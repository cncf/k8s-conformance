I0829 19:28:43.492447      19 e2e.go:126] Starting e2e run "359ac39d-857c-4768-9030-64ce090d54e9" on Ginkgo node 1
Aug 29 19:28:43.513: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1693337323 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 29 19:28:43.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
E0829 19:28:43.617487      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
Aug 29 19:28:43.617: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 29 19:28:43.641: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 29 19:28:43.671: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 29 19:28:43.671: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Aug 29 19:28:43.671: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 29 19:28:43.676: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 29 19:28:43.676: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-ds' (0 seconds elapsed)
Aug 29 19:28:43.676: INFO: e2e test version: v1.26.8
Aug 29 19:28:43.677: INFO: kube-apiserver version: v1.26.8
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Aug 29 19:28:43.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:28:43.681: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.067 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 29 19:28:43.614: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    E0829 19:28:43.617487      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    Aug 29 19:28:43.617: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Aug 29 19:28:43.641: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 29 19:28:43.671: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 29 19:28:43.671: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
    Aug 29 19:28:43.671: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 29 19:28:43.676: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
    Aug 29 19:28:43.676: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy-ds' (0 seconds elapsed)
    Aug 29 19:28:43.676: INFO: e2e test version: v1.26.8
    Aug 29 19:28:43.677: INFO: kube-apiserver version: v1.26.8
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Aug 29 19:28:43.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:28:43.681: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:28:43.704
Aug 29 19:28:43.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename svcaccounts 08/29/23 19:28:43.705
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:28:43.723
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:28:43.725
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Aug 29 19:28:43.750: INFO: created pod pod-service-account-defaultsa
Aug 29 19:28:43.750: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 29 19:28:43.759: INFO: created pod pod-service-account-mountsa
Aug 29 19:28:43.759: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 29 19:28:43.766: INFO: created pod pod-service-account-nomountsa
Aug 29 19:28:43.766: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 29 19:28:43.771: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 29 19:28:43.771: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 29 19:28:43.784: INFO: created pod pod-service-account-mountsa-mountspec
Aug 29 19:28:43.784: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 29 19:28:43.791: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 29 19:28:43.791: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 29 19:28:43.802: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 29 19:28:43.803: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 29 19:28:43.816: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 29 19:28:43.816: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 29 19:28:43.830: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 29 19:28:43.830: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 29 19:28:43.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-487" for this suite. 08/29/23 19:28:43.839
------------------------------
• [0.150 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:28:43.704
    Aug 29 19:28:43.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename svcaccounts 08/29/23 19:28:43.705
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:28:43.723
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:28:43.725
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Aug 29 19:28:43.750: INFO: created pod pod-service-account-defaultsa
    Aug 29 19:28:43.750: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 29 19:28:43.759: INFO: created pod pod-service-account-mountsa
    Aug 29 19:28:43.759: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 29 19:28:43.766: INFO: created pod pod-service-account-nomountsa
    Aug 29 19:28:43.766: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 29 19:28:43.771: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 29 19:28:43.771: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 29 19:28:43.784: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 29 19:28:43.784: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 29 19:28:43.791: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 29 19:28:43.791: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 29 19:28:43.802: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 29 19:28:43.803: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 29 19:28:43.816: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 29 19:28:43.816: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 29 19:28:43.830: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 29 19:28:43.830: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:28:43.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-487" for this suite. 08/29/23 19:28:43.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:28:43.855
Aug 29 19:28:43.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
E0829 19:28:43.856168      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
STEP: Building a namespace api object, basename emptydir 08/29/23 19:28:43.857
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:28:43.877
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:28:43.881
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 08/29/23 19:28:43.884
Aug 29 19:28:43.904: INFO: Waiting up to 5m0s for pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10" in namespace "emptydir-5039" to be "Succeeded or Failed"
Aug 29 19:28:43.910: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485298ms
Aug 29 19:28:45.916: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011656747s
Aug 29 19:28:47.917: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013372321s
Aug 29 19:28:49.917: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Running", Reason="", readiness=true. Elapsed: 6.013511246s
Aug 29 19:28:51.916: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Running", Reason="", readiness=false. Elapsed: 8.012315312s
Aug 29 19:28:53.916: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.01188818s
STEP: Saw pod success 08/29/23 19:28:53.916
Aug 29 19:28:53.916: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10" satisfied condition "Succeeded or Failed"
Aug 29 19:28:53.920: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-8582961c-8d83-48b1-b58f-0facb4aedb10 container test-container: <nil>
STEP: delete the pod 08/29/23 19:28:53.953
Aug 29 19:28:53.969: INFO: Waiting for pod pod-8582961c-8d83-48b1-b58f-0facb4aedb10 to disappear
Aug 29 19:28:53.973: INFO: Pod pod-8582961c-8d83-48b1-b58f-0facb4aedb10 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 19:28:53.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-5039" for this suite. 08/29/23 19:28:53.978
------------------------------
• [SLOW TEST] [10.129 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:28:43.855
    Aug 29 19:28:43.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    E0829 19:28:43.856168      19 progress.go:80] Failed to post progress update to http://localhost:8099/progress: Post "http://localhost:8099/progress": dial tcp [::1]:8099: connect: connection refused
    STEP: Building a namespace api object, basename emptydir 08/29/23 19:28:43.857
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:28:43.877
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:28:43.881
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/29/23 19:28:43.884
    Aug 29 19:28:43.904: INFO: Waiting up to 5m0s for pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10" in namespace "emptydir-5039" to be "Succeeded or Failed"
    Aug 29 19:28:43.910: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Pending", Reason="", readiness=false. Elapsed: 6.485298ms
    Aug 29 19:28:45.916: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011656747s
    Aug 29 19:28:47.917: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013372321s
    Aug 29 19:28:49.917: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Running", Reason="", readiness=true. Elapsed: 6.013511246s
    Aug 29 19:28:51.916: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Running", Reason="", readiness=false. Elapsed: 8.012315312s
    Aug 29 19:28:53.916: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.01188818s
    STEP: Saw pod success 08/29/23 19:28:53.916
    Aug 29 19:28:53.916: INFO: Pod "pod-8582961c-8d83-48b1-b58f-0facb4aedb10" satisfied condition "Succeeded or Failed"
    Aug 29 19:28:53.920: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-8582961c-8d83-48b1-b58f-0facb4aedb10 container test-container: <nil>
    STEP: delete the pod 08/29/23 19:28:53.953
    Aug 29 19:28:53.969: INFO: Waiting for pod pod-8582961c-8d83-48b1-b58f-0facb4aedb10 to disappear
    Aug 29 19:28:53.973: INFO: Pod pod-8582961c-8d83-48b1-b58f-0facb4aedb10 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:28:53.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-5039" for this suite. 08/29/23 19:28:53.978
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:28:53.986
Aug 29 19:28:53.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 19:28:53.988
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:28:54.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:28:54.007
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-14668689-2a02-4e40-a64c-5d7031ae2f95 08/29/23 19:28:54.01
STEP: Creating a pod to test consume secrets 08/29/23 19:28:54.016
Aug 29 19:28:54.031: INFO: Waiting up to 5m0s for pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de" in namespace "secrets-1108" to be "Succeeded or Failed"
Aug 29 19:28:54.034: INFO: Pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.634779ms
Aug 29 19:28:56.039: INFO: Pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007872407s
Aug 29 19:28:58.040: INFO: Pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009368422s
STEP: Saw pod success 08/29/23 19:28:58.04
Aug 29 19:28:58.040: INFO: Pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de" satisfied condition "Succeeded or Failed"
Aug 29 19:28:58.044: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de container secret-volume-test: <nil>
STEP: delete the pod 08/29/23 19:28:58.052
Aug 29 19:28:58.068: INFO: Waiting for pod pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de to disappear
Aug 29 19:28:58.071: INFO: Pod pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 19:28:58.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1108" for this suite. 08/29/23 19:28:58.077
------------------------------
• [4.099 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:28:53.986
    Aug 29 19:28:53.986: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 19:28:53.988
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:28:54.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:28:54.007
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-14668689-2a02-4e40-a64c-5d7031ae2f95 08/29/23 19:28:54.01
    STEP: Creating a pod to test consume secrets 08/29/23 19:28:54.016
    Aug 29 19:28:54.031: INFO: Waiting up to 5m0s for pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de" in namespace "secrets-1108" to be "Succeeded or Failed"
    Aug 29 19:28:54.034: INFO: Pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.634779ms
    Aug 29 19:28:56.039: INFO: Pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007872407s
    Aug 29 19:28:58.040: INFO: Pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009368422s
    STEP: Saw pod success 08/29/23 19:28:58.04
    Aug 29 19:28:58.040: INFO: Pod "pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de" satisfied condition "Succeeded or Failed"
    Aug 29 19:28:58.044: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de container secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 19:28:58.052
    Aug 29 19:28:58.068: INFO: Waiting for pod pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de to disappear
    Aug 29 19:28:58.071: INFO: Pod pod-secrets-a6a8ecd8-db19-4922-b902-f8677bf333de no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:28:58.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1108" for this suite. 08/29/23 19:28:58.077
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:28:58.085
Aug 29 19:28:58.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-runtime 08/29/23 19:28:58.087
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:28:58.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:28:58.111
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 08/29/23 19:28:58.115
STEP: wait for the container to reach Succeeded 08/29/23 19:28:58.127
STEP: get the container status 08/29/23 19:29:03.157
STEP: the container should be terminated 08/29/23 19:29:03.16
STEP: the termination message should be set 08/29/23 19:29:03.16
Aug 29 19:29:03.160: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/29/23 19:29:03.16
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 29 19:29:03.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6180" for this suite. 08/29/23 19:29:03.183
------------------------------
• [SLOW TEST] [5.112 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:28:58.085
    Aug 29 19:28:58.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-runtime 08/29/23 19:28:58.087
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:28:58.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:28:58.111
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 08/29/23 19:28:58.115
    STEP: wait for the container to reach Succeeded 08/29/23 19:28:58.127
    STEP: get the container status 08/29/23 19:29:03.157
    STEP: the container should be terminated 08/29/23 19:29:03.16
    STEP: the termination message should be set 08/29/23 19:29:03.16
    Aug 29 19:29:03.160: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/29/23 19:29:03.16
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:29:03.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6180" for this suite. 08/29/23 19:29:03.183
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:29:03.198
Aug 29 19:29:03.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 19:29:03.199
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:29:03.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:29:03.22
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 08/29/23 19:29:03.224
Aug 29 19:29:03.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0" in namespace "projected-5698" to be "Succeeded or Failed"
Aug 29 19:29:03.241: INFO: Pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469134ms
Aug 29 19:29:05.246: INFO: Pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008415271s
Aug 29 19:29:07.247: INFO: Pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009784314s
STEP: Saw pod success 08/29/23 19:29:07.247
Aug 29 19:29:07.247: INFO: Pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0" satisfied condition "Succeeded or Failed"
Aug 29 19:29:07.251: INFO: Trying to get logs from node loki-15bd39-worker-2 pod downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0 container client-container: <nil>
STEP: delete the pod 08/29/23 19:29:07.278
Aug 29 19:29:07.293: INFO: Waiting for pod downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0 to disappear
Aug 29 19:29:07.296: INFO: Pod downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 19:29:07.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5698" for this suite. 08/29/23 19:29:07.3
------------------------------
• [4.110 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:29:03.198
    Aug 29 19:29:03.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 19:29:03.199
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:29:03.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:29:03.22
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 08/29/23 19:29:03.224
    Aug 29 19:29:03.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0" in namespace "projected-5698" to be "Succeeded or Failed"
    Aug 29 19:29:03.241: INFO: Pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469134ms
    Aug 29 19:29:05.246: INFO: Pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0": Phase="Running", Reason="", readiness=false. Elapsed: 2.008415271s
    Aug 29 19:29:07.247: INFO: Pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009784314s
    STEP: Saw pod success 08/29/23 19:29:07.247
    Aug 29 19:29:07.247: INFO: Pod "downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0" satisfied condition "Succeeded or Failed"
    Aug 29 19:29:07.251: INFO: Trying to get logs from node loki-15bd39-worker-2 pod downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0 container client-container: <nil>
    STEP: delete the pod 08/29/23 19:29:07.278
    Aug 29 19:29:07.293: INFO: Waiting for pod downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0 to disappear
    Aug 29 19:29:07.296: INFO: Pod downwardapi-volume-228d14c0-286d-44e9-b528-9041b42689d0 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:29:07.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5698" for this suite. 08/29/23 19:29:07.3
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:29:07.309
Aug 29 19:29:07.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename statefulset 08/29/23 19:29:07.31
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:29:07.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:29:07.335
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3275 08/29/23 19:29:07.339
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 08/29/23 19:29:07.344
STEP: Creating stateful set ss in namespace statefulset-3275 08/29/23 19:29:07.348
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3275 08/29/23 19:29:07.356
Aug 29 19:29:07.360: INFO: Found 0 stateful pods, waiting for 1
Aug 29 19:29:17.366: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/29/23 19:29:17.366
Aug 29 19:29:17.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 19:29:17.594: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 19:29:17.594: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 19:29:17.594: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 19:29:17.599: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 29 19:29:27.608: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 19:29:27.608: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 19:29:27.623: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999665s
Aug 29 19:29:28.629: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996122815s
Aug 29 19:29:29.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991638677s
Aug 29 19:29:30.637: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987167369s
Aug 29 19:29:31.642: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982983798s
Aug 29 19:29:32.647: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97767072s
Aug 29 19:29:33.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972657306s
Aug 29 19:29:34.657: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968366921s
Aug 29 19:29:35.662: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963346077s
Aug 29 19:29:36.668: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.873716ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3275 08/29/23 19:29:37.668
Aug 29 19:29:37.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 19:29:37.836: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 19:29:37.836: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 19:29:37.836: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 19:29:37.841: INFO: Found 1 stateful pods, waiting for 3
Aug 29 19:29:47.846: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 19:29:47.846: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 19:29:47.846: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
Aug 29 19:29:57.851: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 19:29:57.851: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 19:29:57.851: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/29/23 19:29:57.851
STEP: Scale down will halt with unhealthy stateful pod 08/29/23 19:29:57.851
Aug 29 19:29:57.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 19:29:58.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 19:29:58.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 19:29:58.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 19:29:58.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 19:29:58.210: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 19:29:58.210: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 19:29:58.210: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 19:29:58.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 19:29:58.386: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 19:29:58.386: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 19:29:58.386: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 19:29:58.386: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 19:29:58.391: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 29 19:30:08.403: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 19:30:08.403: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 19:30:08.403: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 19:30:08.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999769s
Aug 29 19:30:09.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996337035s
Aug 29 19:30:10.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989497502s
Aug 29 19:30:11.436: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983939982s
Aug 29 19:30:12.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97821735s
Aug 29 19:30:13.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972213572s
Aug 29 19:30:14.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964558212s
Aug 29 19:30:15.458: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959995678s
Aug 29 19:30:16.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955612749s
Aug 29 19:30:17.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.764962ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3275 08/29/23 19:30:18.47
Aug 29 19:30:18.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 19:30:18.647: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 19:30:18.647: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 19:30:18.647: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 19:30:18.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 19:30:18.809: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 19:30:18.809: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 19:30:18.809: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 19:30:18.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 19:30:18.970: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 19:30:18.970: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 19:30:18.970: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 19:30:18.970: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/29/23 19:30:28.988
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 29 19:30:28.989: INFO: Deleting all statefulset in ns statefulset-3275
Aug 29 19:30:28.993: INFO: Scaling statefulset ss to 0
Aug 29 19:30:29.004: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 19:30:29.006: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 29 19:30:29.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3275" for this suite. 08/29/23 19:30:29.026
------------------------------
• [SLOW TEST] [81.728 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:29:07.309
    Aug 29 19:29:07.309: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename statefulset 08/29/23 19:29:07.31
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:29:07.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:29:07.335
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3275 08/29/23 19:29:07.339
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/29/23 19:29:07.344
    STEP: Creating stateful set ss in namespace statefulset-3275 08/29/23 19:29:07.348
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3275 08/29/23 19:29:07.356
    Aug 29 19:29:07.360: INFO: Found 0 stateful pods, waiting for 1
    Aug 29 19:29:17.366: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/29/23 19:29:17.366
    Aug 29 19:29:17.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 19:29:17.594: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 19:29:17.594: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 19:29:17.594: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 19:29:17.599: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 29 19:29:27.608: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 29 19:29:27.608: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 19:29:27.623: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999665s
    Aug 29 19:29:28.629: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996122815s
    Aug 29 19:29:29.633: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991638677s
    Aug 29 19:29:30.637: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987167369s
    Aug 29 19:29:31.642: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982983798s
    Aug 29 19:29:32.647: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97767072s
    Aug 29 19:29:33.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972657306s
    Aug 29 19:29:34.657: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968366921s
    Aug 29 19:29:35.662: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963346077s
    Aug 29 19:29:36.668: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.873716ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3275 08/29/23 19:29:37.668
    Aug 29 19:29:37.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 19:29:37.836: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 29 19:29:37.836: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 19:29:37.836: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 29 19:29:37.841: INFO: Found 1 stateful pods, waiting for 3
    Aug 29 19:29:47.846: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 19:29:47.846: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 19:29:47.846: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Pending - Ready=false
    Aug 29 19:29:57.851: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 19:29:57.851: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 19:29:57.851: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/29/23 19:29:57.851
    STEP: Scale down will halt with unhealthy stateful pod 08/29/23 19:29:57.851
    Aug 29 19:29:57.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 19:29:58.024: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 19:29:58.024: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 19:29:58.024: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 19:29:58.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 19:29:58.210: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 19:29:58.210: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 19:29:58.210: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 19:29:58.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 19:29:58.386: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 19:29:58.386: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 19:29:58.386: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 19:29:58.386: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 19:29:58.391: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Aug 29 19:30:08.403: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 29 19:30:08.403: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 29 19:30:08.403: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 29 19:30:08.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999769s
    Aug 29 19:30:09.424: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996337035s
    Aug 29 19:30:10.430: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989497502s
    Aug 29 19:30:11.436: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983939982s
    Aug 29 19:30:12.441: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.97821735s
    Aug 29 19:30:13.449: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972213572s
    Aug 29 19:30:14.454: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.964558212s
    Aug 29 19:30:15.458: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959995678s
    Aug 29 19:30:16.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.955612749s
    Aug 29 19:30:17.469: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.764962ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3275 08/29/23 19:30:18.47
    Aug 29 19:30:18.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 19:30:18.647: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 29 19:30:18.647: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 19:30:18.647: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 29 19:30:18.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 19:30:18.809: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 29 19:30:18.809: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 19:30:18.809: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 29 19:30:18.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3275 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 19:30:18.970: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 29 19:30:18.970: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 19:30:18.970: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 29 19:30:18.970: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/29/23 19:30:28.988
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 29 19:30:28.989: INFO: Deleting all statefulset in ns statefulset-3275
    Aug 29 19:30:28.993: INFO: Scaling statefulset ss to 0
    Aug 29 19:30:29.004: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 19:30:29.006: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:30:29.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3275" for this suite. 08/29/23 19:30:29.026
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:30:29.04
Aug 29 19:30:29.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-probe 08/29/23 19:30:29.041
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:30:29.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:30:29.063
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec in namespace container-probe-8841 08/29/23 19:30:29.066
Aug 29 19:30:29.084: INFO: Waiting up to 5m0s for pod "busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec" in namespace "container-probe-8841" to be "not pending"
Aug 29 19:30:29.087: INFO: Pod "busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175993ms
Aug 29 19:30:31.094: INFO: Pod "busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009375745s
Aug 29 19:30:31.094: INFO: Pod "busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec" satisfied condition "not pending"
Aug 29 19:30:31.094: INFO: Started pod busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec in namespace container-probe-8841
STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 19:30:31.094
Aug 29 19:30:31.099: INFO: Initial restart count of pod busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec is 0
STEP: deleting the pod 08/29/23 19:34:31.78
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 29 19:34:31.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8841" for this suite. 08/29/23 19:34:31.804
------------------------------
• [SLOW TEST] [242.776 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:30:29.04
    Aug 29 19:30:29.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-probe 08/29/23 19:30:29.041
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:30:29.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:30:29.063
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec in namespace container-probe-8841 08/29/23 19:30:29.066
    Aug 29 19:30:29.084: INFO: Waiting up to 5m0s for pod "busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec" in namespace "container-probe-8841" to be "not pending"
    Aug 29 19:30:29.087: INFO: Pod "busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.175993ms
    Aug 29 19:30:31.094: INFO: Pod "busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009375745s
    Aug 29 19:30:31.094: INFO: Pod "busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec" satisfied condition "not pending"
    Aug 29 19:30:31.094: INFO: Started pod busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec in namespace container-probe-8841
    STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 19:30:31.094
    Aug 29 19:30:31.099: INFO: Initial restart count of pod busybox-c9b2e159-3e77-4dd7-84a0-a354e05543ec is 0
    STEP: deleting the pod 08/29/23 19:34:31.78
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:34:31.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8841" for this suite. 08/29/23 19:34:31.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:34:31.816
Aug 29 19:34:31.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 19:34:31.818
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:31.845
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:31.848
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 08/29/23 19:34:31.852
Aug 29 19:34:31.863: INFO: Waiting up to 5m0s for pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744" in namespace "downward-api-9063" to be "Succeeded or Failed"
Aug 29 19:34:31.866: INFO: Pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226223ms
Aug 29 19:34:33.872: INFO: Pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008923009s
Aug 29 19:34:35.871: INFO: Pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007941223s
STEP: Saw pod success 08/29/23 19:34:35.871
Aug 29 19:34:35.871: INFO: Pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744" satisfied condition "Succeeded or Failed"
Aug 29 19:34:35.875: INFO: Trying to get logs from node loki-15bd39-worker-2 pod downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744 container dapi-container: <nil>
STEP: delete the pod 08/29/23 19:34:35.896
Aug 29 19:34:35.909: INFO: Waiting for pod downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744 to disappear
Aug 29 19:34:35.912: INFO: Pod downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 29 19:34:35.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9063" for this suite. 08/29/23 19:34:35.917
------------------------------
• [4.110 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:34:31.816
    Aug 29 19:34:31.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 19:34:31.818
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:31.845
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:31.848
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 08/29/23 19:34:31.852
    Aug 29 19:34:31.863: INFO: Waiting up to 5m0s for pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744" in namespace "downward-api-9063" to be "Succeeded or Failed"
    Aug 29 19:34:31.866: INFO: Pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226223ms
    Aug 29 19:34:33.872: INFO: Pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008923009s
    Aug 29 19:34:35.871: INFO: Pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007941223s
    STEP: Saw pod success 08/29/23 19:34:35.871
    Aug 29 19:34:35.871: INFO: Pod "downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744" satisfied condition "Succeeded or Failed"
    Aug 29 19:34:35.875: INFO: Trying to get logs from node loki-15bd39-worker-2 pod downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744 container dapi-container: <nil>
    STEP: delete the pod 08/29/23 19:34:35.896
    Aug 29 19:34:35.909: INFO: Waiting for pod downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744 to disappear
    Aug 29 19:34:35.912: INFO: Pod downward-api-1f4fe916-a878-4133-8b02-10bcbaa0b744 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:34:35.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9063" for this suite. 08/29/23 19:34:35.917
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:34:35.927
Aug 29 19:34:35.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 19:34:35.928
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:35.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:35.951
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 08/29/23 19:34:35.954
Aug 29 19:34:35.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-9032 api-versions'
Aug 29 19:34:36.078: INFO: stderr: ""
Aug 29 19:34:36.078: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 19:34:36.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9032" for this suite. 08/29/23 19:34:36.083
------------------------------
• [0.167 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:34:35.927
    Aug 29 19:34:35.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 19:34:35.928
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:35.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:35.951
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 08/29/23 19:34:35.954
    Aug 29 19:34:35.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-9032 api-versions'
    Aug 29 19:34:36.078: INFO: stderr: ""
    Aug 29 19:34:36.078: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:34:36.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9032" for this suite. 08/29/23 19:34:36.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:34:36.094
Aug 29 19:34:36.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 19:34:36.095
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:36.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:36.117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 19:34:36.139
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:34:36.91
STEP: Deploying the webhook pod 08/29/23 19:34:36.922
STEP: Wait for the deployment to be ready 08/29/23 19:34:36.937
Aug 29 19:34:36.944: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/29/23 19:34:38.956
STEP: Verifying the service has paired with the endpoint 08/29/23 19:34:38.973
Aug 29 19:34:39.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/29/23 19:34:39.979
STEP: create a configmap that should be updated by the webhook 08/29/23 19:34:40
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:34:40.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3255" for this suite. 08/29/23 19:34:40.086
STEP: Destroying namespace "webhook-3255-markers" for this suite. 08/29/23 19:34:40.097
------------------------------
• [4.012 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:34:36.094
    Aug 29 19:34:36.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 19:34:36.095
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:36.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:36.117
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 19:34:36.139
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:34:36.91
    STEP: Deploying the webhook pod 08/29/23 19:34:36.922
    STEP: Wait for the deployment to be ready 08/29/23 19:34:36.937
    Aug 29 19:34:36.944: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/29/23 19:34:38.956
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:34:38.973
    Aug 29 19:34:39.974: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/29/23 19:34:39.979
    STEP: create a configmap that should be updated by the webhook 08/29/23 19:34:40
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:34:40.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3255" for this suite. 08/29/23 19:34:40.086
    STEP: Destroying namespace "webhook-3255-markers" for this suite. 08/29/23 19:34:40.097
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:34:40.107
Aug 29 19:34:40.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 19:34:40.108
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:40.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:40.131
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 19:34:40.154
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:34:40.467
STEP: Deploying the webhook pod 08/29/23 19:34:40.475
STEP: Wait for the deployment to be ready 08/29/23 19:34:40.491
Aug 29 19:34:40.498: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/29/23 19:34:42.509
STEP: Verifying the service has paired with the endpoint 08/29/23 19:34:42.524
Aug 29 19:34:43.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 08/29/23 19:34:43.532
STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 19:34:43.555
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/29/23 19:34:43.564
STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 19:34:43.578
STEP: Patching a validating webhook configuration's rules to include the create operation 08/29/23 19:34:43.589
STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 19:34:43.599
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:34:43.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9619" for this suite. 08/29/23 19:34:43.66
STEP: Destroying namespace "webhook-9619-markers" for this suite. 08/29/23 19:34:43.667
------------------------------
• [3.568 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:34:40.107
    Aug 29 19:34:40.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 19:34:40.108
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:40.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:40.131
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 19:34:40.154
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:34:40.467
    STEP: Deploying the webhook pod 08/29/23 19:34:40.475
    STEP: Wait for the deployment to be ready 08/29/23 19:34:40.491
    Aug 29 19:34:40.498: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/29/23 19:34:42.509
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:34:42.524
    Aug 29 19:34:43.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 08/29/23 19:34:43.532
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 19:34:43.555
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/29/23 19:34:43.564
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 19:34:43.578
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/29/23 19:34:43.589
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 19:34:43.599
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:34:43.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9619" for this suite. 08/29/23 19:34:43.66
    STEP: Destroying namespace "webhook-9619-markers" for this suite. 08/29/23 19:34:43.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:34:43.676
Aug 29 19:34:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 19:34:43.677
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:43.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:43.7
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 08/29/23 19:34:43.703
Aug 29 19:34:43.715: INFO: Waiting up to 5m0s for pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375" in namespace "downward-api-5714" to be "running and ready"
Aug 29 19:34:43.718: INFO: Pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375": Phase="Pending", Reason="", readiness=false. Elapsed: 3.253591ms
Aug 29 19:34:43.718: INFO: The phase of Pod annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:34:45.724: INFO: Pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375": Phase="Running", Reason="", readiness=true. Elapsed: 2.008778451s
Aug 29 19:34:45.724: INFO: The phase of Pod annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375 is Running (Ready = true)
Aug 29 19:34:45.724: INFO: Pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375" satisfied condition "running and ready"
Aug 29 19:34:46.251: INFO: Successfully updated pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 19:34:50.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5714" for this suite. 08/29/23 19:34:50.284
------------------------------
• [SLOW TEST] [6.617 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:34:43.676
    Aug 29 19:34:43.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 19:34:43.677
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:43.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:43.7
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 08/29/23 19:34:43.703
    Aug 29 19:34:43.715: INFO: Waiting up to 5m0s for pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375" in namespace "downward-api-5714" to be "running and ready"
    Aug 29 19:34:43.718: INFO: Pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375": Phase="Pending", Reason="", readiness=false. Elapsed: 3.253591ms
    Aug 29 19:34:43.718: INFO: The phase of Pod annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:34:45.724: INFO: Pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375": Phase="Running", Reason="", readiness=true. Elapsed: 2.008778451s
    Aug 29 19:34:45.724: INFO: The phase of Pod annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375 is Running (Ready = true)
    Aug 29 19:34:45.724: INFO: Pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375" satisfied condition "running and ready"
    Aug 29 19:34:46.251: INFO: Successfully updated pod "annotationupdatec30c599c-23ae-49ba-ac48-b35fffacf375"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:34:50.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5714" for this suite. 08/29/23 19:34:50.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:34:50.293
Aug 29 19:34:50.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename deployment 08/29/23 19:34:50.294
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:50.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:50.318
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 29 19:34:50.333: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 29 19:34:55.338: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/29/23 19:34:55.338
Aug 29 19:34:55.338: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/29/23 19:34:55.351
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 19:34:55.361: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6089  7a78b4cb-24c0-40f5-ad8a-33082023fa83 6064 1 2023-08-29 19:34:55 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-29 19:34:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00431c918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Aug 29 19:34:55.365: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 29 19:34:55.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6089" for this suite. 08/29/23 19:34:55.378
------------------------------
• [SLOW TEST] [5.099 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:34:50.293
    Aug 29 19:34:50.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename deployment 08/29/23 19:34:50.294
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:50.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:50.318
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 29 19:34:50.333: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 29 19:34:55.338: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/29/23 19:34:55.338
    Aug 29 19:34:55.338: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/29/23 19:34:55.351
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 29 19:34:55.361: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6089  7a78b4cb-24c0-40f5-ad8a-33082023fa83 6064 1 2023-08-29 19:34:55 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-29 19:34:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00431c918 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 29 19:34:55.365: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:34:55.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6089" for this suite. 08/29/23 19:34:55.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:34:55.393
Aug 29 19:34:55.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename security-context-test 08/29/23 19:34:55.394
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:55.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:55.446
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Aug 29 19:34:55.465: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543" in namespace "security-context-test-5575" to be "Succeeded or Failed"
Aug 29 19:34:55.468: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997113ms
Aug 29 19:34:57.473: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008573068s
Aug 29 19:34:59.473: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0082739s
Aug 29 19:35:01.473: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008144049s
Aug 29 19:35:01.473: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 29 19:35:01.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5575" for this suite. 08/29/23 19:35:01.479
------------------------------
• [SLOW TEST] [6.095 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:34:55.393
    Aug 29 19:34:55.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename security-context-test 08/29/23 19:34:55.394
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:34:55.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:34:55.446
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Aug 29 19:34:55.465: INFO: Waiting up to 5m0s for pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543" in namespace "security-context-test-5575" to be "Succeeded or Failed"
    Aug 29 19:34:55.468: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997113ms
    Aug 29 19:34:57.473: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008573068s
    Aug 29 19:34:59.473: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0082739s
    Aug 29 19:35:01.473: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008144049s
    Aug 29 19:35:01.473: INFO: Pod "busybox-user-65534-1c6b8029-b11c-4aaa-8b58-7ee181c61543" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:35:01.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5575" for this suite. 08/29/23 19:35:01.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:35:01.489
Aug 29 19:35:01.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 19:35:01.491
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:01.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:01.563
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 08/29/23 19:35:01.567
Aug 29 19:35:01.578: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c" in namespace "emptydir-9" to be "running"
Aug 29 19:35:01.582: INFO: Pod "pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.239097ms
Aug 29 19:35:03.588: INFO: Pod "pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c": Phase="Running", Reason="", readiness=false. Elapsed: 2.009662772s
Aug 29 19:35:03.588: INFO: Pod "pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/29/23 19:35:03.588
Aug 29 19:35:03.588: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9 PodName:pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:35:03.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:35:03.589: INFO: ExecWithOptions: Clientset creation
Aug 29 19:35:03.589: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/emptydir-9/pods/pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 29 19:35:03.663: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 19:35:03.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9" for this suite. 08/29/23 19:35:03.669
------------------------------
• [2.189 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:35:01.489
    Aug 29 19:35:01.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 19:35:01.491
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:01.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:01.563
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 08/29/23 19:35:01.567
    Aug 29 19:35:01.578: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c" in namespace "emptydir-9" to be "running"
    Aug 29 19:35:01.582: INFO: Pod "pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.239097ms
    Aug 29 19:35:03.588: INFO: Pod "pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c": Phase="Running", Reason="", readiness=false. Elapsed: 2.009662772s
    Aug 29 19:35:03.588: INFO: Pod "pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/29/23 19:35:03.588
    Aug 29 19:35:03.588: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9 PodName:pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:35:03.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:35:03.589: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:35:03.589: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/emptydir-9/pods/pod-sharedvolume-8e6d5c4f-4053-46c5-addb-695bb0f7c22c/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 29 19:35:03.663: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:35:03.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9" for this suite. 08/29/23 19:35:03.669
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:35:03.678
Aug 29 19:35:03.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 19:35:03.679
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:03.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:03.702
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 08/29/23 19:35:03.705
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 19:35:03.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2652" for this suite. 08/29/23 19:35:03.714
------------------------------
• [0.044 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:35:03.678
    Aug 29 19:35:03.678: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 19:35:03.679
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:03.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:03.702
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 08/29/23 19:35:03.705
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:35:03.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2652" for this suite. 08/29/23 19:35:03.714
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:35:03.724
Aug 29 19:35:03.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename daemonsets 08/29/23 19:35:03.725
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:03.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:03.749
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834
STEP: Creating simple DaemonSet "daemon-set" 08/29/23 19:35:03.782
STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 19:35:03.789
Aug 29 19:35:03.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:35:03.798: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:35:04.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:35:04.809: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:35:05.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 29 19:35:05.808: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:35:06.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 19:35:06.808: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:35:07.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 19:35:07.807: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:35:08.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 19:35:08.809: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:35:09.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 19:35:09.808: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: listing all DeamonSets 08/29/23 19:35:09.811
STEP: DeleteCollection of the DaemonSets 08/29/23 19:35:09.816
STEP: Verify that ReplicaSets have been deleted 08/29/23 19:35:09.825
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
Aug 29 19:35:09.843: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6293"},"items":null}

Aug 29 19:35:09.849: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6293"},"items":[{"metadata":{"name":"daemon-set-6d6wf","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"338b35f1-7c04-48bf-8a2d-a8b70749f667","resourceVersion":"6288","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ddffd7efe1249e9492eccddd9f3f6c2737d9c8b7480f586f2068605d561108c9","cni.projectcalico.org/podIP":"172.20.17.65/32","cni.projectcalico.org/podIPs":"172.20.17.65/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.17.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rtw8k","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rtw8k","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-master-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-master-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.202","podIP":"172.20.17.65","podIPs":[{"ip":"172.20.17.65"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4376775eb85f1ece795e254541d39a2b0658de0580700065ed5bcf0937237c43","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fv9mw","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"0de0faec-b5ae-446c-a291-fce5404b408b","resourceVersion":"6246","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7960da2f96f6da9b9a5d67fb8d14ef4fd09fb67498df6895f744fe90f1af985c","cni.projectcalico.org/podIP":"172.20.30.144/32","cni.projectcalico.org/podIPs":"172.20.30.144/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l6lb4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l6lb4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.206","podIP":"172.20.30.144","podIPs":[{"ip":"172.20.30.144"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://5d0181fb6b175dd89d6ce7dd8a3b654657bcdf7e2fb64ee78e15b679294459ff","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gbztr","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"4ac47302-c026-4629-8753-3ea582c71fb4","resourceVersion":"6283","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e65a7a4a1118b51ab89761996bb4d1a4c8baa65929a83325438bc92ff17bf013","cni.projectcalico.org/podIP":"172.20.76.129/32","cni.projectcalico.org/podIPs":"172.20.76.129/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.76.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ml6m5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ml6m5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-master-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-master-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.204","podIP":"172.20.76.129","podIPs":[{"ip":"172.20.76.129"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://363e478704f8f9cb5e8d2e7bed48e6f9832011cf54b8613fc42bcb74852f357d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-n6wrt","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"dd64e712-bbe3-4511-ad7f-e717c82388d5","resourceVersion":"6239","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c9d4ada5898ecde681bc9afd276dd7feff47c9be45473093ae206f1351a0a341","cni.projectcalico.org/podIP":"172.20.143.203/32","cni.projectcalico.org/podIPs":"172.20.143.203/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c8ndf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c8ndf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-worker-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-worker-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.198","podIP":"172.20.143.203","podIPs":[{"ip":"172.20.143.203"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4ad5bb0133249162590a7ebc738593c4ca085f91ed34c4f5c334118b3a5e4742","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rll94","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"e6de36fe-542b-42e6-8380-148eb396702c","resourceVersion":"6234","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"eef9886c30940eeb1d3adee9bf8038067e7038f9d5b78039530308a20b1532f9","cni.projectcalico.org/podIP":"172.20.84.141/32","cni.projectcalico.org/podIPs":"172.20.84.141/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-svskz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-svskz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-worker-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-worker-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.199","podIP":"172.20.84.141","podIPs":[{"ip":"172.20.84.141"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://df9f3d64d0ad741325e867f5f8f568c99a47a153115f7cd905b6430904625dc0","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:35:09.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9973" for this suite. 08/29/23 19:35:09.918
------------------------------
• [SLOW TEST] [6.203 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:834

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:35:03.724
    Aug 29 19:35:03.724: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename daemonsets 08/29/23 19:35:03.725
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:03.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:03.749
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:834
    STEP: Creating simple DaemonSet "daemon-set" 08/29/23 19:35:03.782
    STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 19:35:03.789
    Aug 29 19:35:03.798: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:35:03.798: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:35:04.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:35:04.809: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:35:05.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 29 19:35:05.808: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:35:06.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 29 19:35:06.808: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:35:07.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 29 19:35:07.807: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:35:08.809: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 29 19:35:08.809: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:35:09.808: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 19:35:09.808: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: listing all DeamonSets 08/29/23 19:35:09.811
    STEP: DeleteCollection of the DaemonSets 08/29/23 19:35:09.816
    STEP: Verify that ReplicaSets have been deleted 08/29/23 19:35:09.825
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    Aug 29 19:35:09.843: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"6293"},"items":null}

    Aug 29 19:35:09.849: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"6293"},"items":[{"metadata":{"name":"daemon-set-6d6wf","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"338b35f1-7c04-48bf-8a2d-a8b70749f667","resourceVersion":"6288","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"ddffd7efe1249e9492eccddd9f3f6c2737d9c8b7480f586f2068605d561108c9","cni.projectcalico.org/podIP":"172.20.17.65/32","cni.projectcalico.org/podIPs":"172.20.17.65/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.17.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rtw8k","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rtw8k","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-master-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-master-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.202","podIP":"172.20.17.65","podIPs":[{"ip":"172.20.17.65"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4376775eb85f1ece795e254541d39a2b0658de0580700065ed5bcf0937237c43","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fv9mw","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"0de0faec-b5ae-446c-a291-fce5404b408b","resourceVersion":"6246","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"7960da2f96f6da9b9a5d67fb8d14ef4fd09fb67498df6895f744fe90f1af985c","cni.projectcalico.org/podIP":"172.20.30.144/32","cni.projectcalico.org/podIPs":"172.20.30.144/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:06Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.144\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l6lb4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l6lb4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.206","podIP":"172.20.30.144","podIPs":[{"ip":"172.20.30.144"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:05Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://5d0181fb6b175dd89d6ce7dd8a3b654657bcdf7e2fb64ee78e15b679294459ff","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gbztr","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"4ac47302-c026-4629-8753-3ea582c71fb4","resourceVersion":"6283","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e65a7a4a1118b51ab89761996bb4d1a4c8baa65929a83325438bc92ff17bf013","cni.projectcalico.org/podIP":"172.20.76.129/32","cni.projectcalico.org/podIPs":"172.20.76.129/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.76.129\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ml6m5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ml6m5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-master-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-master-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.204","podIP":"172.20.76.129","podIPs":[{"ip":"172.20.76.129"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://363e478704f8f9cb5e8d2e7bed48e6f9832011cf54b8613fc42bcb74852f357d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-n6wrt","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"dd64e712-bbe3-4511-ad7f-e717c82388d5","resourceVersion":"6239","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"c9d4ada5898ecde681bc9afd276dd7feff47c9be45473093ae206f1351a0a341","cni.projectcalico.org/podIP":"172.20.143.203/32","cni.projectcalico.org/podIPs":"172.20.143.203/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.203\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c8ndf","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c8ndf","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-worker-0","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-worker-0"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.198","podIP":"172.20.143.203","podIPs":[{"ip":"172.20.143.203"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://4ad5bb0133249162590a7ebc738593c4ca085f91ed34c4f5c334118b3a5e4742","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rll94","generateName":"daemon-set-","namespace":"daemonsets-9973","uid":"e6de36fe-542b-42e6-8380-148eb396702c","resourceVersion":"6234","creationTimestamp":"2023-08-29T19:35:03Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"eef9886c30940eeb1d3adee9bf8038067e7038f9d5b78039530308a20b1532f9","cni.projectcalico.org/podIP":"172.20.84.141/32","cni.projectcalico.org/podIPs":"172.20.84.141/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"df82b972-1ac8-4cbf-b092-3dcdaa60db5b","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:03Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"df82b972-1ac8-4cbf-b092-3dcdaa60db5b\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:04Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-29T19:35:05Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.141\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-svskz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-svskz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"loki-15bd39-worker-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["loki-15bd39-worker-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:05Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-29T19:35:03Z"}],"hostIP":"10.45.35.199","podIP":"172.20.84.141","podIPs":[{"ip":"172.20.84.141"}],"startTime":"2023-08-29T19:35:03Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-29T19:35:04Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://df9f3d64d0ad741325e867f5f8f568c99a47a153115f7cd905b6430904625dc0","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:35:09.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9973" for this suite. 08/29/23 19:35:09.918
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:35:09.928
Aug 29 19:35:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename csiinlinevolumes 08/29/23 19:35:09.928
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:09.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:09.958
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 08/29/23 19:35:09.962
STEP: getting 08/29/23 19:35:09.985
STEP: listing in namespace 08/29/23 19:35:09.993
STEP: patching 08/29/23 19:35:09.997
STEP: deleting 08/29/23 19:35:10.009
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 29 19:35:10.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-8425" for this suite. 08/29/23 19:35:10.036
------------------------------
• [0.116 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:35:09.928
    Aug 29 19:35:09.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename csiinlinevolumes 08/29/23 19:35:09.928
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:09.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:09.958
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 08/29/23 19:35:09.962
    STEP: getting 08/29/23 19:35:09.985
    STEP: listing in namespace 08/29/23 19:35:09.993
    STEP: patching 08/29/23 19:35:09.997
    STEP: deleting 08/29/23 19:35:10.009
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:35:10.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-8425" for this suite. 08/29/23 19:35:10.036
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:35:10.044
Aug 29 19:35:10.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename podtemplate 08/29/23 19:35:10.045
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:10.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:10.072
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/29/23 19:35:10.076
Aug 29 19:35:10.083: INFO: created test-podtemplate-1
Aug 29 19:35:10.091: INFO: created test-podtemplate-2
Aug 29 19:35:10.099: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/29/23 19:35:10.099
STEP: delete collection of pod templates 08/29/23 19:35:10.102
Aug 29 19:35:10.102: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/29/23 19:35:10.123
Aug 29 19:35:10.123: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 29 19:35:10.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-9809" for this suite. 08/29/23 19:35:10.132
------------------------------
• [0.097 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:35:10.044
    Aug 29 19:35:10.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename podtemplate 08/29/23 19:35:10.045
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:10.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:10.072
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/29/23 19:35:10.076
    Aug 29 19:35:10.083: INFO: created test-podtemplate-1
    Aug 29 19:35:10.091: INFO: created test-podtemplate-2
    Aug 29 19:35:10.099: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/29/23 19:35:10.099
    STEP: delete collection of pod templates 08/29/23 19:35:10.102
    Aug 29 19:35:10.102: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/29/23 19:35:10.123
    Aug 29 19:35:10.123: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:35:10.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-9809" for this suite. 08/29/23 19:35:10.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:35:10.143
Aug 29 19:35:10.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-probe 08/29/23 19:35:10.144
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:10.163
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:10.166
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:10.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7638" for this suite. 08/29/23 19:36:10.194
------------------------------
• [SLOW TEST] [60.061 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:35:10.143
    Aug 29 19:35:10.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-probe 08/29/23 19:35:10.144
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:35:10.163
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:35:10.166
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:10.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7638" for this suite. 08/29/23 19:36:10.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:10.205
Aug 29 19:36:10.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename var-expansion 08/29/23 19:36:10.206
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:10.224
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:10.227
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Aug 29 19:36:10.240: INFO: Waiting up to 2m0s for pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15" in namespace "var-expansion-2799" to be "container 0 failed with reason CreateContainerConfigError"
Aug 29 19:36:10.244: INFO: Pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.503114ms
Aug 29 19:36:12.249: INFO: Pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00866891s
Aug 29 19:36:12.249: INFO: Pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 29 19:36:12.249: INFO: Deleting pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15" in namespace "var-expansion-2799"
Aug 29 19:36:12.259: INFO: Wait up to 5m0s for pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:16.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2799" for this suite. 08/29/23 19:36:16.273
------------------------------
• [SLOW TEST] [6.075 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:10.205
    Aug 29 19:36:10.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename var-expansion 08/29/23 19:36:10.206
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:10.224
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:10.227
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Aug 29 19:36:10.240: INFO: Waiting up to 2m0s for pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15" in namespace "var-expansion-2799" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 29 19:36:10.244: INFO: Pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.503114ms
    Aug 29 19:36:12.249: INFO: Pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00866891s
    Aug 29 19:36:12.249: INFO: Pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 29 19:36:12.249: INFO: Deleting pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15" in namespace "var-expansion-2799"
    Aug 29 19:36:12.259: INFO: Wait up to 5m0s for pod "var-expansion-9721803e-86bd-4480-9551-db421ac4fb15" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:16.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2799" for this suite. 08/29/23 19:36:16.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:16.281
Aug 29 19:36:16.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 19:36:16.282
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:16.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:16.303
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 08/29/23 19:36:16.306
Aug 29 19:36:16.317: INFO: Waiting up to 5m0s for pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4" in namespace "downward-api-1479" to be "Succeeded or Failed"
Aug 29 19:36:16.321: INFO: Pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.529383ms
Aug 29 19:36:18.325: INFO: Pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008278083s
Aug 29 19:36:20.326: INFO: Pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009294231s
STEP: Saw pod success 08/29/23 19:36:20.326
Aug 29 19:36:20.327: INFO: Pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4" satisfied condition "Succeeded or Failed"
Aug 29 19:36:20.330: INFO: Trying to get logs from node loki-15bd39-worker-2 pod downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4 container dapi-container: <nil>
STEP: delete the pod 08/29/23 19:36:20.349
Aug 29 19:36:20.363: INFO: Waiting for pod downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4 to disappear
Aug 29 19:36:20.366: INFO: Pod downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:20.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1479" for this suite. 08/29/23 19:36:20.37
------------------------------
• [4.097 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:16.281
    Aug 29 19:36:16.281: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 19:36:16.282
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:16.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:16.303
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 08/29/23 19:36:16.306
    Aug 29 19:36:16.317: INFO: Waiting up to 5m0s for pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4" in namespace "downward-api-1479" to be "Succeeded or Failed"
    Aug 29 19:36:16.321: INFO: Pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.529383ms
    Aug 29 19:36:18.325: INFO: Pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008278083s
    Aug 29 19:36:20.326: INFO: Pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009294231s
    STEP: Saw pod success 08/29/23 19:36:20.326
    Aug 29 19:36:20.327: INFO: Pod "downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4" satisfied condition "Succeeded or Failed"
    Aug 29 19:36:20.330: INFO: Trying to get logs from node loki-15bd39-worker-2 pod downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4 container dapi-container: <nil>
    STEP: delete the pod 08/29/23 19:36:20.349
    Aug 29 19:36:20.363: INFO: Waiting for pod downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4 to disappear
    Aug 29 19:36:20.366: INFO: Pod downward-api-014bab2a-0e85-41ae-b13d-48d3179b96d4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:20.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1479" for this suite. 08/29/23 19:36:20.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:20.378
Aug 29 19:36:20.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-pred 08/29/23 19:36:20.379
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:20.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:20.4
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 29 19:36:20.403: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 19:36:20.413: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 19:36:20.417: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-master-0 before test
Aug 29 19:36:20.427: INFO: calico-node-f4bng from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.427: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 19:36:20.427: INFO: kube-apiserver-loki-15bd39-master-0 from kube-system started at 2023-08-29 19:11:07 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.427: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 29 19:36:20.427: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 29 19:36:20.427: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 29 19:36:20.427: INFO: kube-proxy-ds-7pn7t from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.427: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 19:36:20.427: INFO: fluent-bit-x2ntj from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.427: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 19:36:20.427: INFO: node-exporter-lsprp from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 19:36:20.427: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 19:36:20.427: INFO: nutanix-csi-node-mqc5m from ntnx-system started at 2023-08-29 19:23:42 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.427: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 19:36:20.427: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 19:36:20.427: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 19:36:20.427: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 19:36:20.427: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 19:36:20.427: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-master-1 before test
Aug 29 19:36:20.439: INFO: calico-node-snrmp from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.439: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 19:36:20.439: INFO: kube-apiserver-loki-15bd39-master-1 from kube-system started at 2023-08-29 19:12:53 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.439: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 29 19:36:20.439: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 29 19:36:20.439: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 29 19:36:20.439: INFO: kube-proxy-ds-c665w from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.439: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 19:36:20.439: INFO: fluent-bit-nmdm2 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.439: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 19:36:20.439: INFO: node-exporter-q5l7x from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.439: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 19:36:20.439: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 19:36:20.439: INFO: nutanix-csi-node-r85kq from ntnx-system started at 2023-08-29 19:23:49 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.439: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 19:36:20.439: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 19:36:20.439: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 19:36:20.439: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 19:36:20.439: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 19:36:20.439: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-0 before test
Aug 29 19:36:20.450: INFO: calico-node-mms8p from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 19:36:20.450: INFO: calico-typha-787bcdb57c-ppmlg from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 19:36:20.450: INFO: coredns-5d88b659b9-6fjb6 from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container coredns ready: true, restart count 0
Aug 29 19:36:20.450: INFO: kube-proxy-ds-nmtgg from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 19:36:20.450: INFO: alertmanager-main-0 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 19:36:20.450: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 19:36:20.450: INFO: blackbox-exporter-7954b6f4db-5vl2v from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 29 19:36:20.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 19:36:20.450: INFO: 	Container module-configmap-reloader ready: true, restart count 0
Aug 29 19:36:20.450: INFO: csi-snapshot-webhook-69668f7b57-g5r2n from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container snapshot-validation ready: true, restart count 0
Aug 29 19:36:20.450: INFO: fluent-bit-lbvlt from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 19:36:20.450: INFO: kubernetes-events-printer-74464fd469-rrxtq from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.450: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 29 19:36:20.450: INFO: node-exporter-wwfmq from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.451: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 19:36:20.451: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 19:36:20.451: INFO: nutanix-csi-node-hql5k from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.451: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 19:36:20.451: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 19:36:20.451: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 19:36:20.451: INFO: prometheus-adapter-6b6d856c7-htsdt from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.451: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 29 19:36:20.451: INFO: prometheus-k8s-1 from ntnx-system started at 2023-08-29 19:21:14 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.451: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 19:36:20.451: INFO: 	Container prometheus ready: true, restart count 0
Aug 29 19:36:20.451: INFO: sonobuoy-e2e-job-161e88352cfa47dc from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.451: INFO: 	Container e2e ready: true, restart count 0
Aug 29 19:36:20.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 19:36:20.451: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 19:36:20.451: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 19:36:20.451: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-1 before test
Aug 29 19:36:20.463: INFO: calico-node-sb7l5 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 19:36:20.463: INFO: calico-typha-787bcdb57c-pgpqd from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 19:36:20.463: INFO: coredns-5d88b659b9-l6c5t from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container coredns ready: true, restart count 0
Aug 29 19:36:20.463: INFO: kube-proxy-ds-jt2tk from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 19:36:20.463: INFO: alertmanager-main-2 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 19:36:20.463: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 19:36:20.463: INFO: csi-snapshot-controller-6d467d46bf-ml6xt from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 29 19:36:20.463: INFO: fluent-bit-zzc42 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 19:36:20.463: INFO: kube-state-metrics-d459f9d68-skd92 from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 29 19:36:20.463: INFO: node-exporter-8gt7t from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 19:36:20.463: INFO: nutanix-csi-node-n48k8 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 19:36:20.463: INFO: prometheus-adapter-6b6d856c7-b9m94 from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 29 19:36:20.463: INFO: prometheus-k8s-0 from ntnx-system started at 2023-08-29 19:21:16 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 	Container prometheus ready: true, restart count 0
Aug 29 19:36:20.463: INFO: prometheus-operator-557c85cd6b-lcd4m from ntnx-system started at 2023-08-29 19:21:02 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 29 19:36:20.463: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.463: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 19:36:20.463: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-2 before test
Aug 29 19:36:20.473: INFO: pod-csi-inline-volumes from csiinlinevolumes-8425 started at 2023-08-29 19:35:09 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
Aug 29 19:36:20.473: INFO: calico-kube-controllers-d9df5649-k5gf4 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 19:36:20.473: INFO: calico-node-9pl5x from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 19:36:20.473: INFO: calico-typha-787bcdb57c-6bcfm from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 19:36:20.473: INFO: kube-proxy-ds-l9vp5 from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 19:36:20.473: INFO: alertmanager-main-1 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 19:36:20.473: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 19:36:20.473: INFO: fluent-bit-6wl9b from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 19:36:20.473: INFO: node-exporter-xhwmw from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 19:36:20.473: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 19:36:20.473: INFO: nutanix-csi-controller-657cc74bd5-7j77l from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (5 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 29 19:36:20.473: INFO: 	Container csi-resizer ready: true, restart count 0
Aug 29 19:36:20.473: INFO: 	Container csi-snapshotter ready: true, restart count 0
Aug 29 19:36:20.473: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 19:36:20.473: INFO: 	Container nutanix-csi-plugin ready: true, restart count 0
Aug 29 19:36:20.473: INFO: nutanix-csi-node-hd5rc from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 19:36:20.473: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 19:36:20.473: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 19:36:20.473: INFO: sonobuoy from sonobuoy started at 2023-08-29 19:28:22 +0000 UTC (1 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 19:36:20.473: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 19:36:20.473: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 19:36:20.473: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node loki-15bd39-master-0 08/29/23 19:36:20.502
STEP: verifying the node has the label node loki-15bd39-master-1 08/29/23 19:36:20.519
STEP: verifying the node has the label node loki-15bd39-worker-0 08/29/23 19:36:20.535
STEP: verifying the node has the label node loki-15bd39-worker-1 08/29/23 19:36:20.552
STEP: verifying the node has the label node loki-15bd39-worker-2 08/29/23 19:36:20.569
Aug 29 19:36:20.600: INFO: Pod pod-csi-inline-volumes requesting resource cpu=0m on Node loki-15bd39-worker-2
Aug 29 19:36:20.600: INFO: Pod calico-kube-controllers-d9df5649-k5gf4 requesting resource cpu=0m on Node loki-15bd39-worker-2
Aug 29 19:36:20.600: INFO: Pod calico-node-9pl5x requesting resource cpu=250m on Node loki-15bd39-worker-2
Aug 29 19:36:20.600: INFO: Pod calico-node-f4bng requesting resource cpu=250m on Node loki-15bd39-master-0
Aug 29 19:36:20.600: INFO: Pod calico-node-mms8p requesting resource cpu=250m on Node loki-15bd39-worker-0
Aug 29 19:36:20.600: INFO: Pod calico-node-sb7l5 requesting resource cpu=250m on Node loki-15bd39-worker-1
Aug 29 19:36:20.600: INFO: Pod calico-node-snrmp requesting resource cpu=250m on Node loki-15bd39-master-1
Aug 29 19:36:20.600: INFO: Pod calico-typha-787bcdb57c-6bcfm requesting resource cpu=0m on Node loki-15bd39-worker-2
Aug 29 19:36:20.600: INFO: Pod calico-typha-787bcdb57c-pgpqd requesting resource cpu=0m on Node loki-15bd39-worker-1
Aug 29 19:36:20.600: INFO: Pod calico-typha-787bcdb57c-ppmlg requesting resource cpu=0m on Node loki-15bd39-worker-0
Aug 29 19:36:20.600: INFO: Pod coredns-5d88b659b9-6fjb6 requesting resource cpu=100m on Node loki-15bd39-worker-0
Aug 29 19:36:20.600: INFO: Pod coredns-5d88b659b9-l6c5t requesting resource cpu=100m on Node loki-15bd39-worker-1
Aug 29 19:36:20.600: INFO: Pod kube-apiserver-loki-15bd39-master-0 requesting resource cpu=300m on Node loki-15bd39-master-0
Aug 29 19:36:20.600: INFO: Pod kube-apiserver-loki-15bd39-master-1 requesting resource cpu=300m on Node loki-15bd39-master-1
Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-7pn7t requesting resource cpu=100m on Node loki-15bd39-master-0
Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-c665w requesting resource cpu=100m on Node loki-15bd39-master-1
Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-jt2tk requesting resource cpu=100m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-l9vp5 requesting resource cpu=100m on Node loki-15bd39-worker-2
Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-nmtgg requesting resource cpu=100m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod alertmanager-main-0 requesting resource cpu=200m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod alertmanager-main-1 requesting resource cpu=200m on Node loki-15bd39-worker-2
Aug 29 19:36:20.601: INFO: Pod alertmanager-main-2 requesting resource cpu=200m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod blackbox-exporter-7954b6f4db-5vl2v requesting resource cpu=30m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod csi-snapshot-controller-6d467d46bf-ml6xt requesting resource cpu=10m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod csi-snapshot-webhook-69668f7b57-g5r2n requesting resource cpu=10m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod fluent-bit-6wl9b requesting resource cpu=100m on Node loki-15bd39-worker-2
Aug 29 19:36:20.601: INFO: Pod fluent-bit-lbvlt requesting resource cpu=100m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod fluent-bit-nmdm2 requesting resource cpu=100m on Node loki-15bd39-master-1
Aug 29 19:36:20.601: INFO: Pod fluent-bit-x2ntj requesting resource cpu=100m on Node loki-15bd39-master-0
Aug 29 19:36:20.601: INFO: Pod fluent-bit-zzc42 requesting resource cpu=100m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod kube-state-metrics-d459f9d68-skd92 requesting resource cpu=130m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod kubernetes-events-printer-74464fd469-rrxtq requesting resource cpu=100m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod node-exporter-8gt7t requesting resource cpu=112m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod node-exporter-lsprp requesting resource cpu=112m on Node loki-15bd39-master-0
Aug 29 19:36:20.601: INFO: Pod node-exporter-q5l7x requesting resource cpu=112m on Node loki-15bd39-master-1
Aug 29 19:36:20.601: INFO: Pod node-exporter-wwfmq requesting resource cpu=112m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod node-exporter-xhwmw requesting resource cpu=112m on Node loki-15bd39-worker-2
Aug 29 19:36:20.601: INFO: Pod nutanix-csi-controller-657cc74bd5-7j77l requesting resource cpu=200m on Node loki-15bd39-worker-2
Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-hd5rc requesting resource cpu=200m on Node loki-15bd39-worker-2
Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-hql5k requesting resource cpu=200m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-mqc5m requesting resource cpu=200m on Node loki-15bd39-master-0
Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-n48k8 requesting resource cpu=200m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-r85kq requesting resource cpu=200m on Node loki-15bd39-master-1
Aug 29 19:36:20.601: INFO: Pod prometheus-adapter-6b6d856c7-b9m94 requesting resource cpu=102m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod prometheus-adapter-6b6d856c7-htsdt requesting resource cpu=102m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod prometheus-k8s-0 requesting resource cpu=600m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod prometheus-k8s-1 requesting resource cpu=600m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod prometheus-operator-557c85cd6b-lcd4m requesting resource cpu=110m on Node loki-15bd39-worker-1
Aug 29 19:36:20.601: INFO: Pod sonobuoy requesting resource cpu=0m on Node loki-15bd39-worker-2
Aug 29 19:36:20.601: INFO: Pod sonobuoy-e2e-job-161e88352cfa47dc requesting resource cpu=0m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g requesting resource cpu=0m on Node loki-15bd39-worker-2
Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm requesting resource cpu=0m on Node loki-15bd39-master-0
Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s requesting resource cpu=0m on Node loki-15bd39-master-1
Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr requesting resource cpu=0m on Node loki-15bd39-worker-0
Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv requesting resource cpu=0m on Node loki-15bd39-worker-1
STEP: Starting Pods to consume most of the cluster CPU. 08/29/23 19:36:20.601
Aug 29 19:36:20.601: INFO: Creating a pod which consumes cpu=2056m on Node loki-15bd39-master-0
Aug 29 19:36:20.612: INFO: Creating a pod which consumes cpu=2056m on Node loki-15bd39-master-1
Aug 29 19:36:20.621: INFO: Creating a pod which consumes cpu=4267m on Node loki-15bd39-worker-0
Aug 29 19:36:20.629: INFO: Creating a pod which consumes cpu=4190m on Node loki-15bd39-worker-1
Aug 29 19:36:20.642: INFO: Creating a pod which consumes cpu=4786m on Node loki-15bd39-worker-2
Aug 29 19:36:20.657: INFO: Waiting up to 5m0s for pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4" in namespace "sched-pred-9822" to be "running"
Aug 29 19:36:20.679: INFO: Pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.61835ms
Aug 29 19:36:22.684: INFO: Pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027359007s
Aug 29 19:36:24.684: INFO: Pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4": Phase="Running", Reason="", readiness=true. Elapsed: 4.026931418s
Aug 29 19:36:24.684: INFO: Pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4" satisfied condition "running"
Aug 29 19:36:24.684: INFO: Waiting up to 5m0s for pod "filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448" in namespace "sched-pred-9822" to be "running"
Aug 29 19:36:24.688: INFO: Pod "filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448": Phase="Running", Reason="", readiness=true. Elapsed: 3.987391ms
Aug 29 19:36:24.688: INFO: Pod "filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448" satisfied condition "running"
Aug 29 19:36:24.688: INFO: Waiting up to 5m0s for pod "filler-pod-c1f10548-45ed-4a85-b683-932b569d0409" in namespace "sched-pred-9822" to be "running"
Aug 29 19:36:24.692: INFO: Pod "filler-pod-c1f10548-45ed-4a85-b683-932b569d0409": Phase="Running", Reason="", readiness=true. Elapsed: 3.554507ms
Aug 29 19:36:24.692: INFO: Pod "filler-pod-c1f10548-45ed-4a85-b683-932b569d0409" satisfied condition "running"
Aug 29 19:36:24.692: INFO: Waiting up to 5m0s for pod "filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9" in namespace "sched-pred-9822" to be "running"
Aug 29 19:36:24.695: INFO: Pod "filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9": Phase="Running", Reason="", readiness=true. Elapsed: 3.104561ms
Aug 29 19:36:24.695: INFO: Pod "filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9" satisfied condition "running"
Aug 29 19:36:24.695: INFO: Waiting up to 5m0s for pod "filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683" in namespace "sched-pred-9822" to be "running"
Aug 29 19:36:24.699: INFO: Pod "filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683": Phase="Running", Reason="", readiness=true. Elapsed: 4.308451ms
Aug 29 19:36:24.699: INFO: Pod "filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/29/23 19:36:24.699
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bd640de340], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448 to loki-15bd39-master-1] 08/29/23 19:36:24.704
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bdbb5e14a5], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bdec7a25ad], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 823.897474ms (823.904395ms including waiting)] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bdef0982e7], Reason = [Created], Message = [Created container filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bdf85d8b27], Reason = [Started], Message = [Started container filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bd6613fd7c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9 to loki-15bd39-worker-1] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bd93141393], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bdcd2a3a5c], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 974.499879ms (974.50771ms including waiting)] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bdcf6d8bc3], Reason = [Created], Message = [Created container filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bdd9243dd2], Reason = [Started], Message = [Started container filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bd62f60e7c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4 to loki-15bd39-master-0] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bd9551dadc], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bdc80dec60], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 851.159094ms (851.168791ms including waiting)] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bdcb47203b], Reason = [Created], Message = [Created container filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bdd41a314d], Reason = [Started], Message = [Started container filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bd6660a6a0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683 to loki-15bd39-worker-2] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bd9a19e1ab], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.705
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bdcb313ea5], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 823.580439ms (823.592784ms including waiting)] 08/29/23 19:36:24.706
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bdcde7ae54], Reason = [Created], Message = [Created container filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683] 08/29/23 19:36:24.706
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bdd6718554], Reason = [Started], Message = [Started container filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683] 08/29/23 19:36:24.706
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bd64322267], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-c1f10548-45ed-4a85-b683-932b569d0409 to loki-15bd39-worker-0] 08/29/23 19:36:24.706
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bd930a26fa], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.706
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bdc445180f], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 825.909067ms (825.917263ms including waiting)] 08/29/23 19:36:24.706
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bdc6a56c61], Reason = [Created], Message = [Created container filler-pod-c1f10548-45ed-4a85-b683-932b569d0409] 08/29/23 19:36:24.706
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bdd05064d7], Reason = [Started], Message = [Started container filler-pod-c1f10548-45ed-4a85-b683-932b569d0409] 08/29/23 19:36:24.706
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.177ff1be56cfbbc5], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod..] 08/29/23 19:36:24.726
STEP: removing the label node off the node loki-15bd39-master-0 08/29/23 19:36:25.721
STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.74
STEP: removing the label node off the node loki-15bd39-master-1 08/29/23 19:36:25.744
STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.776
STEP: removing the label node off the node loki-15bd39-worker-0 08/29/23 19:36:25.78
STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.8
STEP: removing the label node off the node loki-15bd39-worker-1 08/29/23 19:36:25.803
STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.818
STEP: removing the label node off the node loki-15bd39-worker-2 08/29/23 19:36:25.822
STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.837
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:25.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-9822" for this suite. 08/29/23 19:36:25.846
------------------------------
• [SLOW TEST] [5.479 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:20.378
    Aug 29 19:36:20.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-pred 08/29/23 19:36:20.379
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:20.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:20.4
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 29 19:36:20.403: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 29 19:36:20.413: INFO: Waiting for terminating namespaces to be deleted...
    Aug 29 19:36:20.417: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-master-0 before test
    Aug 29 19:36:20.427: INFO: calico-node-f4bng from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.427: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: kube-apiserver-loki-15bd39-master-0 from kube-system started at 2023-08-29 19:11:07 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.427: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: kube-proxy-ds-7pn7t from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.427: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: fluent-bit-x2ntj from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.427: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: node-exporter-lsprp from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.427: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: nutanix-csi-node-mqc5m from ntnx-system started at 2023-08-29 19:23:42 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.427: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.427: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 19:36:20.427: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-master-1 before test
    Aug 29 19:36:20.439: INFO: calico-node-snrmp from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.439: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: kube-apiserver-loki-15bd39-master-1 from kube-system started at 2023-08-29 19:12:53 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.439: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: kube-proxy-ds-c665w from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.439: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: fluent-bit-nmdm2 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.439: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: node-exporter-q5l7x from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.439: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: nutanix-csi-node-r85kq from ntnx-system started at 2023-08-29 19:23:49 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.439: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.439: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 19:36:20.439: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-0 before test
    Aug 29 19:36:20.450: INFO: calico-node-mms8p from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: calico-typha-787bcdb57c-ppmlg from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: coredns-5d88b659b9-6fjb6 from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container coredns ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: kube-proxy-ds-nmtgg from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: alertmanager-main-0 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 19:36:20.450: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: blackbox-exporter-7954b6f4db-5vl2v from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: 	Container module-configmap-reloader ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: csi-snapshot-webhook-69668f7b57-g5r2n from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container snapshot-validation ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: fluent-bit-lbvlt from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: kubernetes-events-printer-74464fd469-rrxtq from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.450: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
    Aug 29 19:36:20.450: INFO: node-exporter-wwfmq from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.451: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: nutanix-csi-node-hql5k from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.451: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: prometheus-adapter-6b6d856c7-htsdt from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.451: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: prometheus-k8s-1 from ntnx-system started at 2023-08-29 19:21:14 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.451: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: 	Container prometheus ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: sonobuoy-e2e-job-161e88352cfa47dc from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.451: INFO: 	Container e2e ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 19:36:20.451: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-1 before test
    Aug 29 19:36:20.463: INFO: calico-node-sb7l5 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: calico-typha-787bcdb57c-pgpqd from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: coredns-5d88b659b9-l6c5t from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container coredns ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: kube-proxy-ds-jt2tk from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: alertmanager-main-2 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 19:36:20.463: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: csi-snapshot-controller-6d467d46bf-ml6xt from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: fluent-bit-zzc42 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: kube-state-metrics-d459f9d68-skd92 from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: node-exporter-8gt7t from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: nutanix-csi-node-n48k8 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: prometheus-adapter-6b6d856c7-b9m94 from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: prometheus-k8s-0 from ntnx-system started at 2023-08-29 19:21:16 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 	Container prometheus ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: prometheus-operator-557c85cd6b-lcd4m from ntnx-system started at 2023-08-29 19:21:02 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 	Container prometheus-operator ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.463: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 19:36:20.463: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-2 before test
    Aug 29 19:36:20.473: INFO: pod-csi-inline-volumes from csiinlinevolumes-8425 started at 2023-08-29 19:35:09 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container pod-csi-inline-volumes ready: false, restart count 0
    Aug 29 19:36:20.473: INFO: calico-kube-controllers-d9df5649-k5gf4 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: calico-node-9pl5x from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: calico-typha-787bcdb57c-6bcfm from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: kube-proxy-ds-l9vp5 from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: alertmanager-main-1 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 19:36:20.473: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: fluent-bit-6wl9b from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: node-exporter-xhwmw from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: nutanix-csi-controller-657cc74bd5-7j77l from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (5 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container csi-provisioner ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: 	Container csi-resizer ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: 	Container nutanix-csi-plugin ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: nutanix-csi-node-hd5rc from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: sonobuoy from sonobuoy started at 2023-08-29 19:28:22 +0000 UTC (1 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 19:36:20.473: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 19:36:20.473: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node loki-15bd39-master-0 08/29/23 19:36:20.502
    STEP: verifying the node has the label node loki-15bd39-master-1 08/29/23 19:36:20.519
    STEP: verifying the node has the label node loki-15bd39-worker-0 08/29/23 19:36:20.535
    STEP: verifying the node has the label node loki-15bd39-worker-1 08/29/23 19:36:20.552
    STEP: verifying the node has the label node loki-15bd39-worker-2 08/29/23 19:36:20.569
    Aug 29 19:36:20.600: INFO: Pod pod-csi-inline-volumes requesting resource cpu=0m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.600: INFO: Pod calico-kube-controllers-d9df5649-k5gf4 requesting resource cpu=0m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.600: INFO: Pod calico-node-9pl5x requesting resource cpu=250m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.600: INFO: Pod calico-node-f4bng requesting resource cpu=250m on Node loki-15bd39-master-0
    Aug 29 19:36:20.600: INFO: Pod calico-node-mms8p requesting resource cpu=250m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.600: INFO: Pod calico-node-sb7l5 requesting resource cpu=250m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.600: INFO: Pod calico-node-snrmp requesting resource cpu=250m on Node loki-15bd39-master-1
    Aug 29 19:36:20.600: INFO: Pod calico-typha-787bcdb57c-6bcfm requesting resource cpu=0m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.600: INFO: Pod calico-typha-787bcdb57c-pgpqd requesting resource cpu=0m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.600: INFO: Pod calico-typha-787bcdb57c-ppmlg requesting resource cpu=0m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.600: INFO: Pod coredns-5d88b659b9-6fjb6 requesting resource cpu=100m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.600: INFO: Pod coredns-5d88b659b9-l6c5t requesting resource cpu=100m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.600: INFO: Pod kube-apiserver-loki-15bd39-master-0 requesting resource cpu=300m on Node loki-15bd39-master-0
    Aug 29 19:36:20.600: INFO: Pod kube-apiserver-loki-15bd39-master-1 requesting resource cpu=300m on Node loki-15bd39-master-1
    Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-7pn7t requesting resource cpu=100m on Node loki-15bd39-master-0
    Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-c665w requesting resource cpu=100m on Node loki-15bd39-master-1
    Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-jt2tk requesting resource cpu=100m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-l9vp5 requesting resource cpu=100m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.601: INFO: Pod kube-proxy-ds-nmtgg requesting resource cpu=100m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod alertmanager-main-0 requesting resource cpu=200m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod alertmanager-main-1 requesting resource cpu=200m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.601: INFO: Pod alertmanager-main-2 requesting resource cpu=200m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod blackbox-exporter-7954b6f4db-5vl2v requesting resource cpu=30m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod csi-snapshot-controller-6d467d46bf-ml6xt requesting resource cpu=10m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod csi-snapshot-webhook-69668f7b57-g5r2n requesting resource cpu=10m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod fluent-bit-6wl9b requesting resource cpu=100m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.601: INFO: Pod fluent-bit-lbvlt requesting resource cpu=100m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod fluent-bit-nmdm2 requesting resource cpu=100m on Node loki-15bd39-master-1
    Aug 29 19:36:20.601: INFO: Pod fluent-bit-x2ntj requesting resource cpu=100m on Node loki-15bd39-master-0
    Aug 29 19:36:20.601: INFO: Pod fluent-bit-zzc42 requesting resource cpu=100m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod kube-state-metrics-d459f9d68-skd92 requesting resource cpu=130m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod kubernetes-events-printer-74464fd469-rrxtq requesting resource cpu=100m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod node-exporter-8gt7t requesting resource cpu=112m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod node-exporter-lsprp requesting resource cpu=112m on Node loki-15bd39-master-0
    Aug 29 19:36:20.601: INFO: Pod node-exporter-q5l7x requesting resource cpu=112m on Node loki-15bd39-master-1
    Aug 29 19:36:20.601: INFO: Pod node-exporter-wwfmq requesting resource cpu=112m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod node-exporter-xhwmw requesting resource cpu=112m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.601: INFO: Pod nutanix-csi-controller-657cc74bd5-7j77l requesting resource cpu=200m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-hd5rc requesting resource cpu=200m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-hql5k requesting resource cpu=200m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-mqc5m requesting resource cpu=200m on Node loki-15bd39-master-0
    Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-n48k8 requesting resource cpu=200m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod nutanix-csi-node-r85kq requesting resource cpu=200m on Node loki-15bd39-master-1
    Aug 29 19:36:20.601: INFO: Pod prometheus-adapter-6b6d856c7-b9m94 requesting resource cpu=102m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod prometheus-adapter-6b6d856c7-htsdt requesting resource cpu=102m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod prometheus-k8s-0 requesting resource cpu=600m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod prometheus-k8s-1 requesting resource cpu=600m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod prometheus-operator-557c85cd6b-lcd4m requesting resource cpu=110m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.601: INFO: Pod sonobuoy requesting resource cpu=0m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.601: INFO: Pod sonobuoy-e2e-job-161e88352cfa47dc requesting resource cpu=0m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g requesting resource cpu=0m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm requesting resource cpu=0m on Node loki-15bd39-master-0
    Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s requesting resource cpu=0m on Node loki-15bd39-master-1
    Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr requesting resource cpu=0m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.601: INFO: Pod sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv requesting resource cpu=0m on Node loki-15bd39-worker-1
    STEP: Starting Pods to consume most of the cluster CPU. 08/29/23 19:36:20.601
    Aug 29 19:36:20.601: INFO: Creating a pod which consumes cpu=2056m on Node loki-15bd39-master-0
    Aug 29 19:36:20.612: INFO: Creating a pod which consumes cpu=2056m on Node loki-15bd39-master-1
    Aug 29 19:36:20.621: INFO: Creating a pod which consumes cpu=4267m on Node loki-15bd39-worker-0
    Aug 29 19:36:20.629: INFO: Creating a pod which consumes cpu=4190m on Node loki-15bd39-worker-1
    Aug 29 19:36:20.642: INFO: Creating a pod which consumes cpu=4786m on Node loki-15bd39-worker-2
    Aug 29 19:36:20.657: INFO: Waiting up to 5m0s for pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4" in namespace "sched-pred-9822" to be "running"
    Aug 29 19:36:20.679: INFO: Pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4": Phase="Pending", Reason="", readiness=false. Elapsed: 21.61835ms
    Aug 29 19:36:22.684: INFO: Pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027359007s
    Aug 29 19:36:24.684: INFO: Pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4": Phase="Running", Reason="", readiness=true. Elapsed: 4.026931418s
    Aug 29 19:36:24.684: INFO: Pod "filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4" satisfied condition "running"
    Aug 29 19:36:24.684: INFO: Waiting up to 5m0s for pod "filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448" in namespace "sched-pred-9822" to be "running"
    Aug 29 19:36:24.688: INFO: Pod "filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448": Phase="Running", Reason="", readiness=true. Elapsed: 3.987391ms
    Aug 29 19:36:24.688: INFO: Pod "filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448" satisfied condition "running"
    Aug 29 19:36:24.688: INFO: Waiting up to 5m0s for pod "filler-pod-c1f10548-45ed-4a85-b683-932b569d0409" in namespace "sched-pred-9822" to be "running"
    Aug 29 19:36:24.692: INFO: Pod "filler-pod-c1f10548-45ed-4a85-b683-932b569d0409": Phase="Running", Reason="", readiness=true. Elapsed: 3.554507ms
    Aug 29 19:36:24.692: INFO: Pod "filler-pod-c1f10548-45ed-4a85-b683-932b569d0409" satisfied condition "running"
    Aug 29 19:36:24.692: INFO: Waiting up to 5m0s for pod "filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9" in namespace "sched-pred-9822" to be "running"
    Aug 29 19:36:24.695: INFO: Pod "filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9": Phase="Running", Reason="", readiness=true. Elapsed: 3.104561ms
    Aug 29 19:36:24.695: INFO: Pod "filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9" satisfied condition "running"
    Aug 29 19:36:24.695: INFO: Waiting up to 5m0s for pod "filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683" in namespace "sched-pred-9822" to be "running"
    Aug 29 19:36:24.699: INFO: Pod "filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683": Phase="Running", Reason="", readiness=true. Elapsed: 4.308451ms
    Aug 29 19:36:24.699: INFO: Pod "filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/29/23 19:36:24.699
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bd640de340], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448 to loki-15bd39-master-1] 08/29/23 19:36:24.704
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bdbb5e14a5], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bdec7a25ad], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 823.897474ms (823.904395ms including waiting)] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bdef0982e7], Reason = [Created], Message = [Created container filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448.177ff1bdf85d8b27], Reason = [Started], Message = [Started container filler-pod-0ffffd96-58e5-4ba0-9851-4d77baa71448] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bd6613fd7c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9 to loki-15bd39-worker-1] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bd93141393], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bdcd2a3a5c], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 974.499879ms (974.50771ms including waiting)] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bdcf6d8bc3], Reason = [Created], Message = [Created container filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9.177ff1bdd9243dd2], Reason = [Started], Message = [Started container filler-pod-21062e6b-548a-45d6-aaa2-336eec16c1a9] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bd62f60e7c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4 to loki-15bd39-master-0] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bd9551dadc], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bdc80dec60], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 851.159094ms (851.168791ms including waiting)] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bdcb47203b], Reason = [Created], Message = [Created container filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4.177ff1bdd41a314d], Reason = [Started], Message = [Started container filler-pod-718f7b3c-d98b-4efa-bbc9-c412f65c86c4] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bd6660a6a0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683 to loki-15bd39-worker-2] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bd9a19e1ab], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.705
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bdcb313ea5], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 823.580439ms (823.592784ms including waiting)] 08/29/23 19:36:24.706
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bdcde7ae54], Reason = [Created], Message = [Created container filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683] 08/29/23 19:36:24.706
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683.177ff1bdd6718554], Reason = [Started], Message = [Started container filler-pod-93b2a630-ec02-49d4-a039-69a30dd86683] 08/29/23 19:36:24.706
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bd64322267], Reason = [Scheduled], Message = [Successfully assigned sched-pred-9822/filler-pod-c1f10548-45ed-4a85-b683-932b569d0409 to loki-15bd39-worker-0] 08/29/23 19:36:24.706
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bd930a26fa], Reason = [Pulling], Message = [Pulling image "registry.k8s.io/pause:3.9"] 08/29/23 19:36:24.706
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bdc445180f], Reason = [Pulled], Message = [Successfully pulled image "registry.k8s.io/pause:3.9" in 825.909067ms (825.917263ms including waiting)] 08/29/23 19:36:24.706
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bdc6a56c61], Reason = [Created], Message = [Created container filler-pod-c1f10548-45ed-4a85-b683-932b569d0409] 08/29/23 19:36:24.706
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c1f10548-45ed-4a85-b683-932b569d0409.177ff1bdd05064d7], Reason = [Started], Message = [Started container filler-pod-c1f10548-45ed-4a85-b683-932b569d0409] 08/29/23 19:36:24.706
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.177ff1be56cfbbc5], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 Insufficient cpu. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod..] 08/29/23 19:36:24.726
    STEP: removing the label node off the node loki-15bd39-master-0 08/29/23 19:36:25.721
    STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.74
    STEP: removing the label node off the node loki-15bd39-master-1 08/29/23 19:36:25.744
    STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.776
    STEP: removing the label node off the node loki-15bd39-worker-0 08/29/23 19:36:25.78
    STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.8
    STEP: removing the label node off the node loki-15bd39-worker-1 08/29/23 19:36:25.803
    STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.818
    STEP: removing the label node off the node loki-15bd39-worker-2 08/29/23 19:36:25.822
    STEP: verifying the node doesn't have the label node 08/29/23 19:36:25.837
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:25.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-9822" for this suite. 08/29/23 19:36:25.846
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:25.858
Aug 29 19:36:25.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 19:36:25.859
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:25.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:25.882
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 08/29/23 19:36:25.885
Aug 29 19:36:25.894: INFO: Waiting up to 5m0s for pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183" in namespace "emptydir-4970" to be "Succeeded or Failed"
Aug 29 19:36:25.898: INFO: Pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183": Phase="Pending", Reason="", readiness=false. Elapsed: 3.87907ms
Aug 29 19:36:27.903: INFO: Pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008236702s
Aug 29 19:36:29.903: INFO: Pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008819566s
STEP: Saw pod success 08/29/23 19:36:29.903
Aug 29 19:36:29.903: INFO: Pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183" satisfied condition "Succeeded or Failed"
Aug 29 19:36:29.907: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-baf2d5e7-c9b3-420f-98e7-48585b828183 container test-container: <nil>
STEP: delete the pod 08/29/23 19:36:29.928
Aug 29 19:36:29.946: INFO: Waiting for pod pod-baf2d5e7-c9b3-420f-98e7-48585b828183 to disappear
Aug 29 19:36:29.950: INFO: Pod pod-baf2d5e7-c9b3-420f-98e7-48585b828183 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:29.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4970" for this suite. 08/29/23 19:36:29.954
------------------------------
• [4.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:25.858
    Aug 29 19:36:25.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 19:36:25.859
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:25.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:25.882
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/29/23 19:36:25.885
    Aug 29 19:36:25.894: INFO: Waiting up to 5m0s for pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183" in namespace "emptydir-4970" to be "Succeeded or Failed"
    Aug 29 19:36:25.898: INFO: Pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183": Phase="Pending", Reason="", readiness=false. Elapsed: 3.87907ms
    Aug 29 19:36:27.903: INFO: Pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008236702s
    Aug 29 19:36:29.903: INFO: Pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008819566s
    STEP: Saw pod success 08/29/23 19:36:29.903
    Aug 29 19:36:29.903: INFO: Pod "pod-baf2d5e7-c9b3-420f-98e7-48585b828183" satisfied condition "Succeeded or Failed"
    Aug 29 19:36:29.907: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-baf2d5e7-c9b3-420f-98e7-48585b828183 container test-container: <nil>
    STEP: delete the pod 08/29/23 19:36:29.928
    Aug 29 19:36:29.946: INFO: Waiting for pod pod-baf2d5e7-c9b3-420f-98e7-48585b828183 to disappear
    Aug 29 19:36:29.950: INFO: Pod pod-baf2d5e7-c9b3-420f-98e7-48585b828183 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:29.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4970" for this suite. 08/29/23 19:36:29.954
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:29.963
Aug 29 19:36:29.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 19:36:29.964
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:29.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:29.994
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-f88fee63-0e80-4038-aac9-4e583a56d5ab 08/29/23 19:36:29.997
STEP: Creating a pod to test consume secrets 08/29/23 19:36:30.003
Aug 29 19:36:30.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd" in namespace "projected-4436" to be "Succeeded or Failed"
Aug 29 19:36:30.014: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.072295ms
Aug 29 19:36:32.022: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011457239s
Aug 29 19:36:34.017: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd": Phase="Running", Reason="", readiness=false. Elapsed: 4.006169927s
Aug 29 19:36:36.021: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009846703s
STEP: Saw pod success 08/29/23 19:36:36.021
Aug 29 19:36:36.021: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd" satisfied condition "Succeeded or Failed"
Aug 29 19:36:36.029: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd container projected-secret-volume-test: <nil>
STEP: delete the pod 08/29/23 19:36:36.036
Aug 29 19:36:36.056: INFO: Waiting for pod pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd to disappear
Aug 29 19:36:36.059: INFO: Pod pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:36.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4436" for this suite. 08/29/23 19:36:36.065
------------------------------
• [SLOW TEST] [6.108 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:29.963
    Aug 29 19:36:29.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 19:36:29.964
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:29.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:29.994
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-f88fee63-0e80-4038-aac9-4e583a56d5ab 08/29/23 19:36:29.997
    STEP: Creating a pod to test consume secrets 08/29/23 19:36:30.003
    Aug 29 19:36:30.011: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd" in namespace "projected-4436" to be "Succeeded or Failed"
    Aug 29 19:36:30.014: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.072295ms
    Aug 29 19:36:32.022: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd": Phase="Running", Reason="", readiness=true. Elapsed: 2.011457239s
    Aug 29 19:36:34.017: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd": Phase="Running", Reason="", readiness=false. Elapsed: 4.006169927s
    Aug 29 19:36:36.021: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009846703s
    STEP: Saw pod success 08/29/23 19:36:36.021
    Aug 29 19:36:36.021: INFO: Pod "pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd" satisfied condition "Succeeded or Failed"
    Aug 29 19:36:36.029: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 19:36:36.036
    Aug 29 19:36:36.056: INFO: Waiting for pod pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd to disappear
    Aug 29 19:36:36.059: INFO: Pod pod-projected-secrets-2634e824-99d8-4096-ac8a-37e1ced72bfd no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:36.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4436" for this suite. 08/29/23 19:36:36.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:36.072
Aug 29 19:36:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename gc 08/29/23 19:36:36.073
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:36.092
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:36.095
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/29/23 19:36:36.097
STEP: Wait for the Deployment to create new ReplicaSet 08/29/23 19:36:36.105
STEP: delete the deployment 08/29/23 19:36:36.223
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/29/23 19:36:36.23
STEP: Gathering metrics 08/29/23 19:36:36.75
W0829 19:36:36.759431      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 19:36:36.759: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:36.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9499" for this suite. 08/29/23 19:36:36.764
------------------------------
• [0.702 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:36.072
    Aug 29 19:36:36.072: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename gc 08/29/23 19:36:36.073
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:36.092
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:36.095
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/29/23 19:36:36.097
    STEP: Wait for the Deployment to create new ReplicaSet 08/29/23 19:36:36.105
    STEP: delete the deployment 08/29/23 19:36:36.223
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/29/23 19:36:36.23
    STEP: Gathering metrics 08/29/23 19:36:36.75
    W0829 19:36:36.759431      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 29 19:36:36.759: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:36.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9499" for this suite. 08/29/23 19:36:36.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:36.774
Aug 29 19:36:36.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename controllerrevisions 08/29/23 19:36:36.776
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:36.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:36.797
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-z6gdt-daemon-set" 08/29/23 19:36:36.836
STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 19:36:36.842
Aug 29 19:36:36.851: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 0
Aug 29 19:36:36.851: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:36:37.863: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 0
Aug 29 19:36:37.863: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:36:38.861: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 4
Aug 29 19:36:38.861: INFO: Node loki-15bd39-worker-0 is running 0 daemon pod, expected 1
Aug 29 19:36:39.861: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 5
Aug 29 19:36:39.861: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-z6gdt-daemon-set
STEP: Confirm DaemonSet "e2e-z6gdt-daemon-set" successfully created with "daemonset-name=e2e-z6gdt-daemon-set" label 08/29/23 19:36:39.864
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-z6gdt-daemon-set" 08/29/23 19:36:39.871
Aug 29 19:36:39.876: INFO: Located ControllerRevision: "e2e-z6gdt-daemon-set-699bd4cbdb"
STEP: Patching ControllerRevision "e2e-z6gdt-daemon-set-699bd4cbdb" 08/29/23 19:36:39.878
Aug 29 19:36:39.890: INFO: e2e-z6gdt-daemon-set-699bd4cbdb has been patched
STEP: Create a new ControllerRevision 08/29/23 19:36:39.89
Aug 29 19:36:39.900: INFO: Created ControllerRevision: e2e-z6gdt-daemon-set-6cd94c9fd4
STEP: Confirm that there are two ControllerRevisions 08/29/23 19:36:39.9
Aug 29 19:36:39.900: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 29 19:36:39.903: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-z6gdt-daemon-set-699bd4cbdb" 08/29/23 19:36:39.903
STEP: Confirm that there is only one ControllerRevision 08/29/23 19:36:39.913
Aug 29 19:36:39.913: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 29 19:36:39.916: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-z6gdt-daemon-set-6cd94c9fd4" 08/29/23 19:36:39.92
Aug 29 19:36:39.931: INFO: e2e-z6gdt-daemon-set-6cd94c9fd4 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/29/23 19:36:39.931
W0829 19:36:39.944202      19 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/29/23 19:36:39.944
Aug 29 19:36:39.944: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 29 19:36:40.948: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 29 19:36:40.952: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-z6gdt-daemon-set-6cd94c9fd4=updated" 08/29/23 19:36:40.952
STEP: Confirm that there is only one ControllerRevision 08/29/23 19:36:40.961
Aug 29 19:36:40.961: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 29 19:36:40.965: INFO: Found 1 ControllerRevisions
Aug 29 19:36:40.968: INFO: ControllerRevision "e2e-z6gdt-daemon-set-64bd9d87b9" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-z6gdt-daemon-set" 08/29/23 19:36:40.972
STEP: deleting DaemonSet.extensions e2e-z6gdt-daemon-set in namespace controllerrevisions-6869, will wait for the garbage collector to delete the pods 08/29/23 19:36:40.972
Aug 29 19:36:41.033: INFO: Deleting DaemonSet.extensions e2e-z6gdt-daemon-set took: 7.508575ms
Aug 29 19:36:41.134: INFO: Terminating DaemonSet.extensions e2e-z6gdt-daemon-set pods took: 100.745325ms
Aug 29 19:36:42.538: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 0
Aug 29 19:36:42.538: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-z6gdt-daemon-set
Aug 29 19:36:42.542: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7169"},"items":null}

Aug 29 19:36:42.545: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7169"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:42.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-6869" for this suite. 08/29/23 19:36:42.572
------------------------------
• [SLOW TEST] [5.808 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:36.774
    Aug 29 19:36:36.775: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename controllerrevisions 08/29/23 19:36:36.776
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:36.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:36.797
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-z6gdt-daemon-set" 08/29/23 19:36:36.836
    STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 19:36:36.842
    Aug 29 19:36:36.851: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 0
    Aug 29 19:36:36.851: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:36:37.863: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 0
    Aug 29 19:36:37.863: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:36:38.861: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 4
    Aug 29 19:36:38.861: INFO: Node loki-15bd39-worker-0 is running 0 daemon pod, expected 1
    Aug 29 19:36:39.861: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 5
    Aug 29 19:36:39.861: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset e2e-z6gdt-daemon-set
    STEP: Confirm DaemonSet "e2e-z6gdt-daemon-set" successfully created with "daemonset-name=e2e-z6gdt-daemon-set" label 08/29/23 19:36:39.864
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-z6gdt-daemon-set" 08/29/23 19:36:39.871
    Aug 29 19:36:39.876: INFO: Located ControllerRevision: "e2e-z6gdt-daemon-set-699bd4cbdb"
    STEP: Patching ControllerRevision "e2e-z6gdt-daemon-set-699bd4cbdb" 08/29/23 19:36:39.878
    Aug 29 19:36:39.890: INFO: e2e-z6gdt-daemon-set-699bd4cbdb has been patched
    STEP: Create a new ControllerRevision 08/29/23 19:36:39.89
    Aug 29 19:36:39.900: INFO: Created ControllerRevision: e2e-z6gdt-daemon-set-6cd94c9fd4
    STEP: Confirm that there are two ControllerRevisions 08/29/23 19:36:39.9
    Aug 29 19:36:39.900: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 29 19:36:39.903: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-z6gdt-daemon-set-699bd4cbdb" 08/29/23 19:36:39.903
    STEP: Confirm that there is only one ControllerRevision 08/29/23 19:36:39.913
    Aug 29 19:36:39.913: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 29 19:36:39.916: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-z6gdt-daemon-set-6cd94c9fd4" 08/29/23 19:36:39.92
    Aug 29 19:36:39.931: INFO: e2e-z6gdt-daemon-set-6cd94c9fd4 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/29/23 19:36:39.931
    W0829 19:36:39.944202      19 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/29/23 19:36:39.944
    Aug 29 19:36:39.944: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 29 19:36:40.948: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 29 19:36:40.952: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-z6gdt-daemon-set-6cd94c9fd4=updated" 08/29/23 19:36:40.952
    STEP: Confirm that there is only one ControllerRevision 08/29/23 19:36:40.961
    Aug 29 19:36:40.961: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 29 19:36:40.965: INFO: Found 1 ControllerRevisions
    Aug 29 19:36:40.968: INFO: ControllerRevision "e2e-z6gdt-daemon-set-64bd9d87b9" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-z6gdt-daemon-set" 08/29/23 19:36:40.972
    STEP: deleting DaemonSet.extensions e2e-z6gdt-daemon-set in namespace controllerrevisions-6869, will wait for the garbage collector to delete the pods 08/29/23 19:36:40.972
    Aug 29 19:36:41.033: INFO: Deleting DaemonSet.extensions e2e-z6gdt-daemon-set took: 7.508575ms
    Aug 29 19:36:41.134: INFO: Terminating DaemonSet.extensions e2e-z6gdt-daemon-set pods took: 100.745325ms
    Aug 29 19:36:42.538: INFO: Number of nodes with available pods controlled by daemonset e2e-z6gdt-daemon-set: 0
    Aug 29 19:36:42.538: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-z6gdt-daemon-set
    Aug 29 19:36:42.542: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7169"},"items":null}

    Aug 29 19:36:42.545: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7169"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:42.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-6869" for this suite. 08/29/23 19:36:42.572
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:42.583
Aug 29 19:36:42.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename security-context 08/29/23 19:36:42.584
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:42.6
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:42.603
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/29/23 19:36:42.606
Aug 29 19:36:42.617: INFO: Waiting up to 5m0s for pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c" in namespace "security-context-1354" to be "Succeeded or Failed"
Aug 29 19:36:42.620: INFO: Pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519977ms
Aug 29 19:36:44.624: INFO: Pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007408596s
Aug 29 19:36:46.626: INFO: Pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009286699s
STEP: Saw pod success 08/29/23 19:36:46.626
Aug 29 19:36:46.626: INFO: Pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c" satisfied condition "Succeeded or Failed"
Aug 29 19:36:46.631: INFO: Trying to get logs from node loki-15bd39-worker-1 pod security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c container test-container: <nil>
STEP: delete the pod 08/29/23 19:36:46.638
Aug 29 19:36:46.651: INFO: Waiting for pod security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c to disappear
Aug 29 19:36:46.655: INFO: Pod security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:46.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1354" for this suite. 08/29/23 19:36:46.66
------------------------------
• [4.084 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:42.583
    Aug 29 19:36:42.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename security-context 08/29/23 19:36:42.584
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:42.6
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:42.603
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/29/23 19:36:42.606
    Aug 29 19:36:42.617: INFO: Waiting up to 5m0s for pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c" in namespace "security-context-1354" to be "Succeeded or Failed"
    Aug 29 19:36:42.620: INFO: Pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519977ms
    Aug 29 19:36:44.624: INFO: Pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007408596s
    Aug 29 19:36:46.626: INFO: Pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009286699s
    STEP: Saw pod success 08/29/23 19:36:46.626
    Aug 29 19:36:46.626: INFO: Pod "security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c" satisfied condition "Succeeded or Failed"
    Aug 29 19:36:46.631: INFO: Trying to get logs from node loki-15bd39-worker-1 pod security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c container test-container: <nil>
    STEP: delete the pod 08/29/23 19:36:46.638
    Aug 29 19:36:46.651: INFO: Waiting for pod security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c to disappear
    Aug 29 19:36:46.655: INFO: Pod security-context-2c831be1-3068-4dc1-a3ca-9c542b88b05c no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:46.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1354" for this suite. 08/29/23 19:36:46.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:46.668
Aug 29 19:36:46.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 19:36:46.67
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:46.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:46.686
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 08/29/23 19:36:46.689
Aug 29 19:36:46.698: INFO: Waiting up to 5m0s for pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec" in namespace "emptydir-1551" to be "Succeeded or Failed"
Aug 29 19:36:46.701: INFO: Pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435103ms
Aug 29 19:36:48.706: INFO: Pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007567988s
Aug 29 19:36:50.708: INFO: Pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009831238s
STEP: Saw pod success 08/29/23 19:36:50.708
Aug 29 19:36:50.708: INFO: Pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec" satisfied condition "Succeeded or Failed"
Aug 29 19:36:50.711: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec container test-container: <nil>
STEP: delete the pod 08/29/23 19:36:50.72
Aug 29 19:36:50.738: INFO: Waiting for pod pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec to disappear
Aug 29 19:36:50.741: INFO: Pod pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:50.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1551" for this suite. 08/29/23 19:36:50.746
------------------------------
• [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:46.668
    Aug 29 19:36:46.669: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 19:36:46.67
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:46.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:46.686
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/29/23 19:36:46.689
    Aug 29 19:36:46.698: INFO: Waiting up to 5m0s for pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec" in namespace "emptydir-1551" to be "Succeeded or Failed"
    Aug 29 19:36:46.701: INFO: Pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435103ms
    Aug 29 19:36:48.706: INFO: Pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007567988s
    Aug 29 19:36:50.708: INFO: Pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009831238s
    STEP: Saw pod success 08/29/23 19:36:50.708
    Aug 29 19:36:50.708: INFO: Pod "pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec" satisfied condition "Succeeded or Failed"
    Aug 29 19:36:50.711: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec container test-container: <nil>
    STEP: delete the pod 08/29/23 19:36:50.72
    Aug 29 19:36:50.738: INFO: Waiting for pod pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec to disappear
    Aug 29 19:36:50.741: INFO: Pod pod-6ac879a6-f692-4f8e-a32e-c1813d08ccec no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:50.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1551" for this suite. 08/29/23 19:36:50.746
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:50.755
Aug 29 19:36:50.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename init-container 08/29/23 19:36:50.756
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:50.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:50.778
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 08/29/23 19:36:50.781
Aug 29 19:36:50.781: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:54.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-6104" for this suite. 08/29/23 19:36:54.259
------------------------------
• [3.513 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:50.755
    Aug 29 19:36:50.755: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename init-container 08/29/23 19:36:50.756
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:50.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:50.778
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 08/29/23 19:36:50.781
    Aug 29 19:36:50.781: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:54.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-6104" for this suite. 08/29/23 19:36:54.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:54.269
Aug 29 19:36:54.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 19:36:54.27
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:54.287
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:54.291
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 08/29/23 19:36:54.294
Aug 29 19:36:54.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6834 create -f -'
Aug 29 19:36:55.806: INFO: stderr: ""
Aug 29 19:36:55.806: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/29/23 19:36:55.806
Aug 29 19:36:55.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6834 diff -f -'
Aug 29 19:36:57.128: INFO: rc: 1
Aug 29 19:36:57.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6834 delete -f -'
Aug 29 19:36:57.206: INFO: stderr: ""
Aug 29 19:36:57.206: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 19:36:57.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6834" for this suite. 08/29/23 19:36:57.211
------------------------------
• [2.950 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:54.269
    Aug 29 19:36:54.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 19:36:54.27
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:54.287
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:54.291
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 08/29/23 19:36:54.294
    Aug 29 19:36:54.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6834 create -f -'
    Aug 29 19:36:55.806: INFO: stderr: ""
    Aug 29 19:36:55.806: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/29/23 19:36:55.806
    Aug 29 19:36:55.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6834 diff -f -'
    Aug 29 19:36:57.128: INFO: rc: 1
    Aug 29 19:36:57.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6834 delete -f -'
    Aug 29 19:36:57.206: INFO: stderr: ""
    Aug 29 19:36:57.206: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:36:57.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6834" for this suite. 08/29/23 19:36:57.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:36:57.22
Aug 29 19:36:57.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename daemonsets 08/29/23 19:36:57.221
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:57.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:57.241
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205
Aug 29 19:36:57.275: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/29/23 19:36:57.281
Aug 29 19:36:57.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:36:57.284: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/29/23 19:36:57.284
Aug 29 19:36:57.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:36:57.309: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 19:36:58.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:36:58.314: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 19:36:59.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 19:36:59.313: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/29/23 19:36:59.316
Aug 29 19:36:59.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 19:36:59.334: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 29 19:37:00.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:00.339: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/29/23 19:37:00.339
Aug 29 19:37:00.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:00.351: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 19:37:01.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:01.357: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 19:37:03.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:03.885: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 19:37:04.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:04.357: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 19:37:05.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:05.356: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 19:37:06.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 29 19:37:06.357: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/29/23 19:37:06.364
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9604, will wait for the garbage collector to delete the pods 08/29/23 19:37:06.364
Aug 29 19:37:06.428: INFO: Deleting DaemonSet.extensions daemon-set took: 8.360575ms
Aug 29 19:37:06.529: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.345268ms
Aug 29 19:37:08.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:08.535: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 19:37:08.538: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7494"},"items":null}

Aug 29 19:37:08.542: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7494"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:37:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-9604" for this suite. 08/29/23 19:37:08.58
------------------------------
• [SLOW TEST] [11.368 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:36:57.22
    Aug 29 19:36:57.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename daemonsets 08/29/23 19:36:57.221
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:36:57.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:36:57.241
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:205
    Aug 29 19:36:57.275: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/29/23 19:36:57.281
    Aug 29 19:36:57.284: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:36:57.284: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/29/23 19:36:57.284
    Aug 29 19:36:57.309: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:36:57.309: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 19:36:58.314: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:36:58.314: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 19:36:59.313: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 29 19:36:59.313: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/29/23 19:36:59.316
    Aug 29 19:36:59.334: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 29 19:36:59.334: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 29 19:37:00.339: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:00.339: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/29/23 19:37:00.339
    Aug 29 19:37:00.351: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:00.351: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 19:37:01.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:01.357: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 19:37:03.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:03.885: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 19:37:04.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:04.357: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 19:37:05.356: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:05.356: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 19:37:06.357: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 29 19:37:06.357: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/29/23 19:37:06.364
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9604, will wait for the garbage collector to delete the pods 08/29/23 19:37:06.364
    Aug 29 19:37:06.428: INFO: Deleting DaemonSet.extensions daemon-set took: 8.360575ms
    Aug 29 19:37:06.529: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.345268ms
    Aug 29 19:37:08.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:08.535: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 29 19:37:08.538: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7494"},"items":null}

    Aug 29 19:37:08.542: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7494"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:37:08.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-9604" for this suite. 08/29/23 19:37:08.58
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:37:08.589
Aug 29 19:37:08.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 19:37:08.59
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:08.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:08.611
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 08/29/23 19:37:08.613
Aug 29 19:37:08.624: INFO: Waiting up to 5m0s for pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5" in namespace "downward-api-799" to be "Succeeded or Failed"
Aug 29 19:37:08.628: INFO: Pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.505647ms
Aug 29 19:37:10.633: INFO: Pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881023s
Aug 29 19:37:12.633: INFO: Pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008629224s
STEP: Saw pod success 08/29/23 19:37:12.633
Aug 29 19:37:12.633: INFO: Pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5" satisfied condition "Succeeded or Failed"
Aug 29 19:37:12.637: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5 container client-container: <nil>
STEP: delete the pod 08/29/23 19:37:12.644
Aug 29 19:37:12.661: INFO: Waiting for pod downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5 to disappear
Aug 29 19:37:12.664: INFO: Pod downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 19:37:12.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-799" for this suite. 08/29/23 19:37:12.669
------------------------------
• [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:37:08.589
    Aug 29 19:37:08.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 19:37:08.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:08.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:08.611
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 08/29/23 19:37:08.613
    Aug 29 19:37:08.624: INFO: Waiting up to 5m0s for pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5" in namespace "downward-api-799" to be "Succeeded or Failed"
    Aug 29 19:37:08.628: INFO: Pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.505647ms
    Aug 29 19:37:10.633: INFO: Pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00881023s
    Aug 29 19:37:12.633: INFO: Pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008629224s
    STEP: Saw pod success 08/29/23 19:37:12.633
    Aug 29 19:37:12.633: INFO: Pod "downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5" satisfied condition "Succeeded or Failed"
    Aug 29 19:37:12.637: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5 container client-container: <nil>
    STEP: delete the pod 08/29/23 19:37:12.644
    Aug 29 19:37:12.661: INFO: Waiting for pod downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5 to disappear
    Aug 29 19:37:12.664: INFO: Pod downwardapi-volume-975058fb-854f-4956-aa09-11d4f60e42e5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:37:12.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-799" for this suite. 08/29/23 19:37:12.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:37:12.685
Aug 29 19:37:12.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename containers 08/29/23 19:37:12.686
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:12.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:12.71
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 08/29/23 19:37:12.712
Aug 29 19:37:12.724: INFO: Waiting up to 5m0s for pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c" in namespace "containers-3642" to be "Succeeded or Failed"
Aug 29 19:37:12.727: INFO: Pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.808752ms
Aug 29 19:37:14.732: INFO: Pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007960187s
Aug 29 19:37:16.731: INFO: Pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00714259s
STEP: Saw pod success 08/29/23 19:37:16.731
Aug 29 19:37:16.731: INFO: Pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c" satisfied condition "Succeeded or Failed"
Aug 29 19:37:16.735: INFO: Trying to get logs from node loki-15bd39-worker-1 pod client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c container agnhost-container: <nil>
STEP: delete the pod 08/29/23 19:37:16.743
Aug 29 19:37:16.755: INFO: Waiting for pod client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c to disappear
Aug 29 19:37:16.758: INFO: Pod client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 29 19:37:16.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-3642" for this suite. 08/29/23 19:37:16.763
------------------------------
• [4.085 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:37:12.685
    Aug 29 19:37:12.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename containers 08/29/23 19:37:12.686
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:12.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:12.71
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 08/29/23 19:37:12.712
    Aug 29 19:37:12.724: INFO: Waiting up to 5m0s for pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c" in namespace "containers-3642" to be "Succeeded or Failed"
    Aug 29 19:37:12.727: INFO: Pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.808752ms
    Aug 29 19:37:14.732: INFO: Pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007960187s
    Aug 29 19:37:16.731: INFO: Pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00714259s
    STEP: Saw pod success 08/29/23 19:37:16.731
    Aug 29 19:37:16.731: INFO: Pod "client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c" satisfied condition "Succeeded or Failed"
    Aug 29 19:37:16.735: INFO: Trying to get logs from node loki-15bd39-worker-1 pod client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 19:37:16.743
    Aug 29 19:37:16.755: INFO: Waiting for pod client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c to disappear
    Aug 29 19:37:16.758: INFO: Pod client-containers-5465e81f-078e-4d2d-80a4-9114eb996d7c no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:37:16.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-3642" for this suite. 08/29/23 19:37:16.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:37:16.772
Aug 29 19:37:16.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pod-network-test 08/29/23 19:37:16.773
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:16.79
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:16.792
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-1959 08/29/23 19:37:16.796
STEP: creating a selector 08/29/23 19:37:16.796
STEP: Creating the service pods in kubernetes 08/29/23 19:37:16.796
Aug 29 19:37:16.796: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 19:37:16.867: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1959" to be "running and ready"
Aug 29 19:37:16.873: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016529ms
Aug 29 19:37:16.873: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:37:18.879: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012424372s
Aug 29 19:37:18.879: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:37:20.878: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01091879s
Aug 29 19:37:20.878: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:37:22.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010173541s
Aug 29 19:37:22.877: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:37:24.879: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012198012s
Aug 29 19:37:24.879: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:37:26.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011320407s
Aug 29 19:37:26.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:37:28.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011145555s
Aug 29 19:37:28.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:37:30.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010776415s
Aug 29 19:37:30.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:37:32.879: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012256516s
Aug 29 19:37:32.879: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:37:34.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011196514s
Aug 29 19:37:34.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:37:36.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010872546s
Aug 29 19:37:36.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:37:38.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010330027s
Aug 29 19:37:38.877: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 29 19:37:38.877: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 29 19:37:38.880: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1959" to be "running and ready"
Aug 29 19:37:38.883: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.95745ms
Aug 29 19:37:38.883: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 29 19:37:38.883: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 29 19:37:38.886: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1959" to be "running and ready"
Aug 29 19:37:38.889: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.451076ms
Aug 29 19:37:38.889: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 29 19:37:38.889: INFO: Pod "netserver-2" satisfied condition "running and ready"
Aug 29 19:37:38.892: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1959" to be "running and ready"
Aug 29 19:37:38.895: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.381892ms
Aug 29 19:37:38.895: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Aug 29 19:37:38.895: INFO: Pod "netserver-3" satisfied condition "running and ready"
Aug 29 19:37:38.898: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1959" to be "running and ready"
Aug 29 19:37:38.901: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.366149ms
Aug 29 19:37:38.901: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Aug 29 19:37:38.901: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 08/29/23 19:37:38.904
Aug 29 19:37:38.910: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1959" to be "running"
Aug 29 19:37:38.915: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.036613ms
Aug 29 19:37:40.920: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010296081s
Aug 29 19:37:40.920: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 29 19:37:40.924: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Aug 29 19:37:40.924: INFO: Breadth first check of 172.20.17.68 on host 10.45.35.202...
Aug 29 19:37:40.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.17.68&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:37:40.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:37:40.928: INFO: ExecWithOptions: Clientset creation
Aug 29 19:37:40.928: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.17.68%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:37:41.000: INFO: Waiting for responses: map[]
Aug 29 19:37:41.000: INFO: reached 172.20.17.68 after 0/1 tries
Aug 29 19:37:41.000: INFO: Breadth first check of 172.20.76.134 on host 10.45.35.204...
Aug 29 19:37:41.003: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.76.134&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:37:41.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:37:41.004: INFO: ExecWithOptions: Clientset creation
Aug 29 19:37:41.004: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.76.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:37:41.078: INFO: Waiting for responses: map[]
Aug 29 19:37:41.078: INFO: reached 172.20.76.134 after 0/1 tries
Aug 29 19:37:41.078: INFO: Breadth first check of 172.20.143.206 on host 10.45.35.198...
Aug 29 19:37:41.082: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.143.206&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:37:41.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:37:41.082: INFO: ExecWithOptions: Clientset creation
Aug 29 19:37:41.083: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.143.206%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:37:41.154: INFO: Waiting for responses: map[]
Aug 29 19:37:41.154: INFO: reached 172.20.143.206 after 0/1 tries
Aug 29 19:37:41.154: INFO: Breadth first check of 172.20.30.157 on host 10.45.35.206...
Aug 29 19:37:41.158: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.30.157&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:37:41.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:37:41.159: INFO: ExecWithOptions: Clientset creation
Aug 29 19:37:41.159: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.30.157%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:37:41.235: INFO: Waiting for responses: map[]
Aug 29 19:37:41.235: INFO: reached 172.20.30.157 after 0/1 tries
Aug 29 19:37:41.235: INFO: Breadth first check of 172.20.84.147 on host 10.45.35.199...
Aug 29 19:37:41.239: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.84.147&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:37:41.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:37:41.240: INFO: ExecWithOptions: Clientset creation
Aug 29 19:37:41.240: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.84.147%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:37:41.308: INFO: Waiting for responses: map[]
Aug 29 19:37:41.308: INFO: reached 172.20.84.147 after 0/1 tries
Aug 29 19:37:41.308: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 29 19:37:41.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-1959" for this suite. 08/29/23 19:37:41.314
------------------------------
• [SLOW TEST] [24.551 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:37:16.772
    Aug 29 19:37:16.772: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pod-network-test 08/29/23 19:37:16.773
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:16.79
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:16.792
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-1959 08/29/23 19:37:16.796
    STEP: creating a selector 08/29/23 19:37:16.796
    STEP: Creating the service pods in kubernetes 08/29/23 19:37:16.796
    Aug 29 19:37:16.796: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 29 19:37:16.867: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1959" to be "running and ready"
    Aug 29 19:37:16.873: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.016529ms
    Aug 29 19:37:16.873: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:37:18.879: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012424372s
    Aug 29 19:37:18.879: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:37:20.878: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01091879s
    Aug 29 19:37:20.878: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:37:22.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010173541s
    Aug 29 19:37:22.877: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:37:24.879: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012198012s
    Aug 29 19:37:24.879: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:37:26.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011320407s
    Aug 29 19:37:26.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:37:28.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011145555s
    Aug 29 19:37:28.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:37:30.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010776415s
    Aug 29 19:37:30.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:37:32.879: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.012256516s
    Aug 29 19:37:32.879: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:37:34.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011196514s
    Aug 29 19:37:34.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:37:36.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.010872546s
    Aug 29 19:37:36.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:37:38.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010330027s
    Aug 29 19:37:38.877: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 29 19:37:38.877: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 29 19:37:38.880: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1959" to be "running and ready"
    Aug 29 19:37:38.883: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.95745ms
    Aug 29 19:37:38.883: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 29 19:37:38.883: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 29 19:37:38.886: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-1959" to be "running and ready"
    Aug 29 19:37:38.889: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.451076ms
    Aug 29 19:37:38.889: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 29 19:37:38.889: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Aug 29 19:37:38.892: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-1959" to be "running and ready"
    Aug 29 19:37:38.895: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.381892ms
    Aug 29 19:37:38.895: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Aug 29 19:37:38.895: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Aug 29 19:37:38.898: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-1959" to be "running and ready"
    Aug 29 19:37:38.901: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.366149ms
    Aug 29 19:37:38.901: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Aug 29 19:37:38.901: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 08/29/23 19:37:38.904
    Aug 29 19:37:38.910: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1959" to be "running"
    Aug 29 19:37:38.915: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.036613ms
    Aug 29 19:37:40.920: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010296081s
    Aug 29 19:37:40.920: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 29 19:37:40.924: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Aug 29 19:37:40.924: INFO: Breadth first check of 172.20.17.68 on host 10.45.35.202...
    Aug 29 19:37:40.927: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.17.68&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:37:40.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:37:40.928: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:37:40.928: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.17.68%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:37:41.000: INFO: Waiting for responses: map[]
    Aug 29 19:37:41.000: INFO: reached 172.20.17.68 after 0/1 tries
    Aug 29 19:37:41.000: INFO: Breadth first check of 172.20.76.134 on host 10.45.35.204...
    Aug 29 19:37:41.003: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.76.134&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:37:41.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:37:41.004: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:37:41.004: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.76.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:37:41.078: INFO: Waiting for responses: map[]
    Aug 29 19:37:41.078: INFO: reached 172.20.76.134 after 0/1 tries
    Aug 29 19:37:41.078: INFO: Breadth first check of 172.20.143.206 on host 10.45.35.198...
    Aug 29 19:37:41.082: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.143.206&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:37:41.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:37:41.082: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:37:41.083: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.143.206%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:37:41.154: INFO: Waiting for responses: map[]
    Aug 29 19:37:41.154: INFO: reached 172.20.143.206 after 0/1 tries
    Aug 29 19:37:41.154: INFO: Breadth first check of 172.20.30.157 on host 10.45.35.206...
    Aug 29 19:37:41.158: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.30.157&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:37:41.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:37:41.159: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:37:41.159: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.30.157%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:37:41.235: INFO: Waiting for responses: map[]
    Aug 29 19:37:41.235: INFO: reached 172.20.30.157 after 0/1 tries
    Aug 29 19:37:41.235: INFO: Breadth first check of 172.20.84.147 on host 10.45.35.199...
    Aug 29 19:37:41.239: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.84.148:9080/dial?request=hostname&protocol=http&host=172.20.84.147&port=8083&tries=1'] Namespace:pod-network-test-1959 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:37:41.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:37:41.240: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:37:41.240: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-1959/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.84.148%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.20.84.147%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:37:41.308: INFO: Waiting for responses: map[]
    Aug 29 19:37:41.308: INFO: reached 172.20.84.147 after 0/1 tries
    Aug 29 19:37:41.308: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:37:41.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-1959" for this suite. 08/29/23 19:37:41.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:37:41.329
Aug 29 19:37:41.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename daemonsets 08/29/23 19:37:41.331
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:41.353
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:41.356
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443
Aug 29 19:37:41.394: INFO: Create a RollingUpdate DaemonSet
Aug 29 19:37:41.400: INFO: Check that daemon pods launch on every node of the cluster
Aug 29 19:37:41.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:41.410: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:37:42.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:42.420: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 19:37:43.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 19:37:43.422: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 19:37:44.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 19:37:44.422: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
Aug 29 19:37:44.422: INFO: Update the DaemonSet to trigger a rollout
Aug 29 19:37:44.433: INFO: Updating DaemonSet daemon-set
Aug 29 19:37:46.452: INFO: Roll back the DaemonSet before rollout is complete
Aug 29 19:37:46.462: INFO: Updating DaemonSet daemon-set
Aug 29 19:37:46.462: INFO: Make sure DaemonSet rollback is complete
Aug 29 19:37:46.470: INFO: Wrong image for pod: daemon-set-sqzd2. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Aug 29 19:37:46.470: INFO: Pod daemon-set-sqzd2 is not available
Aug 29 19:37:49.481: INFO: Pod daemon-set-5m9q7 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/29/23 19:37:49.493
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5621, will wait for the garbage collector to delete the pods 08/29/23 19:37:49.493
Aug 29 19:37:49.555: INFO: Deleting DaemonSet.extensions daemon-set took: 7.909093ms
Aug 29 19:37:49.656: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.304927ms
Aug 29 19:37:52.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 19:37:52.561: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 19:37:52.565: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8031"},"items":null}

Aug 29 19:37:52.568: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8031"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:37:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5621" for this suite. 08/29/23 19:37:52.594
------------------------------
• [SLOW TEST] [11.272 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:37:41.329
    Aug 29 19:37:41.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename daemonsets 08/29/23 19:37:41.331
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:41.353
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:41.356
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:443
    Aug 29 19:37:41.394: INFO: Create a RollingUpdate DaemonSet
    Aug 29 19:37:41.400: INFO: Check that daemon pods launch on every node of the cluster
    Aug 29 19:37:41.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:41.410: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:37:42.420: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:42.420: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 19:37:43.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 19:37:43.422: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 19:37:44.422: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 19:37:44.422: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    Aug 29 19:37:44.422: INFO: Update the DaemonSet to trigger a rollout
    Aug 29 19:37:44.433: INFO: Updating DaemonSet daemon-set
    Aug 29 19:37:46.452: INFO: Roll back the DaemonSet before rollout is complete
    Aug 29 19:37:46.462: INFO: Updating DaemonSet daemon-set
    Aug 29 19:37:46.462: INFO: Make sure DaemonSet rollback is complete
    Aug 29 19:37:46.470: INFO: Wrong image for pod: daemon-set-sqzd2. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Aug 29 19:37:46.470: INFO: Pod daemon-set-sqzd2 is not available
    Aug 29 19:37:49.481: INFO: Pod daemon-set-5m9q7 is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/29/23 19:37:49.493
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5621, will wait for the garbage collector to delete the pods 08/29/23 19:37:49.493
    Aug 29 19:37:49.555: INFO: Deleting DaemonSet.extensions daemon-set took: 7.909093ms
    Aug 29 19:37:49.656: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.304927ms
    Aug 29 19:37:52.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 19:37:52.561: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 29 19:37:52.565: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8031"},"items":null}

    Aug 29 19:37:52.568: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8031"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:37:52.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5621" for this suite. 08/29/23 19:37:52.594
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:37:52.601
Aug 29 19:37:52.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename deployment 08/29/23 19:37:52.603
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:52.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:52.623
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 29 19:37:52.626: INFO: Creating simple deployment test-new-deployment
Aug 29 19:37:52.649: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 08/29/23 19:37:54.662
STEP: updating a scale subresource 08/29/23 19:37:54.665
STEP: verifying the deployment Spec.Replicas was modified 08/29/23 19:37:54.673
STEP: Patch a scale subresource 08/29/23 19:37:54.677
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 19:37:54.695: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-8227  56281bca-1cd6-48d0-92ad-e241425a7a81 8065 3 2023-08-29 19:37:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-29 19:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032acd08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-29 19:37:53 +0000 UTC,LastTransitionTime:2023-08-29 19:37:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-29 19:37:53 +0000 UTC,LastTransitionTime:2023-08-29 19:37:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 19:37:54.701: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8227  2933f70d-8a75-4867-a345-1314b778d714 8068 3 2023-08-29 19:37:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 56281bca-1cd6-48d0-92ad-e241425a7a81 0xc0002cff10 0xc0002cff11}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:37:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-29 19:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56281bca-1cd6-48d0-92ad-e241425a7a81\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002cffd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 19:37:54.706: INFO: Pod "test-new-deployment-7f5969cbc7-hfk8b" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-hfk8b test-new-deployment-7f5969cbc7- deployment-8227  48d30efd-99f7-414e-a1ac-308c1ff9363a 8067 0 2023-08-29 19:37:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 2933f70d-8a75-4867-a345-1314b778d714 0xc0032ad170 0xc0032ad171}] [] [{kube-controller-manager Update v1 2023-08-29 19:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2933f70d-8a75-4867-a345-1314b778d714\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6cx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6cx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:37:54.706: INFO: Pod "test-new-deployment-7f5969cbc7-qq9zz" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qq9zz test-new-deployment-7f5969cbc7- deployment-8227  b400499a-1785-45cd-b99b-531f6e5551ce 8056 0 2023-08-29 19:37:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:790c2c334b3bb70b6c31b4d7f0cfa81bb0b33d4c2711e1ceaf5602f6302a137d cni.projectcalico.org/podIP:172.20.84.150/32 cni.projectcalico.org/podIPs:172.20.84.150/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 2933f70d-8a75-4867-a345-1314b778d714 0xc0032ad2c7 0xc0032ad2c8}] [] [{kube-controller-manager Update v1 2023-08-29 19:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2933f70d-8a75-4867-a345-1314b778d714\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbc77,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbc77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:37:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:37:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.150,StartTime:2023-08-29 19:37:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:37:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://944f50a8fe10f646d8b09651b8deb635cbeecc077fc6f07d7f602327fbde16a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 29 19:37:54.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8227" for this suite. 08/29/23 19:37:54.713
------------------------------
• [2.130 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:37:52.601
    Aug 29 19:37:52.601: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename deployment 08/29/23 19:37:52.603
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:52.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:52.623
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 29 19:37:52.626: INFO: Creating simple deployment test-new-deployment
    Aug 29 19:37:52.649: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 08/29/23 19:37:54.662
    STEP: updating a scale subresource 08/29/23 19:37:54.665
    STEP: verifying the deployment Spec.Replicas was modified 08/29/23 19:37:54.673
    STEP: Patch a scale subresource 08/29/23 19:37:54.677
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 29 19:37:54.695: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-8227  56281bca-1cd6-48d0-92ad-e241425a7a81 8065 3 2023-08-29 19:37:52 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-29 19:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032acd08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-29 19:37:53 +0000 UTC,LastTransitionTime:2023-08-29 19:37:53 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-08-29 19:37:53 +0000 UTC,LastTransitionTime:2023-08-29 19:37:52 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 29 19:37:54.701: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-8227  2933f70d-8a75-4867-a345-1314b778d714 8068 3 2023-08-29 19:37:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 56281bca-1cd6-48d0-92ad-e241425a7a81 0xc0002cff10 0xc0002cff11}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:37:53 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-29 19:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"56281bca-1cd6-48d0-92ad-e241425a7a81\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0002cffd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 19:37:54.706: INFO: Pod "test-new-deployment-7f5969cbc7-hfk8b" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-hfk8b test-new-deployment-7f5969cbc7- deployment-8227  48d30efd-99f7-414e-a1ac-308c1ff9363a 8067 0 2023-08-29 19:37:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 2933f70d-8a75-4867-a345-1314b778d714 0xc0032ad170 0xc0032ad171}] [] [{kube-controller-manager Update v1 2023-08-29 19:37:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2933f70d-8a75-4867-a345-1314b778d714\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6cx6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6cx6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:37:54.706: INFO: Pod "test-new-deployment-7f5969cbc7-qq9zz" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-qq9zz test-new-deployment-7f5969cbc7- deployment-8227  b400499a-1785-45cd-b99b-531f6e5551ce 8056 0 2023-08-29 19:37:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:790c2c334b3bb70b6c31b4d7f0cfa81bb0b33d4c2711e1ceaf5602f6302a137d cni.projectcalico.org/podIP:172.20.84.150/32 cni.projectcalico.org/podIPs:172.20.84.150/32] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 2933f70d-8a75-4867-a345-1314b778d714 0xc0032ad2c7 0xc0032ad2c8}] [] [{kube-controller-manager Update v1 2023-08-29 19:37:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2933f70d-8a75-4867-a345-1314b778d714\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:37:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:37:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbc77,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbc77,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:37:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:37:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:37:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.150,StartTime:2023-08-29 19:37:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:37:53 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://944f50a8fe10f646d8b09651b8deb635cbeecc077fc6f07d7f602327fbde16a6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:37:54.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8227" for this suite. 08/29/23 19:37:54.713
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:37:54.732
Aug 29 19:37:54.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 19:37:54.733
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:54.757
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:54.76
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 08/29/23 19:37:54.763
Aug 29 19:37:54.775: INFO: Waiting up to 5m0s for pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9" in namespace "downward-api-1457" to be "running and ready"
Aug 29 19:37:54.778: INFO: Pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827092ms
Aug 29 19:37:54.778: INFO: The phase of Pod labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:37:56.782: INFO: Pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007165439s
Aug 29 19:37:56.782: INFO: The phase of Pod labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9 is Running (Ready = true)
Aug 29 19:37:56.782: INFO: Pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9" satisfied condition "running and ready"
Aug 29 19:37:57.315: INFO: Successfully updated pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 19:37:59.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1457" for this suite. 08/29/23 19:37:59.336
------------------------------
• [4.612 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:37:54.732
    Aug 29 19:37:54.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 19:37:54.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:54.757
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:54.76
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 08/29/23 19:37:54.763
    Aug 29 19:37:54.775: INFO: Waiting up to 5m0s for pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9" in namespace "downward-api-1457" to be "running and ready"
    Aug 29 19:37:54.778: INFO: Pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827092ms
    Aug 29 19:37:54.778: INFO: The phase of Pod labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:37:56.782: INFO: Pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007165439s
    Aug 29 19:37:56.782: INFO: The phase of Pod labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9 is Running (Ready = true)
    Aug 29 19:37:56.782: INFO: Pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9" satisfied condition "running and ready"
    Aug 29 19:37:57.315: INFO: Successfully updated pod "labelsupdatedfcb5c49-4060-4386-afb3-b03735dffbf9"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:37:59.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1457" for this suite. 08/29/23 19:37:59.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:37:59.345
Aug 29 19:37:59.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-probe 08/29/23 19:37:59.346
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:59.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:59.369
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477 in namespace container-probe-628 08/29/23 19:37:59.372
Aug 29 19:37:59.383: INFO: Waiting up to 5m0s for pod "test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477" in namespace "container-probe-628" to be "not pending"
Aug 29 19:37:59.387: INFO: Pod "test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477": Phase="Pending", Reason="", readiness=false. Elapsed: 3.820599ms
Aug 29 19:38:01.392: INFO: Pod "test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477": Phase="Running", Reason="", readiness=true. Elapsed: 2.009167692s
Aug 29 19:38:01.393: INFO: Pod "test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477" satisfied condition "not pending"
Aug 29 19:38:01.393: INFO: Started pod test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477 in namespace container-probe-628
STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 19:38:01.393
Aug 29 19:38:01.396: INFO: Initial restart count of pod test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477 is 0
STEP: deleting the pod 08/29/23 19:42:02.023
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 29 19:42:02.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-628" for this suite. 08/29/23 19:42:02.049
------------------------------
• [SLOW TEST] [242.713 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:37:59.345
    Aug 29 19:37:59.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-probe 08/29/23 19:37:59.346
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:37:59.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:37:59.369
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477 in namespace container-probe-628 08/29/23 19:37:59.372
    Aug 29 19:37:59.383: INFO: Waiting up to 5m0s for pod "test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477" in namespace "container-probe-628" to be "not pending"
    Aug 29 19:37:59.387: INFO: Pod "test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477": Phase="Pending", Reason="", readiness=false. Elapsed: 3.820599ms
    Aug 29 19:38:01.392: INFO: Pod "test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477": Phase="Running", Reason="", readiness=true. Elapsed: 2.009167692s
    Aug 29 19:38:01.393: INFO: Pod "test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477" satisfied condition "not pending"
    Aug 29 19:38:01.393: INFO: Started pod test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477 in namespace container-probe-628
    STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 19:38:01.393
    Aug 29 19:38:01.396: INFO: Initial restart count of pod test-webserver-dbae1c2e-c063-481e-a944-3a3d31ec3477 is 0
    STEP: deleting the pod 08/29/23 19:42:02.023
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:42:02.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-628" for this suite. 08/29/23 19:42:02.049
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:42:02.058
Aug 29 19:42:02.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 19:42:02.059
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:02.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:02.081
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 08/29/23 19:42:02.085
Aug 29 19:42:02.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: mark a version not serverd 08/29/23 19:42:06.815
STEP: check the unserved version gets removed 08/29/23 19:42:06.845
STEP: check the other version is not changed 08/29/23 19:42:09.528
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:42:13.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-1943" for this suite. 08/29/23 19:42:13.452
------------------------------
• [SLOW TEST] [11.402 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:42:02.058
    Aug 29 19:42:02.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 19:42:02.059
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:02.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:02.081
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 08/29/23 19:42:02.085
    Aug 29 19:42:02.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: mark a version not serverd 08/29/23 19:42:06.815
    STEP: check the unserved version gets removed 08/29/23 19:42:06.845
    STEP: check the other version is not changed 08/29/23 19:42:09.528
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:42:13.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-1943" for this suite. 08/29/23 19:42:13.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:42:13.462
Aug 29 19:42:13.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 19:42:13.463
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:13.485
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:13.488
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 08/29/23 19:42:13.49
Aug 29 19:42:13.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 create -f -'
Aug 29 19:42:14.271: INFO: stderr: ""
Aug 29 19:42:14.271: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/29/23 19:42:14.271
Aug 29 19:42:14.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 19:42:14.373: INFO: stderr: ""
Aug 29 19:42:14.373: INFO: stdout: "update-demo-nautilus-l7mt6 update-demo-nautilus-mls5p "
Aug 29 19:42:14.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:14.454: INFO: stderr: ""
Aug 29 19:42:14.454: INFO: stdout: ""
Aug 29 19:42:14.454: INFO: update-demo-nautilus-l7mt6 is created but not running
Aug 29 19:42:19.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 19:42:19.542: INFO: stderr: ""
Aug 29 19:42:19.542: INFO: stdout: "update-demo-nautilus-l7mt6 update-demo-nautilus-mls5p "
Aug 29 19:42:19.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:19.618: INFO: stderr: ""
Aug 29 19:42:19.618: INFO: stdout: "true"
Aug 29 19:42:19.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 19:42:19.702: INFO: stderr: ""
Aug 29 19:42:19.702: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 19:42:19.702: INFO: validating pod update-demo-nautilus-l7mt6
Aug 29 19:42:19.707: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 19:42:19.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 19:42:19.707: INFO: update-demo-nautilus-l7mt6 is verified up and running
Aug 29 19:42:19.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-mls5p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:19.784: INFO: stderr: ""
Aug 29 19:42:19.784: INFO: stdout: ""
Aug 29 19:42:19.784: INFO: update-demo-nautilus-mls5p is created but not running
Aug 29 19:42:24.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 19:42:24.859: INFO: stderr: ""
Aug 29 19:42:24.859: INFO: stdout: "update-demo-nautilus-l7mt6 update-demo-nautilus-mls5p "
Aug 29 19:42:24.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:24.934: INFO: stderr: ""
Aug 29 19:42:24.934: INFO: stdout: "true"
Aug 29 19:42:24.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 19:42:25.019: INFO: stderr: ""
Aug 29 19:42:25.019: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 19:42:25.019: INFO: validating pod update-demo-nautilus-l7mt6
Aug 29 19:42:25.023: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 19:42:25.024: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 19:42:25.024: INFO: update-demo-nautilus-l7mt6 is verified up and running
Aug 29 19:42:25.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-mls5p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:25.113: INFO: stderr: ""
Aug 29 19:42:25.113: INFO: stdout: "true"
Aug 29 19:42:25.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-mls5p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 19:42:25.189: INFO: stderr: ""
Aug 29 19:42:25.190: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 19:42:25.190: INFO: validating pod update-demo-nautilus-mls5p
Aug 29 19:42:25.195: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 19:42:25.195: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 19:42:25.195: INFO: update-demo-nautilus-mls5p is verified up and running
STEP: scaling down the replication controller 08/29/23 19:42:25.195
Aug 29 19:42:25.197: INFO: scanned /root for discovery docs: <nil>
Aug 29 19:42:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 29 19:42:26.304: INFO: stderr: ""
Aug 29 19:42:26.304: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/29/23 19:42:26.304
Aug 29 19:42:26.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 19:42:26.390: INFO: stderr: ""
Aug 29 19:42:26.390: INFO: stdout: "update-demo-nautilus-l7mt6 update-demo-nautilus-mls5p "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/29/23 19:42:26.39
Aug 29 19:42:31.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 19:42:31.482: INFO: stderr: ""
Aug 29 19:42:31.482: INFO: stdout: "update-demo-nautilus-l7mt6 "
Aug 29 19:42:31.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:31.568: INFO: stderr: ""
Aug 29 19:42:31.568: INFO: stdout: "true"
Aug 29 19:42:31.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 19:42:31.649: INFO: stderr: ""
Aug 29 19:42:31.649: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 19:42:31.649: INFO: validating pod update-demo-nautilus-l7mt6
Aug 29 19:42:31.655: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 19:42:31.655: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 19:42:31.655: INFO: update-demo-nautilus-l7mt6 is verified up and running
STEP: scaling up the replication controller 08/29/23 19:42:31.655
Aug 29 19:42:31.657: INFO: scanned /root for discovery docs: <nil>
Aug 29 19:42:31.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 29 19:42:32.777: INFO: stderr: ""
Aug 29 19:42:32.778: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/29/23 19:42:32.778
Aug 29 19:42:32.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 19:42:32.873: INFO: stderr: ""
Aug 29 19:42:32.873: INFO: stdout: "update-demo-nautilus-8wkn7 update-demo-nautilus-l7mt6 "
Aug 29 19:42:32.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-8wkn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:32.957: INFO: stderr: ""
Aug 29 19:42:32.957: INFO: stdout: ""
Aug 29 19:42:32.957: INFO: update-demo-nautilus-8wkn7 is created but not running
Aug 29 19:42:37.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 19:42:38.061: INFO: stderr: ""
Aug 29 19:42:38.061: INFO: stdout: "update-demo-nautilus-8wkn7 update-demo-nautilus-l7mt6 "
Aug 29 19:42:38.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-8wkn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:38.150: INFO: stderr: ""
Aug 29 19:42:38.150: INFO: stdout: "true"
Aug 29 19:42:38.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-8wkn7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 19:42:38.233: INFO: stderr: ""
Aug 29 19:42:38.233: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 19:42:38.233: INFO: validating pod update-demo-nautilus-8wkn7
Aug 29 19:42:38.238: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 19:42:38.238: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 19:42:38.238: INFO: update-demo-nautilus-8wkn7 is verified up and running
Aug 29 19:42:38.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 19:42:38.320: INFO: stderr: ""
Aug 29 19:42:38.320: INFO: stdout: "true"
Aug 29 19:42:38.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 19:42:38.413: INFO: stderr: ""
Aug 29 19:42:38.413: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 19:42:38.414: INFO: validating pod update-demo-nautilus-l7mt6
Aug 29 19:42:38.418: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 19:42:38.418: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 19:42:38.418: INFO: update-demo-nautilus-l7mt6 is verified up and running
STEP: using delete to clean up resources 08/29/23 19:42:38.418
Aug 29 19:42:38.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 delete --grace-period=0 --force -f -'
Aug 29 19:42:38.514: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 19:42:38.514: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 29 19:42:38.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get rc,svc -l name=update-demo --no-headers'
Aug 29 19:42:38.617: INFO: stderr: "No resources found in kubectl-3308 namespace.\n"
Aug 29 19:42:38.617: INFO: stdout: ""
Aug 29 19:42:38.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 19:42:38.702: INFO: stderr: ""
Aug 29 19:42:38.702: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 19:42:38.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3308" for this suite. 08/29/23 19:42:38.708
------------------------------
• [SLOW TEST] [25.260 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:42:13.462
    Aug 29 19:42:13.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 19:42:13.463
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:13.485
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:13.488
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 08/29/23 19:42:13.49
    Aug 29 19:42:13.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 create -f -'
    Aug 29 19:42:14.271: INFO: stderr: ""
    Aug 29 19:42:14.271: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/29/23 19:42:14.271
    Aug 29 19:42:14.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 19:42:14.373: INFO: stderr: ""
    Aug 29 19:42:14.373: INFO: stdout: "update-demo-nautilus-l7mt6 update-demo-nautilus-mls5p "
    Aug 29 19:42:14.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:14.454: INFO: stderr: ""
    Aug 29 19:42:14.454: INFO: stdout: ""
    Aug 29 19:42:14.454: INFO: update-demo-nautilus-l7mt6 is created but not running
    Aug 29 19:42:19.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 19:42:19.542: INFO: stderr: ""
    Aug 29 19:42:19.542: INFO: stdout: "update-demo-nautilus-l7mt6 update-demo-nautilus-mls5p "
    Aug 29 19:42:19.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:19.618: INFO: stderr: ""
    Aug 29 19:42:19.618: INFO: stdout: "true"
    Aug 29 19:42:19.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 19:42:19.702: INFO: stderr: ""
    Aug 29 19:42:19.702: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 19:42:19.702: INFO: validating pod update-demo-nautilus-l7mt6
    Aug 29 19:42:19.707: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 19:42:19.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 19:42:19.707: INFO: update-demo-nautilus-l7mt6 is verified up and running
    Aug 29 19:42:19.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-mls5p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:19.784: INFO: stderr: ""
    Aug 29 19:42:19.784: INFO: stdout: ""
    Aug 29 19:42:19.784: INFO: update-demo-nautilus-mls5p is created but not running
    Aug 29 19:42:24.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 19:42:24.859: INFO: stderr: ""
    Aug 29 19:42:24.859: INFO: stdout: "update-demo-nautilus-l7mt6 update-demo-nautilus-mls5p "
    Aug 29 19:42:24.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:24.934: INFO: stderr: ""
    Aug 29 19:42:24.934: INFO: stdout: "true"
    Aug 29 19:42:24.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 19:42:25.019: INFO: stderr: ""
    Aug 29 19:42:25.019: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 19:42:25.019: INFO: validating pod update-demo-nautilus-l7mt6
    Aug 29 19:42:25.023: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 19:42:25.024: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 19:42:25.024: INFO: update-demo-nautilus-l7mt6 is verified up and running
    Aug 29 19:42:25.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-mls5p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:25.113: INFO: stderr: ""
    Aug 29 19:42:25.113: INFO: stdout: "true"
    Aug 29 19:42:25.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-mls5p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 19:42:25.189: INFO: stderr: ""
    Aug 29 19:42:25.190: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 19:42:25.190: INFO: validating pod update-demo-nautilus-mls5p
    Aug 29 19:42:25.195: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 19:42:25.195: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 19:42:25.195: INFO: update-demo-nautilus-mls5p is verified up and running
    STEP: scaling down the replication controller 08/29/23 19:42:25.195
    Aug 29 19:42:25.197: INFO: scanned /root for discovery docs: <nil>
    Aug 29 19:42:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 29 19:42:26.304: INFO: stderr: ""
    Aug 29 19:42:26.304: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/29/23 19:42:26.304
    Aug 29 19:42:26.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 19:42:26.390: INFO: stderr: ""
    Aug 29 19:42:26.390: INFO: stdout: "update-demo-nautilus-l7mt6 update-demo-nautilus-mls5p "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/29/23 19:42:26.39
    Aug 29 19:42:31.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 19:42:31.482: INFO: stderr: ""
    Aug 29 19:42:31.482: INFO: stdout: "update-demo-nautilus-l7mt6 "
    Aug 29 19:42:31.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:31.568: INFO: stderr: ""
    Aug 29 19:42:31.568: INFO: stdout: "true"
    Aug 29 19:42:31.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 19:42:31.649: INFO: stderr: ""
    Aug 29 19:42:31.649: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 19:42:31.649: INFO: validating pod update-demo-nautilus-l7mt6
    Aug 29 19:42:31.655: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 19:42:31.655: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 19:42:31.655: INFO: update-demo-nautilus-l7mt6 is verified up and running
    STEP: scaling up the replication controller 08/29/23 19:42:31.655
    Aug 29 19:42:31.657: INFO: scanned /root for discovery docs: <nil>
    Aug 29 19:42:31.657: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 29 19:42:32.777: INFO: stderr: ""
    Aug 29 19:42:32.778: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/29/23 19:42:32.778
    Aug 29 19:42:32.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 19:42:32.873: INFO: stderr: ""
    Aug 29 19:42:32.873: INFO: stdout: "update-demo-nautilus-8wkn7 update-demo-nautilus-l7mt6 "
    Aug 29 19:42:32.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-8wkn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:32.957: INFO: stderr: ""
    Aug 29 19:42:32.957: INFO: stdout: ""
    Aug 29 19:42:32.957: INFO: update-demo-nautilus-8wkn7 is created but not running
    Aug 29 19:42:37.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 19:42:38.061: INFO: stderr: ""
    Aug 29 19:42:38.061: INFO: stdout: "update-demo-nautilus-8wkn7 update-demo-nautilus-l7mt6 "
    Aug 29 19:42:38.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-8wkn7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:38.150: INFO: stderr: ""
    Aug 29 19:42:38.150: INFO: stdout: "true"
    Aug 29 19:42:38.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-8wkn7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 19:42:38.233: INFO: stderr: ""
    Aug 29 19:42:38.233: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 19:42:38.233: INFO: validating pod update-demo-nautilus-8wkn7
    Aug 29 19:42:38.238: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 19:42:38.238: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 19:42:38.238: INFO: update-demo-nautilus-8wkn7 is verified up and running
    Aug 29 19:42:38.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 19:42:38.320: INFO: stderr: ""
    Aug 29 19:42:38.320: INFO: stdout: "true"
    Aug 29 19:42:38.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods update-demo-nautilus-l7mt6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 19:42:38.413: INFO: stderr: ""
    Aug 29 19:42:38.413: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 19:42:38.414: INFO: validating pod update-demo-nautilus-l7mt6
    Aug 29 19:42:38.418: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 19:42:38.418: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 19:42:38.418: INFO: update-demo-nautilus-l7mt6 is verified up and running
    STEP: using delete to clean up resources 08/29/23 19:42:38.418
    Aug 29 19:42:38.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 delete --grace-period=0 --force -f -'
    Aug 29 19:42:38.514: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 19:42:38.514: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 29 19:42:38.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get rc,svc -l name=update-demo --no-headers'
    Aug 29 19:42:38.617: INFO: stderr: "No resources found in kubectl-3308 namespace.\n"
    Aug 29 19:42:38.617: INFO: stdout: ""
    Aug 29 19:42:38.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-3308 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 29 19:42:38.702: INFO: stderr: ""
    Aug 29 19:42:38.702: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:42:38.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3308" for this suite. 08/29/23 19:42:38.708
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:42:38.722
Aug 29 19:42:38.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sysctl 08/29/23 19:42:38.723
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:38.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:38.748
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/29/23 19:42:38.75
STEP: Watching for error events or started pod 08/29/23 19:42:38.768
STEP: Waiting for pod completion 08/29/23 19:42:40.774
Aug 29 19:42:40.774: INFO: Waiting up to 3m0s for pod "sysctl-34edd374-c580-4593-9118-203037047f9b" in namespace "sysctl-982" to be "completed"
Aug 29 19:42:40.778: INFO: Pod "sysctl-34edd374-c580-4593-9118-203037047f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.823318ms
Aug 29 19:42:42.784: INFO: Pod "sysctl-34edd374-c580-4593-9118-203037047f9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009318015s
Aug 29 19:42:42.784: INFO: Pod "sysctl-34edd374-c580-4593-9118-203037047f9b" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/29/23 19:42:42.787
STEP: Getting logs from the pod 08/29/23 19:42:42.787
STEP: Checking that the sysctl is actually updated 08/29/23 19:42:42.813
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:42:42.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-982" for this suite. 08/29/23 19:42:42.818
------------------------------
• [4.104 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:42:38.722
    Aug 29 19:42:38.722: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sysctl 08/29/23 19:42:38.723
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:38.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:38.748
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/29/23 19:42:38.75
    STEP: Watching for error events or started pod 08/29/23 19:42:38.768
    STEP: Waiting for pod completion 08/29/23 19:42:40.774
    Aug 29 19:42:40.774: INFO: Waiting up to 3m0s for pod "sysctl-34edd374-c580-4593-9118-203037047f9b" in namespace "sysctl-982" to be "completed"
    Aug 29 19:42:40.778: INFO: Pod "sysctl-34edd374-c580-4593-9118-203037047f9b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.823318ms
    Aug 29 19:42:42.784: INFO: Pod "sysctl-34edd374-c580-4593-9118-203037047f9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009318015s
    Aug 29 19:42:42.784: INFO: Pod "sysctl-34edd374-c580-4593-9118-203037047f9b" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/29/23 19:42:42.787
    STEP: Getting logs from the pod 08/29/23 19:42:42.787
    STEP: Checking that the sysctl is actually updated 08/29/23 19:42:42.813
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:42:42.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-982" for this suite. 08/29/23 19:42:42.818
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:42:42.827
Aug 29 19:42:42.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 19:42:42.828
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:42.85
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:42.854
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 08/29/23 19:42:42.857
Aug 29 19:42:42.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-9771 create -f -'
Aug 29 19:42:43.689: INFO: stderr: ""
Aug 29 19:42:43.689: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/29/23 19:42:43.689
Aug 29 19:42:44.695: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 19:42:44.695: INFO: Found 0 / 1
Aug 29 19:42:45.695: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 19:42:45.695: INFO: Found 1 / 1
Aug 29 19:42:45.695: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/29/23 19:42:45.695
Aug 29 19:42:45.699: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 19:42:45.699: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 19:42:45.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-9771 patch pod agnhost-primary-95lm9 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 29 19:42:45.787: INFO: stderr: ""
Aug 29 19:42:45.787: INFO: stdout: "pod/agnhost-primary-95lm9 patched\n"
STEP: checking annotations 08/29/23 19:42:45.787
Aug 29 19:42:45.791: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 19:42:45.791: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 19:42:45.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9771" for this suite. 08/29/23 19:42:45.795
------------------------------
• [2.976 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:42:42.827
    Aug 29 19:42:42.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 19:42:42.828
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:42.85
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:42.854
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 08/29/23 19:42:42.857
    Aug 29 19:42:42.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-9771 create -f -'
    Aug 29 19:42:43.689: INFO: stderr: ""
    Aug 29 19:42:43.689: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/29/23 19:42:43.689
    Aug 29 19:42:44.695: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 19:42:44.695: INFO: Found 0 / 1
    Aug 29 19:42:45.695: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 19:42:45.695: INFO: Found 1 / 1
    Aug 29 19:42:45.695: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/29/23 19:42:45.695
    Aug 29 19:42:45.699: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 19:42:45.699: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 29 19:42:45.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-9771 patch pod agnhost-primary-95lm9 -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 29 19:42:45.787: INFO: stderr: ""
    Aug 29 19:42:45.787: INFO: stdout: "pod/agnhost-primary-95lm9 patched\n"
    STEP: checking annotations 08/29/23 19:42:45.787
    Aug 29 19:42:45.791: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 19:42:45.791: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:42:45.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9771" for this suite. 08/29/23 19:42:45.795
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:42:45.803
Aug 29 19:42:45.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 19:42:45.804
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:45.823
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:45.827
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 08/29/23 19:42:45.83
STEP: Creating a ResourceQuota 08/29/23 19:42:50.835
STEP: Ensuring resource quota status is calculated 08/29/23 19:42:50.841
STEP: Creating a ReplicaSet 08/29/23 19:42:52.847
STEP: Ensuring resource quota status captures replicaset creation 08/29/23 19:42:52.864
STEP: Deleting a ReplicaSet 08/29/23 19:42:54.869
STEP: Ensuring resource quota status released usage 08/29/23 19:42:54.876
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 19:42:56.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-1357" for this suite. 08/29/23 19:42:56.887
------------------------------
• [SLOW TEST] [11.092 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:42:45.803
    Aug 29 19:42:45.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 19:42:45.804
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:45.823
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:45.827
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 08/29/23 19:42:45.83
    STEP: Creating a ResourceQuota 08/29/23 19:42:50.835
    STEP: Ensuring resource quota status is calculated 08/29/23 19:42:50.841
    STEP: Creating a ReplicaSet 08/29/23 19:42:52.847
    STEP: Ensuring resource quota status captures replicaset creation 08/29/23 19:42:52.864
    STEP: Deleting a ReplicaSet 08/29/23 19:42:54.869
    STEP: Ensuring resource quota status released usage 08/29/23 19:42:54.876
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:42:56.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-1357" for this suite. 08/29/23 19:42:56.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:42:56.896
Aug 29 19:42:56.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 19:42:56.897
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:56.918
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:56.922
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-1805bd31-f016-4070-bcd7-a6e2d2df49c3 08/29/23 19:42:56.956
STEP: Creating a pod to test consume secrets 08/29/23 19:42:56.961
Aug 29 19:42:56.972: INFO: Waiting up to 5m0s for pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14" in namespace "secrets-604" to be "Succeeded or Failed"
Aug 29 19:42:56.975: INFO: Pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.958587ms
Aug 29 19:42:58.981: INFO: Pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009108083s
Aug 29 19:43:00.979: INFO: Pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007402258s
STEP: Saw pod success 08/29/23 19:43:00.979
Aug 29 19:43:00.980: INFO: Pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14" satisfied condition "Succeeded or Failed"
Aug 29 19:43:00.983: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14 container secret-volume-test: <nil>
STEP: delete the pod 08/29/23 19:43:00.991
Aug 29 19:43:01.007: INFO: Waiting for pod pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14 to disappear
Aug 29 19:43:01.009: INFO: Pod pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 19:43:01.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-604" for this suite. 08/29/23 19:43:01.014
STEP: Destroying namespace "secret-namespace-2746" for this suite. 08/29/23 19:43:01.023
------------------------------
• [4.134 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:42:56.896
    Aug 29 19:42:56.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 19:42:56.897
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:42:56.918
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:42:56.922
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-1805bd31-f016-4070-bcd7-a6e2d2df49c3 08/29/23 19:42:56.956
    STEP: Creating a pod to test consume secrets 08/29/23 19:42:56.961
    Aug 29 19:42:56.972: INFO: Waiting up to 5m0s for pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14" in namespace "secrets-604" to be "Succeeded or Failed"
    Aug 29 19:42:56.975: INFO: Pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.958587ms
    Aug 29 19:42:58.981: INFO: Pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009108083s
    Aug 29 19:43:00.979: INFO: Pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007402258s
    STEP: Saw pod success 08/29/23 19:43:00.979
    Aug 29 19:43:00.980: INFO: Pod "pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14" satisfied condition "Succeeded or Failed"
    Aug 29 19:43:00.983: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14 container secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 19:43:00.991
    Aug 29 19:43:01.007: INFO: Waiting for pod pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14 to disappear
    Aug 29 19:43:01.009: INFO: Pod pod-secrets-28bd1081-83af-4500-85e5-34348a3cea14 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:43:01.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-604" for this suite. 08/29/23 19:43:01.014
    STEP: Destroying namespace "secret-namespace-2746" for this suite. 08/29/23 19:43:01.023
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:43:01.031
Aug 29 19:43:01.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 19:43:01.032
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:43:01.052
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:43:01.055
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 19:43:01.072
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:43:01.255
STEP: Deploying the webhook pod 08/29/23 19:43:01.267
STEP: Wait for the deployment to be ready 08/29/23 19:43:01.283
Aug 29 19:43:01.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 19:43:03.303
STEP: Verifying the service has paired with the endpoint 08/29/23 19:43:03.322
Aug 29 19:43:04.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 08/29/23 19:43:04.327
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/29/23 19:43:04.357
STEP: Creating a configMap that should not be mutated 08/29/23 19:43:04.365
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/29/23 19:43:04.385
STEP: Creating a configMap that should be mutated 08/29/23 19:43:04.399
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:43:04.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5482" for this suite. 08/29/23 19:43:04.482
STEP: Destroying namespace "webhook-5482-markers" for this suite. 08/29/23 19:43:04.488
------------------------------
• [3.468 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:43:01.031
    Aug 29 19:43:01.031: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 19:43:01.032
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:43:01.052
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:43:01.055
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 19:43:01.072
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:43:01.255
    STEP: Deploying the webhook pod 08/29/23 19:43:01.267
    STEP: Wait for the deployment to be ready 08/29/23 19:43:01.283
    Aug 29 19:43:01.292: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 19:43:03.303
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:43:03.322
    Aug 29 19:43:04.322: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 08/29/23 19:43:04.327
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/29/23 19:43:04.357
    STEP: Creating a configMap that should not be mutated 08/29/23 19:43:04.365
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/29/23 19:43:04.385
    STEP: Creating a configMap that should be mutated 08/29/23 19:43:04.399
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:43:04.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5482" for this suite. 08/29/23 19:43:04.482
    STEP: Destroying namespace "webhook-5482-markers" for this suite. 08/29/23 19:43:04.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:43:04.5
Aug 29 19:43:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename var-expansion 08/29/23 19:43:04.501
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:43:04.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:43:04.527
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 08/29/23 19:43:04.53
Aug 29 19:43:04.542: INFO: Waiting up to 2m0s for pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" in namespace "var-expansion-4194" to be "running"
Aug 29 19:43:04.546: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 3.31801ms
Aug 29 19:43:06.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008300024s
Aug 29 19:43:08.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008778627s
Aug 29 19:43:10.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009421798s
Aug 29 19:43:12.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008720094s
Aug 29 19:43:14.550: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008047231s
Aug 29 19:43:16.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009726194s
Aug 29 19:43:18.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008445452s
Aug 29 19:43:20.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008439652s
Aug 29 19:43:22.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009400362s
Aug 29 19:43:24.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008932583s
Aug 29 19:43:26.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009381066s
Aug 29 19:43:28.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008334566s
Aug 29 19:43:30.550: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007526526s
Aug 29 19:43:32.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010918753s
Aug 29 19:43:34.550: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007572493s
Aug 29 19:43:36.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008900654s
Aug 29 19:43:38.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008492215s
Aug 29 19:43:40.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009228913s
Aug 29 19:43:42.556: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01334794s
Aug 29 19:43:44.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00937703s
Aug 29 19:43:46.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009228198s
Aug 29 19:43:48.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010132174s
Aug 29 19:43:50.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009565724s
Aug 29 19:43:52.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00907923s
Aug 29 19:43:54.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008987484s
Aug 29 19:43:56.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008350535s
Aug 29 19:43:58.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009040549s
Aug 29 19:44:00.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008884204s
Aug 29 19:44:02.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010065642s
Aug 29 19:44:04.554: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011261637s
Aug 29 19:44:06.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.01050668s
Aug 29 19:44:08.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008778916s
Aug 29 19:44:10.555: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013018566s
Aug 29 19:44:12.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009241909s
Aug 29 19:44:14.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008437316s
Aug 29 19:44:16.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010785265s
Aug 29 19:44:18.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008835699s
Aug 29 19:44:20.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010057681s
Aug 29 19:44:22.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009334715s
Aug 29 19:44:24.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009139487s
Aug 29 19:44:26.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00873477s
Aug 29 19:44:28.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008931188s
Aug 29 19:44:30.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009731838s
Aug 29 19:44:32.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009639093s
Aug 29 19:44:34.550: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007412908s
Aug 29 19:44:36.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008437811s
Aug 29 19:44:38.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008858322s
Aug 29 19:44:40.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009177824s
Aug 29 19:44:42.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009295648s
Aug 29 19:44:44.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008287321s
Aug 29 19:44:46.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010908794s
Aug 29 19:44:48.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00872542s
Aug 29 19:44:50.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010501754s
Aug 29 19:44:52.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010335175s
Aug 29 19:44:54.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009402177s
Aug 29 19:44:56.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009351157s
Aug 29 19:44:58.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009629986s
Aug 29 19:45:00.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009249854s
Aug 29 19:45:02.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010015188s
Aug 29 19:45:04.555: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01295614s
Aug 29 19:45:04.559: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016906087s
STEP: updating the pod 08/29/23 19:45:04.559
Aug 29 19:45:05.076: INFO: Successfully updated pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094"
STEP: waiting for pod running 08/29/23 19:45:05.076
Aug 29 19:45:05.076: INFO: Waiting up to 2m0s for pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" in namespace "var-expansion-4194" to be "running"
Aug 29 19:45:05.080: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386544ms
Aug 29 19:45:07.085: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Running", Reason="", readiness=true. Elapsed: 2.008731726s
Aug 29 19:45:07.085: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" satisfied condition "running"
STEP: deleting the pod gracefully 08/29/23 19:45:07.085
Aug 29 19:45:07.085: INFO: Deleting pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" in namespace "var-expansion-4194"
Aug 29 19:45:07.095: INFO: Wait up to 5m0s for pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 29 19:45:39.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4194" for this suite. 08/29/23 19:45:39.107
------------------------------
• [SLOW TEST] [154.615 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:43:04.5
    Aug 29 19:43:04.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename var-expansion 08/29/23 19:43:04.501
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:43:04.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:43:04.527
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 08/29/23 19:43:04.53
    Aug 29 19:43:04.542: INFO: Waiting up to 2m0s for pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" in namespace "var-expansion-4194" to be "running"
    Aug 29 19:43:04.546: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 3.31801ms
    Aug 29 19:43:06.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008300024s
    Aug 29 19:43:08.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008778627s
    Aug 29 19:43:10.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009421798s
    Aug 29 19:43:12.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008720094s
    Aug 29 19:43:14.550: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 10.008047231s
    Aug 29 19:43:16.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009726194s
    Aug 29 19:43:18.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008445452s
    Aug 29 19:43:20.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008439652s
    Aug 29 19:43:22.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009400362s
    Aug 29 19:43:24.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008932583s
    Aug 29 19:43:26.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009381066s
    Aug 29 19:43:28.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 24.008334566s
    Aug 29 19:43:30.550: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 26.007526526s
    Aug 29 19:43:32.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010918753s
    Aug 29 19:43:34.550: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007572493s
    Aug 29 19:43:36.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008900654s
    Aug 29 19:43:38.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008492215s
    Aug 29 19:43:40.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 36.009228913s
    Aug 29 19:43:42.556: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 38.01334794s
    Aug 29 19:43:44.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 40.00937703s
    Aug 29 19:43:46.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009228198s
    Aug 29 19:43:48.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010132174s
    Aug 29 19:43:50.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009565724s
    Aug 29 19:43:52.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 48.00907923s
    Aug 29 19:43:54.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008987484s
    Aug 29 19:43:56.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008350535s
    Aug 29 19:43:58.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009040549s
    Aug 29 19:44:00.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008884204s
    Aug 29 19:44:02.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010065642s
    Aug 29 19:44:04.554: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.011261637s
    Aug 29 19:44:06.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.01050668s
    Aug 29 19:44:08.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008778916s
    Aug 29 19:44:10.555: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.013018566s
    Aug 29 19:44:12.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.009241909s
    Aug 29 19:44:14.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.008437316s
    Aug 29 19:44:16.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010785265s
    Aug 29 19:44:18.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008835699s
    Aug 29 19:44:20.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.010057681s
    Aug 29 19:44:22.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009334715s
    Aug 29 19:44:24.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009139487s
    Aug 29 19:44:26.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.00873477s
    Aug 29 19:44:28.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008931188s
    Aug 29 19:44:30.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.009731838s
    Aug 29 19:44:32.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009639093s
    Aug 29 19:44:34.550: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007412908s
    Aug 29 19:44:36.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008437811s
    Aug 29 19:44:38.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008858322s
    Aug 29 19:44:40.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009177824s
    Aug 29 19:44:42.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009295648s
    Aug 29 19:44:44.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008287321s
    Aug 29 19:44:46.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010908794s
    Aug 29 19:44:48.551: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.00872542s
    Aug 29 19:44:50.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010501754s
    Aug 29 19:44:52.553: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.010335175s
    Aug 29 19:44:54.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009402177s
    Aug 29 19:44:56.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009351157s
    Aug 29 19:44:58.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009629986s
    Aug 29 19:45:00.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.009249854s
    Aug 29 19:45:02.552: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010015188s
    Aug 29 19:45:04.555: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01295614s
    Aug 29 19:45:04.559: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.016906087s
    STEP: updating the pod 08/29/23 19:45:04.559
    Aug 29 19:45:05.076: INFO: Successfully updated pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094"
    STEP: waiting for pod running 08/29/23 19:45:05.076
    Aug 29 19:45:05.076: INFO: Waiting up to 2m0s for pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" in namespace "var-expansion-4194" to be "running"
    Aug 29 19:45:05.080: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Pending", Reason="", readiness=false. Elapsed: 3.386544ms
    Aug 29 19:45:07.085: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094": Phase="Running", Reason="", readiness=true. Elapsed: 2.008731726s
    Aug 29 19:45:07.085: INFO: Pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" satisfied condition "running"
    STEP: deleting the pod gracefully 08/29/23 19:45:07.085
    Aug 29 19:45:07.085: INFO: Deleting pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" in namespace "var-expansion-4194"
    Aug 29 19:45:07.095: INFO: Wait up to 5m0s for pod "var-expansion-de37a3e3-7974-4865-a2f1-1c4e0012a094" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:45:39.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4194" for this suite. 08/29/23 19:45:39.107
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:45:39.116
Aug 29 19:45:39.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 19:45:39.117
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:45:39.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:45:39.143
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-d84bc57d-4880-4889-984d-16c486408e38 08/29/23 19:45:39.146
STEP: Creating a pod to test consume configMaps 08/29/23 19:45:39.153
Aug 29 19:45:39.163: INFO: Waiting up to 5m0s for pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd" in namespace "configmap-5535" to be "Succeeded or Failed"
Aug 29 19:45:39.168: INFO: Pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.510349ms
Aug 29 19:45:41.173: INFO: Pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009452029s
Aug 29 19:45:43.176: INFO: Pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012940168s
STEP: Saw pod success 08/29/23 19:45:43.176
Aug 29 19:45:43.176: INFO: Pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd" satisfied condition "Succeeded or Failed"
Aug 29 19:45:43.180: INFO: Trying to get logs from node loki-15bd39-worker-2 pod pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd container agnhost-container: <nil>
STEP: delete the pod 08/29/23 19:45:43.202
Aug 29 19:45:43.214: INFO: Waiting for pod pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd to disappear
Aug 29 19:45:43.217: INFO: Pod pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 19:45:43.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5535" for this suite. 08/29/23 19:45:43.222
------------------------------
• [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:45:39.116
    Aug 29 19:45:39.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 19:45:39.117
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:45:39.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:45:39.143
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-d84bc57d-4880-4889-984d-16c486408e38 08/29/23 19:45:39.146
    STEP: Creating a pod to test consume configMaps 08/29/23 19:45:39.153
    Aug 29 19:45:39.163: INFO: Waiting up to 5m0s for pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd" in namespace "configmap-5535" to be "Succeeded or Failed"
    Aug 29 19:45:39.168: INFO: Pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.510349ms
    Aug 29 19:45:41.173: INFO: Pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009452029s
    Aug 29 19:45:43.176: INFO: Pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012940168s
    STEP: Saw pod success 08/29/23 19:45:43.176
    Aug 29 19:45:43.176: INFO: Pod "pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd" satisfied condition "Succeeded or Failed"
    Aug 29 19:45:43.180: INFO: Trying to get logs from node loki-15bd39-worker-2 pod pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 19:45:43.202
    Aug 29 19:45:43.214: INFO: Waiting for pod pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd to disappear
    Aug 29 19:45:43.217: INFO: Pod pod-configmaps-14ee7cee-0919-44f7-958d-a7a4e3f0c6dd no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:45:43.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5535" for this suite. 08/29/23 19:45:43.222
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:45:43.229
Aug 29 19:45:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename taint-single-pod 08/29/23 19:45:43.23
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:45:43.252
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:45:43.256
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Aug 29 19:45:43.258: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 19:46:43.304: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Aug 29 19:46:43.307: INFO: Starting informer...
STEP: Starting pod... 08/29/23 19:46:43.307
Aug 29 19:46:43.522: INFO: Pod is running on loki-15bd39-worker-1. Tainting Node
STEP: Trying to apply a taint on the Node 08/29/23 19:46:43.522
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/29/23 19:46:43.537
STEP: Waiting short time to make sure Pod is queued for deletion 08/29/23 19:46:43.542
Aug 29 19:46:43.542: INFO: Pod wasn't evicted. Proceeding
Aug 29 19:46:43.542: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/29/23 19:46:43.562
STEP: Waiting some time to make sure that toleration time passed. 08/29/23 19:46:43.574
Aug 29 19:47:58.575: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:47:58.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-8521" for this suite. 08/29/23 19:47:58.581
------------------------------
• [SLOW TEST] [135.360 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:45:43.229
    Aug 29 19:45:43.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename taint-single-pod 08/29/23 19:45:43.23
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:45:43.252
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:45:43.256
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Aug 29 19:45:43.258: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 29 19:46:43.304: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Aug 29 19:46:43.307: INFO: Starting informer...
    STEP: Starting pod... 08/29/23 19:46:43.307
    Aug 29 19:46:43.522: INFO: Pod is running on loki-15bd39-worker-1. Tainting Node
    STEP: Trying to apply a taint on the Node 08/29/23 19:46:43.522
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/29/23 19:46:43.537
    STEP: Waiting short time to make sure Pod is queued for deletion 08/29/23 19:46:43.542
    Aug 29 19:46:43.542: INFO: Pod wasn't evicted. Proceeding
    Aug 29 19:46:43.542: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/29/23 19:46:43.562
    STEP: Waiting some time to make sure that toleration time passed. 08/29/23 19:46:43.574
    Aug 29 19:47:58.575: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:47:58.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-8521" for this suite. 08/29/23 19:47:58.581
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:47:58.589
Aug 29 19:47:58.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename limitrange 08/29/23 19:47:58.59
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:47:58.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:47:58.615
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 08/29/23 19:47:58.618
STEP: Setting up watch 08/29/23 19:47:58.618
STEP: Submitting a LimitRange 08/29/23 19:47:58.722
STEP: Verifying LimitRange creation was observed 08/29/23 19:47:58.73
STEP: Fetching the LimitRange to ensure it has proper values 08/29/23 19:47:58.73
Aug 29 19:47:58.733: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 29 19:47:58.733: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/29/23 19:47:58.733
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/29/23 19:47:58.745
Aug 29 19:47:58.751: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 29 19:47:58.751: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/29/23 19:47:58.751
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/29/23 19:47:58.76
Aug 29 19:47:58.763: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 29 19:47:58.763: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/29/23 19:47:58.763
STEP: Failing to create a Pod with more than max resources 08/29/23 19:47:58.765
STEP: Updating a LimitRange 08/29/23 19:47:58.767
STEP: Verifying LimitRange updating is effective 08/29/23 19:47:58.772
STEP: Creating a Pod with less than former min resources 08/29/23 19:48:00.778
STEP: Failing to create a Pod with more than max resources 08/29/23 19:48:00.795
STEP: Deleting a LimitRange 08/29/23 19:48:00.798
STEP: Verifying the LimitRange was deleted 08/29/23 19:48:00.808
Aug 29 19:48:05.815: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/29/23 19:48:05.815
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:05.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-9905" for this suite. 08/29/23 19:48:05.832
------------------------------
• [SLOW TEST] [7.253 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:47:58.589
    Aug 29 19:47:58.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename limitrange 08/29/23 19:47:58.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:47:58.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:47:58.615
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 08/29/23 19:47:58.618
    STEP: Setting up watch 08/29/23 19:47:58.618
    STEP: Submitting a LimitRange 08/29/23 19:47:58.722
    STEP: Verifying LimitRange creation was observed 08/29/23 19:47:58.73
    STEP: Fetching the LimitRange to ensure it has proper values 08/29/23 19:47:58.73
    Aug 29 19:47:58.733: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 29 19:47:58.733: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/29/23 19:47:58.733
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/29/23 19:47:58.745
    Aug 29 19:47:58.751: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 29 19:47:58.751: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/29/23 19:47:58.751
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/29/23 19:47:58.76
    Aug 29 19:47:58.763: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 29 19:47:58.763: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/29/23 19:47:58.763
    STEP: Failing to create a Pod with more than max resources 08/29/23 19:47:58.765
    STEP: Updating a LimitRange 08/29/23 19:47:58.767
    STEP: Verifying LimitRange updating is effective 08/29/23 19:47:58.772
    STEP: Creating a Pod with less than former min resources 08/29/23 19:48:00.778
    STEP: Failing to create a Pod with more than max resources 08/29/23 19:48:00.795
    STEP: Deleting a LimitRange 08/29/23 19:48:00.798
    STEP: Verifying the LimitRange was deleted 08/29/23 19:48:00.808
    Aug 29 19:48:05.815: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/29/23 19:48:05.815
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:05.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-9905" for this suite. 08/29/23 19:48:05.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:05.842
Aug 29 19:48:05.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pod-network-test 08/29/23 19:48:05.844
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:05.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:05.87
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-5160 08/29/23 19:48:05.874
STEP: creating a selector 08/29/23 19:48:05.874
STEP: Creating the service pods in kubernetes 08/29/23 19:48:05.874
Aug 29 19:48:05.874: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 19:48:05.928: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5160" to be "running and ready"
Aug 29 19:48:05.941: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.144519ms
Aug 29 19:48:05.941: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:48:07.945: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01680751s
Aug 29 19:48:07.945: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:48:09.945: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016725548s
Aug 29 19:48:09.945: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:48:11.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017152448s
Aug 29 19:48:11.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:48:13.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017088948s
Aug 29 19:48:13.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:48:15.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018001808s
Aug 29 19:48:15.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:48:17.945: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.016435122s
Aug 29 19:48:17.945: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 29 19:48:17.945: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 29 19:48:17.949: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5160" to be "running and ready"
Aug 29 19:48:17.953: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.532358ms
Aug 29 19:48:17.953: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 29 19:48:17.953: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 29 19:48:17.956: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5160" to be "running and ready"
Aug 29 19:48:17.959: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.429882ms
Aug 29 19:48:17.959: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 29 19:48:17.959: INFO: Pod "netserver-2" satisfied condition "running and ready"
Aug 29 19:48:17.962: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5160" to be "running and ready"
Aug 29 19:48:17.965: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.681327ms
Aug 29 19:48:17.965: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Aug 29 19:48:17.965: INFO: Pod "netserver-3" satisfied condition "running and ready"
Aug 29 19:48:17.968: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-5160" to be "running and ready"
Aug 29 19:48:17.970: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.573768ms
Aug 29 19:48:17.970: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Aug 29 19:48:17.970: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 08/29/23 19:48:17.973
Aug 29 19:48:17.990: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5160" to be "running"
Aug 29 19:48:17.995: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651138ms
Aug 29 19:48:20.001: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010583179s
Aug 29 19:48:20.001: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 29 19:48:20.004: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5160" to be "running"
Aug 29 19:48:20.008: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.874584ms
Aug 29 19:48:20.008: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 29 19:48:20.011: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Aug 29 19:48:20.011: INFO: Going to poll 172.20.17.70 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 29 19:48:20.014: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.17.70 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:48:20.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:48:20.014: INFO: ExecWithOptions: Clientset creation
Aug 29 19:48:20.014: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.17.70+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 19:48:21.097: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 29 19:48:21.097: INFO: Going to poll 172.20.76.136 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 29 19:48:21.101: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.76.136 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:48:21.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:48:21.102: INFO: ExecWithOptions: Clientset creation
Aug 29 19:48:21.102: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.76.136+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 19:48:22.174: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 29 19:48:22.174: INFO: Going to poll 172.20.143.209 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 29 19:48:22.178: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.143.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:48:22.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:48:22.179: INFO: ExecWithOptions: Clientset creation
Aug 29 19:48:22.179: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.143.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 19:48:23.268: INFO: Found all 1 expected endpoints: [netserver-2]
Aug 29 19:48:23.268: INFO: Going to poll 172.20.30.175 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 29 19:48:23.272: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.30.175 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:48:23.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:48:23.273: INFO: ExecWithOptions: Clientset creation
Aug 29 19:48:23.273: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.30.175+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 19:48:24.349: INFO: Found all 1 expected endpoints: [netserver-3]
Aug 29 19:48:24.349: INFO: Going to poll 172.20.84.157 on port 8081 at least 0 times, with a maximum of 55 tries before failing
Aug 29 19:48:24.354: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.84.157 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:48:24.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:48:24.355: INFO: ExecWithOptions: Clientset creation
Aug 29 19:48:24.355: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.84.157+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 19:48:25.432: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:25.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5160" for this suite. 08/29/23 19:48:25.438
------------------------------
• [SLOW TEST] [19.604 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:05.842
    Aug 29 19:48:05.842: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pod-network-test 08/29/23 19:48:05.844
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:05.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:05.87
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-5160 08/29/23 19:48:05.874
    STEP: creating a selector 08/29/23 19:48:05.874
    STEP: Creating the service pods in kubernetes 08/29/23 19:48:05.874
    Aug 29 19:48:05.874: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 29 19:48:05.928: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5160" to be "running and ready"
    Aug 29 19:48:05.941: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.144519ms
    Aug 29 19:48:05.941: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:48:07.945: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.01680751s
    Aug 29 19:48:07.945: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:48:09.945: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.016725548s
    Aug 29 19:48:09.945: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:48:11.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017152448s
    Aug 29 19:48:11.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:48:13.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.017088948s
    Aug 29 19:48:13.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:48:15.946: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.018001808s
    Aug 29 19:48:15.946: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:48:17.945: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.016435122s
    Aug 29 19:48:17.945: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 29 19:48:17.945: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 29 19:48:17.949: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5160" to be "running and ready"
    Aug 29 19:48:17.953: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.532358ms
    Aug 29 19:48:17.953: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 29 19:48:17.953: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 29 19:48:17.956: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5160" to be "running and ready"
    Aug 29 19:48:17.959: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.429882ms
    Aug 29 19:48:17.959: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 29 19:48:17.959: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Aug 29 19:48:17.962: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-5160" to be "running and ready"
    Aug 29 19:48:17.965: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 2.681327ms
    Aug 29 19:48:17.965: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Aug 29 19:48:17.965: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Aug 29 19:48:17.968: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-5160" to be "running and ready"
    Aug 29 19:48:17.970: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 2.573768ms
    Aug 29 19:48:17.970: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Aug 29 19:48:17.970: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 08/29/23 19:48:17.973
    Aug 29 19:48:17.990: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5160" to be "running"
    Aug 29 19:48:17.995: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651138ms
    Aug 29 19:48:20.001: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010583179s
    Aug 29 19:48:20.001: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 29 19:48:20.004: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5160" to be "running"
    Aug 29 19:48:20.008: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.874584ms
    Aug 29 19:48:20.008: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 29 19:48:20.011: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Aug 29 19:48:20.011: INFO: Going to poll 172.20.17.70 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 19:48:20.014: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.17.70 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:48:20.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:48:20.014: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:48:20.014: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.17.70+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 19:48:21.097: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 29 19:48:21.097: INFO: Going to poll 172.20.76.136 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 19:48:21.101: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.76.136 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:48:21.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:48:21.102: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:48:21.102: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.76.136+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 19:48:22.174: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 29 19:48:22.174: INFO: Going to poll 172.20.143.209 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 19:48:22.178: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.143.209 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:48:22.178: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:48:22.179: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:48:22.179: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.143.209+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 19:48:23.268: INFO: Found all 1 expected endpoints: [netserver-2]
    Aug 29 19:48:23.268: INFO: Going to poll 172.20.30.175 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 19:48:23.272: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.30.175 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:48:23.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:48:23.273: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:48:23.273: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.30.175+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 19:48:24.349: INFO: Found all 1 expected endpoints: [netserver-3]
    Aug 29 19:48:24.349: INFO: Going to poll 172.20.84.157 on port 8081 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 19:48:24.354: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.20.84.157 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5160 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:48:24.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:48:24.355: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:48:24.355: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-5160/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.20.84.157+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 19:48:25.432: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:25.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5160" for this suite. 08/29/23 19:48:25.438
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:25.447
Aug 29 19:48:25.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename runtimeclass 08/29/23 19:48:25.448
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:25.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:25.474
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/29/23 19:48:25.476
STEP: getting /apis/node.k8s.io 08/29/23 19:48:25.479
STEP: getting /apis/node.k8s.io/v1 08/29/23 19:48:25.48
STEP: creating 08/29/23 19:48:25.482
STEP: watching 08/29/23 19:48:25.504
Aug 29 19:48:25.504: INFO: starting watch
STEP: getting 08/29/23 19:48:25.511
STEP: listing 08/29/23 19:48:25.514
STEP: patching 08/29/23 19:48:25.518
STEP: updating 08/29/23 19:48:25.524
Aug 29 19:48:25.528: INFO: waiting for watch events with expected annotations
STEP: deleting 08/29/23 19:48:25.528
STEP: deleting a collection 08/29/23 19:48:25.541
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:25.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5658" for this suite. 08/29/23 19:48:25.565
------------------------------
• [0.126 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:25.447
    Aug 29 19:48:25.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename runtimeclass 08/29/23 19:48:25.448
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:25.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:25.474
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/29/23 19:48:25.476
    STEP: getting /apis/node.k8s.io 08/29/23 19:48:25.479
    STEP: getting /apis/node.k8s.io/v1 08/29/23 19:48:25.48
    STEP: creating 08/29/23 19:48:25.482
    STEP: watching 08/29/23 19:48:25.504
    Aug 29 19:48:25.504: INFO: starting watch
    STEP: getting 08/29/23 19:48:25.511
    STEP: listing 08/29/23 19:48:25.514
    STEP: patching 08/29/23 19:48:25.518
    STEP: updating 08/29/23 19:48:25.524
    Aug 29 19:48:25.528: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/29/23 19:48:25.528
    STEP: deleting a collection 08/29/23 19:48:25.541
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:25.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5658" for this suite. 08/29/23 19:48:25.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:25.575
Aug 29 19:48:25.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename deployment 08/29/23 19:48:25.576
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:25.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:25.6
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 29 19:48:25.603: INFO: Creating deployment "test-recreate-deployment"
Aug 29 19:48:25.610: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 29 19:48:25.617: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 29 19:48:27.625: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 29 19:48:27.629: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 29 19:48:27.641: INFO: Updating deployment test-recreate-deployment
Aug 29 19:48:27.641: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 19:48:27.761: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-459  b83d6b1a-0250-465a-acbf-982a7a1add72 11001 2 2023-08-29 19:48:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040f8588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-29 19:48:27 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-29 19:48:27 +0000 UTC,LastTransitionTime:2023-08-29 19:48:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 29 19:48:27.765: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-459  7ef0c191-ddbc-4ebb-9de5-0d3cbf729a8d 10999 1 2023-08-29 19:48:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b83d6b1a-0250-465a-acbf-982a7a1add72 0xc004342300 0xc004342301}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b83d6b1a-0250-465a-acbf-982a7a1add72\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004342398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 19:48:27.765: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 29 19:48:27.765: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-459  18d1361d-3810-4862-9924-34115859a1ce 10989 2 2023-08-29 19:48:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b83d6b1a-0250-465a-acbf-982a7a1add72 0xc0043421e7 0xc0043421e8}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b83d6b1a-0250-465a-acbf-982a7a1add72\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004342298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 19:48:27.769: INFO: Pod "test-recreate-deployment-cff6dc657-8xkcd" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-8xkcd test-recreate-deployment-cff6dc657- deployment-459  b61d71df-950d-494b-8a10-af2f9477138a 11000 0 2023-08-29 19:48:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7ef0c191-ddbc-4ebb-9de5-0d3cbf729a8d 0xc0040f8900 0xc0040f8901}] [] [{kube-controller-manager Update v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ef0c191-ddbc-4ebb-9de5-0d3cbf729a8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brdz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brdz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:,StartTime:2023-08-29 19:48:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:27.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-459" for this suite. 08/29/23 19:48:27.775
------------------------------
• [2.207 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:25.575
    Aug 29 19:48:25.575: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename deployment 08/29/23 19:48:25.576
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:25.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:25.6
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 29 19:48:25.603: INFO: Creating deployment "test-recreate-deployment"
    Aug 29 19:48:25.610: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 29 19:48:25.617: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 29 19:48:27.625: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 29 19:48:27.629: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 29 19:48:27.641: INFO: Updating deployment test-recreate-deployment
    Aug 29 19:48:27.641: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 29 19:48:27.761: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-459  b83d6b1a-0250-465a-acbf-982a7a1add72 11001 2 2023-08-29 19:48:25 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040f8588 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-29 19:48:27 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-08-29 19:48:27 +0000 UTC,LastTransitionTime:2023-08-29 19:48:25 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 29 19:48:27.765: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-459  7ef0c191-ddbc-4ebb-9de5-0d3cbf729a8d 10999 1 2023-08-29 19:48:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b83d6b1a-0250-465a-acbf-982a7a1add72 0xc004342300 0xc004342301}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b83d6b1a-0250-465a-acbf-982a7a1add72\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004342398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 19:48:27.765: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 29 19:48:27.765: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-459  18d1361d-3810-4862-9924-34115859a1ce 10989 2 2023-08-29 19:48:25 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b83d6b1a-0250-465a-acbf-982a7a1add72 0xc0043421e7 0xc0043421e8}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b83d6b1a-0250-465a-acbf-982a7a1add72\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004342298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 19:48:27.769: INFO: Pod "test-recreate-deployment-cff6dc657-8xkcd" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-8xkcd test-recreate-deployment-cff6dc657- deployment-459  b61d71df-950d-494b-8a10-af2f9477138a 11000 0 2023-08-29 19:48:27 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 7ef0c191-ddbc-4ebb-9de5-0d3cbf729a8d 0xc0040f8900 0xc0040f8901}] [] [{kube-controller-manager Update v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ef0c191-ddbc-4ebb-9de5-0d3cbf729a8d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-29 19:48:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-brdz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-brdz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:48:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:,StartTime:2023-08-29 19:48:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:27.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-459" for this suite. 08/29/23 19:48:27.775
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:27.782
Aug 29 19:48:27.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 19:48:27.783
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:27.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:27.807
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-7f9354ea-6396-4ee6-8d1c-8ad207f77ff7 08/29/23 19:48:27.814
STEP: Creating configMap with name cm-test-opt-upd-2ab0ca8c-af29-48a0-abc7-2804e00d1b28 08/29/23 19:48:27.825
STEP: Creating the pod 08/29/23 19:48:27.83
Aug 29 19:48:27.841: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4" in namespace "configmap-7808" to be "running and ready"
Aug 29 19:48:27.844: INFO: Pod "pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.374602ms
Aug 29 19:48:27.844: INFO: The phase of Pod pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:48:29.850: INFO: Pod "pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009057536s
Aug 29 19:48:29.850: INFO: The phase of Pod pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4 is Running (Ready = true)
Aug 29 19:48:29.850: INFO: Pod "pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-7f9354ea-6396-4ee6-8d1c-8ad207f77ff7 08/29/23 19:48:29.888
STEP: Updating configmap cm-test-opt-upd-2ab0ca8c-af29-48a0-abc7-2804e00d1b28 08/29/23 19:48:29.895
STEP: Creating configMap with name cm-test-opt-create-129fe2a5-1034-48ae-8d82-e51dd3d564d7 08/29/23 19:48:29.902
STEP: waiting to observe update in volume 08/29/23 19:48:29.907
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:33.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7808" for this suite. 08/29/23 19:48:33.961
------------------------------
• [SLOW TEST] [6.187 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:27.782
    Aug 29 19:48:27.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 19:48:27.783
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:27.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:27.807
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-7f9354ea-6396-4ee6-8d1c-8ad207f77ff7 08/29/23 19:48:27.814
    STEP: Creating configMap with name cm-test-opt-upd-2ab0ca8c-af29-48a0-abc7-2804e00d1b28 08/29/23 19:48:27.825
    STEP: Creating the pod 08/29/23 19:48:27.83
    Aug 29 19:48:27.841: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4" in namespace "configmap-7808" to be "running and ready"
    Aug 29 19:48:27.844: INFO: Pod "pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.374602ms
    Aug 29 19:48:27.844: INFO: The phase of Pod pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:48:29.850: INFO: Pod "pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009057536s
    Aug 29 19:48:29.850: INFO: The phase of Pod pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4 is Running (Ready = true)
    Aug 29 19:48:29.850: INFO: Pod "pod-configmaps-7d8bc443-2c4a-435e-ade5-110b69615ec4" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-7f9354ea-6396-4ee6-8d1c-8ad207f77ff7 08/29/23 19:48:29.888
    STEP: Updating configmap cm-test-opt-upd-2ab0ca8c-af29-48a0-abc7-2804e00d1b28 08/29/23 19:48:29.895
    STEP: Creating configMap with name cm-test-opt-create-129fe2a5-1034-48ae-8d82-e51dd3d564d7 08/29/23 19:48:29.902
    STEP: waiting to observe update in volume 08/29/23 19:48:29.907
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:33.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7808" for this suite. 08/29/23 19:48:33.961
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:33.969
Aug 29 19:48:33.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 19:48:33.971
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:33.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:34.003
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 08/29/23 19:48:34.006
Aug 29 19:48:34.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447" in namespace "downward-api-2600" to be "Succeeded or Failed"
Aug 29 19:48:34.019: INFO: Pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447": Phase="Pending", Reason="", readiness=false. Elapsed: 3.549891ms
Aug 29 19:48:36.025: INFO: Pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009427803s
Aug 29 19:48:38.029: INFO: Pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013453996s
STEP: Saw pod success 08/29/23 19:48:38.029
Aug 29 19:48:38.029: INFO: Pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447" satisfied condition "Succeeded or Failed"
Aug 29 19:48:38.033: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447 container client-container: <nil>
STEP: delete the pod 08/29/23 19:48:38.042
Aug 29 19:48:38.056: INFO: Waiting for pod downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447 to disappear
Aug 29 19:48:38.060: INFO: Pod downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:38.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2600" for this suite. 08/29/23 19:48:38.065
------------------------------
• [4.112 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:33.969
    Aug 29 19:48:33.970: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 19:48:33.971
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:33.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:34.003
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 08/29/23 19:48:34.006
    Aug 29 19:48:34.016: INFO: Waiting up to 5m0s for pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447" in namespace "downward-api-2600" to be "Succeeded or Failed"
    Aug 29 19:48:34.019: INFO: Pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447": Phase="Pending", Reason="", readiness=false. Elapsed: 3.549891ms
    Aug 29 19:48:36.025: INFO: Pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009427803s
    Aug 29 19:48:38.029: INFO: Pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013453996s
    STEP: Saw pod success 08/29/23 19:48:38.029
    Aug 29 19:48:38.029: INFO: Pod "downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447" satisfied condition "Succeeded or Failed"
    Aug 29 19:48:38.033: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447 container client-container: <nil>
    STEP: delete the pod 08/29/23 19:48:38.042
    Aug 29 19:48:38.056: INFO: Waiting for pod downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447 to disappear
    Aug 29 19:48:38.060: INFO: Pod downwardapi-volume-99845150-25aa-4379-a7ee-391b82a2c447 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:38.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2600" for this suite. 08/29/23 19:48:38.065
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:38.082
Aug 29 19:48:38.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 19:48:38.083
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:38.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:38.109
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-48265435-7cb9-4868-aca5-249946d672d4 08/29/23 19:48:38.112
STEP: Creating a pod to test consume configMaps 08/29/23 19:48:38.119
Aug 29 19:48:38.129: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d" in namespace "projected-4589" to be "Succeeded or Failed"
Aug 29 19:48:38.132: INFO: Pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170003ms
Aug 29 19:48:40.137: INFO: Pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008720405s
Aug 29 19:48:42.137: INFO: Pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008403741s
STEP: Saw pod success 08/29/23 19:48:42.137
Aug 29 19:48:42.137: INFO: Pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d" satisfied condition "Succeeded or Failed"
Aug 29 19:48:42.141: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d container agnhost-container: <nil>
STEP: delete the pod 08/29/23 19:48:42.15
Aug 29 19:48:42.165: INFO: Waiting for pod pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d to disappear
Aug 29 19:48:42.168: INFO: Pod pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:42.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4589" for this suite. 08/29/23 19:48:42.174
------------------------------
• [4.100 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:38.082
    Aug 29 19:48:38.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 19:48:38.083
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:38.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:38.109
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-48265435-7cb9-4868-aca5-249946d672d4 08/29/23 19:48:38.112
    STEP: Creating a pod to test consume configMaps 08/29/23 19:48:38.119
    Aug 29 19:48:38.129: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d" in namespace "projected-4589" to be "Succeeded or Failed"
    Aug 29 19:48:38.132: INFO: Pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170003ms
    Aug 29 19:48:40.137: INFO: Pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008720405s
    Aug 29 19:48:42.137: INFO: Pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008403741s
    STEP: Saw pod success 08/29/23 19:48:42.137
    Aug 29 19:48:42.137: INFO: Pod "pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d" satisfied condition "Succeeded or Failed"
    Aug 29 19:48:42.141: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 19:48:42.15
    Aug 29 19:48:42.165: INFO: Waiting for pod pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d to disappear
    Aug 29 19:48:42.168: INFO: Pod pod-projected-configmaps-8af9a036-bbdd-404f-b435-791515eee08d no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:42.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4589" for this suite. 08/29/23 19:48:42.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:42.184
Aug 29 19:48:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-webhook 08/29/23 19:48:42.185
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:42.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:42.208
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/29/23 19:48:42.211
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/29/23 19:48:42.527
STEP: Deploying the custom resource conversion webhook pod 08/29/23 19:48:42.538
STEP: Wait for the deployment to be ready 08/29/23 19:48:42.556
Aug 29 19:48:42.566: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 19:48:44.578
STEP: Verifying the service has paired with the endpoint 08/29/23 19:48:44.593
Aug 29 19:48:45.594: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 29 19:48:45.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Creating a v1 custom resource 08/29/23 19:48:48.204
STEP: v2 custom resource should be converted 08/29/23 19:48:48.211
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:48.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7626" for this suite. 08/29/23 19:48:48.788
------------------------------
• [SLOW TEST] [6.613 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:42.184
    Aug 29 19:48:42.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-webhook 08/29/23 19:48:42.185
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:42.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:42.208
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/29/23 19:48:42.211
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/29/23 19:48:42.527
    STEP: Deploying the custom resource conversion webhook pod 08/29/23 19:48:42.538
    STEP: Wait for the deployment to be ready 08/29/23 19:48:42.556
    Aug 29 19:48:42.566: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 19:48:44.578
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:48:44.593
    Aug 29 19:48:45.594: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 29 19:48:45.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Creating a v1 custom resource 08/29/23 19:48:48.204
    STEP: v2 custom resource should be converted 08/29/23 19:48:48.211
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:48.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7626" for this suite. 08/29/23 19:48:48.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:48.798
Aug 29 19:48:48.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 19:48:48.799
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:48.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:48.879
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 19:48:48.9
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:48:49.476
STEP: Deploying the webhook pod 08/29/23 19:48:49.484
STEP: Wait for the deployment to be ready 08/29/23 19:48:49.512
Aug 29 19:48:49.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 19:48:51.533
STEP: Verifying the service has paired with the endpoint 08/29/23 19:48:51.551
Aug 29 19:48:52.551: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Aug 29 19:48:52.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5410-crds.webhook.example.com via the AdmissionRegistration API 08/29/23 19:48:53.078
STEP: Creating a custom resource that should be mutated by the webhook 08/29/23 19:48:53.099
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:48:55.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-450" for this suite. 08/29/23 19:48:55.758
STEP: Destroying namespace "webhook-450-markers" for this suite. 08/29/23 19:48:55.77
------------------------------
• [SLOW TEST] [6.982 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:48.798
    Aug 29 19:48:48.798: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 19:48:48.799
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:48.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:48.879
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 19:48:48.9
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:48:49.476
    STEP: Deploying the webhook pod 08/29/23 19:48:49.484
    STEP: Wait for the deployment to be ready 08/29/23 19:48:49.512
    Aug 29 19:48:49.520: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 19:48:51.533
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:48:51.551
    Aug 29 19:48:52.551: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Aug 29 19:48:52.555: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5410-crds.webhook.example.com via the AdmissionRegistration API 08/29/23 19:48:53.078
    STEP: Creating a custom resource that should be mutated by the webhook 08/29/23 19:48:53.099
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:48:55.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-450" for this suite. 08/29/23 19:48:55.758
    STEP: Destroying namespace "webhook-450-markers" for this suite. 08/29/23 19:48:55.77
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:48:55.781
Aug 29 19:48:55.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 19:48:55.782
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:55.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:55.802
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 08/29/23 19:48:55.806
Aug 29 19:48:55.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d" in namespace "downward-api-7151" to be "Succeeded or Failed"
Aug 29 19:48:55.820: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06247ms
Aug 29 19:48:57.824: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007169728s
Aug 29 19:48:59.825: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0083876s
Aug 29 19:49:01.826: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009431432s
STEP: Saw pod success 08/29/23 19:49:01.826
Aug 29 19:49:01.826: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d" satisfied condition "Succeeded or Failed"
Aug 29 19:49:01.830: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d container client-container: <nil>
STEP: delete the pod 08/29/23 19:49:01.838
Aug 29 19:49:01.851: INFO: Waiting for pod downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d to disappear
Aug 29 19:49:01.855: INFO: Pod downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:01.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-7151" for this suite. 08/29/23 19:49:01.861
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:48:55.781
    Aug 29 19:48:55.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 19:48:55.782
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:48:55.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:48:55.802
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 08/29/23 19:48:55.806
    Aug 29 19:48:55.817: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d" in namespace "downward-api-7151" to be "Succeeded or Failed"
    Aug 29 19:48:55.820: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.06247ms
    Aug 29 19:48:57.824: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007169728s
    Aug 29 19:48:59.825: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0083876s
    Aug 29 19:49:01.826: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009431432s
    STEP: Saw pod success 08/29/23 19:49:01.826
    Aug 29 19:49:01.826: INFO: Pod "downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d" satisfied condition "Succeeded or Failed"
    Aug 29 19:49:01.830: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d container client-container: <nil>
    STEP: delete the pod 08/29/23 19:49:01.838
    Aug 29 19:49:01.851: INFO: Waiting for pod downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d to disappear
    Aug 29 19:49:01.855: INFO: Pod downwardapi-volume-a1d4aaa7-8562-43c1-b6a4-61bb98b5357d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:01.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-7151" for this suite. 08/29/23 19:49:01.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:01.87
Aug 29 19:49:01.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 19:49:01.871
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:01.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:01.903
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/29/23 19:49:01.906
STEP: submitting the pod to kubernetes 08/29/23 19:49:01.906
STEP: verifying QOS class is set on the pod 08/29/23 19:49:01.919
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:01.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-9435" for this suite. 08/29/23 19:49:01.937
------------------------------
• [0.080 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:01.87
    Aug 29 19:49:01.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 19:49:01.871
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:01.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:01.903
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/29/23 19:49:01.906
    STEP: submitting the pod to kubernetes 08/29/23 19:49:01.906
    STEP: verifying QOS class is set on the pod 08/29/23 19:49:01.919
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:01.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-9435" for this suite. 08/29/23 19:49:01.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:01.951
Aug 29 19:49:01.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 19:49:01.952
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:01.974
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:01.977
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-c27567f3-72f3-418e-9f0b-4dca25825508 08/29/23 19:49:01.981
STEP: Creating a pod to test consume secrets 08/29/23 19:49:01.987
Aug 29 19:49:01.998: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7" in namespace "projected-3566" to be "Succeeded or Failed"
Aug 29 19:49:02.002: INFO: Pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.658835ms
Aug 29 19:49:04.008: INFO: Pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009164352s
Aug 29 19:49:06.007: INFO: Pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00852231s
STEP: Saw pod success 08/29/23 19:49:06.007
Aug 29 19:49:06.007: INFO: Pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7" satisfied condition "Succeeded or Failed"
Aug 29 19:49:06.011: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/29/23 19:49:06.018
Aug 29 19:49:06.037: INFO: Waiting for pod pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7 to disappear
Aug 29 19:49:06.041: INFO: Pod pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:06.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3566" for this suite. 08/29/23 19:49:06.047
------------------------------
• [4.106 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:01.951
    Aug 29 19:49:01.951: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 19:49:01.952
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:01.974
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:01.977
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-c27567f3-72f3-418e-9f0b-4dca25825508 08/29/23 19:49:01.981
    STEP: Creating a pod to test consume secrets 08/29/23 19:49:01.987
    Aug 29 19:49:01.998: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7" in namespace "projected-3566" to be "Succeeded or Failed"
    Aug 29 19:49:02.002: INFO: Pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.658835ms
    Aug 29 19:49:04.008: INFO: Pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009164352s
    Aug 29 19:49:06.007: INFO: Pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00852231s
    STEP: Saw pod success 08/29/23 19:49:06.007
    Aug 29 19:49:06.007: INFO: Pod "pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7" satisfied condition "Succeeded or Failed"
    Aug 29 19:49:06.011: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 19:49:06.018
    Aug 29 19:49:06.037: INFO: Waiting for pod pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7 to disappear
    Aug 29 19:49:06.041: INFO: Pod pod-projected-secrets-529ca692-5e4b-4f71-b9b2-eff1154cc0a7 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:06.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3566" for this suite. 08/29/23 19:49:06.047
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:06.058
Aug 29 19:49:06.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 19:49:06.059
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:06.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:06.083
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 08/29/23 19:49:06.086
Aug 29 19:49:06.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02" in namespace "projected-9764" to be "Succeeded or Failed"
Aug 29 19:49:06.100: INFO: Pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034352ms
Aug 29 19:49:08.104: INFO: Pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007250901s
Aug 29 19:49:10.107: INFO: Pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00963249s
STEP: Saw pod success 08/29/23 19:49:10.107
Aug 29 19:49:10.107: INFO: Pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02" satisfied condition "Succeeded or Failed"
Aug 29 19:49:10.109: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02 container client-container: <nil>
STEP: delete the pod 08/29/23 19:49:10.117
Aug 29 19:49:10.132: INFO: Waiting for pod downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02 to disappear
Aug 29 19:49:10.136: INFO: Pod downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:10.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9764" for this suite. 08/29/23 19:49:10.141
------------------------------
• [4.092 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:06.058
    Aug 29 19:49:06.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 19:49:06.059
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:06.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:06.083
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 08/29/23 19:49:06.086
    Aug 29 19:49:06.097: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02" in namespace "projected-9764" to be "Succeeded or Failed"
    Aug 29 19:49:06.100: INFO: Pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02": Phase="Pending", Reason="", readiness=false. Elapsed: 3.034352ms
    Aug 29 19:49:08.104: INFO: Pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007250901s
    Aug 29 19:49:10.107: INFO: Pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00963249s
    STEP: Saw pod success 08/29/23 19:49:10.107
    Aug 29 19:49:10.107: INFO: Pod "downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02" satisfied condition "Succeeded or Failed"
    Aug 29 19:49:10.109: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02 container client-container: <nil>
    STEP: delete the pod 08/29/23 19:49:10.117
    Aug 29 19:49:10.132: INFO: Waiting for pod downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02 to disappear
    Aug 29 19:49:10.136: INFO: Pod downwardapi-volume-cf7eb616-76c3-49e4-9459-4220a7306a02 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:10.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9764" for this suite. 08/29/23 19:49:10.141
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:10.15
Aug 29 19:49:10.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename watch 08/29/23 19:49:10.151
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:10.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:10.174
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/29/23 19:49:10.177
STEP: creating a new configmap 08/29/23 19:49:10.178
STEP: modifying the configmap once 08/29/23 19:49:10.183
STEP: closing the watch once it receives two notifications 08/29/23 19:49:10.193
Aug 29 19:49:10.193: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5697  a0114392-0d01-40db-bf00-b6c9232596cb 11586 0 2023-08-29 19:49:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-29 19:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 19:49:10.193: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5697  a0114392-0d01-40db-bf00-b6c9232596cb 11587 0 2023-08-29 19:49:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-29 19:49:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/29/23 19:49:10.194
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/29/23 19:49:10.202
STEP: deleting the configmap 08/29/23 19:49:10.203
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/29/23 19:49:10.209
Aug 29 19:49:10.210: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5697  a0114392-0d01-40db-bf00-b6c9232596cb 11588 0 2023-08-29 19:49:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-29 19:49:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 19:49:10.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5697  a0114392-0d01-40db-bf00-b6c9232596cb 11589 0 2023-08-29 19:49:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-29 19:49:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:10.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5697" for this suite. 08/29/23 19:49:10.215
------------------------------
• [0.071 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:10.15
    Aug 29 19:49:10.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename watch 08/29/23 19:49:10.151
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:10.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:10.174
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/29/23 19:49:10.177
    STEP: creating a new configmap 08/29/23 19:49:10.178
    STEP: modifying the configmap once 08/29/23 19:49:10.183
    STEP: closing the watch once it receives two notifications 08/29/23 19:49:10.193
    Aug 29 19:49:10.193: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5697  a0114392-0d01-40db-bf00-b6c9232596cb 11586 0 2023-08-29 19:49:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-29 19:49:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 19:49:10.193: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5697  a0114392-0d01-40db-bf00-b6c9232596cb 11587 0 2023-08-29 19:49:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-29 19:49:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/29/23 19:49:10.194
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/29/23 19:49:10.202
    STEP: deleting the configmap 08/29/23 19:49:10.203
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/29/23 19:49:10.209
    Aug 29 19:49:10.210: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5697  a0114392-0d01-40db-bf00-b6c9232596cb 11588 0 2023-08-29 19:49:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-29 19:49:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 19:49:10.210: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5697  a0114392-0d01-40db-bf00-b6c9232596cb 11589 0 2023-08-29 19:49:10 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-29 19:49:10 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:10.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5697" for this suite. 08/29/23 19:49:10.215
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:10.222
Aug 29 19:49:10.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 19:49:10.223
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:10.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:10.249
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 08/29/23 19:49:10.253
Aug 29 19:49:10.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 29 19:49:10.353: INFO: stderr: ""
Aug 29 19:49:10.353: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 08/29/23 19:49:10.353
Aug 29 19:49:10.353: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 29 19:49:10.353: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7314" to be "running and ready, or succeeded"
Aug 29 19:49:10.359: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.498042ms
Aug 29 19:49:10.359: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'loki-15bd39-worker-1' to be 'Running' but was 'Pending'
Aug 29 19:49:12.363: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009883616s
Aug 29 19:49:12.363: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 29 19:49:12.363: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/29/23 19:49:12.363
Aug 29 19:49:12.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator'
Aug 29 19:49:12.477: INFO: stderr: ""
Aug 29 19:49:12.477: INFO: stdout: "I0829 19:49:11.329073       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/64xb 508\nI0829 19:49:11.529230       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/nvg 223\nI0829 19:49:11.729826       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/9lwg 383\nI0829 19:49:11.929179       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/whh 349\nI0829 19:49:12.129607       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/2gjw 215\nI0829 19:49:12.330061       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/58qd 260\n"
STEP: limiting log lines 08/29/23 19:49:12.477
Aug 29 19:49:12.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --tail=1'
Aug 29 19:49:12.586: INFO: stderr: ""
Aug 29 19:49:12.587: INFO: stdout: "I0829 19:49:12.529519       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mgs 239\n"
Aug 29 19:49:12.587: INFO: got output "I0829 19:49:12.529519       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mgs 239\n"
STEP: limiting log bytes 08/29/23 19:49:12.587
Aug 29 19:49:12.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --limit-bytes=1'
Aug 29 19:49:12.685: INFO: stderr: ""
Aug 29 19:49:12.685: INFO: stdout: "I"
Aug 29 19:49:12.685: INFO: got output "I"
STEP: exposing timestamps 08/29/23 19:49:12.685
Aug 29 19:49:12.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 29 19:49:12.784: INFO: stderr: ""
Aug 29 19:49:12.784: INFO: stdout: "2023-08-29T19:49:12.730269366Z I0829 19:49:12.729922       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5hl 369\n"
Aug 29 19:49:12.784: INFO: got output "2023-08-29T19:49:12.730269366Z I0829 19:49:12.729922       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5hl 369\n"
STEP: restricting to a time range 08/29/23 19:49:12.784
Aug 29 19:49:15.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --since=1s'
Aug 29 19:49:15.385: INFO: stderr: ""
Aug 29 19:49:15.385: INFO: stdout: "I0829 19:49:14.529953       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/xv4d 366\nI0829 19:49:14.729230       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/clxd 297\nI0829 19:49:14.929779       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8vw 341\nI0829 19:49:15.129166       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/hck 549\nI0829 19:49:15.329634       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/n6lj 366\n"
Aug 29 19:49:15.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --since=24h'
Aug 29 19:49:15.486: INFO: stderr: ""
Aug 29 19:49:15.486: INFO: stdout: "I0829 19:49:11.329073       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/64xb 508\nI0829 19:49:11.529230       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/nvg 223\nI0829 19:49:11.729826       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/9lwg 383\nI0829 19:49:11.929179       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/whh 349\nI0829 19:49:12.129607       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/2gjw 215\nI0829 19:49:12.330061       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/58qd 260\nI0829 19:49:12.529519       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mgs 239\nI0829 19:49:12.729922       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5hl 369\nI0829 19:49:12.929196       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/xmhd 446\nI0829 19:49:13.129698       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/jk7f 372\nI0829 19:49:13.329195       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/gkms 290\nI0829 19:49:13.529566       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/vfhx 362\nI0829 19:49:13.730030       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/29s8 331\nI0829 19:49:13.929574       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/mr6 579\nI0829 19:49:14.130045       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/kjv 329\nI0829 19:49:14.329500       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/rkcj 305\nI0829 19:49:14.529953       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/xv4d 366\nI0829 19:49:14.729230       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/clxd 297\nI0829 19:49:14.929779       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8vw 341\nI0829 19:49:15.129166       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/hck 549\nI0829 19:49:15.329634       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/n6lj 366\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Aug 29 19:49:15.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 delete pod logs-generator'
Aug 29 19:49:16.963: INFO: stderr: ""
Aug 29 19:49:16.963: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:16.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7314" for this suite. 08/29/23 19:49:16.969
------------------------------
• [SLOW TEST] [6.754 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:10.222
    Aug 29 19:49:10.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 19:49:10.223
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:10.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:10.249
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 08/29/23 19:49:10.253
    Aug 29 19:49:10.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 29 19:49:10.353: INFO: stderr: ""
    Aug 29 19:49:10.353: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 08/29/23 19:49:10.353
    Aug 29 19:49:10.353: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 29 19:49:10.353: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-7314" to be "running and ready, or succeeded"
    Aug 29 19:49:10.359: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.498042ms
    Aug 29 19:49:10.359: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'loki-15bd39-worker-1' to be 'Running' but was 'Pending'
    Aug 29 19:49:12.363: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.009883616s
    Aug 29 19:49:12.363: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 29 19:49:12.363: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/29/23 19:49:12.363
    Aug 29 19:49:12.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator'
    Aug 29 19:49:12.477: INFO: stderr: ""
    Aug 29 19:49:12.477: INFO: stdout: "I0829 19:49:11.329073       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/64xb 508\nI0829 19:49:11.529230       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/nvg 223\nI0829 19:49:11.729826       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/9lwg 383\nI0829 19:49:11.929179       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/whh 349\nI0829 19:49:12.129607       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/2gjw 215\nI0829 19:49:12.330061       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/58qd 260\n"
    STEP: limiting log lines 08/29/23 19:49:12.477
    Aug 29 19:49:12.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --tail=1'
    Aug 29 19:49:12.586: INFO: stderr: ""
    Aug 29 19:49:12.587: INFO: stdout: "I0829 19:49:12.529519       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mgs 239\n"
    Aug 29 19:49:12.587: INFO: got output "I0829 19:49:12.529519       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mgs 239\n"
    STEP: limiting log bytes 08/29/23 19:49:12.587
    Aug 29 19:49:12.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --limit-bytes=1'
    Aug 29 19:49:12.685: INFO: stderr: ""
    Aug 29 19:49:12.685: INFO: stdout: "I"
    Aug 29 19:49:12.685: INFO: got output "I"
    STEP: exposing timestamps 08/29/23 19:49:12.685
    Aug 29 19:49:12.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 29 19:49:12.784: INFO: stderr: ""
    Aug 29 19:49:12.784: INFO: stdout: "2023-08-29T19:49:12.730269366Z I0829 19:49:12.729922       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5hl 369\n"
    Aug 29 19:49:12.784: INFO: got output "2023-08-29T19:49:12.730269366Z I0829 19:49:12.729922       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5hl 369\n"
    STEP: restricting to a time range 08/29/23 19:49:12.784
    Aug 29 19:49:15.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --since=1s'
    Aug 29 19:49:15.385: INFO: stderr: ""
    Aug 29 19:49:15.385: INFO: stdout: "I0829 19:49:14.529953       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/xv4d 366\nI0829 19:49:14.729230       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/clxd 297\nI0829 19:49:14.929779       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8vw 341\nI0829 19:49:15.129166       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/hck 549\nI0829 19:49:15.329634       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/n6lj 366\n"
    Aug 29 19:49:15.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 logs logs-generator logs-generator --since=24h'
    Aug 29 19:49:15.486: INFO: stderr: ""
    Aug 29 19:49:15.486: INFO: stdout: "I0829 19:49:11.329073       1 logs_generator.go:76] 0 PUT /api/v1/namespaces/default/pods/64xb 508\nI0829 19:49:11.529230       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/nvg 223\nI0829 19:49:11.729826       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/9lwg 383\nI0829 19:49:11.929179       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/ns/pods/whh 349\nI0829 19:49:12.129607       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/2gjw 215\nI0829 19:49:12.330061       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/58qd 260\nI0829 19:49:12.529519       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/mgs 239\nI0829 19:49:12.729922       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/5hl 369\nI0829 19:49:12.929196       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/xmhd 446\nI0829 19:49:13.129698       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/jk7f 372\nI0829 19:49:13.329195       1 logs_generator.go:76] 10 POST /api/v1/namespaces/default/pods/gkms 290\nI0829 19:49:13.529566       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/vfhx 362\nI0829 19:49:13.730030       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/29s8 331\nI0829 19:49:13.929574       1 logs_generator.go:76] 13 GET /api/v1/namespaces/default/pods/mr6 579\nI0829 19:49:14.130045       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/kjv 329\nI0829 19:49:14.329500       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/rkcj 305\nI0829 19:49:14.529953       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/xv4d 366\nI0829 19:49:14.729230       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/clxd 297\nI0829 19:49:14.929779       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/8vw 341\nI0829 19:49:15.129166       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/hck 549\nI0829 19:49:15.329634       1 logs_generator.go:76] 20 POST /api/v1/namespaces/kube-system/pods/n6lj 366\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Aug 29 19:49:15.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7314 delete pod logs-generator'
    Aug 29 19:49:16.963: INFO: stderr: ""
    Aug 29 19:49:16.963: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:16.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7314" for this suite. 08/29/23 19:49:16.969
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:16.976
Aug 29 19:49:16.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename containers 08/29/23 19:49:16.977
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:16.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:17.001
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Aug 29 19:49:17.015: INFO: Waiting up to 5m0s for pod "client-containers-2092462f-14e8-40fe-a3f0-522e88625738" in namespace "containers-7398" to be "running"
Aug 29 19:49:17.018: INFO: Pod "client-containers-2092462f-14e8-40fe-a3f0-522e88625738": Phase="Pending", Reason="", readiness=false. Elapsed: 3.601312ms
Aug 29 19:49:19.023: INFO: Pod "client-containers-2092462f-14e8-40fe-a3f0-522e88625738": Phase="Running", Reason="", readiness=true. Elapsed: 2.008078137s
Aug 29 19:49:19.023: INFO: Pod "client-containers-2092462f-14e8-40fe-a3f0-522e88625738" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:19.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-7398" for this suite. 08/29/23 19:49:19.037
------------------------------
• [2.068 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:16.976
    Aug 29 19:49:16.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename containers 08/29/23 19:49:16.977
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:16.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:17.001
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Aug 29 19:49:17.015: INFO: Waiting up to 5m0s for pod "client-containers-2092462f-14e8-40fe-a3f0-522e88625738" in namespace "containers-7398" to be "running"
    Aug 29 19:49:17.018: INFO: Pod "client-containers-2092462f-14e8-40fe-a3f0-522e88625738": Phase="Pending", Reason="", readiness=false. Elapsed: 3.601312ms
    Aug 29 19:49:19.023: INFO: Pod "client-containers-2092462f-14e8-40fe-a3f0-522e88625738": Phase="Running", Reason="", readiness=true. Elapsed: 2.008078137s
    Aug 29 19:49:19.023: INFO: Pod "client-containers-2092462f-14e8-40fe-a3f0-522e88625738" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:19.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-7398" for this suite. 08/29/23 19:49:19.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:19.045
Aug 29 19:49:19.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename security-context-test 08/29/23 19:49:19.046
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:19.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:19.072
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Aug 29 19:49:19.085: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291" in namespace "security-context-test-7920" to be "Succeeded or Failed"
Aug 29 19:49:19.088: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291": Phase="Pending", Reason="", readiness=false. Elapsed: 3.224447ms
Aug 29 19:49:21.094: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008676198s
Aug 29 19:49:23.092: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007089649s
Aug 29 19:49:25.093: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007934394s
Aug 29 19:49:25.093: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:25.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7920" for this suite. 08/29/23 19:49:25.106
------------------------------
• [SLOW TEST] [6.071 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:19.045
    Aug 29 19:49:19.045: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename security-context-test 08/29/23 19:49:19.046
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:19.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:19.072
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Aug 29 19:49:19.085: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291" in namespace "security-context-test-7920" to be "Succeeded or Failed"
    Aug 29 19:49:19.088: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291": Phase="Pending", Reason="", readiness=false. Elapsed: 3.224447ms
    Aug 29 19:49:21.094: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008676198s
    Aug 29 19:49:23.092: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007089649s
    Aug 29 19:49:25.093: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007934394s
    Aug 29 19:49:25.093: INFO: Pod "alpine-nnp-false-2686f2f5-0073-49f2-a1e3-7d4e9cac6291" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:25.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7920" for this suite. 08/29/23 19:49:25.106
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:25.117
Aug 29 19:49:25.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 19:49:25.118
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:25.14
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:25.144
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 08/29/23 19:49:25.148
Aug 29 19:49:25.158: INFO: Waiting up to 5m0s for pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182" in namespace "emptydir-8320" to be "Succeeded or Failed"
Aug 29 19:49:25.161: INFO: Pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110926ms
Aug 29 19:49:27.166: INFO: Pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008075357s
Aug 29 19:49:29.166: INFO: Pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00773906s
STEP: Saw pod success 08/29/23 19:49:29.166
Aug 29 19:49:29.166: INFO: Pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182" satisfied condition "Succeeded or Failed"
Aug 29 19:49:29.170: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-820e3dbf-5e22-47b7-848b-2e91c101c182 container test-container: <nil>
STEP: delete the pod 08/29/23 19:49:29.178
Aug 29 19:49:29.195: INFO: Waiting for pod pod-820e3dbf-5e22-47b7-848b-2e91c101c182 to disappear
Aug 29 19:49:29.198: INFO: Pod pod-820e3dbf-5e22-47b7-848b-2e91c101c182 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:29.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8320" for this suite. 08/29/23 19:49:29.205
------------------------------
• [4.096 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:25.117
    Aug 29 19:49:25.117: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 19:49:25.118
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:25.14
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:25.144
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 08/29/23 19:49:25.148
    Aug 29 19:49:25.158: INFO: Waiting up to 5m0s for pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182" in namespace "emptydir-8320" to be "Succeeded or Failed"
    Aug 29 19:49:25.161: INFO: Pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182": Phase="Pending", Reason="", readiness=false. Elapsed: 3.110926ms
    Aug 29 19:49:27.166: INFO: Pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008075357s
    Aug 29 19:49:29.166: INFO: Pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00773906s
    STEP: Saw pod success 08/29/23 19:49:29.166
    Aug 29 19:49:29.166: INFO: Pod "pod-820e3dbf-5e22-47b7-848b-2e91c101c182" satisfied condition "Succeeded or Failed"
    Aug 29 19:49:29.170: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-820e3dbf-5e22-47b7-848b-2e91c101c182 container test-container: <nil>
    STEP: delete the pod 08/29/23 19:49:29.178
    Aug 29 19:49:29.195: INFO: Waiting for pod pod-820e3dbf-5e22-47b7-848b-2e91c101c182 to disappear
    Aug 29 19:49:29.198: INFO: Pod pod-820e3dbf-5e22-47b7-848b-2e91c101c182 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:29.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8320" for this suite. 08/29/23 19:49:29.205
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:29.214
Aug 29 19:49:29.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 19:49:29.214
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:29.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:29.237
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-d1c5f9ce-a299-44ae-9123-e3b529a9abf8 08/29/23 19:49:29.24
STEP: Creating a pod to test consume secrets 08/29/23 19:49:29.245
Aug 29 19:49:29.254: INFO: Waiting up to 5m0s for pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679" in namespace "secrets-693" to be "Succeeded or Failed"
Aug 29 19:49:29.257: INFO: Pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942809ms
Aug 29 19:49:31.261: INFO: Pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007759356s
Aug 29 19:49:33.262: INFO: Pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007913811s
STEP: Saw pod success 08/29/23 19:49:33.262
Aug 29 19:49:33.262: INFO: Pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679" satisfied condition "Succeeded or Failed"
Aug 29 19:49:33.265: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-f1554a85-2f66-414f-b331-962483d2c679 container secret-env-test: <nil>
STEP: delete the pod 08/29/23 19:49:33.275
Aug 29 19:49:33.295: INFO: Waiting for pod pod-secrets-f1554a85-2f66-414f-b331-962483d2c679 to disappear
Aug 29 19:49:33.298: INFO: Pod pod-secrets-f1554a85-2f66-414f-b331-962483d2c679 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 19:49:33.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-693" for this suite. 08/29/23 19:49:33.304
------------------------------
• [4.100 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:29.214
    Aug 29 19:49:29.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 19:49:29.214
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:29.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:29.237
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-d1c5f9ce-a299-44ae-9123-e3b529a9abf8 08/29/23 19:49:29.24
    STEP: Creating a pod to test consume secrets 08/29/23 19:49:29.245
    Aug 29 19:49:29.254: INFO: Waiting up to 5m0s for pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679" in namespace "secrets-693" to be "Succeeded or Failed"
    Aug 29 19:49:29.257: INFO: Pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.942809ms
    Aug 29 19:49:31.261: INFO: Pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007759356s
    Aug 29 19:49:33.262: INFO: Pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007913811s
    STEP: Saw pod success 08/29/23 19:49:33.262
    Aug 29 19:49:33.262: INFO: Pod "pod-secrets-f1554a85-2f66-414f-b331-962483d2c679" satisfied condition "Succeeded or Failed"
    Aug 29 19:49:33.265: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-f1554a85-2f66-414f-b331-962483d2c679 container secret-env-test: <nil>
    STEP: delete the pod 08/29/23 19:49:33.275
    Aug 29 19:49:33.295: INFO: Waiting for pod pod-secrets-f1554a85-2f66-414f-b331-962483d2c679 to disappear
    Aug 29 19:49:33.298: INFO: Pod pod-secrets-f1554a85-2f66-414f-b331-962483d2c679 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:49:33.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-693" for this suite. 08/29/23 19:49:33.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:49:33.315
Aug 29 19:49:33.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename statefulset 08/29/23 19:49:33.316
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:33.338
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:33.341
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6118 08/29/23 19:49:33.345
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 08/29/23 19:49:33.353
Aug 29 19:49:33.370: INFO: Found 0 stateful pods, waiting for 3
Aug 29 19:49:43.376: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 19:49:43.376: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 19:49:43.376: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/29/23 19:49:43.387
Aug 29 19:49:43.410: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/29/23 19:49:43.41
STEP: Not applying an update when the partition is greater than the number of replicas 08/29/23 19:49:53.429
STEP: Performing a canary update 08/29/23 19:49:53.429
Aug 29 19:49:53.452: INFO: Updating stateful set ss2
Aug 29 19:49:53.460: INFO: Waiting for Pod statefulset-6118/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 08/29/23 19:50:03.469
Aug 29 19:50:03.514: INFO: Found 2 stateful pods, waiting for 3
Aug 29 19:50:13.520: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 19:50:13.520: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 19:50:13.520: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/29/23 19:50:13.528
Aug 29 19:50:13.549: INFO: Updating stateful set ss2
Aug 29 19:50:13.556: INFO: Waiting for Pod statefulset-6118/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Aug 29 19:50:23.589: INFO: Updating stateful set ss2
Aug 29 19:50:23.596: INFO: Waiting for StatefulSet statefulset-6118/ss2 to complete update
Aug 29 19:50:23.596: INFO: Waiting for Pod statefulset-6118/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 29 19:50:33.606: INFO: Deleting all statefulset in ns statefulset-6118
Aug 29 19:50:33.609: INFO: Scaling statefulset ss2 to 0
Aug 29 19:50:43.631: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 19:50:43.634: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 29 19:50:43.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6118" for this suite. 08/29/23 19:50:43.653
------------------------------
• [SLOW TEST] [70.345 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:49:33.315
    Aug 29 19:49:33.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename statefulset 08/29/23 19:49:33.316
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:49:33.338
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:49:33.341
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6118 08/29/23 19:49:33.345
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 08/29/23 19:49:33.353
    Aug 29 19:49:33.370: INFO: Found 0 stateful pods, waiting for 3
    Aug 29 19:49:43.376: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 19:49:43.376: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 19:49:43.376: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/29/23 19:49:43.387
    Aug 29 19:49:43.410: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/29/23 19:49:43.41
    STEP: Not applying an update when the partition is greater than the number of replicas 08/29/23 19:49:53.429
    STEP: Performing a canary update 08/29/23 19:49:53.429
    Aug 29 19:49:53.452: INFO: Updating stateful set ss2
    Aug 29 19:49:53.460: INFO: Waiting for Pod statefulset-6118/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 08/29/23 19:50:03.469
    Aug 29 19:50:03.514: INFO: Found 2 stateful pods, waiting for 3
    Aug 29 19:50:13.520: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 19:50:13.520: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 19:50:13.520: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/29/23 19:50:13.528
    Aug 29 19:50:13.549: INFO: Updating stateful set ss2
    Aug 29 19:50:13.556: INFO: Waiting for Pod statefulset-6118/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Aug 29 19:50:23.589: INFO: Updating stateful set ss2
    Aug 29 19:50:23.596: INFO: Waiting for StatefulSet statefulset-6118/ss2 to complete update
    Aug 29 19:50:23.596: INFO: Waiting for Pod statefulset-6118/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 29 19:50:33.606: INFO: Deleting all statefulset in ns statefulset-6118
    Aug 29 19:50:33.609: INFO: Scaling statefulset ss2 to 0
    Aug 29 19:50:43.631: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 19:50:43.634: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:50:43.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6118" for this suite. 08/29/23 19:50:43.653
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:50:43.66
Aug 29 19:50:43.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 19:50:43.661
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:50:43.683
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:50:43.688
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 08/29/23 19:50:43.691
Aug 29 19:50:43.702: INFO: Waiting up to 5m0s for pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59" in namespace "projected-7963" to be "running and ready"
Aug 29 19:50:43.704: INFO: Pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87265ms
Aug 29 19:50:43.704: INFO: The phase of Pod annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:50:45.709: INFO: Pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59": Phase="Running", Reason="", readiness=true. Elapsed: 2.006987686s
Aug 29 19:50:45.709: INFO: The phase of Pod annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59 is Running (Ready = true)
Aug 29 19:50:45.709: INFO: Pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59" satisfied condition "running and ready"
Aug 29 19:50:46.237: INFO: Successfully updated pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 19:50:48.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7963" for this suite. 08/29/23 19:50:48.265
------------------------------
• [4.613 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:50:43.66
    Aug 29 19:50:43.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 19:50:43.661
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:50:43.683
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:50:43.688
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 08/29/23 19:50:43.691
    Aug 29 19:50:43.702: INFO: Waiting up to 5m0s for pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59" in namespace "projected-7963" to be "running and ready"
    Aug 29 19:50:43.704: INFO: Pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.87265ms
    Aug 29 19:50:43.704: INFO: The phase of Pod annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:50:45.709: INFO: Pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59": Phase="Running", Reason="", readiness=true. Elapsed: 2.006987686s
    Aug 29 19:50:45.709: INFO: The phase of Pod annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59 is Running (Ready = true)
    Aug 29 19:50:45.709: INFO: Pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59" satisfied condition "running and ready"
    Aug 29 19:50:46.237: INFO: Successfully updated pod "annotationupdate3739acf2-cb11-4777-98b2-2b962ca5ae59"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:50:48.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7963" for this suite. 08/29/23 19:50:48.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:50:48.275
Aug 29 19:50:48.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename subpath 08/29/23 19:50:48.276
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:50:48.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:50:48.299
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/29/23 19:50:48.302
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-pvwk 08/29/23 19:50:48.313
STEP: Creating a pod to test atomic-volume-subpath 08/29/23 19:50:48.313
Aug 29 19:50:48.324: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-pvwk" in namespace "subpath-6706" to be "Succeeded or Failed"
Aug 29 19:50:48.332: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.231529ms
Aug 29 19:50:50.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.013171595s
Aug 29 19:50:52.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 4.01457636s
Aug 29 19:50:54.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 6.014644882s
Aug 29 19:50:56.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 8.014255612s
Aug 29 19:50:58.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 10.013279645s
Aug 29 19:51:00.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 12.013983095s
Aug 29 19:51:02.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 14.014049339s
Aug 29 19:51:04.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 16.013565812s
Aug 29 19:51:06.339: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 18.015341143s
Aug 29 19:51:08.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 20.013411682s
Aug 29 19:51:10.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 22.01363672s
Aug 29 19:51:12.336: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=false. Elapsed: 24.012376321s
Aug 29 19:51:14.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013336024s
STEP: Saw pod success 08/29/23 19:51:14.337
Aug 29 19:51:14.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk" satisfied condition "Succeeded or Failed"
Aug 29 19:51:14.341: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-downwardapi-pvwk container test-container-subpath-downwardapi-pvwk: <nil>
STEP: delete the pod 08/29/23 19:51:14.349
Aug 29 19:51:14.366: INFO: Waiting for pod pod-subpath-test-downwardapi-pvwk to disappear
Aug 29 19:51:14.369: INFO: Pod pod-subpath-test-downwardapi-pvwk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-pvwk 08/29/23 19:51:14.369
Aug 29 19:51:14.369: INFO: Deleting pod "pod-subpath-test-downwardapi-pvwk" in namespace "subpath-6706"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 29 19:51:14.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-6706" for this suite. 08/29/23 19:51:14.378
------------------------------
• [SLOW TEST] [26.110 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:50:48.275
    Aug 29 19:50:48.275: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename subpath 08/29/23 19:50:48.276
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:50:48.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:50:48.299
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/29/23 19:50:48.302
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-pvwk 08/29/23 19:50:48.313
    STEP: Creating a pod to test atomic-volume-subpath 08/29/23 19:50:48.313
    Aug 29 19:50:48.324: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-pvwk" in namespace "subpath-6706" to be "Succeeded or Failed"
    Aug 29 19:50:48.332: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Pending", Reason="", readiness=false. Elapsed: 8.231529ms
    Aug 29 19:50:50.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 2.013171595s
    Aug 29 19:50:52.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 4.01457636s
    Aug 29 19:50:54.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 6.014644882s
    Aug 29 19:50:56.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 8.014255612s
    Aug 29 19:50:58.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 10.013279645s
    Aug 29 19:51:00.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 12.013983095s
    Aug 29 19:51:02.338: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 14.014049339s
    Aug 29 19:51:04.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 16.013565812s
    Aug 29 19:51:06.339: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 18.015341143s
    Aug 29 19:51:08.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 20.013411682s
    Aug 29 19:51:10.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=true. Elapsed: 22.01363672s
    Aug 29 19:51:12.336: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Running", Reason="", readiness=false. Elapsed: 24.012376321s
    Aug 29 19:51:14.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.013336024s
    STEP: Saw pod success 08/29/23 19:51:14.337
    Aug 29 19:51:14.337: INFO: Pod "pod-subpath-test-downwardapi-pvwk" satisfied condition "Succeeded or Failed"
    Aug 29 19:51:14.341: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-downwardapi-pvwk container test-container-subpath-downwardapi-pvwk: <nil>
    STEP: delete the pod 08/29/23 19:51:14.349
    Aug 29 19:51:14.366: INFO: Waiting for pod pod-subpath-test-downwardapi-pvwk to disappear
    Aug 29 19:51:14.369: INFO: Pod pod-subpath-test-downwardapi-pvwk no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-pvwk 08/29/23 19:51:14.369
    Aug 29 19:51:14.369: INFO: Deleting pod "pod-subpath-test-downwardapi-pvwk" in namespace "subpath-6706"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:51:14.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-6706" for this suite. 08/29/23 19:51:14.378
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:51:14.385
Aug 29 19:51:14.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 19:51:14.386
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:14.406
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:14.408
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-2394fd67-1e87-48ac-a636-a2f3a4e33e3d 08/29/23 19:51:14.411
STEP: Creating a pod to test consume configMaps 08/29/23 19:51:14.417
Aug 29 19:51:14.428: INFO: Waiting up to 5m0s for pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3" in namespace "configmap-2263" to be "Succeeded or Failed"
Aug 29 19:51:14.431: INFO: Pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520791ms
Aug 29 19:51:16.438: INFO: Pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009633947s
Aug 29 19:51:18.438: INFO: Pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009837915s
STEP: Saw pod success 08/29/23 19:51:18.438
Aug 29 19:51:18.438: INFO: Pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3" satisfied condition "Succeeded or Failed"
Aug 29 19:51:18.443: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3 container agnhost-container: <nil>
STEP: delete the pod 08/29/23 19:51:18.451
Aug 29 19:51:18.468: INFO: Waiting for pod pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3 to disappear
Aug 29 19:51:18.472: INFO: Pod pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 19:51:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2263" for this suite. 08/29/23 19:51:18.477
------------------------------
• [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:51:14.385
    Aug 29 19:51:14.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 19:51:14.386
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:14.406
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:14.408
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-2394fd67-1e87-48ac-a636-a2f3a4e33e3d 08/29/23 19:51:14.411
    STEP: Creating a pod to test consume configMaps 08/29/23 19:51:14.417
    Aug 29 19:51:14.428: INFO: Waiting up to 5m0s for pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3" in namespace "configmap-2263" to be "Succeeded or Failed"
    Aug 29 19:51:14.431: INFO: Pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520791ms
    Aug 29 19:51:16.438: INFO: Pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009633947s
    Aug 29 19:51:18.438: INFO: Pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009837915s
    STEP: Saw pod success 08/29/23 19:51:18.438
    Aug 29 19:51:18.438: INFO: Pod "pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3" satisfied condition "Succeeded or Failed"
    Aug 29 19:51:18.443: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3 container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 19:51:18.451
    Aug 29 19:51:18.468: INFO: Waiting for pod pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3 to disappear
    Aug 29 19:51:18.472: INFO: Pod pod-configmaps-558f13bb-0346-40cf-a763-4467a3ffc8f3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:51:18.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2263" for this suite. 08/29/23 19:51:18.477
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:51:18.485
Aug 29 19:51:18.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename init-container 08/29/23 19:51:18.486
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:18.504
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:18.507
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 08/29/23 19:51:18.51
Aug 29 19:51:18.510: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:51:24.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1187" for this suite. 08/29/23 19:51:24.467
------------------------------
• [SLOW TEST] [5.990 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:51:18.485
    Aug 29 19:51:18.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename init-container 08/29/23 19:51:18.486
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:18.504
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:18.507
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 08/29/23 19:51:18.51
    Aug 29 19:51:18.510: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:51:24.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1187" for this suite. 08/29/23 19:51:24.467
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:51:24.475
Aug 29 19:51:24.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename svcaccounts 08/29/23 19:51:24.477
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:24.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:24.505
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Aug 29 19:51:24.525: INFO: Waiting up to 5m0s for pod "pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf" in namespace "svcaccounts-4043" to be "running"
Aug 29 19:51:24.532: INFO: Pod "pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124085ms
Aug 29 19:51:26.538: INFO: Pod "pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013112153s
Aug 29 19:51:26.538: INFO: Pod "pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf" satisfied condition "running"
STEP: reading a file in the container 08/29/23 19:51:26.538
Aug 29 19:51:26.538: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4043 pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/29/23 19:51:26.717
Aug 29 19:51:26.717: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4043 pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/29/23 19:51:26.884
Aug 29 19:51:26.885: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4043 pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 29 19:51:27.059: INFO: Got root ca configmap in namespace "svcaccounts-4043"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 29 19:51:27.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-4043" for this suite. 08/29/23 19:51:27.066
------------------------------
• [2.600 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:51:24.475
    Aug 29 19:51:24.475: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename svcaccounts 08/29/23 19:51:24.477
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:24.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:24.505
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Aug 29 19:51:24.525: INFO: Waiting up to 5m0s for pod "pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf" in namespace "svcaccounts-4043" to be "running"
    Aug 29 19:51:24.532: INFO: Pod "pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124085ms
    Aug 29 19:51:26.538: INFO: Pod "pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf": Phase="Running", Reason="", readiness=true. Elapsed: 2.013112153s
    Aug 29 19:51:26.538: INFO: Pod "pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf" satisfied condition "running"
    STEP: reading a file in the container 08/29/23 19:51:26.538
    Aug 29 19:51:26.538: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4043 pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/29/23 19:51:26.717
    Aug 29 19:51:26.717: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4043 pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/29/23 19:51:26.884
    Aug 29 19:51:26.885: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4043 pod-service-account-2f0bcd30-8ac9-49d4-ac03-21dfa5d19cbf -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 29 19:51:27.059: INFO: Got root ca configmap in namespace "svcaccounts-4043"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:51:27.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-4043" for this suite. 08/29/23 19:51:27.066
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:51:27.075
Aug 29 19:51:27.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename dns 08/29/23 19:51:27.076
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:27.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:27.101
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/29/23 19:51:27.103
Aug 29 19:51:27.112: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3135  989e9265-e469-4d97-8e2a-694c9eb44b16 12672 0 2023-08-29 19:51:27 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-29 19:51:27 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-742pf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-742pf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:51:27.113: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3135" to be "running and ready"
Aug 29 19:51:27.116: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475524ms
Aug 29 19:51:27.116: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:51:29.120: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007721278s
Aug 29 19:51:29.121: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 29 19:51:29.121: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/29/23 19:51:29.121
Aug 29 19:51:29.121: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3135 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:51:29.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:51:29.121: INFO: ExecWithOptions: Clientset creation
Aug 29 19:51:29.121: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/dns-3135/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/29/23 19:51:29.222
Aug 29 19:51:29.222: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3135 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:51:29.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:51:29.222: INFO: ExecWithOptions: Clientset creation
Aug 29 19:51:29.222: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/dns-3135/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 19:51:29.319: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 29 19:51:29.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-3135" for this suite. 08/29/23 19:51:29.339
------------------------------
• [2.271 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:51:27.075
    Aug 29 19:51:27.075: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename dns 08/29/23 19:51:27.076
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:27.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:27.101
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/29/23 19:51:27.103
    Aug 29 19:51:27.112: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3135  989e9265-e469-4d97-8e2a-694c9eb44b16 12672 0 2023-08-29 19:51:27 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-29 19:51:27 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-742pf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-742pf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:51:27.113: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-3135" to be "running and ready"
    Aug 29 19:51:27.116: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.475524ms
    Aug 29 19:51:27.116: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:51:29.120: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007721278s
    Aug 29 19:51:29.121: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 29 19:51:29.121: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/29/23 19:51:29.121
    Aug 29 19:51:29.121: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3135 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:51:29.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:51:29.121: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:51:29.121: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/dns-3135/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/29/23 19:51:29.222
    Aug 29 19:51:29.222: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3135 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:51:29.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:51:29.222: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:51:29.222: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/dns-3135/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 19:51:29.319: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:51:29.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-3135" for this suite. 08/29/23 19:51:29.339
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:51:29.347
Aug 29 19:51:29.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pod-network-test 08/29/23 19:51:29.348
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:29.369
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:29.372
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-7602 08/29/23 19:51:29.375
STEP: creating a selector 08/29/23 19:51:29.375
STEP: Creating the service pods in kubernetes 08/29/23 19:51:29.375
Aug 29 19:51:29.375: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 19:51:29.440: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7602" to be "running and ready"
Aug 29 19:51:29.450: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.331784ms
Aug 29 19:51:29.450: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:51:31.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014743807s
Aug 29 19:51:31.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:33.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014203983s
Aug 29 19:51:33.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:35.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014507369s
Aug 29 19:51:35.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:37.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014362258s
Aug 29 19:51:37.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:39.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014703646s
Aug 29 19:51:39.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:41.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014484496s
Aug 29 19:51:41.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:43.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016103688s
Aug 29 19:51:43.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:45.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01488538s
Aug 29 19:51:45.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:47.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015248597s
Aug 29 19:51:47.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:49.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014636176s
Aug 29 19:51:49.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 19:51:51.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014027195s
Aug 29 19:51:51.454: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 29 19:51:51.454: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 29 19:51:51.459: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7602" to be "running and ready"
Aug 29 19:51:51.462: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.730734ms
Aug 29 19:51:51.462: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 29 19:51:51.462: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 29 19:51:51.466: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7602" to be "running and ready"
Aug 29 19:51:51.469: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.925817ms
Aug 29 19:51:51.469: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 29 19:51:51.469: INFO: Pod "netserver-2" satisfied condition "running and ready"
Aug 29 19:51:51.472: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7602" to be "running and ready"
Aug 29 19:51:51.475: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.072037ms
Aug 29 19:51:51.475: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Aug 29 19:51:51.475: INFO: Pod "netserver-3" satisfied condition "running and ready"
Aug 29 19:51:51.479: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7602" to be "running and ready"
Aug 29 19:51:51.482: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.230969ms
Aug 29 19:51:51.482: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Aug 29 19:51:51.482: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 08/29/23 19:51:51.485
Aug 29 19:51:51.494: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7602" to be "running"
Aug 29 19:51:51.498: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007606ms
Aug 29 19:51:53.504: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00985357s
Aug 29 19:51:53.504: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 29 19:51:53.507: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Aug 29 19:51:53.507: INFO: Breadth first check of 172.20.17.71 on host 10.45.35.202...
Aug 29 19:51:53.510: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.17.71&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:51:53.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:51:53.511: INFO: ExecWithOptions: Clientset creation
Aug 29 19:51:53.511: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.17.71%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:51:53.605: INFO: Waiting for responses: map[]
Aug 29 19:51:53.605: INFO: reached 172.20.17.71 after 0/1 tries
Aug 29 19:51:53.605: INFO: Breadth first check of 172.20.76.137 on host 10.45.35.204...
Aug 29 19:51:53.608: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.76.137&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:51:53.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:51:53.609: INFO: ExecWithOptions: Clientset creation
Aug 29 19:51:53.609: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.76.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:51:53.698: INFO: Waiting for responses: map[]
Aug 29 19:51:53.698: INFO: reached 172.20.76.137 after 0/1 tries
Aug 29 19:51:53.698: INFO: Breadth first check of 172.20.143.212 on host 10.45.35.198...
Aug 29 19:51:53.703: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.143.212&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:51:53.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:51:53.704: INFO: ExecWithOptions: Clientset creation
Aug 29 19:51:53.704: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.143.212%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:51:53.781: INFO: Waiting for responses: map[]
Aug 29 19:51:53.781: INFO: reached 172.20.143.212 after 0/1 tries
Aug 29 19:51:53.781: INFO: Breadth first check of 172.20.30.144 on host 10.45.35.206...
Aug 29 19:51:53.785: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.30.144&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:51:53.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:51:53.785: INFO: ExecWithOptions: Clientset creation
Aug 29 19:51:53.785: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.30.144%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:51:53.871: INFO: Waiting for responses: map[]
Aug 29 19:51:53.871: INFO: reached 172.20.30.144 after 0/1 tries
Aug 29 19:51:53.871: INFO: Breadth first check of 172.20.84.161 on host 10.45.35.199...
Aug 29 19:51:53.875: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.84.161&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 19:51:53.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 19:51:53.875: INFO: ExecWithOptions: Clientset creation
Aug 29 19:51:53.876: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.84.161%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 29 19:51:53.962: INFO: Waiting for responses: map[]
Aug 29 19:51:53.962: INFO: reached 172.20.84.161 after 0/1 tries
Aug 29 19:51:53.962: INFO: Going to retry 0 out of 5 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 29 19:51:53.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7602" for this suite. 08/29/23 19:51:53.968
------------------------------
• [SLOW TEST] [24.627 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:51:29.347
    Aug 29 19:51:29.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pod-network-test 08/29/23 19:51:29.348
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:29.369
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:29.372
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-7602 08/29/23 19:51:29.375
    STEP: creating a selector 08/29/23 19:51:29.375
    STEP: Creating the service pods in kubernetes 08/29/23 19:51:29.375
    Aug 29 19:51:29.375: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 29 19:51:29.440: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7602" to be "running and ready"
    Aug 29 19:51:29.450: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.331784ms
    Aug 29 19:51:29.450: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:51:31.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.014743807s
    Aug 29 19:51:31.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:33.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014203983s
    Aug 29 19:51:33.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:35.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014507369s
    Aug 29 19:51:35.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:37.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.014362258s
    Aug 29 19:51:37.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:39.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.014703646s
    Aug 29 19:51:39.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:41.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.014484496s
    Aug 29 19:51:41.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:43.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.016103688s
    Aug 29 19:51:43.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:45.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.01488538s
    Aug 29 19:51:45.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:47.456: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.015248597s
    Aug 29 19:51:47.456: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:49.455: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.014636176s
    Aug 29 19:51:49.455: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 19:51:51.454: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.014027195s
    Aug 29 19:51:51.454: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 29 19:51:51.454: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 29 19:51:51.459: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7602" to be "running and ready"
    Aug 29 19:51:51.462: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.730734ms
    Aug 29 19:51:51.462: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 29 19:51:51.462: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 29 19:51:51.466: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7602" to be "running and ready"
    Aug 29 19:51:51.469: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.925817ms
    Aug 29 19:51:51.469: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 29 19:51:51.469: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Aug 29 19:51:51.472: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7602" to be "running and ready"
    Aug 29 19:51:51.475: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.072037ms
    Aug 29 19:51:51.475: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Aug 29 19:51:51.475: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Aug 29 19:51:51.479: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7602" to be "running and ready"
    Aug 29 19:51:51.482: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.230969ms
    Aug 29 19:51:51.482: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Aug 29 19:51:51.482: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 08/29/23 19:51:51.485
    Aug 29 19:51:51.494: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7602" to be "running"
    Aug 29 19:51:51.498: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007606ms
    Aug 29 19:51:53.504: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00985357s
    Aug 29 19:51:53.504: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 29 19:51:53.507: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Aug 29 19:51:53.507: INFO: Breadth first check of 172.20.17.71 on host 10.45.35.202...
    Aug 29 19:51:53.510: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.17.71&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:51:53.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:51:53.511: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:51:53.511: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.17.71%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:51:53.605: INFO: Waiting for responses: map[]
    Aug 29 19:51:53.605: INFO: reached 172.20.17.71 after 0/1 tries
    Aug 29 19:51:53.605: INFO: Breadth first check of 172.20.76.137 on host 10.45.35.204...
    Aug 29 19:51:53.608: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.76.137&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:51:53.608: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:51:53.609: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:51:53.609: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.76.137%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:51:53.698: INFO: Waiting for responses: map[]
    Aug 29 19:51:53.698: INFO: reached 172.20.76.137 after 0/1 tries
    Aug 29 19:51:53.698: INFO: Breadth first check of 172.20.143.212 on host 10.45.35.198...
    Aug 29 19:51:53.703: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.143.212&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:51:53.703: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:51:53.704: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:51:53.704: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.143.212%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:51:53.781: INFO: Waiting for responses: map[]
    Aug 29 19:51:53.781: INFO: reached 172.20.143.212 after 0/1 tries
    Aug 29 19:51:53.781: INFO: Breadth first check of 172.20.30.144 on host 10.45.35.206...
    Aug 29 19:51:53.785: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.30.144&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:51:53.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:51:53.785: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:51:53.785: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.30.144%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:51:53.871: INFO: Waiting for responses: map[]
    Aug 29 19:51:53.871: INFO: reached 172.20.30.144 after 0/1 tries
    Aug 29 19:51:53.871: INFO: Breadth first check of 172.20.84.161 on host 10.45.35.199...
    Aug 29 19:51:53.875: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.20.30.145:9080/dial?request=hostname&protocol=udp&host=172.20.84.161&port=8081&tries=1'] Namespace:pod-network-test-7602 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 19:51:53.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 19:51:53.875: INFO: ExecWithOptions: Clientset creation
    Aug 29 19:51:53.876: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7602/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.20.30.145%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.20.84.161%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 29 19:51:53.962: INFO: Waiting for responses: map[]
    Aug 29 19:51:53.962: INFO: reached 172.20.84.161 after 0/1 tries
    Aug 29 19:51:53.962: INFO: Going to retry 0 out of 5 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:51:53.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7602" for this suite. 08/29/23 19:51:53.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:51:53.975
Aug 29 19:51:53.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename subpath 08/29/23 19:51:53.976
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:53.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:53.999
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/29/23 19:51:54.002
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-vbzf 08/29/23 19:51:54.014
STEP: Creating a pod to test atomic-volume-subpath 08/29/23 19:51:54.015
Aug 29 19:51:54.026: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vbzf" in namespace "subpath-5523" to be "Succeeded or Failed"
Aug 29 19:51:54.030: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900095ms
Aug 29 19:51:56.034: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008175972s
Aug 29 19:51:58.038: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 4.012134456s
Aug 29 19:52:00.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 6.00883472s
Aug 29 19:52:02.036: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 8.010587747s
Aug 29 19:52:04.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 10.008807584s
Aug 29 19:52:06.034: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 12.008634382s
Aug 29 19:52:08.037: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 14.011143061s
Aug 29 19:52:10.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 16.008982451s
Aug 29 19:52:12.036: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 18.010466386s
Aug 29 19:52:14.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 20.009259867s
Aug 29 19:52:16.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=false. Elapsed: 22.009542037s
Aug 29 19:52:18.036: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010135591s
STEP: Saw pod success 08/29/23 19:52:18.036
Aug 29 19:52:18.036: INFO: Pod "pod-subpath-test-configmap-vbzf" satisfied condition "Succeeded or Failed"
Aug 29 19:52:18.040: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-configmap-vbzf container test-container-subpath-configmap-vbzf: <nil>
STEP: delete the pod 08/29/23 19:52:18.048
Aug 29 19:52:18.065: INFO: Waiting for pod pod-subpath-test-configmap-vbzf to disappear
Aug 29 19:52:18.068: INFO: Pod pod-subpath-test-configmap-vbzf no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vbzf 08/29/23 19:52:18.068
Aug 29 19:52:18.068: INFO: Deleting pod "pod-subpath-test-configmap-vbzf" in namespace "subpath-5523"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 29 19:52:18.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5523" for this suite. 08/29/23 19:52:18.077
------------------------------
• [SLOW TEST] [24.109 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:51:53.975
    Aug 29 19:51:53.975: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename subpath 08/29/23 19:51:53.976
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:51:53.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:51:53.999
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/29/23 19:51:54.002
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-vbzf 08/29/23 19:51:54.014
    STEP: Creating a pod to test atomic-volume-subpath 08/29/23 19:51:54.015
    Aug 29 19:51:54.026: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vbzf" in namespace "subpath-5523" to be "Succeeded or Failed"
    Aug 29 19:51:54.030: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.900095ms
    Aug 29 19:51:56.034: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 2.008175972s
    Aug 29 19:51:58.038: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 4.012134456s
    Aug 29 19:52:00.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 6.00883472s
    Aug 29 19:52:02.036: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 8.010587747s
    Aug 29 19:52:04.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 10.008807584s
    Aug 29 19:52:06.034: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 12.008634382s
    Aug 29 19:52:08.037: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 14.011143061s
    Aug 29 19:52:10.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 16.008982451s
    Aug 29 19:52:12.036: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 18.010466386s
    Aug 29 19:52:14.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=true. Elapsed: 20.009259867s
    Aug 29 19:52:16.035: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Running", Reason="", readiness=false. Elapsed: 22.009542037s
    Aug 29 19:52:18.036: INFO: Pod "pod-subpath-test-configmap-vbzf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.010135591s
    STEP: Saw pod success 08/29/23 19:52:18.036
    Aug 29 19:52:18.036: INFO: Pod "pod-subpath-test-configmap-vbzf" satisfied condition "Succeeded or Failed"
    Aug 29 19:52:18.040: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-configmap-vbzf container test-container-subpath-configmap-vbzf: <nil>
    STEP: delete the pod 08/29/23 19:52:18.048
    Aug 29 19:52:18.065: INFO: Waiting for pod pod-subpath-test-configmap-vbzf to disappear
    Aug 29 19:52:18.068: INFO: Pod pod-subpath-test-configmap-vbzf no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-vbzf 08/29/23 19:52:18.068
    Aug 29 19:52:18.068: INFO: Deleting pod "pod-subpath-test-configmap-vbzf" in namespace "subpath-5523"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:52:18.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5523" for this suite. 08/29/23 19:52:18.077
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:52:18.084
Aug 29 19:52:18.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubelet-test 08/29/23 19:52:18.085
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:52:18.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:52:18.108
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 29 19:52:18.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-29" for this suite. 08/29/23 19:52:18.144
------------------------------
• [0.066 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:52:18.084
    Aug 29 19:52:18.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubelet-test 08/29/23 19:52:18.085
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:52:18.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:52:18.108
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:52:18.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-29" for this suite. 08/29/23 19:52:18.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:52:18.152
Aug 29 19:52:18.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 19:52:18.153
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:52:18.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:52:18.175
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 08/29/23 19:52:18.178
Aug 29 19:52:18.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5" in namespace "downward-api-5129" to be "Succeeded or Failed"
Aug 29 19:52:18.191: INFO: Pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.668319ms
Aug 29 19:52:20.198: INFO: Pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010469777s
Aug 29 19:52:22.198: INFO: Pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009951787s
STEP: Saw pod success 08/29/23 19:52:22.198
Aug 29 19:52:22.198: INFO: Pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5" satisfied condition "Succeeded or Failed"
Aug 29 19:52:22.202: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5 container client-container: <nil>
STEP: delete the pod 08/29/23 19:52:22.212
Aug 29 19:52:22.230: INFO: Waiting for pod downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5 to disappear
Aug 29 19:52:22.233: INFO: Pod downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 19:52:22.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5129" for this suite. 08/29/23 19:52:22.239
------------------------------
• [4.096 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:52:18.152
    Aug 29 19:52:18.152: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 19:52:18.153
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:52:18.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:52:18.175
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 08/29/23 19:52:18.178
    Aug 29 19:52:18.188: INFO: Waiting up to 5m0s for pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5" in namespace "downward-api-5129" to be "Succeeded or Failed"
    Aug 29 19:52:18.191: INFO: Pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.668319ms
    Aug 29 19:52:20.198: INFO: Pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010469777s
    Aug 29 19:52:22.198: INFO: Pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009951787s
    STEP: Saw pod success 08/29/23 19:52:22.198
    Aug 29 19:52:22.198: INFO: Pod "downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5" satisfied condition "Succeeded or Failed"
    Aug 29 19:52:22.202: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5 container client-container: <nil>
    STEP: delete the pod 08/29/23 19:52:22.212
    Aug 29 19:52:22.230: INFO: Waiting for pod downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5 to disappear
    Aug 29 19:52:22.233: INFO: Pod downwardapi-volume-81a4930b-d8f2-482e-b89c-dda03595a1f5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:52:22.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5129" for this suite. 08/29/23 19:52:22.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:52:22.251
Aug 29 19:52:22.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 19:52:22.252
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:52:22.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:52:22.272
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-mz6mr" 08/29/23 19:52:22.278
Aug 29 19:52:22.312: INFO: Resource quota "e2e-rq-status-mz6mr" reports spec: hard cpu limit of 500m
Aug 29 19:52:22.312: INFO: Resource quota "e2e-rq-status-mz6mr" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-mz6mr" /status 08/29/23 19:52:22.312
STEP: Confirm /status for "e2e-rq-status-mz6mr" resourceQuota via watch 08/29/23 19:52:22.341
Aug 29 19:52:22.343: INFO: observed resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList(nil)
Aug 29 19:52:22.343: INFO: Found resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 29 19:52:22.343: INFO: ResourceQuota "e2e-rq-status-mz6mr" /status was updated
STEP: Patching hard spec values for cpu & memory 08/29/23 19:52:22.346
Aug 29 19:52:22.353: INFO: Resource quota "e2e-rq-status-mz6mr" reports spec: hard cpu limit of 1
Aug 29 19:52:22.353: INFO: Resource quota "e2e-rq-status-mz6mr" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-mz6mr" /status 08/29/23 19:52:22.353
STEP: Confirm /status for "e2e-rq-status-mz6mr" resourceQuota via watch 08/29/23 19:52:22.362
Aug 29 19:52:22.363: INFO: observed resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Aug 29 19:52:22.364: INFO: Found resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 29 19:52:22.364: INFO: ResourceQuota "e2e-rq-status-mz6mr" /status was patched
STEP: Get "e2e-rq-status-mz6mr" /status 08/29/23 19:52:22.364
Aug 29 19:52:22.368: INFO: Resourcequota "e2e-rq-status-mz6mr" reports status: hard cpu of 1
Aug 29 19:52:22.368: INFO: Resourcequota "e2e-rq-status-mz6mr" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-mz6mr" /status before checking Spec is unchanged 08/29/23 19:52:22.371
Aug 29 19:52:22.382: INFO: Resourcequota "e2e-rq-status-mz6mr" reports status: hard cpu of 2
Aug 29 19:52:22.382: INFO: Resourcequota "e2e-rq-status-mz6mr" reports status: hard memory of 2Gi
Aug 29 19:52:22.383: INFO: observed resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Aug 29 19:52:22.384: INFO: Found resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Aug 29 19:56:17.394: INFO: ResourceQuota "e2e-rq-status-mz6mr" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 19:56:17.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3234" for this suite. 08/29/23 19:56:17.399
------------------------------
• [SLOW TEST] [235.157 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:52:22.251
    Aug 29 19:52:22.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 19:52:22.252
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:52:22.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:52:22.272
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-mz6mr" 08/29/23 19:52:22.278
    Aug 29 19:52:22.312: INFO: Resource quota "e2e-rq-status-mz6mr" reports spec: hard cpu limit of 500m
    Aug 29 19:52:22.312: INFO: Resource quota "e2e-rq-status-mz6mr" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-mz6mr" /status 08/29/23 19:52:22.312
    STEP: Confirm /status for "e2e-rq-status-mz6mr" resourceQuota via watch 08/29/23 19:52:22.341
    Aug 29 19:52:22.343: INFO: observed resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList(nil)
    Aug 29 19:52:22.343: INFO: Found resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 29 19:52:22.343: INFO: ResourceQuota "e2e-rq-status-mz6mr" /status was updated
    STEP: Patching hard spec values for cpu & memory 08/29/23 19:52:22.346
    Aug 29 19:52:22.353: INFO: Resource quota "e2e-rq-status-mz6mr" reports spec: hard cpu limit of 1
    Aug 29 19:52:22.353: INFO: Resource quota "e2e-rq-status-mz6mr" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-mz6mr" /status 08/29/23 19:52:22.353
    STEP: Confirm /status for "e2e-rq-status-mz6mr" resourceQuota via watch 08/29/23 19:52:22.362
    Aug 29 19:52:22.363: INFO: observed resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Aug 29 19:52:22.364: INFO: Found resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 29 19:52:22.364: INFO: ResourceQuota "e2e-rq-status-mz6mr" /status was patched
    STEP: Get "e2e-rq-status-mz6mr" /status 08/29/23 19:52:22.364
    Aug 29 19:52:22.368: INFO: Resourcequota "e2e-rq-status-mz6mr" reports status: hard cpu of 1
    Aug 29 19:52:22.368: INFO: Resourcequota "e2e-rq-status-mz6mr" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-mz6mr" /status before checking Spec is unchanged 08/29/23 19:52:22.371
    Aug 29 19:52:22.382: INFO: Resourcequota "e2e-rq-status-mz6mr" reports status: hard cpu of 2
    Aug 29 19:52:22.382: INFO: Resourcequota "e2e-rq-status-mz6mr" reports status: hard memory of 2Gi
    Aug 29 19:52:22.383: INFO: observed resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Aug 29 19:52:22.384: INFO: Found resourceQuota "e2e-rq-status-mz6mr" in namespace "resourcequota-3234" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Aug 29 19:56:17.394: INFO: ResourceQuota "e2e-rq-status-mz6mr" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:56:17.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3234" for this suite. 08/29/23 19:56:17.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:56:17.409
Aug 29 19:56:17.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replication-controller 08/29/23 19:56:17.41
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:17.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:17.436
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77 08/29/23 19:56:17.439
Aug 29 19:56:17.463: INFO: Pod name my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77: Found 0 pods out of 1
Aug 29 19:56:22.474: INFO: Pod name my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77: Found 1 pods out of 1
Aug 29 19:56:22.474: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77" are running
Aug 29 19:56:22.474: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq" in namespace "replication-controller-6233" to be "running"
Aug 29 19:56:22.478: INFO: Pod "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq": Phase="Running", Reason="", readiness=true. Elapsed: 3.745149ms
Aug 29 19:56:22.478: INFO: Pod "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq" satisfied condition "running"
Aug 29 19:56:22.478: INFO: Pod "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 19:56:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 19:56:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 19:56:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 19:56:17 +0000 UTC Reason: Message:}])
Aug 29 19:56:22.478: INFO: Trying to dial the pod
Aug 29 19:56:27.493: INFO: Controller my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77: Got expected result from replica 1 [my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq]: "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 29 19:56:27.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-6233" for this suite. 08/29/23 19:56:27.498
------------------------------
• [SLOW TEST] [10.095 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:56:17.409
    Aug 29 19:56:17.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replication-controller 08/29/23 19:56:17.41
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:17.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:17.436
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77 08/29/23 19:56:17.439
    Aug 29 19:56:17.463: INFO: Pod name my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77: Found 0 pods out of 1
    Aug 29 19:56:22.474: INFO: Pod name my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77: Found 1 pods out of 1
    Aug 29 19:56:22.474: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77" are running
    Aug 29 19:56:22.474: INFO: Waiting up to 5m0s for pod "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq" in namespace "replication-controller-6233" to be "running"
    Aug 29 19:56:22.478: INFO: Pod "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq": Phase="Running", Reason="", readiness=true. Elapsed: 3.745149ms
    Aug 29 19:56:22.478: INFO: Pod "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq" satisfied condition "running"
    Aug 29 19:56:22.478: INFO: Pod "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 19:56:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 19:56:19 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 19:56:19 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 19:56:17 +0000 UTC Reason: Message:}])
    Aug 29 19:56:22.478: INFO: Trying to dial the pod
    Aug 29 19:56:27.493: INFO: Controller my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77: Got expected result from replica 1 [my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq]: "my-hostname-basic-e5e1092d-a078-4355-8aca-7958fd7d7b77-278gq", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:56:27.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-6233" for this suite. 08/29/23 19:56:27.498
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:56:27.506
Aug 29 19:56:27.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename deployment 08/29/23 19:56:27.507
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:27.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:27.53
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 29 19:56:27.533: INFO: Creating deployment "webserver-deployment"
Aug 29 19:56:27.544: INFO: Waiting for observed generation 1
Aug 29 19:56:29.555: INFO: Waiting for all required pods to come up
Aug 29 19:56:29.561: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/29/23 19:56:29.561
Aug 29 19:56:29.561: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jxvz6" in namespace "deployment-4075" to be "running"
Aug 29 19:56:29.564: INFO: Pod "webserver-deployment-7f5969cbc7-jxvz6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435699ms
Aug 29 19:56:31.570: INFO: Pod "webserver-deployment-7f5969cbc7-jxvz6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008907692s
Aug 29 19:56:31.570: INFO: Pod "webserver-deployment-7f5969cbc7-jxvz6" satisfied condition "running"
Aug 29 19:56:31.570: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 29 19:56:31.578: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 29 19:56:31.588: INFO: Updating deployment webserver-deployment
Aug 29 19:56:31.588: INFO: Waiting for observed generation 2
Aug 29 19:56:33.599: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 29 19:56:33.602: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 29 19:56:33.605: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 29 19:56:33.616: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 29 19:56:33.616: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 29 19:56:33.620: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 29 19:56:33.625: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 29 19:56:33.625: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 29 19:56:33.637: INFO: Updating deployment webserver-deployment
Aug 29 19:56:33.637: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 29 19:56:33.649: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 29 19:56:33.655: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 19:56:33.663: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-4075  3026380a-99b4-4787-9f49-1fa88aed725d 14088 3 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b67638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-29 19:56:29 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-29 19:56:31 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 29 19:56:33.675: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4075  663454a4-35b4-4dbd-acf6-d6336ede0fdc 14091 3 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3026380a-99b4-4787-9f49-1fa88aed725d 0xc002b67b67 0xc002b67b68}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3026380a-99b4-4787-9f49-1fa88aed725d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b67c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 19:56:33.675: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 29 19:56:33.675: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4075  ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 14089 3 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3026380a-99b4-4787-9f49-1fa88aed725d 0xc002b67a77 0xc002b67a78}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3026380a-99b4-4787-9f49-1fa88aed725d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b67b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-2t9cx" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2t9cx webserver-deployment-7f5969cbc7- deployment-4075  d181e22d-ef6a-4a20-b28b-f92467002bdc 13942 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:806d785a426a6e060cfaadb6b501b7e16c84997b3bc1c2d28b07e1b0a876f691 cni.projectcalico.org/podIP:172.20.17.72/32 cni.projectcalico.org/podIPs:172.20.17.72/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0031a3c77 0xc0031a3c78}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.17.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2s6dv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2s6dv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.202,PodIP:172.20.17.72,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://36ff9cb70e2da130468abcdfb1cc4a0cfb10d6683adcdd4fc60d89be57b3109a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.17.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-5bt5s" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5bt5s webserver-deployment-7f5969cbc7- deployment-4075  d242286a-1433-4c38-8b9a-05766a4897c6 13940 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6020c8e42282a904f1345978ca260200ff656bf316386f20459fb5c2e34a58f6 cni.projectcalico.org/podIP:172.20.17.73/32 cni.projectcalico.org/podIPs:172.20.17.73/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0031a3e97 0xc0031a3e98}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.17.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztqdc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztqdc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.202,PodIP:172.20.17.73,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ac6dcbc7bd29c58f7af13b9358d7ea2f422a23d6e6daaa424cddf4dc23e99370,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.17.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-9mdx9" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9mdx9 webserver-deployment-7f5969cbc7- deployment-4075  97e542bf-54b4-426e-b7b4-2b95dd018502 13947 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5545b7252eb46a6655b13ac93c41943d676636944e440a62eee9b5248ff62fad cni.projectcalico.org/podIP:172.20.76.139/32 cni.projectcalico.org/podIPs:172.20.76.139/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e80b7 0xc0043e80b8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.76.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwncx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwncx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.204,PodIP:172.20.76.139,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1cd66e7fb5797d0f3ebf6e47f609a24149a3d760929dc735bd796c60348b5d0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.76.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-jxvz6" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jxvz6 webserver-deployment-7f5969cbc7- deployment-4075  51a7355b-aa71-4a79-9665-f125e341eae0 13969 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f75b5a5cb42f72b261aa08ad7226efe8ec3e5a3da50cee7dc0f7328a4a051e3c cni.projectcalico.org/podIP:172.20.84.163/32 cni.projectcalico.org/podIPs:172.20.84.163/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e82d7 0xc0043e82d8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvzqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvzqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.163,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4e7583801215d7175cacc95abbd7f68c0b5113b24308d51a1e05766988788978,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-p2727" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p2727 webserver-deployment-7f5969cbc7- deployment-4075  9454f915-4f9d-46ac-a5aa-f6a810bfa910 14097 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e84d7 0xc0043e84d8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mf7qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mf7qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-q4ssr" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q4ssr webserver-deployment-7f5969cbc7- deployment-4075  6fac0dc1-0138-411f-ad71-dc397fd48409 13955 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:97e8737790fe5c11669612e6653f33a1583d39f6de8cb7bb514c45127089912d cni.projectcalico.org/podIP:172.20.143.213/32 cni.projectcalico.org/podIPs:172.20.143.213/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8617 0xc0043e8618}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkbbf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkbbf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.198,PodIP:172.20.143.213,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7fa0977ed5d0b65b07bad127ac2788ae2bbe83c5eccf1f5d619b139667c87aba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.143.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-qfmtn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qfmtn webserver-deployment-7f5969cbc7- deployment-4075  ba5ba630-aa44-496c-8256-acf87888c2c5 13949 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:db3b100790c1ad55b7d2d03650ace45fdbde8ee7f2d12d872df3b6064cef631e cni.projectcalico.org/podIP:172.20.76.138/32 cni.projectcalico.org/podIPs:172.20.76.138/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8837 0xc0043e8838}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.76.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zbq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zbq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.204,PodIP:172.20.76.138,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://07b9f90f58371a5b65d82340496438a674fc33a4e086ffe3dfa934b02ca67d9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.76.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-qs826" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qs826 webserver-deployment-7f5969cbc7- deployment-4075  338acda3-3204-497a-a672-0c5e6f7e3dbb 14095 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8a37 0xc0043e8a38}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48x7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48x7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-qsb7v" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qsb7v webserver-deployment-7f5969cbc7- deployment-4075  21431d17-5f1f-493d-8a5d-e2b15aee7f04 14093 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8b77 0xc0043e8b78}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f82cz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f82cz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-rwndn" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rwndn webserver-deployment-7f5969cbc7- deployment-4075  a9fca234-368d-4052-809f-a92df7ded2ed 13953 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5f26719d7924504a8af9674211ab6e0561ddae698c918f774de778726523cda8 cni.projectcalico.org/podIP:172.20.143.214/32 cni.projectcalico.org/podIPs:172.20.143.214/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8cd0 0xc0043e8cd1}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p86dh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p86dh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.198,PodIP:172.20.143.214,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://584ed552189620189267624dfb2f5dd4838f14929c7a58cc5bd0e46c8658903c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.143.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-7f5969cbc7-zrzk5" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zrzk5 webserver-deployment-7f5969cbc7- deployment-4075  1a479eeb-6471-423b-af23-3777ce614740 13921 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ea6b5c2e3c6eb2e86c7ff99e01cc2aada26010e1d0b5f8718af34752864be1e3 cni.projectcalico.org/podIP:172.20.84.162/32 cni.projectcalico.org/podIPs:172.20.84.162/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8ee7 0xc0043e8ee8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zn2mh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zn2mh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.162,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a77fe81000978ad1215b80f7749e1b084cb6ed00a15808cadb22e93c58a6e727,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-5bt9b" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5bt9b webserver-deployment-d9f79cb5- deployment-4075  193a5569-0a18-4ea5-9f37-58b7fbb3231c 14078 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9455e7ad758f80b46212b1e5b1043457cac43938cfd01062dc0a4b86199f59b4 cni.projectcalico.org/podIP:172.20.143.215/32 cni.projectcalico.org/podIPs:172.20.143.215/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e90e7 0xc0043e90e8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pb6fp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pb6fp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.198,PodIP:172.20.143.215,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.143.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-6wpvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6wpvj webserver-deployment-d9f79cb5- deployment-4075  c08446b9-cc66-412c-8918-1bfe3c34b23f 14102 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9317 0xc0043e9318}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4q5g4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4q5g4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-7vd5j" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7vd5j webserver-deployment-d9f79cb5- deployment-4075  1f52d416-cfd7-4182-80d2-45868af91f7d 14083 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e9cc2d4bf3861e590e84da94801bdc8768d939661357a211a3cfb10c88f2adcf cni.projectcalico.org/podIP:172.20.30.151/32 cni.projectcalico.org/podIPs:172.20.30.151/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9487 0xc0043e9488}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72lwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72lwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.151,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-bqbl5" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bqbl5 webserver-deployment-d9f79cb5- deployment-4075  40a4a10f-769b-43ef-a812-7c08ca55b451 14099 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e96b7 0xc0043e96b8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grtcx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grtcx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-fxgrp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fxgrp webserver-deployment-d9f79cb5- deployment-4075  433c65b6-1fed-485e-ba3a-cf43cac5131f 14044 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:52a084d97756ec0b3b36afcae8ad922ce531c6ed6cb0d98d25c5119f2e2df53f cni.projectcalico.org/podIP:172.20.17.74/32 cni.projectcalico.org/podIPs:172.20.17.74/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9827 0xc0043e9828}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqm9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqm9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.202,PodIP:,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.688: INFO: Pod "webserver-deployment-d9f79cb5-jnnpm" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jnnpm webserver-deployment-d9f79cb5- deployment-4075  b54ed08e-81b2-4bc2-910f-5273e0d21e61 14094 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9a27 0xc0043e9a28}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gffb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gffb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.688: INFO: Pod "webserver-deployment-d9f79cb5-phd26" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-phd26 webserver-deployment-d9f79cb5- deployment-4075  a5577e8e-11e6-46eb-b8ed-501c59b38a01 14076 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ab6416d16366ca0c7b4389a48fc81e19c139fac01372fae0d0c5b6c262cbbdca cni.projectcalico.org/podIP:172.20.76.140/32 cni.projectcalico.org/podIPs:172.20.76.140/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9b97 0xc0043e9b98}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.76.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dztqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dztqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.204,PodIP:172.20.76.140,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.76.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 29 19:56:33.688: INFO: Pod "webserver-deployment-d9f79cb5-vdh5j" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vdh5j webserver-deployment-d9f79cb5- deployment-4075  463ad0a2-6a14-4d32-8ef6-da1f3c194fa9 14086 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:98e82d6bc1d0b834959430592615939d925d6de34708dcef07c4c950c795c75e cni.projectcalico.org/podIP:172.20.84.164/32 cni.projectcalico.org/podIPs:172.20.84.164/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9de7 0xc0043e9de8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-df78j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-df78j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.164,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 29 19:56:33.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4075" for this suite. 08/29/23 19:56:33.705
------------------------------
• [SLOW TEST] [6.218 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:56:27.506
    Aug 29 19:56:27.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename deployment 08/29/23 19:56:27.507
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:27.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:27.53
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 29 19:56:27.533: INFO: Creating deployment "webserver-deployment"
    Aug 29 19:56:27.544: INFO: Waiting for observed generation 1
    Aug 29 19:56:29.555: INFO: Waiting for all required pods to come up
    Aug 29 19:56:29.561: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/29/23 19:56:29.561
    Aug 29 19:56:29.561: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-jxvz6" in namespace "deployment-4075" to be "running"
    Aug 29 19:56:29.564: INFO: Pod "webserver-deployment-7f5969cbc7-jxvz6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435699ms
    Aug 29 19:56:31.570: INFO: Pod "webserver-deployment-7f5969cbc7-jxvz6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008907692s
    Aug 29 19:56:31.570: INFO: Pod "webserver-deployment-7f5969cbc7-jxvz6" satisfied condition "running"
    Aug 29 19:56:31.570: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 29 19:56:31.578: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 29 19:56:31.588: INFO: Updating deployment webserver-deployment
    Aug 29 19:56:31.588: INFO: Waiting for observed generation 2
    Aug 29 19:56:33.599: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 29 19:56:33.602: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 29 19:56:33.605: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 29 19:56:33.616: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 29 19:56:33.616: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 29 19:56:33.620: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 29 19:56:33.625: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 29 19:56:33.625: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 29 19:56:33.637: INFO: Updating deployment webserver-deployment
    Aug 29 19:56:33.637: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 29 19:56:33.649: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 29 19:56:33.655: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 29 19:56:33.663: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-4075  3026380a-99b4-4787-9f49-1fa88aed725d 14088 3 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b67638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-29 19:56:29 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-08-29 19:56:31 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 29 19:56:33.675: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-4075  663454a4-35b4-4dbd-acf6-d6336ede0fdc 14091 3 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 3026380a-99b4-4787-9f49-1fa88aed725d 0xc002b67b67 0xc002b67b68}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3026380a-99b4-4787-9f49-1fa88aed725d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b67c18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 19:56:33.675: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 29 19:56:33.675: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-4075  ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 14089 3 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 3026380a-99b4-4787-9f49-1fa88aed725d 0xc002b67a77 0xc002b67a78}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3026380a-99b4-4787-9f49-1fa88aed725d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b67b08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-2t9cx" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-2t9cx webserver-deployment-7f5969cbc7- deployment-4075  d181e22d-ef6a-4a20-b28b-f92467002bdc 13942 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:806d785a426a6e060cfaadb6b501b7e16c84997b3bc1c2d28b07e1b0a876f691 cni.projectcalico.org/podIP:172.20.17.72/32 cni.projectcalico.org/podIPs:172.20.17.72/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0031a3c77 0xc0031a3c78}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.17.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2s6dv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2s6dv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.202,PodIP:172.20.17.72,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://36ff9cb70e2da130468abcdfb1cc4a0cfb10d6683adcdd4fc60d89be57b3109a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.17.72,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-5bt5s" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5bt5s webserver-deployment-7f5969cbc7- deployment-4075  d242286a-1433-4c38-8b9a-05766a4897c6 13940 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:6020c8e42282a904f1345978ca260200ff656bf316386f20459fb5c2e34a58f6 cni.projectcalico.org/podIP:172.20.17.73/32 cni.projectcalico.org/podIPs:172.20.17.73/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0031a3e97 0xc0031a3e98}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.17.73\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ztqdc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ztqdc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.202,PodIP:172.20.17.73,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ac6dcbc7bd29c58f7af13b9358d7ea2f422a23d6e6daaa424cddf4dc23e99370,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.17.73,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-9mdx9" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-9mdx9 webserver-deployment-7f5969cbc7- deployment-4075  97e542bf-54b4-426e-b7b4-2b95dd018502 13947 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5545b7252eb46a6655b13ac93c41943d676636944e440a62eee9b5248ff62fad cni.projectcalico.org/podIP:172.20.76.139/32 cni.projectcalico.org/podIPs:172.20.76.139/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e80b7 0xc0043e80b8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.76.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vwncx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vwncx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.204,PodIP:172.20.76.139,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1cd66e7fb5797d0f3ebf6e47f609a24149a3d760929dc735bd796c60348b5d0b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.76.139,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-jxvz6" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-jxvz6 webserver-deployment-7f5969cbc7- deployment-4075  51a7355b-aa71-4a79-9665-f125e341eae0 13969 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:f75b5a5cb42f72b261aa08ad7226efe8ec3e5a3da50cee7dc0f7328a4a051e3c cni.projectcalico.org/podIP:172.20.84.163/32 cni.projectcalico.org/podIPs:172.20.84.163/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e82d7 0xc0043e82d8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zvzqv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zvzqv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.163,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://4e7583801215d7175cacc95abbd7f68c0b5113b24308d51a1e05766988788978,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.163,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.685: INFO: Pod "webserver-deployment-7f5969cbc7-p2727" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-p2727 webserver-deployment-7f5969cbc7- deployment-4075  9454f915-4f9d-46ac-a5aa-f6a810bfa910 14097 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e84d7 0xc0043e84d8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mf7qd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mf7qd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-q4ssr" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q4ssr webserver-deployment-7f5969cbc7- deployment-4075  6fac0dc1-0138-411f-ad71-dc397fd48409 13955 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:97e8737790fe5c11669612e6653f33a1583d39f6de8cb7bb514c45127089912d cni.projectcalico.org/podIP:172.20.143.213/32 cni.projectcalico.org/podIPs:172.20.143.213/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8617 0xc0043e8618}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkbbf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkbbf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.198,PodIP:172.20.143.213,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7fa0977ed5d0b65b07bad127ac2788ae2bbe83c5eccf1f5d619b139667c87aba,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.143.213,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-qfmtn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qfmtn webserver-deployment-7f5969cbc7- deployment-4075  ba5ba630-aa44-496c-8256-acf87888c2c5 13949 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:db3b100790c1ad55b7d2d03650ace45fdbde8ee7f2d12d872df3b6064cef631e cni.projectcalico.org/podIP:172.20.76.138/32 cni.projectcalico.org/podIPs:172.20.76.138/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8837 0xc0043e8838}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.76.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zbq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zbq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.204,PodIP:172.20.76.138,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://07b9f90f58371a5b65d82340496438a674fc33a4e086ffe3dfa934b02ca67d9b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.76.138,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-qs826" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qs826 webserver-deployment-7f5969cbc7- deployment-4075  338acda3-3204-497a-a672-0c5e6f7e3dbb 14095 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8a37 0xc0043e8a38}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48x7j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48x7j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-qsb7v" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-qsb7v webserver-deployment-7f5969cbc7- deployment-4075  21431d17-5f1f-493d-8a5d-e2b15aee7f04 14093 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8b77 0xc0043e8b78}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f82cz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f82cz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.686: INFO: Pod "webserver-deployment-7f5969cbc7-rwndn" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-rwndn webserver-deployment-7f5969cbc7- deployment-4075  a9fca234-368d-4052-809f-a92df7ded2ed 13953 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:5f26719d7924504a8af9674211ab6e0561ddae698c918f774de778726523cda8 cni.projectcalico.org/podIP:172.20.143.214/32 cni.projectcalico.org/podIPs:172.20.143.214/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8cd0 0xc0043e8cd1}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:29 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p86dh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p86dh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.198,PodIP:172.20.143.214,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://584ed552189620189267624dfb2f5dd4838f14929c7a58cc5bd0e46c8658903c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.143.214,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-7f5969cbc7-zrzk5" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zrzk5 webserver-deployment-7f5969cbc7- deployment-4075  1a479eeb-6471-423b-af23-3777ce614740 13921 0 2023-08-29 19:56:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[cni.projectcalico.org/containerID:ea6b5c2e3c6eb2e86c7ff99e01cc2aada26010e1d0b5f8718af34752864be1e3 cni.projectcalico.org/podIP:172.20.84.162/32 cni.projectcalico.org/podIPs:172.20.84.162/32] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 ff9c3892-9b82-47e3-b86e-e3f6ffd012d0 0xc0043e8ee7 0xc0043e8ee8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff9c3892-9b82-47e3-b86e-e3f6ffd012d0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zn2mh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zn2mh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.162,StartTime:2023-08-29 19:56:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:56:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a77fe81000978ad1215b80f7749e1b084cb6ed00a15808cadb22e93c58a6e727,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-5bt9b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-5bt9b webserver-deployment-d9f79cb5- deployment-4075  193a5569-0a18-4ea5-9f37-58b7fbb3231c 14078 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:9455e7ad758f80b46212b1e5b1043457cac43938cfd01062dc0a4b86199f59b4 cni.projectcalico.org/podIP:172.20.143.215/32 cni.projectcalico.org/podIPs:172.20.143.215/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e90e7 0xc0043e90e8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.215\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pb6fp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pb6fp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.198,PodIP:172.20.143.215,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.143.215,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-6wpvj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-6wpvj webserver-deployment-d9f79cb5- deployment-4075  c08446b9-cc66-412c-8918-1bfe3c34b23f 14102 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9317 0xc0043e9318}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4q5g4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4q5g4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-7vd5j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7vd5j webserver-deployment-d9f79cb5- deployment-4075  1f52d416-cfd7-4182-80d2-45868af91f7d 14083 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:e9cc2d4bf3861e590e84da94801bdc8768d939661357a211a3cfb10c88f2adcf cni.projectcalico.org/podIP:172.20.30.151/32 cni.projectcalico.org/podIPs:172.20.30.151/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9487 0xc0043e9488}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.151\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72lwx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72lwx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.151,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.151,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-bqbl5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-bqbl5 webserver-deployment-d9f79cb5- deployment-4075  40a4a10f-769b-43ef-a812-7c08ca55b451 14099 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e96b7 0xc0043e96b8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grtcx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grtcx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.687: INFO: Pod "webserver-deployment-d9f79cb5-fxgrp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-fxgrp webserver-deployment-d9f79cb5- deployment-4075  433c65b6-1fed-485e-ba3a-cf43cac5131f 14044 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:52a084d97756ec0b3b36afcae8ad922ce531c6ed6cb0d98d25c5119f2e2df53f cni.projectcalico.org/podIP:172.20.17.74/32 cni.projectcalico.org/podIPs:172.20.17.74/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9827 0xc0043e9828}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nqm9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nqm9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.202,PodIP:,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.688: INFO: Pod "webserver-deployment-d9f79cb5-jnnpm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-jnnpm webserver-deployment-d9f79cb5- deployment-4075  b54ed08e-81b2-4bc2-910f-5273e0d21e61 14094 0 2023-08-29 19:56:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9a27 0xc0043e9a28}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gffb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gffb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.688: INFO: Pod "webserver-deployment-d9f79cb5-phd26" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-phd26 webserver-deployment-d9f79cb5- deployment-4075  a5577e8e-11e6-46eb-b8ed-501c59b38a01 14076 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:ab6416d16366ca0c7b4389a48fc81e19c139fac01372fae0d0c5b6c262cbbdca cni.projectcalico.org/podIP:172.20.76.140/32 cni.projectcalico.org/podIPs:172.20.76.140/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9b97 0xc0043e9b98}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.76.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dztqk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dztqk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-master-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.204,PodIP:172.20.76.140,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.76.140,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 29 19:56:33.688: INFO: Pod "webserver-deployment-d9f79cb5-vdh5j" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-vdh5j webserver-deployment-d9f79cb5- deployment-4075  463ad0a2-6a14-4d32-8ef6-da1f3c194fa9 14086 0 2023-08-29 19:56:31 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[cni.projectcalico.org/containerID:98e82d6bc1d0b834959430592615939d925d6de34708dcef07c4c950c795c75e cni.projectcalico.org/podIP:172.20.84.164/32 cni.projectcalico.org/podIPs:172.20.84.164/32] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 663454a4-35b4-4dbd-acf6-d6336ede0fdc 0xc0043e9de7 0xc0043e9de8}] [] [{kube-controller-manager Update v1 2023-08-29 19:56:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"663454a4-35b4-4dbd-acf6-d6336ede0fdc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:56:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:56:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-df78j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-df78j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:56:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.164,StartTime:2023-08-29 19:56:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.164,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:56:33.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4075" for this suite. 08/29/23 19:56:33.705
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:56:33.727
Aug 29 19:56:33.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 19:56:33.728
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:33.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:33.811
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 08/29/23 19:56:33.815
STEP: Getting a ResourceQuota 08/29/23 19:56:33.823
STEP: Updating a ResourceQuota 08/29/23 19:56:33.826
STEP: Verifying a ResourceQuota was modified 08/29/23 19:56:33.84
STEP: Deleting a ResourceQuota 08/29/23 19:56:33.843
STEP: Verifying the deleted ResourceQuota 08/29/23 19:56:33.852
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 19:56:33.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-6279" for this suite. 08/29/23 19:56:33.862
------------------------------
• [0.142 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:56:33.727
    Aug 29 19:56:33.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 19:56:33.728
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:33.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:33.811
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 08/29/23 19:56:33.815
    STEP: Getting a ResourceQuota 08/29/23 19:56:33.823
    STEP: Updating a ResourceQuota 08/29/23 19:56:33.826
    STEP: Verifying a ResourceQuota was modified 08/29/23 19:56:33.84
    STEP: Deleting a ResourceQuota 08/29/23 19:56:33.843
    STEP: Verifying the deleted ResourceQuota 08/29/23 19:56:33.852
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:56:33.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-6279" for this suite. 08/29/23 19:56:33.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:56:33.87
Aug 29 19:56:33.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 19:56:33.871
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:33.891
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:33.895
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 19:56:33.913
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:56:34.316
STEP: Deploying the webhook pod 08/29/23 19:56:34.324
STEP: Wait for the deployment to be ready 08/29/23 19:56:34.339
Aug 29 19:56:34.348: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 19:56:36.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 19, 56, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 56, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 19, 56, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 56, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/29/23 19:56:38.367
STEP: Verifying the service has paired with the endpoint 08/29/23 19:56:38.38
Aug 29 19:56:39.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Aug 29 19:56:39.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4682-crds.webhook.example.com via the AdmissionRegistration API 08/29/23 19:56:39.901
STEP: Creating a custom resource while v1 is storage version 08/29/23 19:56:39.923
STEP: Patching Custom Resource Definition to set v2 as storage 08/29/23 19:56:42.008
STEP: Patching the custom resource while v2 is storage version 08/29/23 19:56:42.029
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:56:42.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1475" for this suite. 08/29/23 19:56:42.708
STEP: Destroying namespace "webhook-1475-markers" for this suite. 08/29/23 19:56:42.715
------------------------------
• [SLOW TEST] [8.855 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:56:33.87
    Aug 29 19:56:33.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 19:56:33.871
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:33.891
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:33.895
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 19:56:33.913
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:56:34.316
    STEP: Deploying the webhook pod 08/29/23 19:56:34.324
    STEP: Wait for the deployment to be ready 08/29/23 19:56:34.339
    Aug 29 19:56:34.348: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 29 19:56:36.361: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 19, 56, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 56, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 19, 56, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 56, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/29/23 19:56:38.367
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:56:38.38
    Aug 29 19:56:39.381: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Aug 29 19:56:39.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4682-crds.webhook.example.com via the AdmissionRegistration API 08/29/23 19:56:39.901
    STEP: Creating a custom resource while v1 is storage version 08/29/23 19:56:39.923
    STEP: Patching Custom Resource Definition to set v2 as storage 08/29/23 19:56:42.008
    STEP: Patching the custom resource while v2 is storage version 08/29/23 19:56:42.029
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:56:42.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1475" for this suite. 08/29/23 19:56:42.708
    STEP: Destroying namespace "webhook-1475-markers" for this suite. 08/29/23 19:56:42.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:56:42.726
Aug 29 19:56:42.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 19:56:42.727
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:42.748
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:42.752
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-164429f3-d944-4754-ac76-c1092d1a012d 08/29/23 19:56:42.755
STEP: Creating a pod to test consume secrets 08/29/23 19:56:42.761
Aug 29 19:56:42.772: INFO: Waiting up to 5m0s for pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc" in namespace "secrets-7389" to be "Succeeded or Failed"
Aug 29 19:56:42.774: INFO: Pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.714224ms
Aug 29 19:56:44.781: INFO: Pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008932624s
Aug 29 19:56:46.781: INFO: Pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009293537s
STEP: Saw pod success 08/29/23 19:56:46.781
Aug 29 19:56:46.781: INFO: Pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc" satisfied condition "Succeeded or Failed"
Aug 29 19:56:46.786: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc container secret-volume-test: <nil>
STEP: delete the pod 08/29/23 19:56:46.808
Aug 29 19:56:46.820: INFO: Waiting for pod pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc to disappear
Aug 29 19:56:46.823: INFO: Pod pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 19:56:46.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7389" for this suite. 08/29/23 19:56:46.829
------------------------------
• [4.112 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:56:42.726
    Aug 29 19:56:42.726: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 19:56:42.727
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:42.748
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:42.752
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-164429f3-d944-4754-ac76-c1092d1a012d 08/29/23 19:56:42.755
    STEP: Creating a pod to test consume secrets 08/29/23 19:56:42.761
    Aug 29 19:56:42.772: INFO: Waiting up to 5m0s for pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc" in namespace "secrets-7389" to be "Succeeded or Failed"
    Aug 29 19:56:42.774: INFO: Pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.714224ms
    Aug 29 19:56:44.781: INFO: Pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008932624s
    Aug 29 19:56:46.781: INFO: Pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009293537s
    STEP: Saw pod success 08/29/23 19:56:46.781
    Aug 29 19:56:46.781: INFO: Pod "pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc" satisfied condition "Succeeded or Failed"
    Aug 29 19:56:46.786: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc container secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 19:56:46.808
    Aug 29 19:56:46.820: INFO: Waiting for pod pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc to disappear
    Aug 29 19:56:46.823: INFO: Pod pod-secrets-2a436375-38e7-4be6-be68-80ef11613afc no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:56:46.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7389" for this suite. 08/29/23 19:56:46.829
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:56:46.84
Aug 29 19:56:46.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 19:56:46.841
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:46.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:46.863
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-2739/configmap-test-7fdccaa6-1c42-49fd-9a6e-5bfdfc37e2c6 08/29/23 19:56:46.866
STEP: Creating a pod to test consume configMaps 08/29/23 19:56:46.873
Aug 29 19:56:46.882: INFO: Waiting up to 5m0s for pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79" in namespace "configmap-2739" to be "Succeeded or Failed"
Aug 29 19:56:46.885: INFO: Pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79": Phase="Pending", Reason="", readiness=false. Elapsed: 3.012817ms
Aug 29 19:56:48.891: INFO: Pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008708213s
Aug 29 19:56:50.891: INFO: Pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008478869s
STEP: Saw pod success 08/29/23 19:56:50.891
Aug 29 19:56:50.891: INFO: Pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79" satisfied condition "Succeeded or Failed"
Aug 29 19:56:50.894: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79 container env-test: <nil>
STEP: delete the pod 08/29/23 19:56:50.902
Aug 29 19:56:50.925: INFO: Waiting for pod pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79 to disappear
Aug 29 19:56:50.929: INFO: Pod pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 19:56:50.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2739" for this suite. 08/29/23 19:56:50.935
------------------------------
• [4.104 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:56:46.84
    Aug 29 19:56:46.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 19:56:46.841
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:46.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:46.863
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-2739/configmap-test-7fdccaa6-1c42-49fd-9a6e-5bfdfc37e2c6 08/29/23 19:56:46.866
    STEP: Creating a pod to test consume configMaps 08/29/23 19:56:46.873
    Aug 29 19:56:46.882: INFO: Waiting up to 5m0s for pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79" in namespace "configmap-2739" to be "Succeeded or Failed"
    Aug 29 19:56:46.885: INFO: Pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79": Phase="Pending", Reason="", readiness=false. Elapsed: 3.012817ms
    Aug 29 19:56:48.891: INFO: Pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008708213s
    Aug 29 19:56:50.891: INFO: Pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008478869s
    STEP: Saw pod success 08/29/23 19:56:50.891
    Aug 29 19:56:50.891: INFO: Pod "pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79" satisfied condition "Succeeded or Failed"
    Aug 29 19:56:50.894: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79 container env-test: <nil>
    STEP: delete the pod 08/29/23 19:56:50.902
    Aug 29 19:56:50.925: INFO: Waiting for pod pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79 to disappear
    Aug 29 19:56:50.929: INFO: Pod pod-configmaps-70211dd9-6b6e-421e-b57f-7b3426d54e79 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:56:50.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2739" for this suite. 08/29/23 19:56:50.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:56:50.944
Aug 29 19:56:50.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-preemption 08/29/23 19:56:50.945
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:50.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:50.972
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 29 19:56:50.997: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 19:57:51.046: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:57:51.05
Aug 29 19:57:51.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-preemption-path 08/29/23 19:57:51.051
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:57:51.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:57:51.074
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:771
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:814
Aug 29 19:57:51.091: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 29 19:57:51.095: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Aug 29 19:57:51.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:787
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:57:51.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5696" for this suite. 08/29/23 19:57:51.202
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-9302" for this suite. 08/29/23 19:57:51.21
------------------------------
• [SLOW TEST] [60.272 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:764
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:814

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:56:50.944
    Aug 29 19:56:50.944: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-preemption 08/29/23 19:56:50.945
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:56:50.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:56:50.972
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 29 19:56:50.997: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 29 19:57:51.046: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:57:51.05
    Aug 29 19:57:51.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-preemption-path 08/29/23 19:57:51.051
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:57:51.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:57:51.074
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:771
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:814
    Aug 29 19:57:51.091: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 29 19:57:51.095: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:57:51.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:787
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:57:51.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5696" for this suite. 08/29/23 19:57:51.202
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-9302" for this suite. 08/29/23 19:57:51.21
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:57:51.217
Aug 29 19:57:51.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 19:57:51.218
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:57:51.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:57:51.238
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/29/23 19:57:51.241
Aug 29 19:57:51.250: INFO: Waiting up to 5m0s for pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8" in namespace "emptydir-7943" to be "Succeeded or Failed"
Aug 29 19:57:51.254: INFO: Pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643099ms
Aug 29 19:57:53.259: INFO: Pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008741947s
Aug 29 19:57:55.260: INFO: Pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009358084s
STEP: Saw pod success 08/29/23 19:57:55.26
Aug 29 19:57:55.260: INFO: Pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8" satisfied condition "Succeeded or Failed"
Aug 29 19:57:55.264: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-036296ff-2b71-48a5-8ed0-e15a867840c8 container test-container: <nil>
STEP: delete the pod 08/29/23 19:57:55.271
Aug 29 19:57:55.287: INFO: Waiting for pod pod-036296ff-2b71-48a5-8ed0-e15a867840c8 to disappear
Aug 29 19:57:55.290: INFO: Pod pod-036296ff-2b71-48a5-8ed0-e15a867840c8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 19:57:55.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7943" for this suite. 08/29/23 19:57:55.295
------------------------------
• [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:57:51.217
    Aug 29 19:57:51.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 19:57:51.218
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:57:51.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:57:51.238
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/29/23 19:57:51.241
    Aug 29 19:57:51.250: INFO: Waiting up to 5m0s for pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8" in namespace "emptydir-7943" to be "Succeeded or Failed"
    Aug 29 19:57:51.254: INFO: Pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.643099ms
    Aug 29 19:57:53.259: INFO: Pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008741947s
    Aug 29 19:57:55.260: INFO: Pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009358084s
    STEP: Saw pod success 08/29/23 19:57:55.26
    Aug 29 19:57:55.260: INFO: Pod "pod-036296ff-2b71-48a5-8ed0-e15a867840c8" satisfied condition "Succeeded or Failed"
    Aug 29 19:57:55.264: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-036296ff-2b71-48a5-8ed0-e15a867840c8 container test-container: <nil>
    STEP: delete the pod 08/29/23 19:57:55.271
    Aug 29 19:57:55.287: INFO: Waiting for pod pod-036296ff-2b71-48a5-8ed0-e15a867840c8 to disappear
    Aug 29 19:57:55.290: INFO: Pod pod-036296ff-2b71-48a5-8ed0-e15a867840c8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:57:55.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7943" for this suite. 08/29/23 19:57:55.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:57:55.303
Aug 29 19:57:55.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 19:57:55.304
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:57:55.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:57:55.331
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 19:57:55.353
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:57:55.74
STEP: Deploying the webhook pod 08/29/23 19:57:55.749
STEP: Wait for the deployment to be ready 08/29/23 19:57:55.765
Aug 29 19:57:55.773: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/29/23 19:57:57.789
STEP: Verifying the service has paired with the endpoint 08/29/23 19:57:57.802
Aug 29 19:57:58.803: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Aug 29 19:57:58.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5339-crds.webhook.example.com via the AdmissionRegistration API 08/29/23 19:57:59.321
STEP: Creating a custom resource that should be mutated by the webhook 08/29/23 19:57:59.343
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:01.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6075" for this suite. 08/29/23 19:58:01.992
STEP: Destroying namespace "webhook-6075-markers" for this suite. 08/29/23 19:58:02.002
------------------------------
• [SLOW TEST] [6.712 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:57:55.303
    Aug 29 19:57:55.303: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 19:57:55.304
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:57:55.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:57:55.331
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 19:57:55.353
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:57:55.74
    STEP: Deploying the webhook pod 08/29/23 19:57:55.749
    STEP: Wait for the deployment to be ready 08/29/23 19:57:55.765
    Aug 29 19:57:55.773: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/29/23 19:57:57.789
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:57:57.802
    Aug 29 19:57:58.803: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Aug 29 19:57:58.808: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5339-crds.webhook.example.com via the AdmissionRegistration API 08/29/23 19:57:59.321
    STEP: Creating a custom resource that should be mutated by the webhook 08/29/23 19:57:59.343
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:01.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6075" for this suite. 08/29/23 19:58:01.992
    STEP: Destroying namespace "webhook-6075-markers" for this suite. 08/29/23 19:58:02.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:02.018
Aug 29 19:58:02.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 19:58:02.02
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:02.04
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:02.044
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 19:58:02.065
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:58:02.694
STEP: Deploying the webhook pod 08/29/23 19:58:02.702
STEP: Wait for the deployment to be ready 08/29/23 19:58:02.718
Aug 29 19:58:02.727: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 19:58:04.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 19, 58, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 58, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 19, 58, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 58, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/29/23 19:58:06.748
STEP: Verifying the service has paired with the endpoint 08/29/23 19:58:06.762
Aug 29 19:58:07.762: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/29/23 19:58:07.765
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/29/23 19:58:07.792
STEP: Creating a dummy validating-webhook-configuration object 08/29/23 19:58:07.804
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/29/23 19:58:07.823
STEP: Creating a dummy mutating-webhook-configuration object 08/29/23 19:58:07.831
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/29/23 19:58:07.841
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:07.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3050" for this suite. 08/29/23 19:58:07.91
STEP: Destroying namespace "webhook-3050-markers" for this suite. 08/29/23 19:58:07.919
------------------------------
• [SLOW TEST] [5.908 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:02.018
    Aug 29 19:58:02.018: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 19:58:02.02
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:02.04
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:02.044
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 19:58:02.065
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:58:02.694
    STEP: Deploying the webhook pod 08/29/23 19:58:02.702
    STEP: Wait for the deployment to be ready 08/29/23 19:58:02.718
    Aug 29 19:58:02.727: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 29 19:58:04.743: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 19, 58, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 58, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 19, 58, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 58, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/29/23 19:58:06.748
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:58:06.762
    Aug 29 19:58:07.762: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/29/23 19:58:07.765
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/29/23 19:58:07.792
    STEP: Creating a dummy validating-webhook-configuration object 08/29/23 19:58:07.804
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/29/23 19:58:07.823
    STEP: Creating a dummy mutating-webhook-configuration object 08/29/23 19:58:07.831
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/29/23 19:58:07.841
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:07.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3050" for this suite. 08/29/23 19:58:07.91
    STEP: Destroying namespace "webhook-3050-markers" for this suite. 08/29/23 19:58:07.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:07.927
Aug 29 19:58:07.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename gc 08/29/23 19:58:07.928
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:07.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:07.959
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/29/23 19:58:07.967
STEP: create the rc2 08/29/23 19:58:07.975
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/29/23 19:58:12.987
STEP: delete the rc simpletest-rc-to-be-deleted 08/29/23 19:58:14.264
STEP: wait for the rc to be deleted 08/29/23 19:58:14.275
Aug 29 19:58:19.295: INFO: 66 pods remaining
Aug 29 19:58:19.295: INFO: 66 pods has nil DeletionTimestamp
Aug 29 19:58:19.295: INFO: 
STEP: Gathering metrics 08/29/23 19:58:24.29
W0829 19:58:24.305601      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 19:58:24.305: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 29 19:58:24.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-26ppr" in namespace "gc-9826"
Aug 29 19:58:24.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qm2g" in namespace "gc-9826"
Aug 29 19:58:24.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-47l4p" in namespace "gc-9826"
Aug 29 19:58:24.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nq9b" in namespace "gc-9826"
Aug 29 19:58:24.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w2vf" in namespace "gc-9826"
Aug 29 19:58:24.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-5697j" in namespace "gc-9826"
Aug 29 19:58:24.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-585wq" in namespace "gc-9826"
Aug 29 19:58:24.441: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hmsb" in namespace "gc-9826"
Aug 29 19:58:24.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p4q2" in namespace "gc-9826"
Aug 29 19:58:24.487: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kssd" in namespace "gc-9826"
Aug 29 19:58:24.517: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pj69" in namespace "gc-9826"
Aug 29 19:58:24.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ttcf" in namespace "gc-9826"
Aug 29 19:58:24.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vkph" in namespace "gc-9826"
Aug 29 19:58:24.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vlhg" in namespace "gc-9826"
Aug 29 19:58:24.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kbpj" in namespace "gc-9826"
Aug 29 19:58:24.628: INFO: Deleting pod "simpletest-rc-to-be-deleted-88nm7" in namespace "gc-9826"
Aug 29 19:58:24.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f297" in namespace "gc-9826"
Aug 29 19:58:24.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-8s5jf" in namespace "gc-9826"
Aug 29 19:58:24.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-9877c" in namespace "gc-9826"
Aug 29 19:58:24.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lx9g" in namespace "gc-9826"
Aug 29 19:58:24.762: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mqd5" in namespace "gc-9826"
Aug 29 19:58:24.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t959" in namespace "gc-9826"
Aug 29 19:58:24.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdr82" in namespace "gc-9826"
Aug 29 19:58:24.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgpfm" in namespace "gc-9826"
Aug 29 19:58:24.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2f69" in namespace "gc-9826"
Aug 29 19:58:24.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvn6h" in namespace "gc-9826"
Aug 29 19:58:24.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw6gm" in namespace "gc-9826"
Aug 29 19:58:24.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxdmr" in namespace "gc-9826"
Aug 29 19:58:24.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8nb7" in namespace "gc-9826"
Aug 29 19:58:24.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9vgb" in namespace "gc-9826"
Aug 29 19:58:25.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcc4t" in namespace "gc-9826"
Aug 29 19:58:25.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-dx4lr" in namespace "gc-9826"
Aug 29 19:58:25.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-flg44" in namespace "gc-9826"
Aug 29 19:58:25.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsfhc" in namespace "gc-9826"
Aug 29 19:58:25.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvt8c" in namespace "gc-9826"
Aug 29 19:58:25.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp257" in namespace "gc-9826"
Aug 29 19:58:25.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-h67rq" in namespace "gc-9826"
Aug 29 19:58:25.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-hs7x9" in namespace "gc-9826"
Aug 29 19:58:25.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6qjc" in namespace "gc-9826"
Aug 29 19:58:25.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7z2q" in namespace "gc-9826"
Aug 29 19:58:25.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9552" in namespace "gc-9826"
Aug 29 19:58:25.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdfr5" in namespace "gc-9826"
Aug 29 19:58:25.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhmh5" in namespace "gc-9826"
Aug 29 19:58:25.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwvfr" in namespace "gc-9826"
Aug 29 19:58:25.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzbg4" in namespace "gc-9826"
Aug 29 19:58:25.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-kgg8j" in namespace "gc-9826"
Aug 29 19:58:25.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-kl9h6" in namespace "gc-9826"
Aug 29 19:58:25.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-km8gz" in namespace "gc-9826"
Aug 29 19:58:25.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-kwnvw" in namespace "gc-9826"
Aug 29 19:58:25.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-lfhk9" in namespace "gc-9826"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:25.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9826" for this suite. 08/29/23 19:58:25.703
------------------------------
• [SLOW TEST] [17.803 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:07.927
    Aug 29 19:58:07.927: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename gc 08/29/23 19:58:07.928
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:07.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:07.959
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/29/23 19:58:07.967
    STEP: create the rc2 08/29/23 19:58:07.975
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/29/23 19:58:12.987
    STEP: delete the rc simpletest-rc-to-be-deleted 08/29/23 19:58:14.264
    STEP: wait for the rc to be deleted 08/29/23 19:58:14.275
    Aug 29 19:58:19.295: INFO: 66 pods remaining
    Aug 29 19:58:19.295: INFO: 66 pods has nil DeletionTimestamp
    Aug 29 19:58:19.295: INFO: 
    STEP: Gathering metrics 08/29/23 19:58:24.29
    W0829 19:58:24.305601      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 29 19:58:24.305: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 29 19:58:24.305: INFO: Deleting pod "simpletest-rc-to-be-deleted-26ppr" in namespace "gc-9826"
    Aug 29 19:58:24.325: INFO: Deleting pod "simpletest-rc-to-be-deleted-2qm2g" in namespace "gc-9826"
    Aug 29 19:58:24.341: INFO: Deleting pod "simpletest-rc-to-be-deleted-47l4p" in namespace "gc-9826"
    Aug 29 19:58:24.359: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nq9b" in namespace "gc-9826"
    Aug 29 19:58:24.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-4w2vf" in namespace "gc-9826"
    Aug 29 19:58:24.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-5697j" in namespace "gc-9826"
    Aug 29 19:58:24.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-585wq" in namespace "gc-9826"
    Aug 29 19:58:24.441: INFO: Deleting pod "simpletest-rc-to-be-deleted-5hmsb" in namespace "gc-9826"
    Aug 29 19:58:24.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-5p4q2" in namespace "gc-9826"
    Aug 29 19:58:24.487: INFO: Deleting pod "simpletest-rc-to-be-deleted-6kssd" in namespace "gc-9826"
    Aug 29 19:58:24.517: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pj69" in namespace "gc-9826"
    Aug 29 19:58:24.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ttcf" in namespace "gc-9826"
    Aug 29 19:58:24.568: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vkph" in namespace "gc-9826"
    Aug 29 19:58:24.593: INFO: Deleting pod "simpletest-rc-to-be-deleted-6vlhg" in namespace "gc-9826"
    Aug 29 19:58:24.609: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kbpj" in namespace "gc-9826"
    Aug 29 19:58:24.628: INFO: Deleting pod "simpletest-rc-to-be-deleted-88nm7" in namespace "gc-9826"
    Aug 29 19:58:24.650: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f297" in namespace "gc-9826"
    Aug 29 19:58:24.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-8s5jf" in namespace "gc-9826"
    Aug 29 19:58:24.700: INFO: Deleting pod "simpletest-rc-to-be-deleted-9877c" in namespace "gc-9826"
    Aug 29 19:58:24.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-9lx9g" in namespace "gc-9826"
    Aug 29 19:58:24.762: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mqd5" in namespace "gc-9826"
    Aug 29 19:58:24.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t959" in namespace "gc-9826"
    Aug 29 19:58:24.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdr82" in namespace "gc-9826"
    Aug 29 19:58:24.847: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgpfm" in namespace "gc-9826"
    Aug 29 19:58:24.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-c2f69" in namespace "gc-9826"
    Aug 29 19:58:24.891: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvn6h" in namespace "gc-9826"
    Aug 29 19:58:24.922: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw6gm" in namespace "gc-9826"
    Aug 29 19:58:24.952: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxdmr" in namespace "gc-9826"
    Aug 29 19:58:24.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-d8nb7" in namespace "gc-9826"
    Aug 29 19:58:24.999: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9vgb" in namespace "gc-9826"
    Aug 29 19:58:25.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcc4t" in namespace "gc-9826"
    Aug 29 19:58:25.074: INFO: Deleting pod "simpletest-rc-to-be-deleted-dx4lr" in namespace "gc-9826"
    Aug 29 19:58:25.115: INFO: Deleting pod "simpletest-rc-to-be-deleted-flg44" in namespace "gc-9826"
    Aug 29 19:58:25.145: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsfhc" in namespace "gc-9826"
    Aug 29 19:58:25.172: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvt8c" in namespace "gc-9826"
    Aug 29 19:58:25.211: INFO: Deleting pod "simpletest-rc-to-be-deleted-gp257" in namespace "gc-9826"
    Aug 29 19:58:25.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-h67rq" in namespace "gc-9826"
    Aug 29 19:58:25.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-hs7x9" in namespace "gc-9826"
    Aug 29 19:58:25.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-j6qjc" in namespace "gc-9826"
    Aug 29 19:58:25.361: INFO: Deleting pod "simpletest-rc-to-be-deleted-j7z2q" in namespace "gc-9826"
    Aug 29 19:58:25.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-j9552" in namespace "gc-9826"
    Aug 29 19:58:25.418: INFO: Deleting pod "simpletest-rc-to-be-deleted-jdfr5" in namespace "gc-9826"
    Aug 29 19:58:25.439: INFO: Deleting pod "simpletest-rc-to-be-deleted-jhmh5" in namespace "gc-9826"
    Aug 29 19:58:25.464: INFO: Deleting pod "simpletest-rc-to-be-deleted-jwvfr" in namespace "gc-9826"
    Aug 29 19:58:25.491: INFO: Deleting pod "simpletest-rc-to-be-deleted-jzbg4" in namespace "gc-9826"
    Aug 29 19:58:25.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-kgg8j" in namespace "gc-9826"
    Aug 29 19:58:25.551: INFO: Deleting pod "simpletest-rc-to-be-deleted-kl9h6" in namespace "gc-9826"
    Aug 29 19:58:25.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-km8gz" in namespace "gc-9826"
    Aug 29 19:58:25.601: INFO: Deleting pod "simpletest-rc-to-be-deleted-kwnvw" in namespace "gc-9826"
    Aug 29 19:58:25.647: INFO: Deleting pod "simpletest-rc-to-be-deleted-lfhk9" in namespace "gc-9826"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:25.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9826" for this suite. 08/29/23 19:58:25.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:25.731
Aug 29 19:58:25.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 19:58:25.732
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:25.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:25.783
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-3f7afecd-4719-4032-8439-bf85a7fabb54 08/29/23 19:58:25.803
STEP: Creating the pod 08/29/23 19:58:25.816
Aug 29 19:58:25.838: INFO: Waiting up to 5m0s for pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c" in namespace "configmap-4659" to be "running"
Aug 29 19:58:25.844: INFO: Pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.721301ms
Aug 29 19:58:27.849: INFO: Pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011124636s
Aug 29 19:58:29.848: INFO: Pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c": Phase="Running", Reason="", readiness=false. Elapsed: 4.010463845s
Aug 29 19:58:29.848: INFO: Pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c" satisfied condition "running"
STEP: Waiting for pod with text data 08/29/23 19:58:29.848
STEP: Waiting for pod with binary data 08/29/23 19:58:29.858
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:29.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4659" for this suite. 08/29/23 19:58:29.87
------------------------------
• [4.147 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:25.731
    Aug 29 19:58:25.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 19:58:25.732
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:25.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:25.783
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-3f7afecd-4719-4032-8439-bf85a7fabb54 08/29/23 19:58:25.803
    STEP: Creating the pod 08/29/23 19:58:25.816
    Aug 29 19:58:25.838: INFO: Waiting up to 5m0s for pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c" in namespace "configmap-4659" to be "running"
    Aug 29 19:58:25.844: INFO: Pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.721301ms
    Aug 29 19:58:27.849: INFO: Pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011124636s
    Aug 29 19:58:29.848: INFO: Pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c": Phase="Running", Reason="", readiness=false. Elapsed: 4.010463845s
    Aug 29 19:58:29.848: INFO: Pod "pod-configmaps-afb4294e-eb4f-4d4c-83ea-6aa80c99d31c" satisfied condition "running"
    STEP: Waiting for pod with text data 08/29/23 19:58:29.848
    STEP: Waiting for pod with binary data 08/29/23 19:58:29.858
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:29.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4659" for this suite. 08/29/23 19:58:29.87
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:29.879
Aug 29 19:58:29.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename endpointslice 08/29/23 19:58:29.88
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:29.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:29.903
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:31.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-7032" for this suite. 08/29/23 19:58:31.966
------------------------------
• [2.099 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:29.879
    Aug 29 19:58:29.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename endpointslice 08/29/23 19:58:29.88
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:29.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:29.903
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:31.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-7032" for this suite. 08/29/23 19:58:31.966
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:31.978
Aug 29 19:58:31.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename discovery 08/29/23 19:58:31.98
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:32.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:32.011
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/29/23 19:58:32.016
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 29 19:58:32.262: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 29 19:58:32.263: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 29 19:58:32.263: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 29 19:58:32.263: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 29 19:58:32.263: INFO: Checking APIGroup: apps
Aug 29 19:58:32.264: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 29 19:58:32.264: INFO: Versions found [{apps/v1 v1}]
Aug 29 19:58:32.264: INFO: apps/v1 matches apps/v1
Aug 29 19:58:32.264: INFO: Checking APIGroup: events.k8s.io
Aug 29 19:58:32.265: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 29 19:58:32.265: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 29 19:58:32.265: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 29 19:58:32.265: INFO: Checking APIGroup: authentication.k8s.io
Aug 29 19:58:32.266: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 29 19:58:32.266: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 29 19:58:32.266: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 29 19:58:32.266: INFO: Checking APIGroup: authorization.k8s.io
Aug 29 19:58:32.267: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 29 19:58:32.267: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 29 19:58:32.267: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 29 19:58:32.267: INFO: Checking APIGroup: autoscaling
Aug 29 19:58:32.268: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 29 19:58:32.268: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Aug 29 19:58:32.268: INFO: autoscaling/v2 matches autoscaling/v2
Aug 29 19:58:32.268: INFO: Checking APIGroup: batch
Aug 29 19:58:32.270: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 29 19:58:32.270: INFO: Versions found [{batch/v1 v1}]
Aug 29 19:58:32.270: INFO: batch/v1 matches batch/v1
Aug 29 19:58:32.270: INFO: Checking APIGroup: certificates.k8s.io
Aug 29 19:58:32.271: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 29 19:58:32.271: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 29 19:58:32.271: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 29 19:58:32.271: INFO: Checking APIGroup: networking.k8s.io
Aug 29 19:58:32.272: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 29 19:58:32.272: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 29 19:58:32.272: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 29 19:58:32.272: INFO: Checking APIGroup: policy
Aug 29 19:58:32.273: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 29 19:58:32.273: INFO: Versions found [{policy/v1 v1}]
Aug 29 19:58:32.273: INFO: policy/v1 matches policy/v1
Aug 29 19:58:32.273: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 29 19:58:32.274: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 29 19:58:32.274: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 29 19:58:32.274: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 29 19:58:32.274: INFO: Checking APIGroup: storage.k8s.io
Aug 29 19:58:32.276: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 29 19:58:32.276: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 29 19:58:32.276: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 29 19:58:32.276: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 29 19:58:32.277: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 29 19:58:32.277: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 29 19:58:32.277: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 29 19:58:32.277: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 29 19:58:32.278: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 29 19:58:32.278: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 29 19:58:32.278: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 29 19:58:32.278: INFO: Checking APIGroup: scheduling.k8s.io
Aug 29 19:58:32.279: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 29 19:58:32.279: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 29 19:58:32.279: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 29 19:58:32.279: INFO: Checking APIGroup: coordination.k8s.io
Aug 29 19:58:32.281: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 29 19:58:32.281: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 29 19:58:32.281: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 29 19:58:32.281: INFO: Checking APIGroup: node.k8s.io
Aug 29 19:58:32.282: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 29 19:58:32.282: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 29 19:58:32.282: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 29 19:58:32.282: INFO: Checking APIGroup: discovery.k8s.io
Aug 29 19:58:32.283: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 29 19:58:32.283: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 29 19:58:32.283: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 29 19:58:32.283: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 29 19:58:32.284: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Aug 29 19:58:32.284: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Aug 29 19:58:32.284: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Aug 29 19:58:32.284: INFO: Checking APIGroup: crd.projectcalico.org
Aug 29 19:58:32.285: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Aug 29 19:58:32.285: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Aug 29 19:58:32.285: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Aug 29 19:58:32.285: INFO: Checking APIGroup: monitoring.coreos.com
Aug 29 19:58:32.286: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Aug 29 19:58:32.286: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Aug 29 19:58:32.286: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Aug 29 19:58:32.286: INFO: Checking APIGroup: snapshot.storage.k8s.io
Aug 29 19:58:32.287: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Aug 29 19:58:32.287: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Aug 29 19:58:32.287: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Aug 29 19:58:32.287: INFO: Checking APIGroup: metrics.k8s.io
Aug 29 19:58:32.288: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Aug 29 19:58:32.288: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Aug 29 19:58:32.288: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:32.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-247" for this suite. 08/29/23 19:58:32.293
------------------------------
• [0.324 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:31.978
    Aug 29 19:58:31.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename discovery 08/29/23 19:58:31.98
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:32.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:32.011
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/29/23 19:58:32.016
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 29 19:58:32.262: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 29 19:58:32.263: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 29 19:58:32.263: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 29 19:58:32.263: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 29 19:58:32.263: INFO: Checking APIGroup: apps
    Aug 29 19:58:32.264: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 29 19:58:32.264: INFO: Versions found [{apps/v1 v1}]
    Aug 29 19:58:32.264: INFO: apps/v1 matches apps/v1
    Aug 29 19:58:32.264: INFO: Checking APIGroup: events.k8s.io
    Aug 29 19:58:32.265: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 29 19:58:32.265: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 29 19:58:32.265: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 29 19:58:32.265: INFO: Checking APIGroup: authentication.k8s.io
    Aug 29 19:58:32.266: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 29 19:58:32.266: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 29 19:58:32.266: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 29 19:58:32.266: INFO: Checking APIGroup: authorization.k8s.io
    Aug 29 19:58:32.267: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 29 19:58:32.267: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 29 19:58:32.267: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 29 19:58:32.267: INFO: Checking APIGroup: autoscaling
    Aug 29 19:58:32.268: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 29 19:58:32.268: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Aug 29 19:58:32.268: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 29 19:58:32.268: INFO: Checking APIGroup: batch
    Aug 29 19:58:32.270: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 29 19:58:32.270: INFO: Versions found [{batch/v1 v1}]
    Aug 29 19:58:32.270: INFO: batch/v1 matches batch/v1
    Aug 29 19:58:32.270: INFO: Checking APIGroup: certificates.k8s.io
    Aug 29 19:58:32.271: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 29 19:58:32.271: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 29 19:58:32.271: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 29 19:58:32.271: INFO: Checking APIGroup: networking.k8s.io
    Aug 29 19:58:32.272: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 29 19:58:32.272: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 29 19:58:32.272: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 29 19:58:32.272: INFO: Checking APIGroup: policy
    Aug 29 19:58:32.273: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 29 19:58:32.273: INFO: Versions found [{policy/v1 v1}]
    Aug 29 19:58:32.273: INFO: policy/v1 matches policy/v1
    Aug 29 19:58:32.273: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 29 19:58:32.274: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 29 19:58:32.274: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 29 19:58:32.274: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 29 19:58:32.274: INFO: Checking APIGroup: storage.k8s.io
    Aug 29 19:58:32.276: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 29 19:58:32.276: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 29 19:58:32.276: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 29 19:58:32.276: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 29 19:58:32.277: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 29 19:58:32.277: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 29 19:58:32.277: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 29 19:58:32.277: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 29 19:58:32.278: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 29 19:58:32.278: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 29 19:58:32.278: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 29 19:58:32.278: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 29 19:58:32.279: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 29 19:58:32.279: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 29 19:58:32.279: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 29 19:58:32.279: INFO: Checking APIGroup: coordination.k8s.io
    Aug 29 19:58:32.281: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 29 19:58:32.281: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 29 19:58:32.281: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 29 19:58:32.281: INFO: Checking APIGroup: node.k8s.io
    Aug 29 19:58:32.282: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 29 19:58:32.282: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 29 19:58:32.282: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 29 19:58:32.282: INFO: Checking APIGroup: discovery.k8s.io
    Aug 29 19:58:32.283: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 29 19:58:32.283: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 29 19:58:32.283: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 29 19:58:32.283: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 29 19:58:32.284: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Aug 29 19:58:32.284: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Aug 29 19:58:32.284: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Aug 29 19:58:32.284: INFO: Checking APIGroup: crd.projectcalico.org
    Aug 29 19:58:32.285: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Aug 29 19:58:32.285: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Aug 29 19:58:32.285: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Aug 29 19:58:32.285: INFO: Checking APIGroup: monitoring.coreos.com
    Aug 29 19:58:32.286: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Aug 29 19:58:32.286: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Aug 29 19:58:32.286: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Aug 29 19:58:32.286: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Aug 29 19:58:32.287: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Aug 29 19:58:32.287: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Aug 29 19:58:32.287: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Aug 29 19:58:32.287: INFO: Checking APIGroup: metrics.k8s.io
    Aug 29 19:58:32.288: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Aug 29 19:58:32.288: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Aug 29 19:58:32.288: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:32.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-247" for this suite. 08/29/23 19:58:32.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:32.304
Aug 29 19:58:32.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 19:58:32.305
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:32.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:32.33
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 19:58:32.349
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:58:33.215
STEP: Deploying the webhook pod 08/29/23 19:58:33.224
STEP: Wait for the deployment to be ready 08/29/23 19:58:33.248
Aug 29 19:58:33.254: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/29/23 19:58:35.269
STEP: Verifying the service has paired with the endpoint 08/29/23 19:58:35.282
Aug 29 19:58:36.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/29/23 19:58:36.288
Aug 29 19:58:36.310: INFO: Waiting for webhook configuration to be ready...
STEP: create a namespace for the webhook 08/29/23 19:58:36.423
STEP: create a configmap should be unconditionally rejected by the webhook 08/29/23 19:58:36.433
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:36.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-7645" for this suite. 08/29/23 19:58:36.522
STEP: Destroying namespace "webhook-7645-markers" for this suite. 08/29/23 19:58:36.529
------------------------------
• [4.239 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:32.304
    Aug 29 19:58:32.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 19:58:32.305
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:32.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:32.33
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 19:58:32.349
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 19:58:33.215
    STEP: Deploying the webhook pod 08/29/23 19:58:33.224
    STEP: Wait for the deployment to be ready 08/29/23 19:58:33.248
    Aug 29 19:58:33.254: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/29/23 19:58:35.269
    STEP: Verifying the service has paired with the endpoint 08/29/23 19:58:35.282
    Aug 29 19:58:36.284: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/29/23 19:58:36.288
    Aug 29 19:58:36.310: INFO: Waiting for webhook configuration to be ready...
    STEP: create a namespace for the webhook 08/29/23 19:58:36.423
    STEP: create a configmap should be unconditionally rejected by the webhook 08/29/23 19:58:36.433
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:36.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-7645" for this suite. 08/29/23 19:58:36.522
    STEP: Destroying namespace "webhook-7645-markers" for this suite. 08/29/23 19:58:36.529
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:36.543
Aug 29 19:58:36.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename deployment 08/29/23 19:58:36.545
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:36.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:36.574
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/29/23 19:58:36.582
Aug 29 19:58:36.582: INFO: Creating simple deployment test-deployment-5jqmk
Aug 29 19:58:36.598: INFO: deployment "test-deployment-5jqmk" doesn't have the required revision set
STEP: Getting /status 08/29/23 19:58:38.611
Aug 29 19:58:38.615: INFO: Deployment test-deployment-5jqmk has Conditions: [{Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 08/29/23 19:58:38.615
Aug 29 19:58:38.627: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 19, 58, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 58, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 19, 58, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 58, 36, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5jqmk-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/29/23 19:58:38.627
Aug 29 19:58:38.629: INFO: Observed &Deployment event: ADDED
Aug 29 19:58:38.629: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5jqmk-54bc444df"}
Aug 29 19:58:38.629: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.629: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5jqmk-54bc444df"}
Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 19:58:38.630: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5jqmk-54bc444df" is progressing.}
Aug 29 19:58:38.630: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}
Aug 29 19:58:38.630: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}
Aug 29 19:58:38.630: INFO: Found Deployment test-deployment-5jqmk in namespace deployment-3103 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 19:58:38.630: INFO: Deployment test-deployment-5jqmk has an updated status
STEP: patching the Statefulset Status 08/29/23 19:58:38.63
Aug 29 19:58:38.630: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 19:58:38.645: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/29/23 19:58:38.645
Aug 29 19:58:38.647: INFO: Observed &Deployment event: ADDED
Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5jqmk-54bc444df"}
Aug 29 19:58:38.647: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5jqmk-54bc444df"}
Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 19:58:38.647: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5jqmk-54bc444df" is progressing.}
Aug 29 19:58:38.648: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}
Aug 29 19:58:38.648: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}
Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 19:58:38.648: INFO: Observed &Deployment event: MODIFIED
Aug 29 19:58:38.648: INFO: Found deployment test-deployment-5jqmk in namespace deployment-3103 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 29 19:58:38.648: INFO: Deployment test-deployment-5jqmk has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 19:58:38.651: INFO: Deployment "test-deployment-5jqmk":
&Deployment{ObjectMeta:{test-deployment-5jqmk  deployment-3103  420dc555-c48a-42f5-b8bc-12d68e2febb8 17731 1 2023-08-29 19:58:36 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-29 19:58:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-29 19:58:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-29 19:58:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00201c648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 19:58:38.656: INFO: New ReplicaSet "test-deployment-5jqmk-54bc444df" of Deployment "test-deployment-5jqmk":
&ReplicaSet{ObjectMeta:{test-deployment-5jqmk-54bc444df  deployment-3103  70380256-5477-4511-9f03-c5ed8fbcacc0 17721 1 2023-08-29 19:58:36 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5jqmk 420dc555-c48a-42f5-b8bc-12d68e2febb8 0xc0043421d0 0xc0043421d1}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:58:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"420dc555-c48a-42f5-b8bc-12d68e2febb8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:58:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004342278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 19:58:38.660: INFO: Pod "test-deployment-5jqmk-54bc444df-mjnjx" is available:
&Pod{ObjectMeta:{test-deployment-5jqmk-54bc444df-mjnjx test-deployment-5jqmk-54bc444df- deployment-3103  f83b820e-b0a6-46a8-8cee-89b22801fec4 17720 0 2023-08-29 19:58:36 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:49e7318525896b54bcac52a23b5796c58c13496978910bd75ef6313c55d7f89c cni.projectcalico.org/podIP:172.20.30.179/32 cni.projectcalico.org/podIPs:172.20.30.179/32] [{apps/v1 ReplicaSet test-deployment-5jqmk-54bc444df 70380256-5477-4511-9f03-c5ed8fbcacc0 0xc004342640 0xc004342641}] [] [{kube-controller-manager Update v1 2023-08-29 19:58:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70380256-5477-4511-9f03-c5ed8fbcacc0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:58:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvxsr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvxsr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:58:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:58:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:58:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:58:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.179,StartTime:2023-08-29 19:58:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:58:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ace9e281b3af7e785babe4c7924646d6aac04d5ed5d2fd4c8d628c44e3b5633f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:38.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-3103" for this suite. 08/29/23 19:58:38.669
------------------------------
• [2.135 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:36.543
    Aug 29 19:58:36.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename deployment 08/29/23 19:58:36.545
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:36.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:36.574
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/29/23 19:58:36.582
    Aug 29 19:58:36.582: INFO: Creating simple deployment test-deployment-5jqmk
    Aug 29 19:58:36.598: INFO: deployment "test-deployment-5jqmk" doesn't have the required revision set
    STEP: Getting /status 08/29/23 19:58:38.611
    Aug 29 19:58:38.615: INFO: Deployment test-deployment-5jqmk has Conditions: [{Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 08/29/23 19:58:38.615
    Aug 29 19:58:38.627: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 19, 58, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 58, 38, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 19, 58, 38, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 19, 58, 36, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-5jqmk-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/29/23 19:58:38.627
    Aug 29 19:58:38.629: INFO: Observed &Deployment event: ADDED
    Aug 29 19:58:38.629: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5jqmk-54bc444df"}
    Aug 29 19:58:38.629: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.629: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5jqmk-54bc444df"}
    Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 29 19:58:38.630: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5jqmk-54bc444df" is progressing.}
    Aug 29 19:58:38.630: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}
    Aug 29 19:58:38.630: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 29 19:58:38.630: INFO: Observed Deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}
    Aug 29 19:58:38.630: INFO: Found Deployment test-deployment-5jqmk in namespace deployment-3103 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 29 19:58:38.630: INFO: Deployment test-deployment-5jqmk has an updated status
    STEP: patching the Statefulset Status 08/29/23 19:58:38.63
    Aug 29 19:58:38.630: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 29 19:58:38.645: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/29/23 19:58:38.645
    Aug 29 19:58:38.647: INFO: Observed &Deployment event: ADDED
    Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5jqmk-54bc444df"}
    Aug 29 19:58:38.647: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-5jqmk-54bc444df"}
    Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 29 19:58:38.647: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 29 19:58:38.647: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:36 +0000 UTC 2023-08-29 19:58:36 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-5jqmk-54bc444df" is progressing.}
    Aug 29 19:58:38.648: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}
    Aug 29 19:58:38.648: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-29 19:58:38 +0000 UTC 2023-08-29 19:58:36 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-5jqmk-54bc444df" has successfully progressed.}
    Aug 29 19:58:38.648: INFO: Observed deployment test-deployment-5jqmk in namespace deployment-3103 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 29 19:58:38.648: INFO: Observed &Deployment event: MODIFIED
    Aug 29 19:58:38.648: INFO: Found deployment test-deployment-5jqmk in namespace deployment-3103 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 29 19:58:38.648: INFO: Deployment test-deployment-5jqmk has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 29 19:58:38.651: INFO: Deployment "test-deployment-5jqmk":
    &Deployment{ObjectMeta:{test-deployment-5jqmk  deployment-3103  420dc555-c48a-42f5-b8bc-12d68e2febb8 17731 1 2023-08-29 19:58:36 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-29 19:58:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-29 19:58:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-29 19:58:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00201c648 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 29 19:58:38.656: INFO: New ReplicaSet "test-deployment-5jqmk-54bc444df" of Deployment "test-deployment-5jqmk":
    &ReplicaSet{ObjectMeta:{test-deployment-5jqmk-54bc444df  deployment-3103  70380256-5477-4511-9f03-c5ed8fbcacc0 17721 1 2023-08-29 19:58:36 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-5jqmk 420dc555-c48a-42f5-b8bc-12d68e2febb8 0xc0043421d0 0xc0043421d1}] [] [{kube-controller-manager Update apps/v1 2023-08-29 19:58:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"420dc555-c48a-42f5-b8bc-12d68e2febb8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 19:58:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004342278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 19:58:38.660: INFO: Pod "test-deployment-5jqmk-54bc444df-mjnjx" is available:
    &Pod{ObjectMeta:{test-deployment-5jqmk-54bc444df-mjnjx test-deployment-5jqmk-54bc444df- deployment-3103  f83b820e-b0a6-46a8-8cee-89b22801fec4 17720 0 2023-08-29 19:58:36 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[cni.projectcalico.org/containerID:49e7318525896b54bcac52a23b5796c58c13496978910bd75ef6313c55d7f89c cni.projectcalico.org/podIP:172.20.30.179/32 cni.projectcalico.org/podIPs:172.20.30.179/32] [{apps/v1 ReplicaSet test-deployment-5jqmk-54bc444df 70380256-5477-4511-9f03-c5ed8fbcacc0 0xc004342640 0xc004342641}] [] [{kube-controller-manager Update v1 2023-08-29 19:58:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"70380256-5477-4511-9f03-c5ed8fbcacc0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 19:58:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 19:58:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.179\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pvxsr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pvxsr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:58:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:58:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:58:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 19:58:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.179,StartTime:2023-08-29 19:58:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 19:58:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ace9e281b3af7e785babe4c7924646d6aac04d5ed5d2fd4c8d628c44e3b5633f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.179,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:38.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-3103" for this suite. 08/29/23 19:58:38.669
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:38.681
Aug 29 19:58:38.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 19:58:38.682
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:38.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:38.702
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 08/29/23 19:58:38.705
STEP: Getting a ResourceQuota 08/29/23 19:58:38.711
STEP: Listing all ResourceQuotas with LabelSelector 08/29/23 19:58:38.715
STEP: Patching the ResourceQuota 08/29/23 19:58:38.718
STEP: Deleting a Collection of ResourceQuotas 08/29/23 19:58:38.729
STEP: Verifying the deleted ResourceQuota 08/29/23 19:58:38.744
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:38.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2302" for this suite. 08/29/23 19:58:38.752
------------------------------
• [0.079 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:38.681
    Aug 29 19:58:38.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 19:58:38.682
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:38.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:38.702
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 08/29/23 19:58:38.705
    STEP: Getting a ResourceQuota 08/29/23 19:58:38.711
    STEP: Listing all ResourceQuotas with LabelSelector 08/29/23 19:58:38.715
    STEP: Patching the ResourceQuota 08/29/23 19:58:38.718
    STEP: Deleting a Collection of ResourceQuotas 08/29/23 19:58:38.729
    STEP: Verifying the deleted ResourceQuota 08/29/23 19:58:38.744
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:38.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2302" for this suite. 08/29/23 19:58:38.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:38.761
Aug 29 19:58:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename var-expansion 08/29/23 19:58:38.763
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:38.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:38.785
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 08/29/23 19:58:38.788
Aug 29 19:58:38.801: INFO: Waiting up to 5m0s for pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b" in namespace "var-expansion-4266" to be "Succeeded or Failed"
Aug 29 19:58:38.812: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.290595ms
Aug 29 19:58:40.817: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016173991s
Aug 29 19:58:42.820: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018954001s
Aug 29 19:58:44.818: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016857478s
STEP: Saw pod success 08/29/23 19:58:44.818
Aug 29 19:58:44.818: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b" satisfied condition "Succeeded or Failed"
Aug 29 19:58:44.821: INFO: Trying to get logs from node loki-15bd39-worker-1 pod var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b container dapi-container: <nil>
STEP: delete the pod 08/29/23 19:58:44.829
Aug 29 19:58:44.847: INFO: Waiting for pod var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b to disappear
Aug 29 19:58:44.850: INFO: Pod var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:44.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4266" for this suite. 08/29/23 19:58:44.855
------------------------------
• [SLOW TEST] [6.101 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:38.761
    Aug 29 19:58:38.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename var-expansion 08/29/23 19:58:38.763
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:38.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:38.785
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 08/29/23 19:58:38.788
    Aug 29 19:58:38.801: INFO: Waiting up to 5m0s for pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b" in namespace "var-expansion-4266" to be "Succeeded or Failed"
    Aug 29 19:58:38.812: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.290595ms
    Aug 29 19:58:40.817: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016173991s
    Aug 29 19:58:42.820: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018954001s
    Aug 29 19:58:44.818: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.016857478s
    STEP: Saw pod success 08/29/23 19:58:44.818
    Aug 29 19:58:44.818: INFO: Pod "var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b" satisfied condition "Succeeded or Failed"
    Aug 29 19:58:44.821: INFO: Trying to get logs from node loki-15bd39-worker-1 pod var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b container dapi-container: <nil>
    STEP: delete the pod 08/29/23 19:58:44.829
    Aug 29 19:58:44.847: INFO: Waiting for pod var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b to disappear
    Aug 29 19:58:44.850: INFO: Pod var-expansion-442f715a-09a0-4df1-83ca-c5df9397976b no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:44.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4266" for this suite. 08/29/23 19:58:44.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:44.864
Aug 29 19:58:44.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 19:58:44.865
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:44.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:44.892
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Aug 29 19:58:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/29/23 19:58:47.245
Aug 29 19:58:47.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 --namespace=crd-publish-openapi-9034 create -f -'
Aug 29 19:58:48.264: INFO: stderr: ""
Aug 29 19:58:48.264: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 29 19:58:48.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 --namespace=crd-publish-openapi-9034 delete e2e-test-crd-publish-openapi-3722-crds test-cr'
Aug 29 19:58:48.369: INFO: stderr: ""
Aug 29 19:58:48.369: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 29 19:58:48.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 --namespace=crd-publish-openapi-9034 apply -f -'
Aug 29 19:58:49.289: INFO: stderr: ""
Aug 29 19:58:49.289: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 29 19:58:49.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 --namespace=crd-publish-openapi-9034 delete e2e-test-crd-publish-openapi-3722-crds test-cr'
Aug 29 19:58:49.388: INFO: stderr: ""
Aug 29 19:58:49.388: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/29/23 19:58:49.388
Aug 29 19:58:49.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 explain e2e-test-crd-publish-openapi-3722-crds'
Aug 29 19:58:49.703: INFO: stderr: ""
Aug 29 19:58:49.703: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3722-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:52.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9034" for this suite. 08/29/23 19:58:52.497
------------------------------
• [SLOW TEST] [7.642 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:44.864
    Aug 29 19:58:44.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 19:58:44.865
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:44.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:44.892
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Aug 29 19:58:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/29/23 19:58:47.245
    Aug 29 19:58:47.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 --namespace=crd-publish-openapi-9034 create -f -'
    Aug 29 19:58:48.264: INFO: stderr: ""
    Aug 29 19:58:48.264: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 29 19:58:48.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 --namespace=crd-publish-openapi-9034 delete e2e-test-crd-publish-openapi-3722-crds test-cr'
    Aug 29 19:58:48.369: INFO: stderr: ""
    Aug 29 19:58:48.369: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 29 19:58:48.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 --namespace=crd-publish-openapi-9034 apply -f -'
    Aug 29 19:58:49.289: INFO: stderr: ""
    Aug 29 19:58:49.289: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 29 19:58:49.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 --namespace=crd-publish-openapi-9034 delete e2e-test-crd-publish-openapi-3722-crds test-cr'
    Aug 29 19:58:49.388: INFO: stderr: ""
    Aug 29 19:58:49.388: INFO: stdout: "e2e-test-crd-publish-openapi-3722-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/29/23 19:58:49.388
    Aug 29 19:58:49.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-9034 explain e2e-test-crd-publish-openapi-3722-crds'
    Aug 29 19:58:49.703: INFO: stderr: ""
    Aug 29 19:58:49.703: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-3722-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:52.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9034" for this suite. 08/29/23 19:58:52.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:52.507
Aug 29 19:58:52.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubelet-test 08/29/23 19:58:52.508
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:52.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:52.534
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 29 19:58:52.546: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395" in namespace "kubelet-test-9078" to be "running and ready"
Aug 29 19:58:52.549: INFO: Pod "busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395": Phase="Pending", Reason="", readiness=false. Elapsed: 3.204718ms
Aug 29 19:58:52.549: INFO: The phase of Pod busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:58:54.554: INFO: Pod "busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395": Phase="Running", Reason="", readiness=true. Elapsed: 2.007890403s
Aug 29 19:58:54.554: INFO: The phase of Pod busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395 is Running (Ready = true)
Aug 29 19:58:54.554: INFO: Pod "busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:54.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-9078" for this suite. 08/29/23 19:58:54.582
------------------------------
• [2.083 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:52.507
    Aug 29 19:58:52.507: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubelet-test 08/29/23 19:58:52.508
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:52.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:52.534
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 29 19:58:52.546: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395" in namespace "kubelet-test-9078" to be "running and ready"
    Aug 29 19:58:52.549: INFO: Pod "busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395": Phase="Pending", Reason="", readiness=false. Elapsed: 3.204718ms
    Aug 29 19:58:52.549: INFO: The phase of Pod busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:58:54.554: INFO: Pod "busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395": Phase="Running", Reason="", readiness=true. Elapsed: 2.007890403s
    Aug 29 19:58:54.554: INFO: The phase of Pod busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395 is Running (Ready = true)
    Aug 29 19:58:54.554: INFO: Pod "busybox-readonly-fs4db56fe7-d0e6-461b-a1cc-8a6015e06395" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:54.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-9078" for this suite. 08/29/23 19:58:54.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:54.592
Aug 29 19:58:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename security-context-test 08/29/23 19:58:54.593
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:54.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:54.615
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Aug 29 19:58:54.627: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a" in namespace "security-context-test-5178" to be "Succeeded or Failed"
Aug 29 19:58:54.632: INFO: Pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146245ms
Aug 29 19:58:56.637: INFO: Pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010452061s
Aug 29 19:58:58.636: INFO: Pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00963066s
Aug 29 19:58:58.637: INFO: Pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 29 19:58:58.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-5178" for this suite. 08/29/23 19:58:58.641
------------------------------
• [4.061 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:54.592
    Aug 29 19:58:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename security-context-test 08/29/23 19:58:54.593
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:54.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:54.615
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Aug 29 19:58:54.627: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a" in namespace "security-context-test-5178" to be "Succeeded or Failed"
    Aug 29 19:58:54.632: INFO: Pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.146245ms
    Aug 29 19:58:56.637: INFO: Pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010452061s
    Aug 29 19:58:58.636: INFO: Pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00963066s
    Aug 29 19:58:58.637: INFO: Pod "busybox-readonly-false-43e04384-6641-43c2-bdc9-0ee6db91004a" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:58:58.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-5178" for this suite. 08/29/23 19:58:58.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:58:58.653
Aug 29 19:58:58.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replication-controller 08/29/23 19:58:58.654
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:58.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:58.676
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-j9nfs" 08/29/23 19:58:58.678
Aug 29 19:58:58.686: INFO: Get Replication Controller "e2e-rc-j9nfs" to confirm replicas
Aug 29 19:58:59.689: INFO: Get Replication Controller "e2e-rc-j9nfs" to confirm replicas
Aug 29 19:58:59.693: INFO: Found 1 replicas for "e2e-rc-j9nfs" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-j9nfs" 08/29/23 19:58:59.694
STEP: Updating a scale subresource 08/29/23 19:58:59.697
STEP: Verifying replicas where modified for replication controller "e2e-rc-j9nfs" 08/29/23 19:58:59.702
Aug 29 19:58:59.702: INFO: Get Replication Controller "e2e-rc-j9nfs" to confirm replicas
Aug 29 19:59:00.706: INFO: Get Replication Controller "e2e-rc-j9nfs" to confirm replicas
Aug 29 19:59:00.709: INFO: Found 2 replicas for "e2e-rc-j9nfs" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 29 19:59:00.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5331" for this suite. 08/29/23 19:59:00.714
------------------------------
• [2.070 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:58:58.653
    Aug 29 19:58:58.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replication-controller 08/29/23 19:58:58.654
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:58:58.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:58:58.676
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-j9nfs" 08/29/23 19:58:58.678
    Aug 29 19:58:58.686: INFO: Get Replication Controller "e2e-rc-j9nfs" to confirm replicas
    Aug 29 19:58:59.689: INFO: Get Replication Controller "e2e-rc-j9nfs" to confirm replicas
    Aug 29 19:58:59.693: INFO: Found 1 replicas for "e2e-rc-j9nfs" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-j9nfs" 08/29/23 19:58:59.694
    STEP: Updating a scale subresource 08/29/23 19:58:59.697
    STEP: Verifying replicas where modified for replication controller "e2e-rc-j9nfs" 08/29/23 19:58:59.702
    Aug 29 19:58:59.702: INFO: Get Replication Controller "e2e-rc-j9nfs" to confirm replicas
    Aug 29 19:59:00.706: INFO: Get Replication Controller "e2e-rc-j9nfs" to confirm replicas
    Aug 29 19:59:00.709: INFO: Found 2 replicas for "e2e-rc-j9nfs" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:59:00.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5331" for this suite. 08/29/23 19:59:00.714
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:59:00.723
Aug 29 19:59:00.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename proxy 08/29/23 19:59:00.724
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:00.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:00.749
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/29/23 19:59:00.765
STEP: creating replication controller proxy-service-4t846 in namespace proxy-7275 08/29/23 19:59:00.766
I0829 19:59:00.784575      19 runners.go:193] Created replication controller with name: proxy-service-4t846, namespace: proxy-7275, replica count: 1
I0829 19:59:01.835602      19 runners.go:193] proxy-service-4t846 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0829 19:59:02.836674      19 runners.go:193] proxy-service-4t846 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 19:59:02.840: INFO: setup took 2.089058061s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/29/23 19:59:02.84
Aug 29 19:59:02.850: INFO: (0) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 8.316621ms)
Aug 29 19:59:02.850: INFO: (0) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 9.156452ms)
Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 9.757007ms)
Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 9.590022ms)
Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.280551ms)
Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.389998ms)
Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.738922ms)
Aug 29 19:59:02.852: INFO: (0) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.920261ms)
Aug 29 19:59:02.852: INFO: (0) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.665829ms)
Aug 29 19:59:02.852: INFO: (0) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.878533ms)
Aug 29 19:59:02.854: INFO: (0) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 12.406962ms)
Aug 29 19:59:02.860: INFO: (0) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 18.34839ms)
Aug 29 19:59:02.860: INFO: (0) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 19.226068ms)
Aug 29 19:59:02.860: INFO: (0) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 19.523549ms)
Aug 29 19:59:02.860: INFO: (0) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 19.021807ms)
Aug 29 19:59:02.861: INFO: (0) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 19.352873ms)
Aug 29 19:59:02.865: INFO: (1) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 3.961839ms)
Aug 29 19:59:02.865: INFO: (1) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.14092ms)
Aug 29 19:59:02.866: INFO: (1) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.198429ms)
Aug 29 19:59:02.867: INFO: (1) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.661988ms)
Aug 29 19:59:02.867: INFO: (1) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.551988ms)
Aug 29 19:59:02.867: INFO: (1) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.9218ms)
Aug 29 19:59:02.867: INFO: (1) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.800648ms)
Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 6.547303ms)
Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.502371ms)
Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.158969ms)
Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.638181ms)
Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 6.913831ms)
Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.735475ms)
Aug 29 19:59:02.869: INFO: (1) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.183551ms)
Aug 29 19:59:02.870: INFO: (1) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 8.323276ms)
Aug 29 19:59:02.870: INFO: (1) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 8.205456ms)
Aug 29 19:59:02.876: INFO: (2) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.768024ms)
Aug 29 19:59:02.876: INFO: (2) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.960005ms)
Aug 29 19:59:02.876: INFO: (2) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.784769ms)
Aug 29 19:59:02.878: INFO: (2) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 6.891912ms)
Aug 29 19:59:02.878: INFO: (2) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.191006ms)
Aug 29 19:59:02.879: INFO: (2) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.732775ms)
Aug 29 19:59:02.879: INFO: (2) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.408367ms)
Aug 29 19:59:02.879: INFO: (2) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 8.05391ms)
Aug 29 19:59:02.879: INFO: (2) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 8.74805ms)
Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.528597ms)
Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 8.87315ms)
Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 9.299747ms)
Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 9.169055ms)
Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.450234ms)
Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 10.070693ms)
Aug 29 19:59:02.881: INFO: (2) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.392384ms)
Aug 29 19:59:02.886: INFO: (3) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.515287ms)
Aug 29 19:59:02.887: INFO: (3) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 6.137096ms)
Aug 29 19:59:02.888: INFO: (3) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 6.514945ms)
Aug 29 19:59:02.888: INFO: (3) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.027484ms)
Aug 29 19:59:02.888: INFO: (3) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.601425ms)
Aug 29 19:59:02.888: INFO: (3) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 6.707747ms)
Aug 29 19:59:02.889: INFO: (3) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.695841ms)
Aug 29 19:59:02.889: INFO: (3) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 7.765859ms)
Aug 29 19:59:02.890: INFO: (3) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.552331ms)
Aug 29 19:59:02.890: INFO: (3) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 8.137323ms)
Aug 29 19:59:02.891: INFO: (3) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.728292ms)
Aug 29 19:59:02.891: INFO: (3) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 9.654284ms)
Aug 29 19:59:02.891: INFO: (3) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 9.384749ms)
Aug 29 19:59:02.891: INFO: (3) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.338208ms)
Aug 29 19:59:02.892: INFO: (3) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 9.979019ms)
Aug 29 19:59:02.892: INFO: (3) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 10.592051ms)
Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 7.725706ms)
Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 7.730783ms)
Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 8.319533ms)
Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.303231ms)
Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 8.345798ms)
Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 8.744611ms)
Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.526644ms)
Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 8.736726ms)
Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.794881ms)
Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.085647ms)
Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 8.929396ms)
Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 9.348268ms)
Aug 29 19:59:02.902: INFO: (4) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 9.484571ms)
Aug 29 19:59:02.902: INFO: (4) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.987271ms)
Aug 29 19:59:02.904: INFO: (4) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 11.533587ms)
Aug 29 19:59:02.904: INFO: (4) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 12.20355ms)
Aug 29 19:59:02.907: INFO: (5) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 2.90873ms)
Aug 29 19:59:02.912: INFO: (5) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.812183ms)
Aug 29 19:59:02.913: INFO: (5) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 8.064988ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.346601ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 10.749578ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.675438ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.549905ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 10.497176ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 11.237481ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.950189ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 10.386706ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 11.184199ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 10.680695ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 10.337921ms)
Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 11.387822ms)
Aug 29 19:59:02.918: INFO: (5) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 12.800952ms)
Aug 29 19:59:02.921: INFO: (6) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 3.274253ms)
Aug 29 19:59:02.922: INFO: (6) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 4.490894ms)
Aug 29 19:59:02.922: INFO: (6) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 4.124497ms)
Aug 29 19:59:02.923: INFO: (6) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.878363ms)
Aug 29 19:59:02.923: INFO: (6) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 4.622316ms)
Aug 29 19:59:02.923: INFO: (6) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 4.745679ms)
Aug 29 19:59:02.924: INFO: (6) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.329569ms)
Aug 29 19:59:02.924: INFO: (6) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.34433ms)
Aug 29 19:59:02.924: INFO: (6) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.775842ms)
Aug 29 19:59:02.924: INFO: (6) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.010471ms)
Aug 29 19:59:02.925: INFO: (6) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.533286ms)
Aug 29 19:59:02.925: INFO: (6) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 6.51579ms)
Aug 29 19:59:02.925: INFO: (6) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.122909ms)
Aug 29 19:59:02.926: INFO: (6) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.245237ms)
Aug 29 19:59:02.926: INFO: (6) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.609683ms)
Aug 29 19:59:02.926: INFO: (6) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 7.835478ms)
Aug 29 19:59:02.930: INFO: (7) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 3.116452ms)
Aug 29 19:59:02.930: INFO: (7) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 4.468181ms)
Aug 29 19:59:02.930: INFO: (7) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 3.809671ms)
Aug 29 19:59:02.931: INFO: (7) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.46683ms)
Aug 29 19:59:02.931: INFO: (7) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.676499ms)
Aug 29 19:59:02.932: INFO: (7) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 6.030323ms)
Aug 29 19:59:02.932: INFO: (7) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.887191ms)
Aug 29 19:59:02.933: INFO: (7) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 5.984223ms)
Aug 29 19:59:02.933: INFO: (7) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.338978ms)
Aug 29 19:59:02.933: INFO: (7) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.839312ms)
Aug 29 19:59:02.933: INFO: (7) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 6.684305ms)
Aug 29 19:59:02.934: INFO: (7) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.2168ms)
Aug 29 19:59:02.934: INFO: (7) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 7.466362ms)
Aug 29 19:59:02.935: INFO: (7) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.907215ms)
Aug 29 19:59:02.935: INFO: (7) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 8.54494ms)
Aug 29 19:59:02.935: INFO: (7) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 8.673885ms)
Aug 29 19:59:02.940: INFO: (8) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.54481ms)
Aug 29 19:59:02.940: INFO: (8) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 4.906975ms)
Aug 29 19:59:02.942: INFO: (8) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.937827ms)
Aug 29 19:59:02.942: INFO: (8) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.982119ms)
Aug 29 19:59:02.943: INFO: (8) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 7.163117ms)
Aug 29 19:59:02.943: INFO: (8) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 7.298266ms)
Aug 29 19:59:02.943: INFO: (8) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 7.584248ms)
Aug 29 19:59:02.945: INFO: (8) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.070812ms)
Aug 29 19:59:02.945: INFO: (8) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 8.922553ms)
Aug 29 19:59:02.945: INFO: (8) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 9.338094ms)
Aug 29 19:59:02.945: INFO: (8) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.660865ms)
Aug 29 19:59:02.946: INFO: (8) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 9.993115ms)
Aug 29 19:59:02.946: INFO: (8) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 10.039073ms)
Aug 29 19:59:02.946: INFO: (8) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 10.592737ms)
Aug 29 19:59:02.947: INFO: (8) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 11.364004ms)
Aug 29 19:59:02.948: INFO: (8) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 11.870664ms)
Aug 29 19:59:02.952: INFO: (9) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 3.816013ms)
Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 4.474819ms)
Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.532326ms)
Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.235776ms)
Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.462351ms)
Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 4.902452ms)
Aug 29 19:59:02.954: INFO: (9) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.771511ms)
Aug 29 19:59:02.954: INFO: (9) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.324379ms)
Aug 29 19:59:02.954: INFO: (9) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.414992ms)
Aug 29 19:59:02.954: INFO: (9) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 5.443143ms)
Aug 29 19:59:02.955: INFO: (9) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 6.593915ms)
Aug 29 19:59:02.955: INFO: (9) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.944773ms)
Aug 29 19:59:02.955: INFO: (9) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 6.898292ms)
Aug 29 19:59:02.955: INFO: (9) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.029252ms)
Aug 29 19:59:02.956: INFO: (9) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.488594ms)
Aug 29 19:59:02.956: INFO: (9) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.258066ms)
Aug 29 19:59:02.960: INFO: (10) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 3.405842ms)
Aug 29 19:59:02.960: INFO: (10) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 4.252407ms)
Aug 29 19:59:02.960: INFO: (10) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.419963ms)
Aug 29 19:59:02.961: INFO: (10) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.607453ms)
Aug 29 19:59:02.961: INFO: (10) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.233014ms)
Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.961812ms)
Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 4.979702ms)
Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.878017ms)
Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.019282ms)
Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 5.832572ms)
Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.736585ms)
Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 6.258827ms)
Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 6.445089ms)
Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 6.978804ms)
Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.195511ms)
Aug 29 19:59:02.964: INFO: (10) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.440699ms)
Aug 29 19:59:02.970: INFO: (11) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.284551ms)
Aug 29 19:59:02.972: INFO: (11) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.96654ms)
Aug 29 19:59:02.972: INFO: (11) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.020436ms)
Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.481317ms)
Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.879117ms)
Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 9.273554ms)
Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 9.753397ms)
Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 9.561602ms)
Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 9.627449ms)
Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 10.399767ms)
Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.287759ms)
Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.342185ms)
Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 10.574303ms)
Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.542897ms)
Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 10.59621ms)
Aug 29 19:59:02.976: INFO: (11) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 11.816913ms)
Aug 29 19:59:02.979: INFO: (12) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 3.144148ms)
Aug 29 19:59:02.980: INFO: (12) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 3.671996ms)
Aug 29 19:59:02.982: INFO: (12) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.218599ms)
Aug 29 19:59:02.982: INFO: (12) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.221266ms)
Aug 29 19:59:02.982: INFO: (12) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.06535ms)
Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 6.449408ms)
Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.312668ms)
Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.245725ms)
Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.042429ms)
Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.536979ms)
Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.055184ms)
Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.390167ms)
Aug 29 19:59:02.984: INFO: (12) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 6.713755ms)
Aug 29 19:59:02.985: INFO: (12) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 8.121819ms)
Aug 29 19:59:02.985: INFO: (12) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 8.635051ms)
Aug 29 19:59:02.986: INFO: (12) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 8.737264ms)
Aug 29 19:59:02.990: INFO: (13) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.222581ms)
Aug 29 19:59:02.990: INFO: (13) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 4.686321ms)
Aug 29 19:59:02.991: INFO: (13) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.113225ms)
Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.469747ms)
Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.459181ms)
Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.816944ms)
Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.188396ms)
Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.827352ms)
Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 5.795199ms)
Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 6.828731ms)
Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 6.773932ms)
Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 5.969945ms)
Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 6.266773ms)
Aug 29 19:59:02.994: INFO: (13) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 7.181589ms)
Aug 29 19:59:02.994: INFO: (13) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 6.828588ms)
Aug 29 19:59:02.994: INFO: (13) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 7.313009ms)
Aug 29 19:59:02.997: INFO: (14) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 3.636358ms)
Aug 29 19:59:02.999: INFO: (14) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.71808ms)
Aug 29 19:59:02.999: INFO: (14) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.563804ms)
Aug 29 19:59:02.999: INFO: (14) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 4.647222ms)
Aug 29 19:59:03.000: INFO: (14) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 4.659634ms)
Aug 29 19:59:03.000: INFO: (14) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.928697ms)
Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.726826ms)
Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 6.449393ms)
Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 6.770239ms)
Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.861693ms)
Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 6.88998ms)
Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.127659ms)
Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 6.851968ms)
Aug 29 19:59:03.002: INFO: (14) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.166504ms)
Aug 29 19:59:03.002: INFO: (14) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.348809ms)
Aug 29 19:59:03.003: INFO: (14) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 8.23898ms)
Aug 29 19:59:03.009: INFO: (15) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.748598ms)
Aug 29 19:59:03.012: INFO: (15) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 7.776006ms)
Aug 29 19:59:03.013: INFO: (15) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 8.896755ms)
Aug 29 19:59:03.013: INFO: (15) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 8.802683ms)
Aug 29 19:59:03.013: INFO: (15) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.117654ms)
Aug 29 19:59:03.013: INFO: (15) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 9.78753ms)
Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 9.020618ms)
Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 9.533608ms)
Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 9.190566ms)
Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 10.307464ms)
Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 9.401905ms)
Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.4462ms)
Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 9.87975ms)
Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.278421ms)
Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 9.786962ms)
Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.31817ms)
Aug 29 19:59:03.021: INFO: (16) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.94729ms)
Aug 29 19:59:03.021: INFO: (16) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.353501ms)
Aug 29 19:59:03.022: INFO: (16) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.499885ms)
Aug 29 19:59:03.022: INFO: (16) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.608465ms)
Aug 29 19:59:03.022: INFO: (16) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.007029ms)
Aug 29 19:59:03.022: INFO: (16) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.678951ms)
Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.664113ms)
Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.204124ms)
Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.213758ms)
Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 6.555224ms)
Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.121565ms)
Aug 29 19:59:03.024: INFO: (16) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 6.827014ms)
Aug 29 19:59:03.024: INFO: (16) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.486201ms)
Aug 29 19:59:03.024: INFO: (16) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 7.630174ms)
Aug 29 19:59:03.024: INFO: (16) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.966574ms)
Aug 29 19:59:03.025: INFO: (16) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.586908ms)
Aug 29 19:59:03.029: INFO: (17) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.054849ms)
Aug 29 19:59:03.029: INFO: (17) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 4.091677ms)
Aug 29 19:59:03.030: INFO: (17) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.69561ms)
Aug 29 19:59:03.031: INFO: (17) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 5.118598ms)
Aug 29 19:59:03.031: INFO: (17) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.762077ms)
Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 7.473791ms)
Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 8.179765ms)
Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 7.658382ms)
Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 8.038731ms)
Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.715871ms)
Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.852964ms)
Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.955893ms)
Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.09425ms)
Aug 29 19:59:03.034: INFO: (17) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.080191ms)
Aug 29 19:59:03.035: INFO: (17) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 9.728505ms)
Aug 29 19:59:03.035: INFO: (17) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 10.224282ms)
Aug 29 19:59:03.040: INFO: (18) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 4.09872ms)
Aug 29 19:59:03.040: INFO: (18) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.155505ms)
Aug 29 19:59:03.041: INFO: (18) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.234762ms)
Aug 29 19:59:03.041: INFO: (18) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 4.979105ms)
Aug 29 19:59:03.041: INFO: (18) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.318959ms)
Aug 29 19:59:03.041: INFO: (18) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.542427ms)
Aug 29 19:59:03.042: INFO: (18) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.853449ms)
Aug 29 19:59:03.042: INFO: (18) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.986156ms)
Aug 29 19:59:03.042: INFO: (18) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.1809ms)
Aug 29 19:59:03.042: INFO: (18) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.243653ms)
Aug 29 19:59:03.043: INFO: (18) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.097396ms)
Aug 29 19:59:03.044: INFO: (18) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 8.491739ms)
Aug 29 19:59:03.045: INFO: (18) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.108209ms)
Aug 29 19:59:03.045: INFO: (18) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 9.255333ms)
Aug 29 19:59:03.045: INFO: (18) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.367655ms)
Aug 29 19:59:03.045: INFO: (18) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 9.309178ms)
Aug 29 19:59:03.052: INFO: (19) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 6.494136ms)
Aug 29 19:59:03.052: INFO: (19) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 6.153662ms)
Aug 29 19:59:03.052: INFO: (19) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.053696ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.702775ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 9.692682ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.455986ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 9.662642ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.60841ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 10.079267ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 10.462277ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 9.880617ms)
Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.353475ms)
Aug 29 19:59:03.057: INFO: (19) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 10.668037ms)
Aug 29 19:59:03.057: INFO: (19) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 10.936498ms)
Aug 29 19:59:03.058: INFO: (19) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 11.196716ms)
Aug 29 19:59:03.058: INFO: (19) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 11.54291ms)
STEP: deleting ReplicationController proxy-service-4t846 in namespace proxy-7275, will wait for the garbage collector to delete the pods 08/29/23 19:59:03.058
Aug 29 19:59:03.121: INFO: Deleting ReplicationController proxy-service-4t846 took: 7.849156ms
Aug 29 19:59:03.222: INFO: Terminating ReplicationController proxy-service-4t846 pods took: 100.620044ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 29 19:59:05.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-7275" for this suite. 08/29/23 19:59:05.228
------------------------------
• [4.512 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:59:00.723
    Aug 29 19:59:00.723: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename proxy 08/29/23 19:59:00.724
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:00.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:00.749
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/29/23 19:59:00.765
    STEP: creating replication controller proxy-service-4t846 in namespace proxy-7275 08/29/23 19:59:00.766
    I0829 19:59:00.784575      19 runners.go:193] Created replication controller with name: proxy-service-4t846, namespace: proxy-7275, replica count: 1
    I0829 19:59:01.835602      19 runners.go:193] proxy-service-4t846 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0829 19:59:02.836674      19 runners.go:193] proxy-service-4t846 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 19:59:02.840: INFO: setup took 2.089058061s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/29/23 19:59:02.84
    Aug 29 19:59:02.850: INFO: (0) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 8.316621ms)
    Aug 29 19:59:02.850: INFO: (0) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 9.156452ms)
    Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 9.757007ms)
    Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 9.590022ms)
    Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.280551ms)
    Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.389998ms)
    Aug 29 19:59:02.851: INFO: (0) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.738922ms)
    Aug 29 19:59:02.852: INFO: (0) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.920261ms)
    Aug 29 19:59:02.852: INFO: (0) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.665829ms)
    Aug 29 19:59:02.852: INFO: (0) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.878533ms)
    Aug 29 19:59:02.854: INFO: (0) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 12.406962ms)
    Aug 29 19:59:02.860: INFO: (0) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 18.34839ms)
    Aug 29 19:59:02.860: INFO: (0) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 19.226068ms)
    Aug 29 19:59:02.860: INFO: (0) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 19.523549ms)
    Aug 29 19:59:02.860: INFO: (0) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 19.021807ms)
    Aug 29 19:59:02.861: INFO: (0) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 19.352873ms)
    Aug 29 19:59:02.865: INFO: (1) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 3.961839ms)
    Aug 29 19:59:02.865: INFO: (1) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.14092ms)
    Aug 29 19:59:02.866: INFO: (1) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.198429ms)
    Aug 29 19:59:02.867: INFO: (1) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.661988ms)
    Aug 29 19:59:02.867: INFO: (1) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.551988ms)
    Aug 29 19:59:02.867: INFO: (1) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.9218ms)
    Aug 29 19:59:02.867: INFO: (1) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.800648ms)
    Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 6.547303ms)
    Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.502371ms)
    Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.158969ms)
    Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.638181ms)
    Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 6.913831ms)
    Aug 29 19:59:02.868: INFO: (1) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.735475ms)
    Aug 29 19:59:02.869: INFO: (1) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.183551ms)
    Aug 29 19:59:02.870: INFO: (1) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 8.323276ms)
    Aug 29 19:59:02.870: INFO: (1) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 8.205456ms)
    Aug 29 19:59:02.876: INFO: (2) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.768024ms)
    Aug 29 19:59:02.876: INFO: (2) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.960005ms)
    Aug 29 19:59:02.876: INFO: (2) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.784769ms)
    Aug 29 19:59:02.878: INFO: (2) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 6.891912ms)
    Aug 29 19:59:02.878: INFO: (2) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.191006ms)
    Aug 29 19:59:02.879: INFO: (2) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.732775ms)
    Aug 29 19:59:02.879: INFO: (2) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.408367ms)
    Aug 29 19:59:02.879: INFO: (2) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 8.05391ms)
    Aug 29 19:59:02.879: INFO: (2) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 8.74805ms)
    Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.528597ms)
    Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 8.87315ms)
    Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 9.299747ms)
    Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 9.169055ms)
    Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.450234ms)
    Aug 29 19:59:02.880: INFO: (2) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 10.070693ms)
    Aug 29 19:59:02.881: INFO: (2) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.392384ms)
    Aug 29 19:59:02.886: INFO: (3) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.515287ms)
    Aug 29 19:59:02.887: INFO: (3) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 6.137096ms)
    Aug 29 19:59:02.888: INFO: (3) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 6.514945ms)
    Aug 29 19:59:02.888: INFO: (3) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.027484ms)
    Aug 29 19:59:02.888: INFO: (3) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.601425ms)
    Aug 29 19:59:02.888: INFO: (3) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 6.707747ms)
    Aug 29 19:59:02.889: INFO: (3) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.695841ms)
    Aug 29 19:59:02.889: INFO: (3) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 7.765859ms)
    Aug 29 19:59:02.890: INFO: (3) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.552331ms)
    Aug 29 19:59:02.890: INFO: (3) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 8.137323ms)
    Aug 29 19:59:02.891: INFO: (3) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.728292ms)
    Aug 29 19:59:02.891: INFO: (3) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 9.654284ms)
    Aug 29 19:59:02.891: INFO: (3) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 9.384749ms)
    Aug 29 19:59:02.891: INFO: (3) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.338208ms)
    Aug 29 19:59:02.892: INFO: (3) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 9.979019ms)
    Aug 29 19:59:02.892: INFO: (3) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 10.592051ms)
    Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 7.725706ms)
    Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 7.730783ms)
    Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 8.319533ms)
    Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.303231ms)
    Aug 29 19:59:02.900: INFO: (4) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 8.345798ms)
    Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 8.744611ms)
    Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.526644ms)
    Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 8.736726ms)
    Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.794881ms)
    Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.085647ms)
    Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 8.929396ms)
    Aug 29 19:59:02.901: INFO: (4) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 9.348268ms)
    Aug 29 19:59:02.902: INFO: (4) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 9.484571ms)
    Aug 29 19:59:02.902: INFO: (4) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.987271ms)
    Aug 29 19:59:02.904: INFO: (4) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 11.533587ms)
    Aug 29 19:59:02.904: INFO: (4) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 12.20355ms)
    Aug 29 19:59:02.907: INFO: (5) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 2.90873ms)
    Aug 29 19:59:02.912: INFO: (5) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.812183ms)
    Aug 29 19:59:02.913: INFO: (5) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 8.064988ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.346601ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 10.749578ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.675438ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.549905ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 10.497176ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 11.237481ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.950189ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 10.386706ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 11.184199ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 10.680695ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 10.337921ms)
    Aug 29 19:59:02.916: INFO: (5) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 11.387822ms)
    Aug 29 19:59:02.918: INFO: (5) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 12.800952ms)
    Aug 29 19:59:02.921: INFO: (6) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 3.274253ms)
    Aug 29 19:59:02.922: INFO: (6) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 4.490894ms)
    Aug 29 19:59:02.922: INFO: (6) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 4.124497ms)
    Aug 29 19:59:02.923: INFO: (6) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.878363ms)
    Aug 29 19:59:02.923: INFO: (6) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 4.622316ms)
    Aug 29 19:59:02.923: INFO: (6) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 4.745679ms)
    Aug 29 19:59:02.924: INFO: (6) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.329569ms)
    Aug 29 19:59:02.924: INFO: (6) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.34433ms)
    Aug 29 19:59:02.924: INFO: (6) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.775842ms)
    Aug 29 19:59:02.924: INFO: (6) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.010471ms)
    Aug 29 19:59:02.925: INFO: (6) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.533286ms)
    Aug 29 19:59:02.925: INFO: (6) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 6.51579ms)
    Aug 29 19:59:02.925: INFO: (6) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.122909ms)
    Aug 29 19:59:02.926: INFO: (6) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.245237ms)
    Aug 29 19:59:02.926: INFO: (6) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.609683ms)
    Aug 29 19:59:02.926: INFO: (6) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 7.835478ms)
    Aug 29 19:59:02.930: INFO: (7) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 3.116452ms)
    Aug 29 19:59:02.930: INFO: (7) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 4.468181ms)
    Aug 29 19:59:02.930: INFO: (7) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 3.809671ms)
    Aug 29 19:59:02.931: INFO: (7) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.46683ms)
    Aug 29 19:59:02.931: INFO: (7) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.676499ms)
    Aug 29 19:59:02.932: INFO: (7) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 6.030323ms)
    Aug 29 19:59:02.932: INFO: (7) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.887191ms)
    Aug 29 19:59:02.933: INFO: (7) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 5.984223ms)
    Aug 29 19:59:02.933: INFO: (7) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.338978ms)
    Aug 29 19:59:02.933: INFO: (7) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.839312ms)
    Aug 29 19:59:02.933: INFO: (7) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 6.684305ms)
    Aug 29 19:59:02.934: INFO: (7) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.2168ms)
    Aug 29 19:59:02.934: INFO: (7) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 7.466362ms)
    Aug 29 19:59:02.935: INFO: (7) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.907215ms)
    Aug 29 19:59:02.935: INFO: (7) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 8.54494ms)
    Aug 29 19:59:02.935: INFO: (7) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 8.673885ms)
    Aug 29 19:59:02.940: INFO: (8) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.54481ms)
    Aug 29 19:59:02.940: INFO: (8) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 4.906975ms)
    Aug 29 19:59:02.942: INFO: (8) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.937827ms)
    Aug 29 19:59:02.942: INFO: (8) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.982119ms)
    Aug 29 19:59:02.943: INFO: (8) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 7.163117ms)
    Aug 29 19:59:02.943: INFO: (8) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 7.298266ms)
    Aug 29 19:59:02.943: INFO: (8) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 7.584248ms)
    Aug 29 19:59:02.945: INFO: (8) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.070812ms)
    Aug 29 19:59:02.945: INFO: (8) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 8.922553ms)
    Aug 29 19:59:02.945: INFO: (8) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 9.338094ms)
    Aug 29 19:59:02.945: INFO: (8) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.660865ms)
    Aug 29 19:59:02.946: INFO: (8) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 9.993115ms)
    Aug 29 19:59:02.946: INFO: (8) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 10.039073ms)
    Aug 29 19:59:02.946: INFO: (8) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 10.592737ms)
    Aug 29 19:59:02.947: INFO: (8) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 11.364004ms)
    Aug 29 19:59:02.948: INFO: (8) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 11.870664ms)
    Aug 29 19:59:02.952: INFO: (9) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 3.816013ms)
    Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 4.474819ms)
    Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.532326ms)
    Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.235776ms)
    Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.462351ms)
    Aug 29 19:59:02.953: INFO: (9) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 4.902452ms)
    Aug 29 19:59:02.954: INFO: (9) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.771511ms)
    Aug 29 19:59:02.954: INFO: (9) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.324379ms)
    Aug 29 19:59:02.954: INFO: (9) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.414992ms)
    Aug 29 19:59:02.954: INFO: (9) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 5.443143ms)
    Aug 29 19:59:02.955: INFO: (9) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 6.593915ms)
    Aug 29 19:59:02.955: INFO: (9) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.944773ms)
    Aug 29 19:59:02.955: INFO: (9) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 6.898292ms)
    Aug 29 19:59:02.955: INFO: (9) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.029252ms)
    Aug 29 19:59:02.956: INFO: (9) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.488594ms)
    Aug 29 19:59:02.956: INFO: (9) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.258066ms)
    Aug 29 19:59:02.960: INFO: (10) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 3.405842ms)
    Aug 29 19:59:02.960: INFO: (10) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 4.252407ms)
    Aug 29 19:59:02.960: INFO: (10) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.419963ms)
    Aug 29 19:59:02.961: INFO: (10) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.607453ms)
    Aug 29 19:59:02.961: INFO: (10) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.233014ms)
    Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.961812ms)
    Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 4.979702ms)
    Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.878017ms)
    Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.019282ms)
    Aug 29 19:59:02.962: INFO: (10) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 5.832572ms)
    Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.736585ms)
    Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 6.258827ms)
    Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 6.445089ms)
    Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 6.978804ms)
    Aug 29 19:59:02.963: INFO: (10) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.195511ms)
    Aug 29 19:59:02.964: INFO: (10) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.440699ms)
    Aug 29 19:59:02.970: INFO: (11) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.284551ms)
    Aug 29 19:59:02.972: INFO: (11) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.96654ms)
    Aug 29 19:59:02.972: INFO: (11) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.020436ms)
    Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.481317ms)
    Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.879117ms)
    Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 9.273554ms)
    Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 9.753397ms)
    Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 9.561602ms)
    Aug 29 19:59:02.974: INFO: (11) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 9.627449ms)
    Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 10.399767ms)
    Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.287759ms)
    Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.342185ms)
    Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 10.574303ms)
    Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.542897ms)
    Aug 29 19:59:02.975: INFO: (11) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 10.59621ms)
    Aug 29 19:59:02.976: INFO: (11) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 11.816913ms)
    Aug 29 19:59:02.979: INFO: (12) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 3.144148ms)
    Aug 29 19:59:02.980: INFO: (12) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 3.671996ms)
    Aug 29 19:59:02.982: INFO: (12) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.218599ms)
    Aug 29 19:59:02.982: INFO: (12) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.221266ms)
    Aug 29 19:59:02.982: INFO: (12) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.06535ms)
    Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 6.449408ms)
    Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.312668ms)
    Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.245725ms)
    Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.042429ms)
    Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.536979ms)
    Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.055184ms)
    Aug 29 19:59:02.983: INFO: (12) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.390167ms)
    Aug 29 19:59:02.984: INFO: (12) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 6.713755ms)
    Aug 29 19:59:02.985: INFO: (12) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 8.121819ms)
    Aug 29 19:59:02.985: INFO: (12) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 8.635051ms)
    Aug 29 19:59:02.986: INFO: (12) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 8.737264ms)
    Aug 29 19:59:02.990: INFO: (13) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.222581ms)
    Aug 29 19:59:02.990: INFO: (13) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 4.686321ms)
    Aug 29 19:59:02.991: INFO: (13) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.113225ms)
    Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.469747ms)
    Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.459181ms)
    Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.816944ms)
    Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.188396ms)
    Aug 29 19:59:02.992: INFO: (13) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.827352ms)
    Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 5.795199ms)
    Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 6.828731ms)
    Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 6.773932ms)
    Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 5.969945ms)
    Aug 29 19:59:02.993: INFO: (13) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 6.266773ms)
    Aug 29 19:59:02.994: INFO: (13) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 7.181589ms)
    Aug 29 19:59:02.994: INFO: (13) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 6.828588ms)
    Aug 29 19:59:02.994: INFO: (13) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 7.313009ms)
    Aug 29 19:59:02.997: INFO: (14) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 3.636358ms)
    Aug 29 19:59:02.999: INFO: (14) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.71808ms)
    Aug 29 19:59:02.999: INFO: (14) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.563804ms)
    Aug 29 19:59:02.999: INFO: (14) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 4.647222ms)
    Aug 29 19:59:03.000: INFO: (14) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 4.659634ms)
    Aug 29 19:59:03.000: INFO: (14) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.928697ms)
    Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.726826ms)
    Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 6.449393ms)
    Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 6.770239ms)
    Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 5.861693ms)
    Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 6.88998ms)
    Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.127659ms)
    Aug 29 19:59:03.001: INFO: (14) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 6.851968ms)
    Aug 29 19:59:03.002: INFO: (14) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.166504ms)
    Aug 29 19:59:03.002: INFO: (14) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.348809ms)
    Aug 29 19:59:03.003: INFO: (14) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 8.23898ms)
    Aug 29 19:59:03.009: INFO: (15) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.748598ms)
    Aug 29 19:59:03.012: INFO: (15) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 7.776006ms)
    Aug 29 19:59:03.013: INFO: (15) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 8.896755ms)
    Aug 29 19:59:03.013: INFO: (15) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 8.802683ms)
    Aug 29 19:59:03.013: INFO: (15) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.117654ms)
    Aug 29 19:59:03.013: INFO: (15) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 9.78753ms)
    Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 9.020618ms)
    Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 9.533608ms)
    Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 9.190566ms)
    Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 10.307464ms)
    Aug 29 19:59:03.014: INFO: (15) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 9.401905ms)
    Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.4462ms)
    Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 9.87975ms)
    Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 10.278421ms)
    Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 9.786962ms)
    Aug 29 19:59:03.015: INFO: (15) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.31817ms)
    Aug 29 19:59:03.021: INFO: (16) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.94729ms)
    Aug 29 19:59:03.021: INFO: (16) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.353501ms)
    Aug 29 19:59:03.022: INFO: (16) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.499885ms)
    Aug 29 19:59:03.022: INFO: (16) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 6.608465ms)
    Aug 29 19:59:03.022: INFO: (16) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 6.007029ms)
    Aug 29 19:59:03.022: INFO: (16) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 5.678951ms)
    Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.664113ms)
    Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.204124ms)
    Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.213758ms)
    Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 6.555224ms)
    Aug 29 19:59:03.023: INFO: (16) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 6.121565ms)
    Aug 29 19:59:03.024: INFO: (16) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 6.827014ms)
    Aug 29 19:59:03.024: INFO: (16) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.486201ms)
    Aug 29 19:59:03.024: INFO: (16) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 7.630174ms)
    Aug 29 19:59:03.024: INFO: (16) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.966574ms)
    Aug 29 19:59:03.025: INFO: (16) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 7.586908ms)
    Aug 29 19:59:03.029: INFO: (17) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 4.054849ms)
    Aug 29 19:59:03.029: INFO: (17) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 4.091677ms)
    Aug 29 19:59:03.030: INFO: (17) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 4.69561ms)
    Aug 29 19:59:03.031: INFO: (17) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 5.118598ms)
    Aug 29 19:59:03.031: INFO: (17) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.762077ms)
    Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 7.473791ms)
    Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 8.179765ms)
    Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 7.658382ms)
    Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 8.038731ms)
    Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 7.715871ms)
    Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 7.852964ms)
    Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 7.955893ms)
    Aug 29 19:59:03.033: INFO: (17) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 8.09425ms)
    Aug 29 19:59:03.034: INFO: (17) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 8.080191ms)
    Aug 29 19:59:03.035: INFO: (17) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 9.728505ms)
    Aug 29 19:59:03.035: INFO: (17) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 10.224282ms)
    Aug 29 19:59:03.040: INFO: (18) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 4.09872ms)
    Aug 29 19:59:03.040: INFO: (18) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 4.155505ms)
    Aug 29 19:59:03.041: INFO: (18) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 5.234762ms)
    Aug 29 19:59:03.041: INFO: (18) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 4.979105ms)
    Aug 29 19:59:03.041: INFO: (18) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 5.318959ms)
    Aug 29 19:59:03.041: INFO: (18) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 5.542427ms)
    Aug 29 19:59:03.042: INFO: (18) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.853449ms)
    Aug 29 19:59:03.042: INFO: (18) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 5.986156ms)
    Aug 29 19:59:03.042: INFO: (18) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.1809ms)
    Aug 29 19:59:03.042: INFO: (18) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.243653ms)
    Aug 29 19:59:03.043: INFO: (18) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 7.097396ms)
    Aug 29 19:59:03.044: INFO: (18) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 8.491739ms)
    Aug 29 19:59:03.045: INFO: (18) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 9.108209ms)
    Aug 29 19:59:03.045: INFO: (18) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 9.255333ms)
    Aug 29 19:59:03.045: INFO: (18) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 9.367655ms)
    Aug 29 19:59:03.045: INFO: (18) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 9.309178ms)
    Aug 29 19:59:03.052: INFO: (19) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:460/proxy/: tls baz (200; 6.494136ms)
    Aug 29 19:59:03.052: INFO: (19) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:1080/proxy/rewriteme">... (200; 6.153662ms)
    Aug 29 19:59:03.052: INFO: (19) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:162/proxy/: bar (200; 6.053696ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.702775ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/http:proxy-service-4t846-j7m6g:162/proxy/: bar (200; 9.692682ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname1/proxy/: foo (200; 10.455986ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g/proxy/rewriteme">test</a> (200; 9.662642ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:160/proxy/: foo (200; 9.60841ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:462/proxy/: tls qux (200; 10.079267ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname2/proxy/: tls qux (200; 10.462277ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/proxy-service-4t846-j7m6g:1080/proxy/rewriteme">test<... (200; 9.880617ms)
    Aug 29 19:59:03.056: INFO: (19) /api/v1/namespaces/proxy-7275/services/http:proxy-service-4t846:portname2/proxy/: bar (200; 10.353475ms)
    Aug 29 19:59:03.057: INFO: (19) /api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/: <a href="/api/v1/namespaces/proxy-7275/pods/https:proxy-service-4t846-j7m6g:443/proxy/tlsrewritem... (200; 10.668037ms)
    Aug 29 19:59:03.057: INFO: (19) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname2/proxy/: bar (200; 10.936498ms)
    Aug 29 19:59:03.058: INFO: (19) /api/v1/namespaces/proxy-7275/services/https:proxy-service-4t846:tlsportname1/proxy/: tls baz (200; 11.196716ms)
    Aug 29 19:59:03.058: INFO: (19) /api/v1/namespaces/proxy-7275/services/proxy-service-4t846:portname1/proxy/: foo (200; 11.54291ms)
    STEP: deleting ReplicationController proxy-service-4t846 in namespace proxy-7275, will wait for the garbage collector to delete the pods 08/29/23 19:59:03.058
    Aug 29 19:59:03.121: INFO: Deleting ReplicationController proxy-service-4t846 took: 7.849156ms
    Aug 29 19:59:03.222: INFO: Terminating ReplicationController proxy-service-4t846 pods took: 100.620044ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:59:05.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-7275" for this suite. 08/29/23 19:59:05.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:59:05.236
Aug 29 19:59:05.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 19:59:05.238
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:05.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:05.258
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-380435e4-ecdc-4dfe-861b-61d3ec867a0c 08/29/23 19:59:05.279
STEP: Creating configMap with name cm-test-opt-upd-e89f135c-e1c1-444d-80ad-b7f331d9f51b 08/29/23 19:59:05.285
STEP: Creating the pod 08/29/23 19:59:05.291
Aug 29 19:59:05.303: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8" in namespace "projected-2848" to be "running and ready"
Aug 29 19:59:05.307: INFO: Pod "pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.365093ms
Aug 29 19:59:05.307: INFO: The phase of Pod pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 19:59:07.311: INFO: Pod "pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.007695634s
Aug 29 19:59:07.311: INFO: The phase of Pod pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8 is Running (Ready = true)
Aug 29 19:59:07.311: INFO: Pod "pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-380435e4-ecdc-4dfe-861b-61d3ec867a0c 08/29/23 19:59:07.338
STEP: Updating configmap cm-test-opt-upd-e89f135c-e1c1-444d-80ad-b7f331d9f51b 08/29/23 19:59:07.347
STEP: Creating configMap with name cm-test-opt-create-edad6170-da3a-4d15-a60f-8ed9c95e90ed 08/29/23 19:59:07.358
STEP: waiting to observe update in volume 08/29/23 19:59:07.363
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 19:59:09.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2848" for this suite. 08/29/23 19:59:09.408
------------------------------
• [4.180 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:59:05.236
    Aug 29 19:59:05.236: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 19:59:05.238
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:05.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:05.258
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-380435e4-ecdc-4dfe-861b-61d3ec867a0c 08/29/23 19:59:05.279
    STEP: Creating configMap with name cm-test-opt-upd-e89f135c-e1c1-444d-80ad-b7f331d9f51b 08/29/23 19:59:05.285
    STEP: Creating the pod 08/29/23 19:59:05.291
    Aug 29 19:59:05.303: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8" in namespace "projected-2848" to be "running and ready"
    Aug 29 19:59:05.307: INFO: Pod "pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.365093ms
    Aug 29 19:59:05.307: INFO: The phase of Pod pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 19:59:07.311: INFO: Pod "pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.007695634s
    Aug 29 19:59:07.311: INFO: The phase of Pod pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8 is Running (Ready = true)
    Aug 29 19:59:07.311: INFO: Pod "pod-projected-configmaps-43a5d349-147d-47c8-809a-fb811e3817b8" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-380435e4-ecdc-4dfe-861b-61d3ec867a0c 08/29/23 19:59:07.338
    STEP: Updating configmap cm-test-opt-upd-e89f135c-e1c1-444d-80ad-b7f331d9f51b 08/29/23 19:59:07.347
    STEP: Creating configMap with name cm-test-opt-create-edad6170-da3a-4d15-a60f-8ed9c95e90ed 08/29/23 19:59:07.358
    STEP: waiting to observe update in volume 08/29/23 19:59:07.363
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:59:09.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2848" for this suite. 08/29/23 19:59:09.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:59:09.417
Aug 29 19:59:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 19:59:09.418
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:09.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:09.439
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-552 08/29/23 19:59:09.442
STEP: changing the ExternalName service to type=ClusterIP 08/29/23 19:59:09.451
STEP: creating replication controller externalname-service in namespace services-552 08/29/23 19:59:09.47
I0829 19:59:09.480566      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-552, replica count: 2
I0829 19:59:12.530882      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 19:59:12.530: INFO: Creating new exec pod
Aug 29 19:59:12.540: INFO: Waiting up to 5m0s for pod "execpodq56fc" in namespace "services-552" to be "running"
Aug 29 19:59:12.544: INFO: Pod "execpodq56fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879801ms
Aug 29 19:59:14.547: INFO: Pod "execpodq56fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007483286s
Aug 29 19:59:14.547: INFO: Pod "execpodq56fc" satisfied condition "running"
Aug 29 19:59:15.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-552 exec execpodq56fc -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 29 19:59:15.736: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 19:59:15.736: INFO: stdout: ""
Aug 29 19:59:15.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-552 exec execpodq56fc -- /bin/sh -x -c nc -v -z -w 2 172.19.140.23 80'
Aug 29 19:59:15.895: INFO: stderr: "+ nc -v -z -w 2 172.19.140.23 80\nConnection to 172.19.140.23 80 port [tcp/http] succeeded!\n"
Aug 29 19:59:15.895: INFO: stdout: ""
Aug 29 19:59:15.895: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 19:59:15.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-552" for this suite. 08/29/23 19:59:15.93
------------------------------
• [SLOW TEST] [6.525 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:59:09.417
    Aug 29 19:59:09.417: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 19:59:09.418
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:09.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:09.439
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-552 08/29/23 19:59:09.442
    STEP: changing the ExternalName service to type=ClusterIP 08/29/23 19:59:09.451
    STEP: creating replication controller externalname-service in namespace services-552 08/29/23 19:59:09.47
    I0829 19:59:09.480566      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-552, replica count: 2
    I0829 19:59:12.530882      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 19:59:12.530: INFO: Creating new exec pod
    Aug 29 19:59:12.540: INFO: Waiting up to 5m0s for pod "execpodq56fc" in namespace "services-552" to be "running"
    Aug 29 19:59:12.544: INFO: Pod "execpodq56fc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.879801ms
    Aug 29 19:59:14.547: INFO: Pod "execpodq56fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.007483286s
    Aug 29 19:59:14.547: INFO: Pod "execpodq56fc" satisfied condition "running"
    Aug 29 19:59:15.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-552 exec execpodq56fc -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 29 19:59:15.736: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 29 19:59:15.736: INFO: stdout: ""
    Aug 29 19:59:15.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-552 exec execpodq56fc -- /bin/sh -x -c nc -v -z -w 2 172.19.140.23 80'
    Aug 29 19:59:15.895: INFO: stderr: "+ nc -v -z -w 2 172.19.140.23 80\nConnection to 172.19.140.23 80 port [tcp/http] succeeded!\n"
    Aug 29 19:59:15.895: INFO: stdout: ""
    Aug 29 19:59:15.895: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:59:15.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-552" for this suite. 08/29/23 19:59:15.93
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:59:15.943
Aug 29 19:59:15.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename namespaces 08/29/23 19:59:15.944
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:15.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:15.963
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 08/29/23 19:59:15.969
Aug 29 19:59:15.972: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/29/23 19:59:15.973
Aug 29 19:59:15.978: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/29/23 19:59:15.978
Aug 29 19:59:15.987: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 19:59:15.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-3698" for this suite. 08/29/23 19:59:15.992
------------------------------
• [0.064 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:59:15.943
    Aug 29 19:59:15.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename namespaces 08/29/23 19:59:15.944
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:15.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:15.963
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 08/29/23 19:59:15.969
    Aug 29 19:59:15.972: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/29/23 19:59:15.973
    Aug 29 19:59:15.978: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/29/23 19:59:15.978
    Aug 29 19:59:15.987: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:59:15.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-3698" for this suite. 08/29/23 19:59:15.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:59:16.007
Aug 29 19:59:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename dns 08/29/23 19:59:16.008
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:16.024
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:16.026
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9492.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9492.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/29/23 19:59:16.03
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9492.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9492.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/29/23 19:59:16.03
STEP: creating a pod to probe /etc/hosts 08/29/23 19:59:16.03
STEP: submitting the pod to kubernetes 08/29/23 19:59:16.03
Aug 29 19:59:16.044: INFO: Waiting up to 15m0s for pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a" in namespace "dns-9492" to be "running"
Aug 29 19:59:16.048: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.449682ms
Aug 29 19:59:18.054: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009575205s
Aug 29 19:59:20.053: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009142293s
Aug 29 19:59:22.053: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0091942s
Aug 29 19:59:24.053: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009011618s
Aug 29 19:59:26.052: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Running", Reason="", readiness=true. Elapsed: 10.008097594s
Aug 29 19:59:26.052: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a" satisfied condition "running"
STEP: retrieving the pod 08/29/23 19:59:26.052
STEP: looking for the results for each expected name from probers 08/29/23 19:59:26.055
Aug 29 19:59:26.071: INFO: DNS probes using dns-9492/dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a succeeded

STEP: deleting the pod 08/29/23 19:59:26.071
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 29 19:59:26.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9492" for this suite. 08/29/23 19:59:26.091
------------------------------
• [SLOW TEST] [10.092 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:59:16.007
    Aug 29 19:59:16.007: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename dns 08/29/23 19:59:16.008
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:16.024
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:16.026
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9492.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9492.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/29/23 19:59:16.03
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9492.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9492.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/29/23 19:59:16.03
    STEP: creating a pod to probe /etc/hosts 08/29/23 19:59:16.03
    STEP: submitting the pod to kubernetes 08/29/23 19:59:16.03
    Aug 29 19:59:16.044: INFO: Waiting up to 15m0s for pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a" in namespace "dns-9492" to be "running"
    Aug 29 19:59:16.048: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.449682ms
    Aug 29 19:59:18.054: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009575205s
    Aug 29 19:59:20.053: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009142293s
    Aug 29 19:59:22.053: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0091942s
    Aug 29 19:59:24.053: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009011618s
    Aug 29 19:59:26.052: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a": Phase="Running", Reason="", readiness=true. Elapsed: 10.008097594s
    Aug 29 19:59:26.052: INFO: Pod "dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 19:59:26.052
    STEP: looking for the results for each expected name from probers 08/29/23 19:59:26.055
    Aug 29 19:59:26.071: INFO: DNS probes using dns-9492/dns-test-a886e9d0-5e83-46e7-aa85-3d82ad2a131a succeeded

    STEP: deleting the pod 08/29/23 19:59:26.071
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 29 19:59:26.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9492" for this suite. 08/29/23 19:59:26.091
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 19:59:26.1
Aug 29 19:59:26.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-preemption 08/29/23 19:59:26.101
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:26.12
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:26.123
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 29 19:59:26.143: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 20:00:26.203: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224
STEP: Create pods that use 4/5 of node resources. 08/29/23 20:00:26.206
Aug 29 20:00:26.336: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 29 20:00:26.483: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 29 20:00:26.793: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 29 20:00:26.836: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 29 20:00:27.130: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 29 20:00:27.199: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Aug 29 20:00:27.237: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Aug 29 20:00:27.254: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Aug 29 20:00:27.283: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Aug 29 20:00:27.292: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/29/23 20:00:27.292
Aug 29 20:00:27.292: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:27.296: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054431ms
Aug 29 20:00:29.301: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008660776s
Aug 29 20:00:29.301: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 29 20:00:29.301: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.305: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.545369ms
Aug 29 20:00:29.305: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:00:29.305: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.308: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.148522ms
Aug 29 20:00:29.308: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:00:29.308: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.311: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.981282ms
Aug 29 20:00:29.311: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:00:29.311: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.316: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.600268ms
Aug 29 20:00:29.317: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:00:29.317: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.319: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.810339ms
Aug 29 20:00:29.319: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:00:29.319: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.323: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.466084ms
Aug 29 20:00:29.323: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:00:29.323: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.326: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.121726ms
Aug 29 20:00:29.326: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:00:29.326: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.329: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.94844ms
Aug 29 20:00:29.329: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:00:29.329: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
Aug 29 20:00:29.332: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.864001ms
Aug 29 20:00:29.332: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/29/23 20:00:29.332
Aug 29 20:00:29.347: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 29 20:00:29.350: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263534ms
Aug 29 20:00:31.355: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008096753s
Aug 29 20:00:33.355: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007430555s
Aug 29 20:00:35.357: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009760969s
Aug 29 20:00:35.357: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:00:35.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5372" for this suite. 08/29/23 20:00:35.481
------------------------------
• [SLOW TEST] [69.388 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 19:59:26.1
    Aug 29 19:59:26.100: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-preemption 08/29/23 19:59:26.101
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 19:59:26.12
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 19:59:26.123
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 29 19:59:26.143: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 29 20:00:26.203: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:224
    STEP: Create pods that use 4/5 of node resources. 08/29/23 20:00:26.206
    Aug 29 20:00:26.336: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 29 20:00:26.483: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 29 20:00:26.793: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 29 20:00:26.836: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 29 20:00:27.130: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 29 20:00:27.199: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Aug 29 20:00:27.237: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Aug 29 20:00:27.254: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Aug 29 20:00:27.283: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Aug 29 20:00:27.292: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/29/23 20:00:27.292
    Aug 29 20:00:27.292: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:27.296: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054431ms
    Aug 29 20:00:29.301: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008660776s
    Aug 29 20:00:29.301: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 29 20:00:29.301: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.305: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.545369ms
    Aug 29 20:00:29.305: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:00:29.305: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.308: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.148522ms
    Aug 29 20:00:29.308: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:00:29.308: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.311: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.981282ms
    Aug 29 20:00:29.311: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:00:29.311: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.316: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 5.600268ms
    Aug 29 20:00:29.317: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:00:29.317: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.319: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.810339ms
    Aug 29 20:00:29.319: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:00:29.319: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.323: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.466084ms
    Aug 29 20:00:29.323: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:00:29.323: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.326: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.121726ms
    Aug 29 20:00:29.326: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:00:29.326: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.329: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.94844ms
    Aug 29 20:00:29.329: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:00:29.329: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-5372" to be "running"
    Aug 29 20:00:29.332: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.864001ms
    Aug 29 20:00:29.332: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/29/23 20:00:29.332
    Aug 29 20:00:29.347: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 29 20:00:29.350: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.263534ms
    Aug 29 20:00:31.355: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008096753s
    Aug 29 20:00:33.355: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007430555s
    Aug 29 20:00:35.357: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009760969s
    Aug 29 20:00:35.357: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:00:35.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5372" for this suite. 08/29/23 20:00:35.481
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:00:35.489
Aug 29 20:00:35.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:00:35.49
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:35.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:35.511
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-6514/secret-test-9acc33fb-6f81-400f-beaa-127fa04df5d8 08/29/23 20:00:35.514
STEP: Creating a pod to test consume secrets 08/29/23 20:00:35.52
Aug 29 20:00:35.530: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4" in namespace "secrets-6514" to be "Succeeded or Failed"
Aug 29 20:00:35.534: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00449ms
Aug 29 20:00:37.540: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009552273s
Aug 29 20:00:39.540: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4": Phase="Running", Reason="", readiness=false. Elapsed: 4.009627794s
Aug 29 20:00:41.540: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009840969s
STEP: Saw pod success 08/29/23 20:00:41.54
Aug 29 20:00:41.541: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4" satisfied condition "Succeeded or Failed"
Aug 29 20:00:41.552: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4 container env-test: <nil>
STEP: delete the pod 08/29/23 20:00:41.579
Aug 29 20:00:41.602: INFO: Waiting for pod pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4 to disappear
Aug 29 20:00:41.606: INFO: Pod pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:00:41.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6514" for this suite. 08/29/23 20:00:41.611
------------------------------
• [SLOW TEST] [6.133 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:00:35.489
    Aug 29 20:00:35.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:00:35.49
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:35.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:35.511
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-6514/secret-test-9acc33fb-6f81-400f-beaa-127fa04df5d8 08/29/23 20:00:35.514
    STEP: Creating a pod to test consume secrets 08/29/23 20:00:35.52
    Aug 29 20:00:35.530: INFO: Waiting up to 5m0s for pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4" in namespace "secrets-6514" to be "Succeeded or Failed"
    Aug 29 20:00:35.534: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00449ms
    Aug 29 20:00:37.540: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009552273s
    Aug 29 20:00:39.540: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4": Phase="Running", Reason="", readiness=false. Elapsed: 4.009627794s
    Aug 29 20:00:41.540: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009840969s
    STEP: Saw pod success 08/29/23 20:00:41.54
    Aug 29 20:00:41.541: INFO: Pod "pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4" satisfied condition "Succeeded or Failed"
    Aug 29 20:00:41.552: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4 container env-test: <nil>
    STEP: delete the pod 08/29/23 20:00:41.579
    Aug 29 20:00:41.602: INFO: Waiting for pod pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4 to disappear
    Aug 29 20:00:41.606: INFO: Pod pod-configmaps-2ff367ac-3f01-4276-9b79-8f1676cc75f4 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:00:41.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6514" for this suite. 08/29/23 20:00:41.611
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:00:41.622
Aug 29 20:00:41.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:00:41.623
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:41.643
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:41.652
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:00:41.655
Aug 29 20:00:41.677: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a" in namespace "projected-2987" to be "Succeeded or Failed"
Aug 29 20:00:41.682: INFO: Pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.689181ms
Aug 29 20:00:43.688: INFO: Pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010996202s
Aug 29 20:00:45.688: INFO: Pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011025659s
STEP: Saw pod success 08/29/23 20:00:45.688
Aug 29 20:00:45.688: INFO: Pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a" satisfied condition "Succeeded or Failed"
Aug 29 20:00:45.692: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a container client-container: <nil>
STEP: delete the pod 08/29/23 20:00:45.699
Aug 29 20:00:45.729: INFO: Waiting for pod downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a to disappear
Aug 29 20:00:45.732: INFO: Pod downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 20:00:45.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2987" for this suite. 08/29/23 20:00:45.736
------------------------------
• [4.123 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:00:41.622
    Aug 29 20:00:41.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:00:41.623
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:41.643
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:41.652
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:00:41.655
    Aug 29 20:00:41.677: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a" in namespace "projected-2987" to be "Succeeded or Failed"
    Aug 29 20:00:41.682: INFO: Pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.689181ms
    Aug 29 20:00:43.688: INFO: Pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010996202s
    Aug 29 20:00:45.688: INFO: Pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011025659s
    STEP: Saw pod success 08/29/23 20:00:45.688
    Aug 29 20:00:45.688: INFO: Pod "downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a" satisfied condition "Succeeded or Failed"
    Aug 29 20:00:45.692: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a container client-container: <nil>
    STEP: delete the pod 08/29/23 20:00:45.699
    Aug 29 20:00:45.729: INFO: Waiting for pod downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a to disappear
    Aug 29 20:00:45.732: INFO: Pod downwardapi-volume-10a4846a-bf43-4ef8-9803-40856ab0ab1a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:00:45.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2987" for this suite. 08/29/23 20:00:45.736
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:00:45.745
Aug 29 20:00:45.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename proxy 08/29/23 20:00:45.746
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:45.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:45.767
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 29 20:00:45.769: INFO: Creating pod...
Aug 29 20:00:45.785: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5392" to be "running"
Aug 29 20:00:45.788: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.045676ms
Aug 29 20:00:47.793: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007944419s
Aug 29 20:00:47.793: INFO: Pod "agnhost" satisfied condition "running"
Aug 29 20:00:47.793: INFO: Creating service...
Aug 29 20:00:47.808: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/DELETE
Aug 29 20:00:47.814: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 20:00:47.814: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/GET
Aug 29 20:00:47.823: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 29 20:00:47.823: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/HEAD
Aug 29 20:00:47.827: INFO: http.Client request:HEAD | StatusCode:200
Aug 29 20:00:47.827: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 29 20:00:47.837: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 20:00:47.837: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/PATCH
Aug 29 20:00:47.842: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 20:00:47.842: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/POST
Aug 29 20:00:47.846: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 20:00:47.846: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/PUT
Aug 29 20:00:47.852: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 29 20:00:47.852: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/DELETE
Aug 29 20:00:47.857: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 20:00:47.857: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/GET
Aug 29 20:00:47.862: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 29 20:00:47.862: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/HEAD
Aug 29 20:00:47.867: INFO: http.Client request:HEAD | StatusCode:200
Aug 29 20:00:47.867: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/OPTIONS
Aug 29 20:00:47.871: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 20:00:47.871: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/PATCH
Aug 29 20:00:47.876: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 20:00:47.876: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/POST
Aug 29 20:00:47.880: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 20:00:47.880: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/PUT
Aug 29 20:00:47.884: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 29 20:00:47.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5392" for this suite. 08/29/23 20:00:47.889
------------------------------
• [2.154 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:00:45.745
    Aug 29 20:00:45.745: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename proxy 08/29/23 20:00:45.746
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:45.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:45.767
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 29 20:00:45.769: INFO: Creating pod...
    Aug 29 20:00:45.785: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5392" to be "running"
    Aug 29 20:00:45.788: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.045676ms
    Aug 29 20:00:47.793: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007944419s
    Aug 29 20:00:47.793: INFO: Pod "agnhost" satisfied condition "running"
    Aug 29 20:00:47.793: INFO: Creating service...
    Aug 29 20:00:47.808: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/DELETE
    Aug 29 20:00:47.814: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 29 20:00:47.814: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/GET
    Aug 29 20:00:47.823: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 29 20:00:47.823: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/HEAD
    Aug 29 20:00:47.827: INFO: http.Client request:HEAD | StatusCode:200
    Aug 29 20:00:47.827: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 29 20:00:47.837: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 29 20:00:47.837: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/PATCH
    Aug 29 20:00:47.842: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 29 20:00:47.842: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/POST
    Aug 29 20:00:47.846: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 29 20:00:47.846: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/pods/agnhost/proxy/some/path/with/PUT
    Aug 29 20:00:47.852: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 29 20:00:47.852: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/DELETE
    Aug 29 20:00:47.857: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 29 20:00:47.857: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/GET
    Aug 29 20:00:47.862: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 29 20:00:47.862: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/HEAD
    Aug 29 20:00:47.867: INFO: http.Client request:HEAD | StatusCode:200
    Aug 29 20:00:47.867: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/OPTIONS
    Aug 29 20:00:47.871: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 29 20:00:47.871: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/PATCH
    Aug 29 20:00:47.876: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 29 20:00:47.876: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/POST
    Aug 29 20:00:47.880: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 29 20:00:47.880: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5392/services/test-service/proxy/some/path/with/PUT
    Aug 29 20:00:47.884: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:00:47.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5392" for this suite. 08/29/23 20:00:47.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:00:47.899
Aug 29 20:00:47.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:00:47.901
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:47.919
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:47.922
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/29/23 20:00:47.925
Aug 29 20:00:47.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6350 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 29 20:00:48.027: INFO: stderr: ""
Aug 29 20:00:48.027: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/29/23 20:00:48.027
STEP: verifying the pod e2e-test-httpd-pod was created 08/29/23 20:00:53.078
Aug 29 20:00:53.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6350 get pod e2e-test-httpd-pod -o json'
Aug 29 20:00:53.180: INFO: stderr: ""
Aug 29 20:00:53.180: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a7d35900204793e989e82fcb3e0a0872ab33e7cad7dcce2d8da20e02e2b52e2e\",\n            \"cni.projectcalico.org/podIP\": \"172.20.30.137/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.20.30.137/32\"\n        },\n        \"creationTimestamp\": \"2023-08-29T20:00:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6350\",\n        \"resourceVersion\": \"18943\",\n        \"uid\": \"f56fb8d9-aacb-40cc-932b-858d5e80c8e8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-4wcvg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"loki-15bd39-worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-4wcvg\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-29T20:00:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-29T20:00:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-29T20:00:49Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-29T20:00:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://3113f85a3ba5f6abf8488dbde62eaa335b887a1a0f28fa79c56d0850028a5d94\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-29T20:00:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.35.206\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.30.137\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.20.30.137\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-29T20:00:48Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/29/23 20:00:53.18
Aug 29 20:00:53.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6350 replace -f -'
Aug 29 20:00:54.264: INFO: stderr: ""
Aug 29 20:00:54.264: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/29/23 20:00:54.264
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Aug 29 20:00:54.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6350 delete pods e2e-test-httpd-pod'
Aug 29 20:00:57.681: INFO: stderr: ""
Aug 29 20:00:57.681: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:00:57.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6350" for this suite. 08/29/23 20:00:57.69
------------------------------
• [SLOW TEST] [9.799 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:00:47.899
    Aug 29 20:00:47.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:00:47.901
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:47.919
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:47.922
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/29/23 20:00:47.925
    Aug 29 20:00:47.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6350 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 29 20:00:48.027: INFO: stderr: ""
    Aug 29 20:00:48.027: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/29/23 20:00:48.027
    STEP: verifying the pod e2e-test-httpd-pod was created 08/29/23 20:00:53.078
    Aug 29 20:00:53.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6350 get pod e2e-test-httpd-pod -o json'
    Aug 29 20:00:53.180: INFO: stderr: ""
    Aug 29 20:00:53.180: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"a7d35900204793e989e82fcb3e0a0872ab33e7cad7dcce2d8da20e02e2b52e2e\",\n            \"cni.projectcalico.org/podIP\": \"172.20.30.137/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.20.30.137/32\"\n        },\n        \"creationTimestamp\": \"2023-08-29T20:00:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-6350\",\n        \"resourceVersion\": \"18943\",\n        \"uid\": \"f56fb8d9-aacb-40cc-932b-858d5e80c8e8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-4wcvg\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"loki-15bd39-worker-1\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-4wcvg\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-29T20:00:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-29T20:00:49Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-29T20:00:49Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-29T20:00:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://3113f85a3ba5f6abf8488dbde62eaa335b887a1a0f28fa79c56d0850028a5d94\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-29T20:00:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.45.35.206\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.20.30.137\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.20.30.137\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-29T20:00:48Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/29/23 20:00:53.18
    Aug 29 20:00:53.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6350 replace -f -'
    Aug 29 20:00:54.264: INFO: stderr: ""
    Aug 29 20:00:54.264: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 08/29/23 20:00:54.264
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Aug 29 20:00:54.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6350 delete pods e2e-test-httpd-pod'
    Aug 29 20:00:57.681: INFO: stderr: ""
    Aug 29 20:00:57.681: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:00:57.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6350" for this suite. 08/29/23 20:00:57.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:00:57.699
Aug 29 20:00:57.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:00:57.7
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:57.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:57.722
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-4d0bacac-fc54-455a-bfb4-6792d281f109 08/29/23 20:00:57.726
STEP: Creating a pod to test consume secrets 08/29/23 20:00:57.734
Aug 29 20:00:57.757: INFO: Waiting up to 5m0s for pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd" in namespace "secrets-1802" to be "Succeeded or Failed"
Aug 29 20:00:57.762: INFO: Pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.107477ms
Aug 29 20:00:59.766: INFO: Pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008960989s
Aug 29 20:01:01.769: INFO: Pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01240392s
STEP: Saw pod success 08/29/23 20:01:01.769
Aug 29 20:01:01.769: INFO: Pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd" satisfied condition "Succeeded or Failed"
Aug 29 20:01:01.773: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd container secret-volume-test: <nil>
STEP: delete the pod 08/29/23 20:01:01.782
Aug 29 20:01:01.805: INFO: Waiting for pod pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd to disappear
Aug 29 20:01:01.809: INFO: Pod pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:01.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1802" for this suite. 08/29/23 20:01:01.815
------------------------------
• [4.128 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:00:57.699
    Aug 29 20:00:57.699: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:00:57.7
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:00:57.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:00:57.722
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-4d0bacac-fc54-455a-bfb4-6792d281f109 08/29/23 20:00:57.726
    STEP: Creating a pod to test consume secrets 08/29/23 20:00:57.734
    Aug 29 20:00:57.757: INFO: Waiting up to 5m0s for pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd" in namespace "secrets-1802" to be "Succeeded or Failed"
    Aug 29 20:00:57.762: INFO: Pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.107477ms
    Aug 29 20:00:59.766: INFO: Pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008960989s
    Aug 29 20:01:01.769: INFO: Pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01240392s
    STEP: Saw pod success 08/29/23 20:01:01.769
    Aug 29 20:01:01.769: INFO: Pod "pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd" satisfied condition "Succeeded or Failed"
    Aug 29 20:01:01.773: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd container secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:01:01.782
    Aug 29 20:01:01.805: INFO: Waiting for pod pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd to disappear
    Aug 29 20:01:01.809: INFO: Pod pod-secrets-0ef6e74d-6858-442e-9476-fdc0583980bd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:01.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1802" for this suite. 08/29/23 20:01:01.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:01.827
Aug 29 20:01:01.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:01:01.828
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:01.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:01.871
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/29/23 20:01:01.875
Aug 29 20:01:01.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7032 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Aug 29 20:01:02.000: INFO: stderr: ""
Aug 29 20:01:02.000: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/29/23 20:01:02
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Aug 29 20:01:02.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7032 delete pods e2e-test-httpd-pod'
Aug 29 20:01:04.726: INFO: stderr: ""
Aug 29 20:01:04.726: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:04.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7032" for this suite. 08/29/23 20:01:04.731
------------------------------
• [2.912 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:01.827
    Aug 29 20:01:01.827: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:01:01.828
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:01.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:01.871
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/29/23 20:01:01.875
    Aug 29 20:01:01.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7032 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Aug 29 20:01:02.000: INFO: stderr: ""
    Aug 29 20:01:02.000: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/29/23 20:01:02
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Aug 29 20:01:02.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7032 delete pods e2e-test-httpd-pod'
    Aug 29 20:01:04.726: INFO: stderr: ""
    Aug 29 20:01:04.726: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:04.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7032" for this suite. 08/29/23 20:01:04.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:04.74
Aug 29 20:01:04.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:01:04.74
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:04.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:04.762
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Aug 29 20:01:04.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/29/23 20:01:06.978
Aug 29 20:01:06.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 --namespace=crd-publish-openapi-5099 create -f -'
Aug 29 20:01:08.105: INFO: stderr: ""
Aug 29 20:01:08.105: INFO: stdout: "e2e-test-crd-publish-openapi-35-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 29 20:01:08.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 --namespace=crd-publish-openapi-5099 delete e2e-test-crd-publish-openapi-35-crds test-cr'
Aug 29 20:01:08.245: INFO: stderr: ""
Aug 29 20:01:08.245: INFO: stdout: "e2e-test-crd-publish-openapi-35-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 29 20:01:08.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 --namespace=crd-publish-openapi-5099 apply -f -'
Aug 29 20:01:09.106: INFO: stderr: ""
Aug 29 20:01:09.106: INFO: stdout: "e2e-test-crd-publish-openapi-35-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 29 20:01:09.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 --namespace=crd-publish-openapi-5099 delete e2e-test-crd-publish-openapi-35-crds test-cr'
Aug 29 20:01:09.209: INFO: stderr: ""
Aug 29 20:01:09.209: INFO: stdout: "e2e-test-crd-publish-openapi-35-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/29/23 20:01:09.209
Aug 29 20:01:09.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 explain e2e-test-crd-publish-openapi-35-crds'
Aug 29 20:01:09.525: INFO: stderr: ""
Aug 29 20:01:09.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-35-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:12.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5099" for this suite. 08/29/23 20:01:12.271
------------------------------
• [SLOW TEST] [7.540 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:04.74
    Aug 29 20:01:04.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:01:04.74
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:04.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:04.762
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Aug 29 20:01:04.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/29/23 20:01:06.978
    Aug 29 20:01:06.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 --namespace=crd-publish-openapi-5099 create -f -'
    Aug 29 20:01:08.105: INFO: stderr: ""
    Aug 29 20:01:08.105: INFO: stdout: "e2e-test-crd-publish-openapi-35-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 29 20:01:08.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 --namespace=crd-publish-openapi-5099 delete e2e-test-crd-publish-openapi-35-crds test-cr'
    Aug 29 20:01:08.245: INFO: stderr: ""
    Aug 29 20:01:08.245: INFO: stdout: "e2e-test-crd-publish-openapi-35-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 29 20:01:08.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 --namespace=crd-publish-openapi-5099 apply -f -'
    Aug 29 20:01:09.106: INFO: stderr: ""
    Aug 29 20:01:09.106: INFO: stdout: "e2e-test-crd-publish-openapi-35-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 29 20:01:09.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 --namespace=crd-publish-openapi-5099 delete e2e-test-crd-publish-openapi-35-crds test-cr'
    Aug 29 20:01:09.209: INFO: stderr: ""
    Aug 29 20:01:09.209: INFO: stdout: "e2e-test-crd-publish-openapi-35-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/29/23 20:01:09.209
    Aug 29 20:01:09.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-5099 explain e2e-test-crd-publish-openapi-35-crds'
    Aug 29 20:01:09.525: INFO: stderr: ""
    Aug 29 20:01:09.525: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-35-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:12.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5099" for this suite. 08/29/23 20:01:12.271
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:12.28
Aug 29 20:01:12.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:01:12.281
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:12.296
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:12.299
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:01:12.316
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:01:12.733
STEP: Deploying the webhook pod 08/29/23 20:01:12.744
STEP: Wait for the deployment to be ready 08/29/23 20:01:12.758
Aug 29 20:01:12.768: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 20:01:14.783
STEP: Verifying the service has paired with the endpoint 08/29/23 20:01:14.796
Aug 29 20:01:15.797: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 08/29/23 20:01:15.899
STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 20:01:15.935
STEP: Deleting the collection of validation webhooks 08/29/23 20:01:15.965
STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 20:01:16.036
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:16.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5784" for this suite. 08/29/23 20:01:16.123
STEP: Destroying namespace "webhook-5784-markers" for this suite. 08/29/23 20:01:16.135
------------------------------
• [3.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:12.28
    Aug 29 20:01:12.280: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:01:12.281
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:12.296
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:12.299
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:01:12.316
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:01:12.733
    STEP: Deploying the webhook pod 08/29/23 20:01:12.744
    STEP: Wait for the deployment to be ready 08/29/23 20:01:12.758
    Aug 29 20:01:12.768: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 20:01:14.783
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:01:14.796
    Aug 29 20:01:15.797: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 08/29/23 20:01:15.899
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 20:01:15.935
    STEP: Deleting the collection of validation webhooks 08/29/23 20:01:15.965
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/29/23 20:01:16.036
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:16.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5784" for this suite. 08/29/23 20:01:16.123
    STEP: Destroying namespace "webhook-5784-markers" for this suite. 08/29/23 20:01:16.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:16.149
Aug 29 20:01:16.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:01:16.15
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:16.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:16.17
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 08/29/23 20:01:16.179
STEP: waiting for available Endpoint 08/29/23 20:01:16.186
STEP: listing all Endpoints 08/29/23 20:01:16.187
STEP: updating the Endpoint 08/29/23 20:01:16.191
STEP: fetching the Endpoint 08/29/23 20:01:16.2
STEP: patching the Endpoint 08/29/23 20:01:16.204
STEP: fetching the Endpoint 08/29/23 20:01:16.212
STEP: deleting the Endpoint by Collection 08/29/23 20:01:16.217
STEP: waiting for Endpoint deletion 08/29/23 20:01:16.228
STEP: fetching the Endpoint 08/29/23 20:01:16.229
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3756" for this suite. 08/29/23 20:01:16.24
------------------------------
• [0.104 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:16.149
    Aug 29 20:01:16.149: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:01:16.15
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:16.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:16.17
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 08/29/23 20:01:16.179
    STEP: waiting for available Endpoint 08/29/23 20:01:16.186
    STEP: listing all Endpoints 08/29/23 20:01:16.187
    STEP: updating the Endpoint 08/29/23 20:01:16.191
    STEP: fetching the Endpoint 08/29/23 20:01:16.2
    STEP: patching the Endpoint 08/29/23 20:01:16.204
    STEP: fetching the Endpoint 08/29/23 20:01:16.212
    STEP: deleting the Endpoint by Collection 08/29/23 20:01:16.217
    STEP: waiting for Endpoint deletion 08/29/23 20:01:16.228
    STEP: fetching the Endpoint 08/29/23 20:01:16.229
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3756" for this suite. 08/29/23 20:01:16.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:16.254
Aug 29 20:01:16.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-pred 08/29/23 20:01:16.256
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:16.272
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:16.275
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 29 20:01:16.278: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 20:01:16.289: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 20:01:16.292: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-master-0 before test
Aug 29 20:01:16.303: INFO: calico-node-f4bng from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.303: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:01:16.303: INFO: kube-apiserver-loki-15bd39-master-0 from kube-system started at 2023-08-29 19:11:07 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.303: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 29 20:01:16.303: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 29 20:01:16.303: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 29 20:01:16.303: INFO: kube-proxy-ds-7pn7t from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.303: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:01:16.303: INFO: fluent-bit-x2ntj from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.303: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:01:16.303: INFO: node-exporter-lsprp from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.303: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:01:16.303: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:01:16.303: INFO: nutanix-csi-node-mqc5m from ntnx-system started at 2023-08-29 19:23:42 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.303: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:01:16.303: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:01:16.303: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:01:16.303: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.303: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:01:16.303: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 20:01:16.303: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-master-1 before test
Aug 29 20:01:16.314: INFO: calico-node-snrmp from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.314: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:01:16.314: INFO: calico-typha-787bcdb57c-c7xxf from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.314: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 20:01:16.314: INFO: kube-apiserver-loki-15bd39-master-1 from kube-system started at 2023-08-29 19:12:53 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.314: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 29 20:01:16.314: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 29 20:01:16.314: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 29 20:01:16.314: INFO: kube-proxy-ds-c665w from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.314: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:01:16.314: INFO: fluent-bit-nmdm2 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.314: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:01:16.314: INFO: node-exporter-q5l7x from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.314: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:01:16.314: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:01:16.314: INFO: nutanix-csi-node-r85kq from ntnx-system started at 2023-08-29 19:23:49 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.314: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:01:16.314: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:01:16.314: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:01:16.314: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:01:16.314: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 20:01:16.314: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-0 before test
Aug 29 20:01:16.325: INFO: calico-node-mms8p from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.325: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:01:16.325: INFO: calico-typha-787bcdb57c-ppmlg from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.325: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 20:01:16.325: INFO: coredns-5d88b659b9-6fjb6 from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.325: INFO: 	Container coredns ready: true, restart count 0
Aug 29 20:01:16.325: INFO: kube-proxy-ds-nmtgg from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.325: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:01:16.325: INFO: alertmanager-main-0 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.325: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 20:01:16.325: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:01:16.325: INFO: blackbox-exporter-7954b6f4db-5vl2v from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 	Container module-configmap-reloader ready: true, restart count 0
Aug 29 20:01:16.326: INFO: csi-snapshot-controller-6d467d46bf-fbxgt from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 29 20:01:16.326: INFO: csi-snapshot-webhook-69668f7b57-g5r2n from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container snapshot-validation ready: true, restart count 0
Aug 29 20:01:16.326: INFO: fluent-bit-lbvlt from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:01:16.326: INFO: kubernetes-events-printer-74464fd469-rrxtq from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 29 20:01:16.326: INFO: node-exporter-wwfmq from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:01:16.326: INFO: nutanix-csi-node-hql5k from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:01:16.326: INFO: prometheus-adapter-6b6d856c7-htsdt from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 29 20:01:16.326: INFO: prometheus-k8s-1 from ntnx-system started at 2023-08-29 19:21:14 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 	Container prometheus ready: true, restart count 0
Aug 29 20:01:16.326: INFO: sonobuoy-e2e-job-161e88352cfa47dc from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container e2e ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:01:16.326: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 20:01:16.326: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-1 before test
Aug 29 20:01:16.339: INFO: calico-node-sb7l5 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:01:16.339: INFO: kube-proxy-ds-jt2tk from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:01:16.339: INFO: alertmanager-main-2 from ntnx-system started at 2023-08-29 19:46:48 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container alertmanager ready: true, restart count 0
Aug 29 20:01:16.339: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:01:16.339: INFO: fluent-bit-z54v4 from ntnx-system started at 2023-08-29 19:46:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:01:16.339: INFO: node-exporter-8gt7t from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:01:16.339: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:01:16.339: INFO: nutanix-csi-node-qtc8x from ntnx-system started at 2023-08-29 19:46:53 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:01:16.339: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:01:16.339: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:01:16.339: INFO: prometheus-adapter-6b6d856c7-zwgvq from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 29 20:01:16.339: INFO: prometheus-k8s-0 from ntnx-system started at 2023-08-29 19:47:04 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:01:16.339: INFO: 	Container prometheus ready: true, restart count 0
Aug 29 20:01:16.339: INFO: prometheus-operator-557c85cd6b-r5v4w from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:01:16.339: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 29 20:01:16.339: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.339: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:01:16.339: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 20:01:16.339: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-2 before test
Aug 29 20:01:16.352: INFO: calico-kube-controllers-d9df5649-k5gf4 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 20:01:16.352: INFO: calico-node-9pl5x from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:01:16.352: INFO: calico-typha-787bcdb57c-6bcfm from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 20:01:16.352: INFO: coredns-5d88b659b9-fqwjz from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container coredns ready: true, restart count 0
Aug 29 20:01:16.352: INFO: kube-proxy-ds-l9vp5 from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:01:16.352: INFO: alertmanager-main-1 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 20:01:16.352: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:01:16.352: INFO: fluent-bit-6wl9b from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:01:16.352: INFO: kube-state-metrics-d459f9d68-z5xtj from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 29 20:01:16.352: INFO: node-exporter-xhwmw from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:01:16.352: INFO: nutanix-csi-controller-657cc74bd5-7j77l from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (5 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container csi-resizer ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container csi-snapshotter ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container nutanix-csi-plugin ready: true, restart count 0
Aug 29 20:01:16.352: INFO: nutanix-csi-node-hd5rc from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:01:16.352: INFO: sonobuoy from sonobuoy started at 2023-08-29 19:28:22 +0000 UTC (1 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 20:01:16.352: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:01:16.352: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:01:16.352: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/29/23 20:01:16.352
Aug 29 20:01:16.363: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2975" to be "running"
Aug 29 20:01:16.367: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.447173ms
Aug 29 20:01:18.372: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008868984s
Aug 29 20:01:18.372: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/29/23 20:01:18.376
STEP: Trying to apply a random label on the found node. 08/29/23 20:01:18.392
STEP: verifying the node has the label kubernetes.io/e2e-26341e73-fbd4-407b-b297-21af705f221e 42 08/29/23 20:01:18.404
STEP: Trying to relaunch the pod, now with labels. 08/29/23 20:01:18.409
Aug 29 20:01:18.417: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2975" to be "not pending"
Aug 29 20:01:18.420: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018498ms
Aug 29 20:01:20.425: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.007646913s
Aug 29 20:01:20.425: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-26341e73-fbd4-407b-b297-21af705f221e off the node loki-15bd39-worker-1 08/29/23 20:01:20.427
STEP: verifying the node doesn't have the label kubernetes.io/e2e-26341e73-fbd4-407b-b297-21af705f221e 08/29/23 20:01:20.442
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:20.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2975" for this suite. 08/29/23 20:01:20.451
------------------------------
• [4.206 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:16.254
    Aug 29 20:01:16.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-pred 08/29/23 20:01:16.256
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:16.272
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:16.275
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 29 20:01:16.278: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 29 20:01:16.289: INFO: Waiting for terminating namespaces to be deleted...
    Aug 29 20:01:16.292: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-master-0 before test
    Aug 29 20:01:16.303: INFO: calico-node-f4bng from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.303: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: kube-apiserver-loki-15bd39-master-0 from kube-system started at 2023-08-29 19:11:07 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.303: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: kube-proxy-ds-7pn7t from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.303: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: fluent-bit-x2ntj from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.303: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: node-exporter-lsprp from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.303: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: nutanix-csi-node-mqc5m from ntnx-system started at 2023-08-29 19:23:42 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.303: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.303: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 20:01:16.303: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-master-1 before test
    Aug 29 20:01:16.314: INFO: calico-node-snrmp from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.314: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: calico-typha-787bcdb57c-c7xxf from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.314: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: kube-apiserver-loki-15bd39-master-1 from kube-system started at 2023-08-29 19:12:53 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.314: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: kube-proxy-ds-c665w from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.314: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: fluent-bit-nmdm2 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.314: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: node-exporter-q5l7x from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.314: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: nutanix-csi-node-r85kq from ntnx-system started at 2023-08-29 19:23:49 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.314: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.314: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 20:01:16.314: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-0 before test
    Aug 29 20:01:16.325: INFO: calico-node-mms8p from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.325: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:01:16.325: INFO: calico-typha-787bcdb57c-ppmlg from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.325: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 20:01:16.325: INFO: coredns-5d88b659b9-6fjb6 from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.325: INFO: 	Container coredns ready: true, restart count 0
    Aug 29 20:01:16.325: INFO: kube-proxy-ds-nmtgg from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.325: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:01:16.325: INFO: alertmanager-main-0 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.325: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 20:01:16.325: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:01:16.325: INFO: blackbox-exporter-7954b6f4db-5vl2v from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 	Container module-configmap-reloader ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: csi-snapshot-controller-6d467d46bf-fbxgt from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: csi-snapshot-webhook-69668f7b57-g5r2n from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container snapshot-validation ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: fluent-bit-lbvlt from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: kubernetes-events-printer-74464fd469-rrxtq from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: node-exporter-wwfmq from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: nutanix-csi-node-hql5k from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: prometheus-adapter-6b6d856c7-htsdt from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: prometheus-k8s-1 from ntnx-system started at 2023-08-29 19:21:14 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 	Container prometheus ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: sonobuoy-e2e-job-161e88352cfa47dc from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container e2e ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.326: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 20:01:16.326: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-1 before test
    Aug 29 20:01:16.339: INFO: calico-node-sb7l5 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: kube-proxy-ds-jt2tk from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: alertmanager-main-2 from ntnx-system started at 2023-08-29 19:46:48 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container alertmanager ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: fluent-bit-z54v4 from ntnx-system started at 2023-08-29 19:46:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: node-exporter-8gt7t from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: nutanix-csi-node-qtc8x from ntnx-system started at 2023-08-29 19:46:53 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: prometheus-adapter-6b6d856c7-zwgvq from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: prometheus-k8s-0 from ntnx-system started at 2023-08-29 19:47:04 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: 	Container prometheus ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: prometheus-operator-557c85cd6b-r5v4w from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: 	Container prometheus-operator ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.339: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 20:01:16.339: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-2 before test
    Aug 29 20:01:16.352: INFO: calico-kube-controllers-d9df5649-k5gf4 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: calico-node-9pl5x from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: calico-typha-787bcdb57c-6bcfm from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: coredns-5d88b659b9-fqwjz from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container coredns ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: kube-proxy-ds-l9vp5 from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: alertmanager-main-1 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 20:01:16.352: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: fluent-bit-6wl9b from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: kube-state-metrics-d459f9d68-z5xtj from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: node-exporter-xhwmw from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: nutanix-csi-controller-657cc74bd5-7j77l from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (5 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container csi-provisioner ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container csi-resizer ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container nutanix-csi-plugin ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: nutanix-csi-node-hd5rc from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: sonobuoy from sonobuoy started at 2023-08-29 19:28:22 +0000 UTC (1 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:01:16.352: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:01:16.352: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/29/23 20:01:16.352
    Aug 29 20:01:16.363: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2975" to be "running"
    Aug 29 20:01:16.367: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 4.447173ms
    Aug 29 20:01:18.372: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008868984s
    Aug 29 20:01:18.372: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/29/23 20:01:18.376
    STEP: Trying to apply a random label on the found node. 08/29/23 20:01:18.392
    STEP: verifying the node has the label kubernetes.io/e2e-26341e73-fbd4-407b-b297-21af705f221e 42 08/29/23 20:01:18.404
    STEP: Trying to relaunch the pod, now with labels. 08/29/23 20:01:18.409
    Aug 29 20:01:18.417: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-2975" to be "not pending"
    Aug 29 20:01:18.420: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018498ms
    Aug 29 20:01:20.425: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.007646913s
    Aug 29 20:01:20.425: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-26341e73-fbd4-407b-b297-21af705f221e off the node loki-15bd39-worker-1 08/29/23 20:01:20.427
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-26341e73-fbd4-407b-b297-21af705f221e 08/29/23 20:01:20.442
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:20.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2975" for this suite. 08/29/23 20:01:20.451
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:20.461
Aug 29 20:01:20.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replication-controller 08/29/23 20:01:20.462
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:20.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:20.482
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 08/29/23 20:01:20.484
STEP: When the matched label of one of its pods change 08/29/23 20:01:20.492
Aug 29 20:01:20.495: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 29 20:01:25.501: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/29/23 20:01:25.517
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:26.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4571" for this suite. 08/29/23 20:01:26.531
------------------------------
• [SLOW TEST] [6.082 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:20.461
    Aug 29 20:01:20.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replication-controller 08/29/23 20:01:20.462
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:20.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:20.482
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 08/29/23 20:01:20.484
    STEP: When the matched label of one of its pods change 08/29/23 20:01:20.492
    Aug 29 20:01:20.495: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 29 20:01:25.501: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/29/23 20:01:25.517
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:26.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4571" for this suite. 08/29/23 20:01:26.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:26.544
Aug 29 20:01:26.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 20:01:26.545
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:26.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:26.567
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:01:26.571
Aug 29 20:01:26.582: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d" in namespace "downward-api-5316" to be "Succeeded or Failed"
Aug 29 20:01:26.585: INFO: Pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357741ms
Aug 29 20:01:28.590: INFO: Pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008245325s
Aug 29 20:01:30.589: INFO: Pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007417445s
STEP: Saw pod success 08/29/23 20:01:30.589
Aug 29 20:01:30.589: INFO: Pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d" satisfied condition "Succeeded or Failed"
Aug 29 20:01:30.593: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d container client-container: <nil>
STEP: delete the pod 08/29/23 20:01:30.601
Aug 29 20:01:30.617: INFO: Waiting for pod downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d to disappear
Aug 29 20:01:30.620: INFO: Pod downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:30.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5316" for this suite. 08/29/23 20:01:30.624
------------------------------
• [4.088 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:26.544
    Aug 29 20:01:26.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 20:01:26.545
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:26.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:26.567
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:01:26.571
    Aug 29 20:01:26.582: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d" in namespace "downward-api-5316" to be "Succeeded or Failed"
    Aug 29 20:01:26.585: INFO: Pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.357741ms
    Aug 29 20:01:28.590: INFO: Pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008245325s
    Aug 29 20:01:30.589: INFO: Pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007417445s
    STEP: Saw pod success 08/29/23 20:01:30.589
    Aug 29 20:01:30.589: INFO: Pod "downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d" satisfied condition "Succeeded or Failed"
    Aug 29 20:01:30.593: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d container client-container: <nil>
    STEP: delete the pod 08/29/23 20:01:30.601
    Aug 29 20:01:30.617: INFO: Waiting for pod downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d to disappear
    Aug 29 20:01:30.620: INFO: Pod downwardapi-volume-b302b41a-804b-4415-8a21-b1ba41c4e22d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:30.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5316" for this suite. 08/29/23 20:01:30.624
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:30.632
Aug 29 20:01:30.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-runtime 08/29/23 20:01:30.634
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:30.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:30.654
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 08/29/23 20:01:30.657
STEP: wait for the container to reach Succeeded 08/29/23 20:01:30.669
STEP: get the container status 08/29/23 20:01:34.692
STEP: the container should be terminated 08/29/23 20:01:34.696
STEP: the termination message should be set 08/29/23 20:01:34.697
Aug 29 20:01:34.697: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/29/23 20:01:34.697
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 29 20:01:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1130" for this suite. 08/29/23 20:01:34.722
------------------------------
• [4.099 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:30.632
    Aug 29 20:01:30.632: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-runtime 08/29/23 20:01:30.634
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:30.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:30.654
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 08/29/23 20:01:30.657
    STEP: wait for the container to reach Succeeded 08/29/23 20:01:30.669
    STEP: get the container status 08/29/23 20:01:34.692
    STEP: the container should be terminated 08/29/23 20:01:34.696
    STEP: the termination message should be set 08/29/23 20:01:34.697
    Aug 29 20:01:34.697: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/29/23 20:01:34.697
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:01:34.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1130" for this suite. 08/29/23 20:01:34.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:01:34.732
Aug 29 20:01:34.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename statefulset 08/29/23 20:01:34.733
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:34.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:34.752
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3487 08/29/23 20:01:34.754
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-3487 08/29/23 20:01:34.764
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3487 08/29/23 20:01:34.776
Aug 29 20:01:34.780: INFO: Found 0 stateful pods, waiting for 1
Aug 29 20:01:44.786: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/29/23 20:01:44.786
Aug 29 20:01:44.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 20:01:44.971: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 20:01:44.971: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 20:01:44.971: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 20:01:44.976: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 29 20:01:54.982: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 20:01:54.982: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 20:01:55.003: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Aug 29 20:01:55.003: INFO: ss-0  loki-15bd39-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:34 +0000 UTC  }]
Aug 29 20:01:55.003: INFO: 
Aug 29 20:01:55.003: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 29 20:01:56.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995214898s
Aug 29 20:01:57.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990425404s
Aug 29 20:01:58.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98565322s
Aug 29 20:01:59.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979696025s
Aug 29 20:02:00.029: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974153363s
Aug 29 20:02:01.034: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969101758s
Aug 29 20:02:02.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964355341s
Aug 29 20:02:03.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959214093s
Aug 29 20:02:04.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.118625ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3487 08/29/23 20:02:05.049
Aug 29 20:02:05.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 20:02:05.217: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 20:02:05.217: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 20:02:05.217: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 20:02:05.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 20:02:05.379: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 29 20:02:05.379: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 20:02:05.379: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 20:02:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 20:02:05.539: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 29 20:02:05.539: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 20:02:05.540: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 29 20:02:05.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 29 20:02:15.551: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 20:02:15.551: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 20:02:15.551: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/29/23 20:02:15.551
Aug 29 20:02:15.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 20:02:15.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 20:02:15.735: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 20:02:15.735: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 20:02:15.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 20:02:15.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 20:02:15.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 20:02:15.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 20:02:15.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 20:02:16.086: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 20:02:16.086: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 20:02:16.086: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 20:02:16.086: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 20:02:16.090: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 29 20:02:26.101: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 20:02:26.101: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 20:02:26.101: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 29 20:02:26.118: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Aug 29 20:02:26.118: INFO: ss-0  loki-15bd39-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:34 +0000 UTC  }]
Aug 29 20:02:26.118: INFO: ss-1  loki-15bd39-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  }]
Aug 29 20:02:26.118: INFO: ss-2  loki-15bd39-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  }]
Aug 29 20:02:26.118: INFO: 
Aug 29 20:02:26.118: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 29 20:02:27.122: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
Aug 29 20:02:27.123: INFO: ss-1  loki-15bd39-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  }]
Aug 29 20:02:27.123: INFO: 
Aug 29 20:02:27.123: INFO: StatefulSet ss has not reached scale 0, at 1
Aug 29 20:02:28.126: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.99176355s
Aug 29 20:02:29.130: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987820282s
Aug 29 20:02:30.136: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983066949s
Aug 29 20:02:31.140: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97874147s
Aug 29 20:02:32.145: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973754779s
Aug 29 20:02:33.149: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969353872s
Aug 29 20:02:34.153: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965183914s
Aug 29 20:02:35.158: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.270551ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3487 08/29/23 20:02:36.158
Aug 29 20:02:36.162: INFO: Scaling statefulset ss to 0
Aug 29 20:02:36.173: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 29 20:02:36.177: INFO: Deleting all statefulset in ns statefulset-3487
Aug 29 20:02:36.180: INFO: Scaling statefulset ss to 0
Aug 29 20:02:36.189: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 20:02:36.192: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:02:36.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3487" for this suite. 08/29/23 20:02:36.221
------------------------------
• [SLOW TEST] [61.508 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:01:34.732
    Aug 29 20:01:34.732: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename statefulset 08/29/23 20:01:34.733
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:01:34.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:01:34.752
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3487 08/29/23 20:01:34.754
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-3487 08/29/23 20:01:34.764
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3487 08/29/23 20:01:34.776
    Aug 29 20:01:34.780: INFO: Found 0 stateful pods, waiting for 1
    Aug 29 20:01:44.786: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/29/23 20:01:44.786
    Aug 29 20:01:44.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 20:01:44.971: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 20:01:44.971: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 20:01:44.971: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 20:01:44.976: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 29 20:01:54.982: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 29 20:01:54.982: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 20:01:55.003: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
    Aug 29 20:01:55.003: INFO: ss-0  loki-15bd39-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:45 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:34 +0000 UTC  }]
    Aug 29 20:01:55.003: INFO: 
    Aug 29 20:01:55.003: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 29 20:01:56.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.995214898s
    Aug 29 20:01:57.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990425404s
    Aug 29 20:01:58.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98565322s
    Aug 29 20:01:59.024: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979696025s
    Aug 29 20:02:00.029: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974153363s
    Aug 29 20:02:01.034: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969101758s
    Aug 29 20:02:02.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.964355341s
    Aug 29 20:02:03.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959214093s
    Aug 29 20:02:04.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.118625ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3487 08/29/23 20:02:05.049
    Aug 29 20:02:05.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 20:02:05.217: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 29 20:02:05.217: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 20:02:05.217: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 29 20:02:05.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 20:02:05.379: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 29 20:02:05.379: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 20:02:05.379: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 29 20:02:05.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 20:02:05.539: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 29 20:02:05.539: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 20:02:05.540: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 29 20:02:05.544: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Aug 29 20:02:15.551: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 20:02:15.551: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 20:02:15.551: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/29/23 20:02:15.551
    Aug 29 20:02:15.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 20:02:15.735: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 20:02:15.735: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 20:02:15.735: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 20:02:15.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 20:02:15.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 20:02:15.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 20:02:15.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 20:02:15.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-3487 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 20:02:16.086: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 20:02:16.086: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 20:02:16.086: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 20:02:16.086: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 20:02:16.090: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Aug 29 20:02:26.101: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 29 20:02:26.101: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 29 20:02:26.101: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 29 20:02:26.118: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
    Aug 29 20:02:26.118: INFO: ss-0  loki-15bd39-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:34 +0000 UTC  }]
    Aug 29 20:02:26.118: INFO: ss-1  loki-15bd39-worker-0  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  }]
    Aug 29 20:02:26.118: INFO: ss-2  loki-15bd39-worker-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  }]
    Aug 29 20:02:26.118: INFO: 
    Aug 29 20:02:26.118: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 29 20:02:27.122: INFO: POD   NODE                  PHASE    GRACE  CONDITIONS
    Aug 29 20:02:27.123: INFO: ss-1  loki-15bd39-worker-0  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:02:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:01:55 +0000 UTC  }]
    Aug 29 20:02:27.123: INFO: 
    Aug 29 20:02:27.123: INFO: StatefulSet ss has not reached scale 0, at 1
    Aug 29 20:02:28.126: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.99176355s
    Aug 29 20:02:29.130: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987820282s
    Aug 29 20:02:30.136: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983066949s
    Aug 29 20:02:31.140: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.97874147s
    Aug 29 20:02:32.145: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.973754779s
    Aug 29 20:02:33.149: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.969353872s
    Aug 29 20:02:34.153: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.965183914s
    Aug 29 20:02:35.158: INFO: Verifying statefulset ss doesn't scale past 0 for another 961.270551ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3487 08/29/23 20:02:36.158
    Aug 29 20:02:36.162: INFO: Scaling statefulset ss to 0
    Aug 29 20:02:36.173: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 29 20:02:36.177: INFO: Deleting all statefulset in ns statefulset-3487
    Aug 29 20:02:36.180: INFO: Scaling statefulset ss to 0
    Aug 29 20:02:36.189: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 20:02:36.192: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:02:36.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3487" for this suite. 08/29/23 20:02:36.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:02:36.241
Aug 29 20:02:36.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:02:36.242
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:02:36.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:02:36.273
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-df85639c-ab76-479d-a0a6-47ac2a454e80 08/29/23 20:02:36.276
STEP: Creating a pod to test consume secrets 08/29/23 20:02:36.282
Aug 29 20:02:36.304: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d" in namespace "projected-1456" to be "Succeeded or Failed"
Aug 29 20:02:36.309: INFO: Pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016584ms
Aug 29 20:02:38.313: INFO: Pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008475021s
Aug 29 20:02:40.313: INFO: Pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00890194s
STEP: Saw pod success 08/29/23 20:02:40.314
Aug 29 20:02:40.314: INFO: Pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d" satisfied condition "Succeeded or Failed"
Aug 29 20:02:40.318: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d container projected-secret-volume-test: <nil>
STEP: delete the pod 08/29/23 20:02:40.33
Aug 29 20:02:40.345: INFO: Waiting for pod pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d to disappear
Aug 29 20:02:40.348: INFO: Pod pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 29 20:02:40.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1456" for this suite. 08/29/23 20:02:40.353
------------------------------
• [4.119 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:02:36.241
    Aug 29 20:02:36.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:02:36.242
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:02:36.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:02:36.273
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-df85639c-ab76-479d-a0a6-47ac2a454e80 08/29/23 20:02:36.276
    STEP: Creating a pod to test consume secrets 08/29/23 20:02:36.282
    Aug 29 20:02:36.304: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d" in namespace "projected-1456" to be "Succeeded or Failed"
    Aug 29 20:02:36.309: INFO: Pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.016584ms
    Aug 29 20:02:38.313: INFO: Pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008475021s
    Aug 29 20:02:40.313: INFO: Pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00890194s
    STEP: Saw pod success 08/29/23 20:02:40.314
    Aug 29 20:02:40.314: INFO: Pod "pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d" satisfied condition "Succeeded or Failed"
    Aug 29 20:02:40.318: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:02:40.33
    Aug 29 20:02:40.345: INFO: Waiting for pod pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d to disappear
    Aug 29 20:02:40.348: INFO: Pod pod-projected-secrets-bee3f23a-d5b9-4535-b44c-59ff481db80d no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:02:40.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1456" for this suite. 08/29/23 20:02:40.353
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:02:40.361
Aug 29 20:02:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename endpointslice 08/29/23 20:02:40.362
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:02:40.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:02:40.385
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 08/29/23 20:02:40.389
STEP: getting /apis/discovery.k8s.io 08/29/23 20:02:40.391
STEP: getting /apis/discovery.k8s.iov1 08/29/23 20:02:40.393
STEP: creating 08/29/23 20:02:40.394
STEP: getting 08/29/23 20:02:40.416
STEP: listing 08/29/23 20:02:40.421
STEP: watching 08/29/23 20:02:40.425
Aug 29 20:02:40.425: INFO: starting watch
STEP: cluster-wide listing 08/29/23 20:02:40.426
STEP: cluster-wide watching 08/29/23 20:02:40.43
Aug 29 20:02:40.430: INFO: starting watch
STEP: patching 08/29/23 20:02:40.431
STEP: updating 08/29/23 20:02:40.437
Aug 29 20:02:40.447: INFO: waiting for watch events with expected annotations
Aug 29 20:02:40.447: INFO: saw patched and updated annotations
STEP: deleting 08/29/23 20:02:40.447
STEP: deleting a collection 08/29/23 20:02:40.46
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 29 20:02:40.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1769" for this suite. 08/29/23 20:02:40.481
------------------------------
• [0.130 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:02:40.361
    Aug 29 20:02:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename endpointslice 08/29/23 20:02:40.362
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:02:40.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:02:40.385
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 08/29/23 20:02:40.389
    STEP: getting /apis/discovery.k8s.io 08/29/23 20:02:40.391
    STEP: getting /apis/discovery.k8s.iov1 08/29/23 20:02:40.393
    STEP: creating 08/29/23 20:02:40.394
    STEP: getting 08/29/23 20:02:40.416
    STEP: listing 08/29/23 20:02:40.421
    STEP: watching 08/29/23 20:02:40.425
    Aug 29 20:02:40.425: INFO: starting watch
    STEP: cluster-wide listing 08/29/23 20:02:40.426
    STEP: cluster-wide watching 08/29/23 20:02:40.43
    Aug 29 20:02:40.430: INFO: starting watch
    STEP: patching 08/29/23 20:02:40.431
    STEP: updating 08/29/23 20:02:40.437
    Aug 29 20:02:40.447: INFO: waiting for watch events with expected annotations
    Aug 29 20:02:40.447: INFO: saw patched and updated annotations
    STEP: deleting 08/29/23 20:02:40.447
    STEP: deleting a collection 08/29/23 20:02:40.46
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:02:40.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1769" for this suite. 08/29/23 20:02:40.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:02:40.492
Aug 29 20:02:40.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 20:02:40.493
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:02:40.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:02:40.514
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 08/29/23 20:02:40.517
STEP: Ensuring ResourceQuota status is calculated 08/29/23 20:02:40.526
STEP: Creating a ResourceQuota with not best effort scope 08/29/23 20:02:42.531
STEP: Ensuring ResourceQuota status is calculated 08/29/23 20:02:42.543
STEP: Creating a best-effort pod 08/29/23 20:02:44.548
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/29/23 20:02:44.574
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/29/23 20:02:46.579
STEP: Deleting the pod 08/29/23 20:02:48.584
STEP: Ensuring resource quota status released the pod usage 08/29/23 20:02:48.602
STEP: Creating a not best-effort pod 08/29/23 20:02:50.607
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/29/23 20:02:50.62
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/29/23 20:02:52.626
STEP: Deleting the pod 08/29/23 20:02:54.631
STEP: Ensuring resource quota status released the pod usage 08/29/23 20:02:54.645
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 20:02:56.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-3885" for this suite. 08/29/23 20:02:56.655
------------------------------
• [SLOW TEST] [16.170 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:02:40.492
    Aug 29 20:02:40.492: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 20:02:40.493
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:02:40.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:02:40.514
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 08/29/23 20:02:40.517
    STEP: Ensuring ResourceQuota status is calculated 08/29/23 20:02:40.526
    STEP: Creating a ResourceQuota with not best effort scope 08/29/23 20:02:42.531
    STEP: Ensuring ResourceQuota status is calculated 08/29/23 20:02:42.543
    STEP: Creating a best-effort pod 08/29/23 20:02:44.548
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/29/23 20:02:44.574
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/29/23 20:02:46.579
    STEP: Deleting the pod 08/29/23 20:02:48.584
    STEP: Ensuring resource quota status released the pod usage 08/29/23 20:02:48.602
    STEP: Creating a not best-effort pod 08/29/23 20:02:50.607
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/29/23 20:02:50.62
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/29/23 20:02:52.626
    STEP: Deleting the pod 08/29/23 20:02:54.631
    STEP: Ensuring resource quota status released the pod usage 08/29/23 20:02:54.645
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:02:56.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-3885" for this suite. 08/29/23 20:02:56.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:02:56.662
Aug 29 20:02:56.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:02:56.663
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:02:56.68
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:02:56.683
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-9325 08/29/23 20:02:56.685
STEP: creating service affinity-nodeport in namespace services-9325 08/29/23 20:02:56.685
STEP: creating replication controller affinity-nodeport in namespace services-9325 08/29/23 20:02:56.705
I0829 20:02:56.720553      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9325, replica count: 3
I0829 20:02:59.771677      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 20:02:59.784: INFO: Creating new exec pod
Aug 29 20:02:59.790: INFO: Waiting up to 5m0s for pod "execpod-affinityv4n59" in namespace "services-9325" to be "running"
Aug 29 20:02:59.795: INFO: Pod "execpod-affinityv4n59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.391681ms
Aug 29 20:03:01.799: INFO: Pod "execpod-affinityv4n59": Phase="Running", Reason="", readiness=true. Elapsed: 2.009226561s
Aug 29 20:03:01.799: INFO: Pod "execpod-affinityv4n59" satisfied condition "running"
Aug 29 20:03:02.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Aug 29 20:03:02.967: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 29 20:03:02.967: INFO: stdout: ""
Aug 29 20:03:02.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c nc -v -z -w 2 172.19.75.76 80'
Aug 29 20:03:03.134: INFO: stderr: "+ nc -v -z -w 2 172.19.75.76 80\nConnection to 172.19.75.76 80 port [tcp/http] succeeded!\n"
Aug 29 20:03:03.134: INFO: stdout: ""
Aug 29 20:03:03.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.202 30075'
Aug 29 20:03:03.310: INFO: stderr: "+ nc -v -z -w 2 10.45.35.202 30075\nConnection to 10.45.35.202 30075 port [tcp/*] succeeded!\n"
Aug 29 20:03:03.310: INFO: stdout: ""
Aug 29 20:03:03.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.199 30075'
Aug 29 20:03:03.470: INFO: stderr: "+ nc -v -z -w 2 10.45.35.199 30075\nConnection to 10.45.35.199 30075 port [tcp/*] succeeded!\n"
Aug 29 20:03:03.470: INFO: stdout: ""
Aug 29 20:03:03.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:30075/ ; done'
Aug 29 20:03:03.756: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n"
Aug 29 20:03:03.756: INFO: stdout: "\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5"
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
Aug 29 20:03:03.756: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-9325, will wait for the garbage collector to delete the pods 08/29/23 20:03:03.773
Aug 29 20:03:03.836: INFO: Deleting ReplicationController affinity-nodeport took: 7.717191ms
Aug 29 20:03:03.936: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.166329ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:06.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9325" for this suite. 08/29/23 20:03:06.275
------------------------------
• [SLOW TEST] [9.621 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:02:56.662
    Aug 29 20:02:56.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:02:56.663
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:02:56.68
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:02:56.683
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-9325 08/29/23 20:02:56.685
    STEP: creating service affinity-nodeport in namespace services-9325 08/29/23 20:02:56.685
    STEP: creating replication controller affinity-nodeport in namespace services-9325 08/29/23 20:02:56.705
    I0829 20:02:56.720553      19 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-9325, replica count: 3
    I0829 20:02:59.771677      19 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 20:02:59.784: INFO: Creating new exec pod
    Aug 29 20:02:59.790: INFO: Waiting up to 5m0s for pod "execpod-affinityv4n59" in namespace "services-9325" to be "running"
    Aug 29 20:02:59.795: INFO: Pod "execpod-affinityv4n59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.391681ms
    Aug 29 20:03:01.799: INFO: Pod "execpod-affinityv4n59": Phase="Running", Reason="", readiness=true. Elapsed: 2.009226561s
    Aug 29 20:03:01.799: INFO: Pod "execpod-affinityv4n59" satisfied condition "running"
    Aug 29 20:03:02.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Aug 29 20:03:02.967: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 29 20:03:02.967: INFO: stdout: ""
    Aug 29 20:03:02.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c nc -v -z -w 2 172.19.75.76 80'
    Aug 29 20:03:03.134: INFO: stderr: "+ nc -v -z -w 2 172.19.75.76 80\nConnection to 172.19.75.76 80 port [tcp/http] succeeded!\n"
    Aug 29 20:03:03.134: INFO: stdout: ""
    Aug 29 20:03:03.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.202 30075'
    Aug 29 20:03:03.310: INFO: stderr: "+ nc -v -z -w 2 10.45.35.202 30075\nConnection to 10.45.35.202 30075 port [tcp/*] succeeded!\n"
    Aug 29 20:03:03.310: INFO: stdout: ""
    Aug 29 20:03:03.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.199 30075'
    Aug 29 20:03:03.470: INFO: stderr: "+ nc -v -z -w 2 10.45.35.199 30075\nConnection to 10.45.35.199 30075 port [tcp/*] succeeded!\n"
    Aug 29 20:03:03.470: INFO: stdout: ""
    Aug 29 20:03:03.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-9325 exec execpod-affinityv4n59 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:30075/ ; done'
    Aug 29 20:03:03.756: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:30075/\n"
    Aug 29 20:03:03.756: INFO: stdout: "\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5\naffinity-nodeport-8t2d5"
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Received response from host: affinity-nodeport-8t2d5
    Aug 29 20:03:03.756: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-9325, will wait for the garbage collector to delete the pods 08/29/23 20:03:03.773
    Aug 29 20:03:03.836: INFO: Deleting ReplicationController affinity-nodeport took: 7.717191ms
    Aug 29 20:03:03.936: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.166329ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:06.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9325" for this suite. 08/29/23 20:03:06.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:06.284
Aug 29 20:03:06.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename watch 08/29/23 20:03:06.285
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:06.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:06.305
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/29/23 20:03:06.308
STEP: modifying the configmap once 08/29/23 20:03:06.313
STEP: modifying the configmap a second time 08/29/23 20:03:06.324
STEP: deleting the configmap 08/29/23 20:03:06.334
STEP: creating a watch on configmaps from the resource version returned by the first update 08/29/23 20:03:06.342
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/29/23 20:03:06.344
Aug 29 20:03:06.344: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-482  8bd9b1d4-39d5-4e0b-88ec-75d81c3c08ce 20163 0 2023-08-29 20:03:06 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-29 20:03:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:03:06.344: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-482  8bd9b1d4-39d5-4e0b-88ec-75d81c3c08ce 20164 0 2023-08-29 20:03:06 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-29 20:03:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:06.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-482" for this suite. 08/29/23 20:03:06.35
------------------------------
• [0.081 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:06.284
    Aug 29 20:03:06.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename watch 08/29/23 20:03:06.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:06.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:06.305
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/29/23 20:03:06.308
    STEP: modifying the configmap once 08/29/23 20:03:06.313
    STEP: modifying the configmap a second time 08/29/23 20:03:06.324
    STEP: deleting the configmap 08/29/23 20:03:06.334
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/29/23 20:03:06.342
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/29/23 20:03:06.344
    Aug 29 20:03:06.344: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-482  8bd9b1d4-39d5-4e0b-88ec-75d81c3c08ce 20163 0 2023-08-29 20:03:06 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-29 20:03:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:03:06.344: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-482  8bd9b1d4-39d5-4e0b-88ec-75d81c3c08ce 20164 0 2023-08-29 20:03:06 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-29 20:03:06 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:06.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-482" for this suite. 08/29/23 20:03:06.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:06.366
Aug 29 20:03:06.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename conformance-tests 08/29/23 20:03:06.367
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:06.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:06.39
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/29/23 20:03:06.393
Aug 29 20:03:06.394: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:06.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-6971" for this suite. 08/29/23 20:03:06.406
------------------------------
• [0.049 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:06.366
    Aug 29 20:03:06.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename conformance-tests 08/29/23 20:03:06.367
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:06.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:06.39
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/29/23 20:03:06.393
    Aug 29 20:03:06.394: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:06.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-6971" for this suite. 08/29/23 20:03:06.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:06.416
Aug 29 20:03:06.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename podtemplate 08/29/23 20:03:06.418
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:06.444
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:06.448
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/29/23 20:03:06.451
STEP: Replace a pod template 08/29/23 20:03:06.457
Aug 29 20:03:06.468: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:06.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-7173" for this suite. 08/29/23 20:03:06.474
------------------------------
• [0.067 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:06.416
    Aug 29 20:03:06.416: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename podtemplate 08/29/23 20:03:06.418
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:06.444
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:06.448
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/29/23 20:03:06.451
    STEP: Replace a pod template 08/29/23 20:03:06.457
    Aug 29 20:03:06.468: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:06.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-7173" for this suite. 08/29/23 20:03:06.474
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:06.484
Aug 29 20:03:06.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pod-network-test 08/29/23 20:03:06.486
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:06.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:06.516
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-7265 08/29/23 20:03:06.52
STEP: creating a selector 08/29/23 20:03:06.52
STEP: Creating the service pods in kubernetes 08/29/23 20:03:06.52
Aug 29 20:03:06.520: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 29 20:03:06.597: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7265" to be "running and ready"
Aug 29 20:03:06.603: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874804ms
Aug 29 20:03:06.603: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:03:08.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011818094s
Aug 29 20:03:08.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:10.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011579536s
Aug 29 20:03:10.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:12.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010357337s
Aug 29 20:03:12.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:14.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009857666s
Aug 29 20:03:14.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:16.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010212171s
Aug 29 20:03:16.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:18.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010442724s
Aug 29 20:03:18.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:20.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011791486s
Aug 29 20:03:20.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:22.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010552944s
Aug 29 20:03:22.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:24.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01034453s
Aug 29 20:03:24.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:26.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00890546s
Aug 29 20:03:26.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 29 20:03:28.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010774707s
Aug 29 20:03:28.608: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 29 20:03:28.608: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 29 20:03:28.611: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7265" to be "running and ready"
Aug 29 20:03:28.620: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.926271ms
Aug 29 20:03:28.620: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 29 20:03:28.620: INFO: Pod "netserver-1" satisfied condition "running and ready"
Aug 29 20:03:28.624: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7265" to be "running and ready"
Aug 29 20:03:28.628: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.741743ms
Aug 29 20:03:28.628: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Aug 29 20:03:28.628: INFO: Pod "netserver-2" satisfied condition "running and ready"
Aug 29 20:03:28.631: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7265" to be "running and ready"
Aug 29 20:03:28.635: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.515439ms
Aug 29 20:03:28.635: INFO: The phase of Pod netserver-3 is Running (Ready = true)
Aug 29 20:03:28.635: INFO: Pod "netserver-3" satisfied condition "running and ready"
Aug 29 20:03:28.638: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7265" to be "running and ready"
Aug 29 20:03:28.642: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.796588ms
Aug 29 20:03:28.642: INFO: The phase of Pod netserver-4 is Running (Ready = true)
Aug 29 20:03:28.642: INFO: Pod "netserver-4" satisfied condition "running and ready"
STEP: Creating test pods 08/29/23 20:03:28.646
Aug 29 20:03:28.664: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7265" to be "running"
Aug 29 20:03:28.669: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351573ms
Aug 29 20:03:30.673: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009083828s
Aug 29 20:03:30.673: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 29 20:03:30.678: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7265" to be "running"
Aug 29 20:03:30.681: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.617779ms
Aug 29 20:03:30.681: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 29 20:03:30.684: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
Aug 29 20:03:30.684: INFO: Going to poll 172.20.17.97 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 29 20:03:30.687: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.17.97:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:03:30.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:03:30.688: INFO: ExecWithOptions: Clientset creation
Aug 29 20:03:30.688: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.17.97%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 20:03:30.785: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 29 20:03:30.785: INFO: Going to poll 172.20.76.162 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 29 20:03:30.789: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.76.162:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:03:30.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:03:30.789: INFO: ExecWithOptions: Clientset creation
Aug 29 20:03:30.789: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.76.162%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 20:03:30.875: INFO: Found all 1 expected endpoints: [netserver-1]
Aug 29 20:03:30.875: INFO: Going to poll 172.20.143.242 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 29 20:03:30.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.143.242:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:03:30.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:03:30.880: INFO: ExecWithOptions: Clientset creation
Aug 29 20:03:30.880: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.143.242%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 20:03:30.972: INFO: Found all 1 expected endpoints: [netserver-2]
Aug 29 20:03:30.972: INFO: Going to poll 172.20.30.151 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 29 20:03:30.976: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.30.151:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:03:30.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:03:30.977: INFO: ExecWithOptions: Clientset creation
Aug 29 20:03:30.977: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.30.151%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 20:03:31.061: INFO: Found all 1 expected endpoints: [netserver-3]
Aug 29 20:03:31.061: INFO: Going to poll 172.20.84.190 on port 8083 at least 0 times, with a maximum of 55 tries before failing
Aug 29 20:03:31.066: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.84.190:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:03:31.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:03:31.066: INFO: ExecWithOptions: Clientset creation
Aug 29 20:03:31.066: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.84.190%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 29 20:03:31.156: INFO: Found all 1 expected endpoints: [netserver-4]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:31.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-7265" for this suite. 08/29/23 20:03:31.162
------------------------------
• [SLOW TEST] [24.686 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:06.484
    Aug 29 20:03:06.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pod-network-test 08/29/23 20:03:06.486
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:06.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:06.516
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-7265 08/29/23 20:03:06.52
    STEP: creating a selector 08/29/23 20:03:06.52
    STEP: Creating the service pods in kubernetes 08/29/23 20:03:06.52
    Aug 29 20:03:06.520: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 29 20:03:06.597: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-7265" to be "running and ready"
    Aug 29 20:03:06.603: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.874804ms
    Aug 29 20:03:06.603: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:03:08.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011818094s
    Aug 29 20:03:08.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:10.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.011579536s
    Aug 29 20:03:10.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:12.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.010357337s
    Aug 29 20:03:12.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:14.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.009857666s
    Aug 29 20:03:14.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:16.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010212171s
    Aug 29 20:03:16.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:18.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.010442724s
    Aug 29 20:03:18.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:20.609: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.011791486s
    Aug 29 20:03:20.609: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:22.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010552944s
    Aug 29 20:03:22.608: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:24.607: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.01034453s
    Aug 29 20:03:24.607: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:26.606: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.00890546s
    Aug 29 20:03:26.606: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 29 20:03:28.608: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010774707s
    Aug 29 20:03:28.608: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 29 20:03:28.608: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 29 20:03:28.611: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-7265" to be "running and ready"
    Aug 29 20:03:28.620: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 8.926271ms
    Aug 29 20:03:28.620: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 29 20:03:28.620: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Aug 29 20:03:28.624: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-7265" to be "running and ready"
    Aug 29 20:03:28.628: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 3.741743ms
    Aug 29 20:03:28.628: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Aug 29 20:03:28.628: INFO: Pod "netserver-2" satisfied condition "running and ready"
    Aug 29 20:03:28.631: INFO: Waiting up to 5m0s for pod "netserver-3" in namespace "pod-network-test-7265" to be "running and ready"
    Aug 29 20:03:28.635: INFO: Pod "netserver-3": Phase="Running", Reason="", readiness=true. Elapsed: 3.515439ms
    Aug 29 20:03:28.635: INFO: The phase of Pod netserver-3 is Running (Ready = true)
    Aug 29 20:03:28.635: INFO: Pod "netserver-3" satisfied condition "running and ready"
    Aug 29 20:03:28.638: INFO: Waiting up to 5m0s for pod "netserver-4" in namespace "pod-network-test-7265" to be "running and ready"
    Aug 29 20:03:28.642: INFO: Pod "netserver-4": Phase="Running", Reason="", readiness=true. Elapsed: 3.796588ms
    Aug 29 20:03:28.642: INFO: The phase of Pod netserver-4 is Running (Ready = true)
    Aug 29 20:03:28.642: INFO: Pod "netserver-4" satisfied condition "running and ready"
    STEP: Creating test pods 08/29/23 20:03:28.646
    Aug 29 20:03:28.664: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-7265" to be "running"
    Aug 29 20:03:28.669: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.351573ms
    Aug 29 20:03:30.673: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009083828s
    Aug 29 20:03:30.673: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 29 20:03:30.678: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-7265" to be "running"
    Aug 29 20:03:30.681: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.617779ms
    Aug 29 20:03:30.681: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 29 20:03:30.684: INFO: Setting MaxTries for pod polling to 55 for networking test based on endpoint count 5
    Aug 29 20:03:30.684: INFO: Going to poll 172.20.17.97 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 20:03:30.687: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.17.97:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:03:30.687: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:03:30.688: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:03:30.688: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.17.97%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 20:03:30.785: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 29 20:03:30.785: INFO: Going to poll 172.20.76.162 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 20:03:30.789: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.76.162:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:03:30.789: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:03:30.789: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:03:30.789: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.76.162%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 20:03:30.875: INFO: Found all 1 expected endpoints: [netserver-1]
    Aug 29 20:03:30.875: INFO: Going to poll 172.20.143.242 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 20:03:30.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.143.242:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:03:30.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:03:30.880: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:03:30.880: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.143.242%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 20:03:30.972: INFO: Found all 1 expected endpoints: [netserver-2]
    Aug 29 20:03:30.972: INFO: Going to poll 172.20.30.151 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 20:03:30.976: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.30.151:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:03:30.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:03:30.977: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:03:30.977: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.30.151%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 20:03:31.061: INFO: Found all 1 expected endpoints: [netserver-3]
    Aug 29 20:03:31.061: INFO: Going to poll 172.20.84.190 on port 8083 at least 0 times, with a maximum of 55 tries before failing
    Aug 29 20:03:31.066: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.20.84.190:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7265 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:03:31.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:03:31.066: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:03:31.066: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/pod-network-test-7265/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.20.84.190%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 29 20:03:31.156: INFO: Found all 1 expected endpoints: [netserver-4]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:31.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-7265" for this suite. 08/29/23 20:03:31.162
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:31.171
Aug 29 20:03:31.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-runtime 08/29/23 20:03:31.172
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:31.192
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:31.196
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 08/29/23 20:03:31.199
STEP: wait for the container to reach Succeeded 08/29/23 20:03:31.21
STEP: get the container status 08/29/23 20:03:36.238
STEP: the container should be terminated 08/29/23 20:03:36.245
STEP: the termination message should be set 08/29/23 20:03:36.246
Aug 29 20:03:36.246: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/29/23 20:03:36.246
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:36.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1041" for this suite. 08/29/23 20:03:36.29
------------------------------
• [SLOW TEST] [5.133 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:31.171
    Aug 29 20:03:31.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-runtime 08/29/23 20:03:31.172
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:31.192
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:31.196
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 08/29/23 20:03:31.199
    STEP: wait for the container to reach Succeeded 08/29/23 20:03:31.21
    STEP: get the container status 08/29/23 20:03:36.238
    STEP: the container should be terminated 08/29/23 20:03:36.245
    STEP: the termination message should be set 08/29/23 20:03:36.246
    Aug 29 20:03:36.246: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/29/23 20:03:36.246
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:36.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1041" for this suite. 08/29/23 20:03:36.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:36.305
Aug 29 20:03:36.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-pred 08/29/23 20:03:36.307
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:36.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:36.337
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 29 20:03:36.341: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 20:03:36.355: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 20:03:36.360: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-master-0 before test
Aug 29 20:03:36.385: INFO: calico-node-f4bng from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.385: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:03:36.385: INFO: kube-apiserver-loki-15bd39-master-0 from kube-system started at 2023-08-29 19:11:07 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.385: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 29 20:03:36.385: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 29 20:03:36.385: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 29 20:03:36.385: INFO: kube-proxy-ds-7pn7t from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.385: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:03:36.385: INFO: fluent-bit-x2ntj from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.385: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:03:36.385: INFO: node-exporter-lsprp from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.385: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:03:36.385: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:03:36.385: INFO: nutanix-csi-node-mqc5m from ntnx-system started at 2023-08-29 19:23:42 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.385: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:03:36.385: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:03:36.385: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:03:36.385: INFO: netserver-0 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.385: INFO: 	Container webserver ready: true, restart count 0
Aug 29 20:03:36.385: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.385: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:03:36.385: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 20:03:36.385: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-master-1 before test
Aug 29 20:03:36.400: INFO: calico-node-snrmp from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:03:36.401: INFO: calico-typha-787bcdb57c-c7xxf from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 20:03:36.401: INFO: kube-apiserver-loki-15bd39-master-1 from kube-system started at 2023-08-29 19:12:53 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 29 20:03:36.401: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 29 20:03:36.401: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 29 20:03:36.401: INFO: kube-proxy-ds-c665w from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:03:36.401: INFO: fluent-bit-nmdm2 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:03:36.401: INFO: node-exporter-q5l7x from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:03:36.401: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:03:36.401: INFO: nutanix-csi-node-r85kq from ntnx-system started at 2023-08-29 19:23:49 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:03:36.401: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:03:36.401: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:03:36.401: INFO: netserver-1 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container webserver ready: true, restart count 0
Aug 29 20:03:36.401: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:03:36.401: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 20:03:36.401: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-0 before test
Aug 29 20:03:36.420: INFO: calico-node-mms8p from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:03:36.420: INFO: calico-typha-787bcdb57c-ppmlg from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 20:03:36.420: INFO: coredns-5d88b659b9-6fjb6 from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container coredns ready: true, restart count 0
Aug 29 20:03:36.420: INFO: kube-proxy-ds-nmtgg from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:03:36.420: INFO: alertmanager-main-0 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 20:03:36.420: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:03:36.420: INFO: blackbox-exporter-7954b6f4db-5vl2v from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 	Container module-configmap-reloader ready: true, restart count 0
Aug 29 20:03:36.420: INFO: csi-snapshot-controller-6d467d46bf-fbxgt from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 29 20:03:36.420: INFO: csi-snapshot-webhook-69668f7b57-g5r2n from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container snapshot-validation ready: true, restart count 0
Aug 29 20:03:36.420: INFO: fluent-bit-lbvlt from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:03:36.420: INFO: kubernetes-events-printer-74464fd469-rrxtq from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 29 20:03:36.420: INFO: node-exporter-wwfmq from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:03:36.420: INFO: nutanix-csi-node-hql5k from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:03:36.420: INFO: prometheus-adapter-6b6d856c7-htsdt from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 29 20:03:36.420: INFO: prometheus-k8s-1 from ntnx-system started at 2023-08-29 19:21:14 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 	Container prometheus ready: true, restart count 0
Aug 29 20:03:36.420: INFO: netserver-2 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container webserver ready: true, restart count 0
Aug 29 20:03:36.420: INFO: sonobuoy-e2e-job-161e88352cfa47dc from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container e2e ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:03:36.420: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 20:03:36.420: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-1 before test
Aug 29 20:03:36.455: INFO: calico-node-sb7l5 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:03:36.455: INFO: kube-proxy-ds-jt2tk from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:03:36.455: INFO: alertmanager-main-2 from ntnx-system started at 2023-08-29 19:46:48 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container alertmanager ready: true, restart count 0
Aug 29 20:03:36.455: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:03:36.455: INFO: fluent-bit-z54v4 from ntnx-system started at 2023-08-29 19:46:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:03:36.455: INFO: node-exporter-8gt7t from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:03:36.455: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:03:36.455: INFO: nutanix-csi-node-qtc8x from ntnx-system started at 2023-08-29 19:46:53 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:03:36.455: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:03:36.455: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:03:36.455: INFO: prometheus-adapter-6b6d856c7-zwgvq from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 29 20:03:36.455: INFO: prometheus-k8s-0 from ntnx-system started at 2023-08-29 19:47:04 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:03:36.455: INFO: 	Container prometheus ready: true, restart count 0
Aug 29 20:03:36.455: INFO: prometheus-operator-557c85cd6b-r5v4w from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:03:36.455: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 29 20:03:36.455: INFO: netserver-3 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container webserver ready: true, restart count 0
Aug 29 20:03:36.455: INFO: test-container-pod from pod-network-test-7265 started at 2023-08-29 20:03:28 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container webserver ready: true, restart count 0
Aug 29 20:03:36.455: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:03:36.455: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 20:03:36.455: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-2 before test
Aug 29 20:03:36.474: INFO: calico-kube-controllers-d9df5649-k5gf4 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 20:03:36.474: INFO: calico-node-9pl5x from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 20:03:36.474: INFO: calico-typha-787bcdb57c-6bcfm from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 20:03:36.474: INFO: coredns-5d88b659b9-fqwjz from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container coredns ready: true, restart count 0
Aug 29 20:03:36.474: INFO: kube-proxy-ds-l9vp5 from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 20:03:36.474: INFO: alertmanager-main-1 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 20:03:36.474: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 20:03:36.474: INFO: fluent-bit-6wl9b from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 20:03:36.474: INFO: kube-state-metrics-d459f9d68-z5xtj from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 29 20:03:36.474: INFO: node-exporter-xhwmw from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 20:03:36.474: INFO: nutanix-csi-controller-657cc74bd5-7j77l from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (5 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container csi-resizer ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container csi-snapshotter ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container nutanix-csi-plugin ready: true, restart count 0
Aug 29 20:03:36.474: INFO: nutanix-csi-node-hd5rc from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 20:03:36.474: INFO: netserver-4 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container webserver ready: true, restart count 0
Aug 29 20:03:36.474: INFO: sonobuoy from sonobuoy started at 2023-08-29 19:28:22 +0000 UTC (1 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 20:03:36.474: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 20:03:36.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 20:03:36.474: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/29/23 20:03:36.475
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177ff33a4663741a], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 08/29/23 20:03:36.544
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:37.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-89" for this suite. 08/29/23 20:03:37.538
------------------------------
• [1.240 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:36.305
    Aug 29 20:03:36.305: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-pred 08/29/23 20:03:36.307
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:36.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:36.337
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 29 20:03:36.341: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 29 20:03:36.355: INFO: Waiting for terminating namespaces to be deleted...
    Aug 29 20:03:36.360: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-master-0 before test
    Aug 29 20:03:36.385: INFO: calico-node-f4bng from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.385: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: kube-apiserver-loki-15bd39-master-0 from kube-system started at 2023-08-29 19:11:07 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.385: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: kube-proxy-ds-7pn7t from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.385: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: fluent-bit-x2ntj from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.385: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: node-exporter-lsprp from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.385: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: nutanix-csi-node-mqc5m from ntnx-system started at 2023-08-29 19:23:42 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.385: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: netserver-0 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.385: INFO: 	Container webserver ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.385: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 20:03:36.385: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-master-1 before test
    Aug 29 20:03:36.400: INFO: calico-node-snrmp from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: calico-typha-787bcdb57c-c7xxf from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: kube-apiserver-loki-15bd39-master-1 from kube-system started at 2023-08-29 19:12:53 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: kube-proxy-ds-c665w from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: fluent-bit-nmdm2 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: node-exporter-q5l7x from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: nutanix-csi-node-r85kq from ntnx-system started at 2023-08-29 19:23:49 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: netserver-1 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container webserver ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.401: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 20:03:36.401: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-0 before test
    Aug 29 20:03:36.420: INFO: calico-node-mms8p from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: calico-typha-787bcdb57c-ppmlg from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: coredns-5d88b659b9-6fjb6 from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container coredns ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: kube-proxy-ds-nmtgg from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: alertmanager-main-0 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 20:03:36.420: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: blackbox-exporter-7954b6f4db-5vl2v from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 	Container module-configmap-reloader ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: csi-snapshot-controller-6d467d46bf-fbxgt from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: csi-snapshot-webhook-69668f7b57-g5r2n from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container snapshot-validation ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: fluent-bit-lbvlt from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: kubernetes-events-printer-74464fd469-rrxtq from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: node-exporter-wwfmq from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: nutanix-csi-node-hql5k from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: prometheus-adapter-6b6d856c7-htsdt from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: prometheus-k8s-1 from ntnx-system started at 2023-08-29 19:21:14 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 	Container prometheus ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: netserver-2 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container webserver ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: sonobuoy-e2e-job-161e88352cfa47dc from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container e2e ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.420: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 20:03:36.420: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-1 before test
    Aug 29 20:03:36.455: INFO: calico-node-sb7l5 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: kube-proxy-ds-jt2tk from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: alertmanager-main-2 from ntnx-system started at 2023-08-29 19:46:48 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container alertmanager ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: fluent-bit-z54v4 from ntnx-system started at 2023-08-29 19:46:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: node-exporter-8gt7t from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: nutanix-csi-node-qtc8x from ntnx-system started at 2023-08-29 19:46:53 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: prometheus-adapter-6b6d856c7-zwgvq from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: prometheus-k8s-0 from ntnx-system started at 2023-08-29 19:47:04 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: 	Container prometheus ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: prometheus-operator-557c85cd6b-r5v4w from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: 	Container prometheus-operator ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: netserver-3 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container webserver ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: test-container-pod from pod-network-test-7265 started at 2023-08-29 20:03:28 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container webserver ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.455: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 20:03:36.455: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-2 before test
    Aug 29 20:03:36.474: INFO: calico-kube-controllers-d9df5649-k5gf4 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: calico-node-9pl5x from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: calico-typha-787bcdb57c-6bcfm from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: coredns-5d88b659b9-fqwjz from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container coredns ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: kube-proxy-ds-l9vp5 from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: alertmanager-main-1 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 20:03:36.474: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: fluent-bit-6wl9b from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: kube-state-metrics-d459f9d68-z5xtj from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: node-exporter-xhwmw from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: nutanix-csi-controller-657cc74bd5-7j77l from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (5 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container csi-provisioner ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container csi-resizer ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container nutanix-csi-plugin ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: nutanix-csi-node-hd5rc from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: netserver-4 from pod-network-test-7265 started at 2023-08-29 20:03:06 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container webserver ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: sonobuoy from sonobuoy started at 2023-08-29 19:28:22 +0000 UTC (1 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 20:03:36.474: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 20:03:36.474: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/29/23 20:03:36.475
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177ff33a4663741a], Reason = [FailedScheduling], Message = [0/5 nodes are available: 5 node(s) didn't match Pod's node affinity/selector. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling..] 08/29/23 20:03:36.544
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:37.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-89" for this suite. 08/29/23 20:03:37.538
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:37.546
Aug 29 20:03:37.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename namespaces 08/29/23 20:03:37.547
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:37.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:37.571
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 08/29/23 20:03:37.574
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:37.593
STEP: Creating a service in the namespace 08/29/23 20:03:37.596
STEP: Deleting the namespace 08/29/23 20:03:37.617
STEP: Waiting for the namespace to be removed. 08/29/23 20:03:37.629
STEP: Recreating the namespace 08/29/23 20:03:43.633
STEP: Verifying there is no service in the namespace 08/29/23 20:03:43.652
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:43.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5311" for this suite. 08/29/23 20:03:43.66
STEP: Destroying namespace "nsdeletetest-3319" for this suite. 08/29/23 20:03:43.667
Aug 29 20:03:43.670: INFO: Namespace nsdeletetest-3319 was already deleted
STEP: Destroying namespace "nsdeletetest-3946" for this suite. 08/29/23 20:03:43.67
------------------------------
• [SLOW TEST] [6.131 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:37.546
    Aug 29 20:03:37.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename namespaces 08/29/23 20:03:37.547
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:37.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:37.571
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 08/29/23 20:03:37.574
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:37.593
    STEP: Creating a service in the namespace 08/29/23 20:03:37.596
    STEP: Deleting the namespace 08/29/23 20:03:37.617
    STEP: Waiting for the namespace to be removed. 08/29/23 20:03:37.629
    STEP: Recreating the namespace 08/29/23 20:03:43.633
    STEP: Verifying there is no service in the namespace 08/29/23 20:03:43.652
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:43.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5311" for this suite. 08/29/23 20:03:43.66
    STEP: Destroying namespace "nsdeletetest-3319" for this suite. 08/29/23 20:03:43.667
    Aug 29 20:03:43.670: INFO: Namespace nsdeletetest-3319 was already deleted
    STEP: Destroying namespace "nsdeletetest-3946" for this suite. 08/29/23 20:03:43.67
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:43.679
Aug 29 20:03:43.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 20:03:43.68
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:43.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:43.699
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 08/29/23 20:03:43.702
Aug 29 20:03:43.712: INFO: Waiting up to 5m0s for pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2" in namespace "downward-api-6138" to be "Succeeded or Failed"
Aug 29 20:03:43.715: INFO: Pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226119ms
Aug 29 20:03:45.720: INFO: Pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008066536s
Aug 29 20:03:47.721: INFO: Pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009079436s
STEP: Saw pod success 08/29/23 20:03:47.721
Aug 29 20:03:47.721: INFO: Pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2" satisfied condition "Succeeded or Failed"
Aug 29 20:03:47.724: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2 container dapi-container: <nil>
STEP: delete the pod 08/29/23 20:03:47.732
Aug 29 20:03:47.749: INFO: Waiting for pod downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2 to disappear
Aug 29 20:03:47.752: INFO: Pod downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:47.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6138" for this suite. 08/29/23 20:03:47.757
------------------------------
• [4.085 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:43.679
    Aug 29 20:03:43.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 20:03:43.68
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:43.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:43.699
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 08/29/23 20:03:43.702
    Aug 29 20:03:43.712: INFO: Waiting up to 5m0s for pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2" in namespace "downward-api-6138" to be "Succeeded or Failed"
    Aug 29 20:03:43.715: INFO: Pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.226119ms
    Aug 29 20:03:45.720: INFO: Pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008066536s
    Aug 29 20:03:47.721: INFO: Pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009079436s
    STEP: Saw pod success 08/29/23 20:03:47.721
    Aug 29 20:03:47.721: INFO: Pod "downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2" satisfied condition "Succeeded or Failed"
    Aug 29 20:03:47.724: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2 container dapi-container: <nil>
    STEP: delete the pod 08/29/23 20:03:47.732
    Aug 29 20:03:47.749: INFO: Waiting for pod downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2 to disappear
    Aug 29 20:03:47.752: INFO: Pod downward-api-863c384d-4039-430f-b48b-75fa1ffd86c2 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:47.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6138" for this suite. 08/29/23 20:03:47.757
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:47.765
Aug 29 20:03:47.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:03:47.766
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:47.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:47.785
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 29 20:03:47.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:51.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3810" for this suite. 08/29/23 20:03:51.043
------------------------------
• [3.286 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:47.765
    Aug 29 20:03:47.765: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:03:47.766
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:47.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:47.785
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 29 20:03:47.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:51.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3810" for this suite. 08/29/23 20:03:51.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:51.055
Aug 29 20:03:51.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename limitrange 08/29/23 20:03:51.056
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:51.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:51.076
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-vlgr5" in namespace "limitrange-5738" 08/29/23 20:03:51.079
STEP: Creating another limitRange in another namespace 08/29/23 20:03:51.085
Aug 29 20:03:51.101: INFO: Namespace "e2e-limitrange-vlgr5-4816" created
Aug 29 20:03:51.101: INFO: Creating LimitRange "e2e-limitrange-vlgr5" in namespace "e2e-limitrange-vlgr5-4816"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vlgr5" 08/29/23 20:03:51.105
Aug 29 20:03:51.109: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-vlgr5" in "limitrange-5738" namespace 08/29/23 20:03:51.109
Aug 29 20:03:51.116: INFO: LimitRange "e2e-limitrange-vlgr5" has been patched
STEP: Delete LimitRange "e2e-limitrange-vlgr5" by Collection with labelSelector: "e2e-limitrange-vlgr5=patched" 08/29/23 20:03:51.116
STEP: Confirm that the limitRange "e2e-limitrange-vlgr5" has been deleted 08/29/23 20:03:51.127
Aug 29 20:03:51.127: INFO: Requesting list of LimitRange to confirm quantity
Aug 29 20:03:51.130: INFO: Found 0 LimitRange with label "e2e-limitrange-vlgr5=patched"
Aug 29 20:03:51.130: INFO: LimitRange "e2e-limitrange-vlgr5" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vlgr5" 08/29/23 20:03:51.13
Aug 29 20:03:51.133: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:51.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5738" for this suite. 08/29/23 20:03:51.138
STEP: Destroying namespace "e2e-limitrange-vlgr5-4816" for this suite. 08/29/23 20:03:51.147
------------------------------
• [0.099 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:51.055
    Aug 29 20:03:51.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename limitrange 08/29/23 20:03:51.056
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:51.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:51.076
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-vlgr5" in namespace "limitrange-5738" 08/29/23 20:03:51.079
    STEP: Creating another limitRange in another namespace 08/29/23 20:03:51.085
    Aug 29 20:03:51.101: INFO: Namespace "e2e-limitrange-vlgr5-4816" created
    Aug 29 20:03:51.101: INFO: Creating LimitRange "e2e-limitrange-vlgr5" in namespace "e2e-limitrange-vlgr5-4816"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-vlgr5" 08/29/23 20:03:51.105
    Aug 29 20:03:51.109: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-vlgr5" in "limitrange-5738" namespace 08/29/23 20:03:51.109
    Aug 29 20:03:51.116: INFO: LimitRange "e2e-limitrange-vlgr5" has been patched
    STEP: Delete LimitRange "e2e-limitrange-vlgr5" by Collection with labelSelector: "e2e-limitrange-vlgr5=patched" 08/29/23 20:03:51.116
    STEP: Confirm that the limitRange "e2e-limitrange-vlgr5" has been deleted 08/29/23 20:03:51.127
    Aug 29 20:03:51.127: INFO: Requesting list of LimitRange to confirm quantity
    Aug 29 20:03:51.130: INFO: Found 0 LimitRange with label "e2e-limitrange-vlgr5=patched"
    Aug 29 20:03:51.130: INFO: LimitRange "e2e-limitrange-vlgr5" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-vlgr5" 08/29/23 20:03:51.13
    Aug 29 20:03:51.133: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:51.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5738" for this suite. 08/29/23 20:03:51.138
    STEP: Destroying namespace "e2e-limitrange-vlgr5-4816" for this suite. 08/29/23 20:03:51.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:51.156
Aug 29 20:03:51.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename job 08/29/23 20:03:51.157
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:51.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:51.179
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 08/29/23 20:03:51.182
STEP: Ensure pods equal to parallelism count is attached to the job 08/29/23 20:03:51.203
STEP: patching /status 08/29/23 20:03:53.21
STEP: updating /status 08/29/23 20:03:53.219
STEP: get /status 08/29/23 20:03:53.25
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:53.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4087" for this suite. 08/29/23 20:03:53.259
------------------------------
• [2.110 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:51.156
    Aug 29 20:03:51.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename job 08/29/23 20:03:51.157
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:51.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:51.179
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 08/29/23 20:03:51.182
    STEP: Ensure pods equal to parallelism count is attached to the job 08/29/23 20:03:51.203
    STEP: patching /status 08/29/23 20:03:53.21
    STEP: updating /status 08/29/23 20:03:53.219
    STEP: get /status 08/29/23 20:03:53.25
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:53.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4087" for this suite. 08/29/23 20:03:53.259
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:53.266
Aug 29 20:03:53.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replication-controller 08/29/23 20:03:53.267
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:53.289
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:53.293
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 08/29/23 20:03:53.301
STEP: waiting for RC to be added 08/29/23 20:03:53.309
STEP: waiting for available Replicas 08/29/23 20:03:53.309
STEP: patching ReplicationController 08/29/23 20:03:54.34
STEP: waiting for RC to be modified 08/29/23 20:03:54.351
STEP: patching ReplicationController status 08/29/23 20:03:54.351
STEP: waiting for RC to be modified 08/29/23 20:03:54.359
STEP: waiting for available Replicas 08/29/23 20:03:54.36
STEP: fetching ReplicationController status 08/29/23 20:03:54.366
STEP: patching ReplicationController scale 08/29/23 20:03:54.37
STEP: waiting for RC to be modified 08/29/23 20:03:54.378
STEP: waiting for ReplicationController's scale to be the max amount 08/29/23 20:03:54.379
STEP: fetching ReplicationController; ensuring that it's patched 08/29/23 20:03:55.828
STEP: updating ReplicationController status 08/29/23 20:03:55.831
STEP: waiting for RC to be modified 08/29/23 20:03:55.84
STEP: listing all ReplicationControllers 08/29/23 20:03:55.84
STEP: checking that ReplicationController has expected values 08/29/23 20:03:55.844
STEP: deleting ReplicationControllers by collection 08/29/23 20:03:55.844
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/29/23 20:03:55.854
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:55.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-3889" for this suite. 08/29/23 20:03:55.898
------------------------------
• [2.639 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:53.266
    Aug 29 20:03:53.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replication-controller 08/29/23 20:03:53.267
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:53.289
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:53.293
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 08/29/23 20:03:53.301
    STEP: waiting for RC to be added 08/29/23 20:03:53.309
    STEP: waiting for available Replicas 08/29/23 20:03:53.309
    STEP: patching ReplicationController 08/29/23 20:03:54.34
    STEP: waiting for RC to be modified 08/29/23 20:03:54.351
    STEP: patching ReplicationController status 08/29/23 20:03:54.351
    STEP: waiting for RC to be modified 08/29/23 20:03:54.359
    STEP: waiting for available Replicas 08/29/23 20:03:54.36
    STEP: fetching ReplicationController status 08/29/23 20:03:54.366
    STEP: patching ReplicationController scale 08/29/23 20:03:54.37
    STEP: waiting for RC to be modified 08/29/23 20:03:54.378
    STEP: waiting for ReplicationController's scale to be the max amount 08/29/23 20:03:54.379
    STEP: fetching ReplicationController; ensuring that it's patched 08/29/23 20:03:55.828
    STEP: updating ReplicationController status 08/29/23 20:03:55.831
    STEP: waiting for RC to be modified 08/29/23 20:03:55.84
    STEP: listing all ReplicationControllers 08/29/23 20:03:55.84
    STEP: checking that ReplicationController has expected values 08/29/23 20:03:55.844
    STEP: deleting ReplicationControllers by collection 08/29/23 20:03:55.844
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/29/23 20:03:55.854
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:55.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-3889" for this suite. 08/29/23 20:03:55.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:55.906
Aug 29 20:03:55.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:03:55.907
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:55.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:55.926
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-92844d80-4559-43b1-99a8-894ec0cd1a2a 08/29/23 20:03:55.929
STEP: Creating a pod to test consume secrets 08/29/23 20:03:55.935
Aug 29 20:03:55.945: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a" in namespace "projected-8806" to be "Succeeded or Failed"
Aug 29 20:03:55.949: INFO: Pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618326ms
Aug 29 20:03:57.954: INFO: Pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008684976s
Aug 29 20:03:59.954: INFO: Pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008894657s
STEP: Saw pod success 08/29/23 20:03:59.954
Aug 29 20:03:59.955: INFO: Pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a" satisfied condition "Succeeded or Failed"
Aug 29 20:03:59.958: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a container projected-secret-volume-test: <nil>
STEP: delete the pod 08/29/23 20:03:59.966
Aug 29 20:03:59.978: INFO: Waiting for pod pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a to disappear
Aug 29 20:03:59.981: INFO: Pod pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 29 20:03:59.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8806" for this suite. 08/29/23 20:03:59.985
------------------------------
• [4.087 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:55.906
    Aug 29 20:03:55.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:03:55.907
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:03:55.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:03:55.926
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-92844d80-4559-43b1-99a8-894ec0cd1a2a 08/29/23 20:03:55.929
    STEP: Creating a pod to test consume secrets 08/29/23 20:03:55.935
    Aug 29 20:03:55.945: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a" in namespace "projected-8806" to be "Succeeded or Failed"
    Aug 29 20:03:55.949: INFO: Pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618326ms
    Aug 29 20:03:57.954: INFO: Pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008684976s
    Aug 29 20:03:59.954: INFO: Pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008894657s
    STEP: Saw pod success 08/29/23 20:03:59.954
    Aug 29 20:03:59.955: INFO: Pod "pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a" satisfied condition "Succeeded or Failed"
    Aug 29 20:03:59.958: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:03:59.966
    Aug 29 20:03:59.978: INFO: Waiting for pod pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a to disappear
    Aug 29 20:03:59.981: INFO: Pod pod-projected-secrets-4a7225f5-a980-4754-9207-98261641c50a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:03:59.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8806" for this suite. 08/29/23 20:03:59.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:03:59.993
Aug 29 20:03:59.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename job 08/29/23 20:03:59.994
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:00.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:00.018
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 08/29/23 20:04:00.025
STEP: Patching the Job 08/29/23 20:04:00.031
STEP: Watching for Job to be patched 08/29/23 20:04:00.058
Aug 29 20:04:00.060: INFO: Event ADDED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 29 20:04:00.060: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 29 20:04:00.060: INFO: Event MODIFIED found for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/29/23 20:04:00.06
STEP: Watching for Job to be updated 08/29/23 20:04:00.07
Aug 29 20:04:00.072: INFO: Event MODIFIED found for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 29 20:04:00.072: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/29/23 20:04:00.072
Aug 29 20:04:00.075: INFO: Job: e2e-pdlzd as labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched]
STEP: Waiting for job to complete 08/29/23 20:04:00.075
STEP: Delete a job collection with a labelselector 08/29/23 20:04:08.08
STEP: Watching for Job to be deleted 08/29/23 20:04:08.09
Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 29 20:04:08.092: INFO: Event DELETED found for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/29/23 20:04:08.092
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 29 20:04:08.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5739" for this suite. 08/29/23 20:04:08.111
------------------------------
• [SLOW TEST] [8.128 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:03:59.993
    Aug 29 20:03:59.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename job 08/29/23 20:03:59.994
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:00.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:00.018
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 08/29/23 20:04:00.025
    STEP: Patching the Job 08/29/23 20:04:00.031
    STEP: Watching for Job to be patched 08/29/23 20:04:00.058
    Aug 29 20:04:00.060: INFO: Event ADDED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 29 20:04:00.060: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 29 20:04:00.060: INFO: Event MODIFIED found for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/29/23 20:04:00.06
    STEP: Watching for Job to be updated 08/29/23 20:04:00.07
    Aug 29 20:04:00.072: INFO: Event MODIFIED found for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 29 20:04:00.072: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/29/23 20:04:00.072
    Aug 29 20:04:00.075: INFO: Job: e2e-pdlzd as labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched]
    STEP: Waiting for job to complete 08/29/23 20:04:00.075
    STEP: Delete a job collection with a labelselector 08/29/23 20:04:08.08
    STEP: Watching for Job to be deleted 08/29/23 20:04:08.09
    Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 29 20:04:08.092: INFO: Event MODIFIED observed for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 29 20:04:08.092: INFO: Event DELETED found for Job e2e-pdlzd in namespace job-5739 with labels: map[e2e-job-label:e2e-pdlzd e2e-pdlzd:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/29/23 20:04:08.092
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:04:08.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5739" for this suite. 08/29/23 20:04:08.111
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:04:08.123
Aug 29 20:04:08.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename statefulset 08/29/23 20:04:08.124
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:08.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:08.162
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5280 08/29/23 20:04:08.165
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-5280 08/29/23 20:04:08.171
Aug 29 20:04:08.189: INFO: Found 0 stateful pods, waiting for 1
Aug 29 20:04:18.195: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/29/23 20:04:18.201
STEP: updating a scale subresource 08/29/23 20:04:18.205
STEP: verifying the statefulset Spec.Replicas was modified 08/29/23 20:04:18.212
STEP: Patch a scale subresource 08/29/23 20:04:18.214
STEP: verifying the statefulset Spec.Replicas was modified 08/29/23 20:04:18.224
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 29 20:04:18.233: INFO: Deleting all statefulset in ns statefulset-5280
Aug 29 20:04:18.237: INFO: Scaling statefulset ss to 0
Aug 29 20:04:28.260: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 20:04:28.263: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:04:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5280" for this suite. 08/29/23 20:04:28.295
------------------------------
• [SLOW TEST] [20.183 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:04:08.123
    Aug 29 20:04:08.123: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename statefulset 08/29/23 20:04:08.124
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:08.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:08.162
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5280 08/29/23 20:04:08.165
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-5280 08/29/23 20:04:08.171
    Aug 29 20:04:08.189: INFO: Found 0 stateful pods, waiting for 1
    Aug 29 20:04:18.195: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/29/23 20:04:18.201
    STEP: updating a scale subresource 08/29/23 20:04:18.205
    STEP: verifying the statefulset Spec.Replicas was modified 08/29/23 20:04:18.212
    STEP: Patch a scale subresource 08/29/23 20:04:18.214
    STEP: verifying the statefulset Spec.Replicas was modified 08/29/23 20:04:18.224
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 29 20:04:18.233: INFO: Deleting all statefulset in ns statefulset-5280
    Aug 29 20:04:18.237: INFO: Scaling statefulset ss to 0
    Aug 29 20:04:28.260: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 20:04:28.263: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:04:28.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5280" for this suite. 08/29/23 20:04:28.295
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:04:28.307
Aug 29 20:04:28.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:04:28.308
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:28.332
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:28.335
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Aug 29 20:04:28.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: creating the pod 08/29/23 20:04:28.339
STEP: submitting the pod to kubernetes 08/29/23 20:04:28.339
Aug 29 20:04:28.354: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d" in namespace "pods-1923" to be "running and ready"
Aug 29 20:04:28.360: INFO: Pod "pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141531ms
Aug 29 20:04:28.360: INFO: The phase of Pod pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:04:30.365: INFO: Pod "pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010532973s
Aug 29 20:04:30.365: INFO: The phase of Pod pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d is Running (Ready = true)
Aug 29 20:04:30.365: INFO: Pod "pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:04:30.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1923" for this suite. 08/29/23 20:04:30.473
------------------------------
• [2.176 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:04:28.307
    Aug 29 20:04:28.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:04:28.308
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:28.332
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:28.335
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Aug 29 20:04:28.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: creating the pod 08/29/23 20:04:28.339
    STEP: submitting the pod to kubernetes 08/29/23 20:04:28.339
    Aug 29 20:04:28.354: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d" in namespace "pods-1923" to be "running and ready"
    Aug 29 20:04:28.360: INFO: Pod "pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141531ms
    Aug 29 20:04:28.360: INFO: The phase of Pod pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:04:30.365: INFO: Pod "pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d": Phase="Running", Reason="", readiness=true. Elapsed: 2.010532973s
    Aug 29 20:04:30.365: INFO: The phase of Pod pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d is Running (Ready = true)
    Aug 29 20:04:30.365: INFO: Pod "pod-exec-websocket-c737ca06-9dee-497f-ad28-9721605f833d" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:04:30.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1923" for this suite. 08/29/23 20:04:30.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:04:30.483
Aug 29 20:04:30.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename events 08/29/23 20:04:30.484
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:30.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:30.506
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/29/23 20:04:30.509
STEP: listing all events in all namespaces 08/29/23 20:04:30.514
STEP: patching the test event 08/29/23 20:04:30.627
STEP: fetching the test event 08/29/23 20:04:30.64
STEP: updating the test event 08/29/23 20:04:30.643
STEP: getting the test event 08/29/23 20:04:30.655
STEP: deleting the test event 08/29/23 20:04:30.659
STEP: listing all events in all namespaces 08/29/23 20:04:30.668
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 29 20:04:30.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-1669" for this suite. 08/29/23 20:04:30.69
------------------------------
• [0.214 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:04:30.483
    Aug 29 20:04:30.483: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename events 08/29/23 20:04:30.484
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:30.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:30.506
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/29/23 20:04:30.509
    STEP: listing all events in all namespaces 08/29/23 20:04:30.514
    STEP: patching the test event 08/29/23 20:04:30.627
    STEP: fetching the test event 08/29/23 20:04:30.64
    STEP: updating the test event 08/29/23 20:04:30.643
    STEP: getting the test event 08/29/23 20:04:30.655
    STEP: deleting the test event 08/29/23 20:04:30.659
    STEP: listing all events in all namespaces 08/29/23 20:04:30.668
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:04:30.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-1669" for this suite. 08/29/23 20:04:30.69
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:04:30.698
Aug 29 20:04:30.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-runtime 08/29/23 20:04:30.7
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:30.719
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:30.722
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 08/29/23 20:04:30.724
STEP: wait for the container to reach Failed 08/29/23 20:04:30.735
STEP: get the container status 08/29/23 20:04:34.772
STEP: the container should be terminated 08/29/23 20:04:34.775
STEP: the termination message should be set 08/29/23 20:04:34.775
Aug 29 20:04:34.776: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/29/23 20:04:34.776
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 29 20:04:34.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-5873" for this suite. 08/29/23 20:04:34.801
------------------------------
• [4.112 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:04:30.698
    Aug 29 20:04:30.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-runtime 08/29/23 20:04:30.7
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:30.719
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:30.722
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 08/29/23 20:04:30.724
    STEP: wait for the container to reach Failed 08/29/23 20:04:30.735
    STEP: get the container status 08/29/23 20:04:34.772
    STEP: the container should be terminated 08/29/23 20:04:34.775
    STEP: the termination message should be set 08/29/23 20:04:34.775
    Aug 29 20:04:34.776: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/29/23 20:04:34.776
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:04:34.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-5873" for this suite. 08/29/23 20:04:34.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:04:34.811
Aug 29 20:04:34.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:04:34.812
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:34.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:34.834
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 29 20:04:34.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:04:35.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6286" for this suite. 08/29/23 20:04:35.399
------------------------------
• [0.595 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:04:34.811
    Aug 29 20:04:34.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:04:34.812
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:34.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:34.834
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 29 20:04:34.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:04:35.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6286" for this suite. 08/29/23 20:04:35.399
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:04:35.408
Aug 29 20:04:35.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename statefulset 08/29/23 20:04:35.409
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:35.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:35.428
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5497 08/29/23 20:04:35.431
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-5497 08/29/23 20:04:35.445
Aug 29 20:04:35.457: INFO: Found 0 stateful pods, waiting for 1
Aug 29 20:04:45.462: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/29/23 20:04:45.47
STEP: Getting /status 08/29/23 20:04:45.48
Aug 29 20:04:45.484: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/29/23 20:04:45.484
Aug 29 20:04:45.501: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/29/23 20:04:45.501
Aug 29 20:04:45.503: INFO: Observed &StatefulSet event: ADDED
Aug 29 20:04:45.503: INFO: Found Statefulset ss in namespace statefulset-5497 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 20:04:45.503: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/29/23 20:04:45.503
Aug 29 20:04:45.503: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 20:04:45.514: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/29/23 20:04:45.514
Aug 29 20:04:45.516: INFO: Observed &StatefulSet event: ADDED
Aug 29 20:04:45.516: INFO: Observed Statefulset ss in namespace statefulset-5497 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 20:04:45.517: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 29 20:04:45.517: INFO: Deleting all statefulset in ns statefulset-5497
Aug 29 20:04:45.521: INFO: Scaling statefulset ss to 0
Aug 29 20:04:55.543: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 20:04:55.546: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:04:55.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5497" for this suite. 08/29/23 20:04:55.579
------------------------------
• [SLOW TEST] [20.181 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:04:35.408
    Aug 29 20:04:35.408: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename statefulset 08/29/23 20:04:35.409
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:35.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:35.428
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5497 08/29/23 20:04:35.431
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-5497 08/29/23 20:04:35.445
    Aug 29 20:04:35.457: INFO: Found 0 stateful pods, waiting for 1
    Aug 29 20:04:45.462: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/29/23 20:04:45.47
    STEP: Getting /status 08/29/23 20:04:45.48
    Aug 29 20:04:45.484: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/29/23 20:04:45.484
    Aug 29 20:04:45.501: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/29/23 20:04:45.501
    Aug 29 20:04:45.503: INFO: Observed &StatefulSet event: ADDED
    Aug 29 20:04:45.503: INFO: Found Statefulset ss in namespace statefulset-5497 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 29 20:04:45.503: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/29/23 20:04:45.503
    Aug 29 20:04:45.503: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 29 20:04:45.514: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/29/23 20:04:45.514
    Aug 29 20:04:45.516: INFO: Observed &StatefulSet event: ADDED
    Aug 29 20:04:45.516: INFO: Observed Statefulset ss in namespace statefulset-5497 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 29 20:04:45.517: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 29 20:04:45.517: INFO: Deleting all statefulset in ns statefulset-5497
    Aug 29 20:04:45.521: INFO: Scaling statefulset ss to 0
    Aug 29 20:04:55.543: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 20:04:55.546: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:04:55.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5497" for this suite. 08/29/23 20:04:55.579
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:04:55.589
Aug 29 20:04:55.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:04:55.59
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:55.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:55.616
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e31db532-3655-4ad6-9894-20033de0c08b 08/29/23 20:04:55.624
STEP: Creating the pod 08/29/23 20:04:55.632
Aug 29 20:04:55.643: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d" in namespace "projected-239" to be "running and ready"
Aug 29 20:04:55.647: INFO: Pod "pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131854ms
Aug 29 20:04:55.647: INFO: The phase of Pod pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:04:57.651: INFO: Pod "pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008074308s
Aug 29 20:04:57.651: INFO: The phase of Pod pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d is Running (Ready = true)
Aug 29 20:04:57.651: INFO: Pod "pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-e31db532-3655-4ad6-9894-20033de0c08b 08/29/23 20:04:57.662
STEP: waiting to observe update in volume 08/29/23 20:04:57.669
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:04:59.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-239" for this suite. 08/29/23 20:04:59.697
------------------------------
• [4.115 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:04:55.589
    Aug 29 20:04:55.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:04:55.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:55.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:55.616
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-e31db532-3655-4ad6-9894-20033de0c08b 08/29/23 20:04:55.624
    STEP: Creating the pod 08/29/23 20:04:55.632
    Aug 29 20:04:55.643: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d" in namespace "projected-239" to be "running and ready"
    Aug 29 20:04:55.647: INFO: Pod "pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131854ms
    Aug 29 20:04:55.647: INFO: The phase of Pod pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:04:57.651: INFO: Pod "pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d": Phase="Running", Reason="", readiness=true. Elapsed: 2.008074308s
    Aug 29 20:04:57.651: INFO: The phase of Pod pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d is Running (Ready = true)
    Aug 29 20:04:57.651: INFO: Pod "pod-projected-configmaps-69ce9a77-e4d3-4d0a-807b-c2634cac247d" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-e31db532-3655-4ad6-9894-20033de0c08b 08/29/23 20:04:57.662
    STEP: waiting to observe update in volume 08/29/23 20:04:57.669
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:04:59.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-239" for this suite. 08/29/23 20:04:59.697
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:04:59.707
Aug 29 20:04:59.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:04:59.708
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:59.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:59.737
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-91a900ab-a595-41ff-8fdd-97f71ead41d8 08/29/23 20:04:59.74
STEP: Creating a pod to test consume secrets 08/29/23 20:04:59.746
Aug 29 20:04:59.757: INFO: Waiting up to 5m0s for pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d" in namespace "secrets-7243" to be "Succeeded or Failed"
Aug 29 20:04:59.766: INFO: Pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.954662ms
Aug 29 20:05:01.772: INFO: Pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015564478s
Aug 29 20:05:03.770: INFO: Pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01347021s
STEP: Saw pod success 08/29/23 20:05:03.77
Aug 29 20:05:03.771: INFO: Pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d" satisfied condition "Succeeded or Failed"
Aug 29 20:05:03.774: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d container secret-volume-test: <nil>
STEP: delete the pod 08/29/23 20:05:03.782
Aug 29 20:05:03.797: INFO: Waiting for pod pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d to disappear
Aug 29 20:05:03.799: INFO: Pod pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:05:03.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7243" for this suite. 08/29/23 20:05:03.804
------------------------------
• [4.105 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:04:59.707
    Aug 29 20:04:59.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:04:59.708
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:04:59.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:04:59.737
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-91a900ab-a595-41ff-8fdd-97f71ead41d8 08/29/23 20:04:59.74
    STEP: Creating a pod to test consume secrets 08/29/23 20:04:59.746
    Aug 29 20:04:59.757: INFO: Waiting up to 5m0s for pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d" in namespace "secrets-7243" to be "Succeeded or Failed"
    Aug 29 20:04:59.766: INFO: Pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.954662ms
    Aug 29 20:05:01.772: INFO: Pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015564478s
    Aug 29 20:05:03.770: INFO: Pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01347021s
    STEP: Saw pod success 08/29/23 20:05:03.77
    Aug 29 20:05:03.771: INFO: Pod "pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d" satisfied condition "Succeeded or Failed"
    Aug 29 20:05:03.774: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d container secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:05:03.782
    Aug 29 20:05:03.797: INFO: Waiting for pod pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d to disappear
    Aug 29 20:05:03.799: INFO: Pod pod-secrets-283802dc-485c-4f79-81bc-f4c89c595a0d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:05:03.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7243" for this suite. 08/29/23 20:05:03.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:05:03.813
Aug 29 20:05:03.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replicaset 08/29/23 20:05:03.814
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:03.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:03.831
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/29/23 20:05:03.834
Aug 29 20:05:03.845: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7087" to be "running and ready"
Aug 29 20:05:03.849: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.248647ms
Aug 29 20:05:03.849: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:05:05.854: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008720965s
Aug 29 20:05:05.854: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 29 20:05:05.854: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/29/23 20:05:05.858
STEP: Then the orphan pod is adopted 08/29/23 20:05:05.866
STEP: When the matched label of one of its pods change 08/29/23 20:05:06.873
Aug 29 20:05:06.876: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/29/23 20:05:06.889
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:05:07.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7087" for this suite. 08/29/23 20:05:07.903
------------------------------
• [4.097 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:05:03.813
    Aug 29 20:05:03.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replicaset 08/29/23 20:05:03.814
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:03.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:03.831
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/29/23 20:05:03.834
    Aug 29 20:05:03.845: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-7087" to be "running and ready"
    Aug 29 20:05:03.849: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.248647ms
    Aug 29 20:05:03.849: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:05:05.854: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008720965s
    Aug 29 20:05:05.854: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 29 20:05:05.854: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/29/23 20:05:05.858
    STEP: Then the orphan pod is adopted 08/29/23 20:05:05.866
    STEP: When the matched label of one of its pods change 08/29/23 20:05:06.873
    Aug 29 20:05:06.876: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/29/23 20:05:06.889
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:05:07.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7087" for this suite. 08/29/23 20:05:07.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:05:07.911
Aug 29 20:05:07.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:05:07.912
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:07.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:07.929
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:05:07.949
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:05:08.862
STEP: Deploying the webhook pod 08/29/23 20:05:08.869
STEP: Wait for the deployment to be ready 08/29/23 20:05:08.885
Aug 29 20:05:08.896: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 20:05:10.909
STEP: Verifying the service has paired with the endpoint 08/29/23 20:05:10.932
Aug 29 20:05:11.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/29/23 20:05:11.937
STEP: create a pod that should be updated by the webhook 08/29/23 20:05:11.958
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:05:11.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2465" for this suite. 08/29/23 20:05:12.058
STEP: Destroying namespace "webhook-2465-markers" for this suite. 08/29/23 20:05:12.066
------------------------------
• [4.165 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:05:07.911
    Aug 29 20:05:07.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:05:07.912
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:07.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:07.929
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:05:07.949
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:05:08.862
    STEP: Deploying the webhook pod 08/29/23 20:05:08.869
    STEP: Wait for the deployment to be ready 08/29/23 20:05:08.885
    Aug 29 20:05:08.896: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 20:05:10.909
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:05:10.932
    Aug 29 20:05:11.933: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/29/23 20:05:11.937
    STEP: create a pod that should be updated by the webhook 08/29/23 20:05:11.958
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:05:11.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2465" for this suite. 08/29/23 20:05:12.058
    STEP: Destroying namespace "webhook-2465-markers" for this suite. 08/29/23 20:05:12.066
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:05:12.076
Aug 29 20:05:12.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename job 08/29/23 20:05:12.078
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:12.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:12.096
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 08/29/23 20:05:12.099
STEP: Ensuring job reaches completions 08/29/23 20:05:12.107
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 29 20:05:24.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-43" for this suite. 08/29/23 20:05:24.118
------------------------------
• [SLOW TEST] [12.051 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:05:12.076
    Aug 29 20:05:12.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename job 08/29/23 20:05:12.078
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:12.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:12.096
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 08/29/23 20:05:12.099
    STEP: Ensuring job reaches completions 08/29/23 20:05:12.107
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:05:24.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-43" for this suite. 08/29/23 20:05:24.118
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:05:24.128
Aug 29 20:05:24.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replicaset 08/29/23 20:05:24.129
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:24.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:24.152
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/29/23 20:05:24.159
STEP: Verify that the required pods have come up. 08/29/23 20:05:24.168
Aug 29 20:05:24.172: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 20:05:29.178: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/29/23 20:05:29.178
STEP: Getting /status 08/29/23 20:05:29.179
Aug 29 20:05:29.184: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/29/23 20:05:29.184
Aug 29 20:05:29.196: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/29/23 20:05:29.196
Aug 29 20:05:29.200: INFO: Observed &ReplicaSet event: ADDED
Aug 29 20:05:29.200: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 20:05:29.200: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 20:05:29.200: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 20:05:29.200: INFO: Found replicaset test-rs in namespace replicaset-4139 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 20:05:29.200: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/29/23 20:05:29.2
Aug 29 20:05:29.200: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 29 20:05:29.207: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/29/23 20:05:29.207
Aug 29 20:05:29.209: INFO: Observed &ReplicaSet event: ADDED
Aug 29 20:05:29.209: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 20:05:29.210: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 20:05:29.210: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 20:05:29.210: INFO: Observed replicaset test-rs in namespace replicaset-4139 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 29 20:05:29.210: INFO: Observed &ReplicaSet event: MODIFIED
Aug 29 20:05:29.210: INFO: Found replicaset test-rs in namespace replicaset-4139 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 29 20:05:29.210: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:05:29.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-4139" for this suite. 08/29/23 20:05:29.214
------------------------------
• [SLOW TEST] [5.096 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:05:24.128
    Aug 29 20:05:24.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replicaset 08/29/23 20:05:24.129
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:24.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:24.152
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/29/23 20:05:24.159
    STEP: Verify that the required pods have come up. 08/29/23 20:05:24.168
    Aug 29 20:05:24.172: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 29 20:05:29.178: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/29/23 20:05:29.178
    STEP: Getting /status 08/29/23 20:05:29.179
    Aug 29 20:05:29.184: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/29/23 20:05:29.184
    Aug 29 20:05:29.196: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/29/23 20:05:29.196
    Aug 29 20:05:29.200: INFO: Observed &ReplicaSet event: ADDED
    Aug 29 20:05:29.200: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 29 20:05:29.200: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 29 20:05:29.200: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 29 20:05:29.200: INFO: Found replicaset test-rs in namespace replicaset-4139 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 29 20:05:29.200: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/29/23 20:05:29.2
    Aug 29 20:05:29.200: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 29 20:05:29.207: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/29/23 20:05:29.207
    Aug 29 20:05:29.209: INFO: Observed &ReplicaSet event: ADDED
    Aug 29 20:05:29.209: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 29 20:05:29.210: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 29 20:05:29.210: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 29 20:05:29.210: INFO: Observed replicaset test-rs in namespace replicaset-4139 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 29 20:05:29.210: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 29 20:05:29.210: INFO: Found replicaset test-rs in namespace replicaset-4139 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 29 20:05:29.210: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:05:29.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-4139" for this suite. 08/29/23 20:05:29.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:05:29.225
Aug 29 20:05:29.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename endpointslice 08/29/23 20:05:29.227
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:29.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:29.249
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 08/29/23 20:05:34.343
STEP: referencing matching pods with named port 08/29/23 20:05:39.351
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/29/23 20:05:44.358
STEP: recreating EndpointSlices after they've been deleted 08/29/23 20:05:49.367
Aug 29 20:05:49.396: INFO: EndpointSlice for Service endpointslice-1585/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 29 20:05:59.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-1585" for this suite. 08/29/23 20:05:59.411
------------------------------
• [SLOW TEST] [30.193 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:05:29.225
    Aug 29 20:05:29.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename endpointslice 08/29/23 20:05:29.227
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:29.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:29.249
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 08/29/23 20:05:34.343
    STEP: referencing matching pods with named port 08/29/23 20:05:39.351
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/29/23 20:05:44.358
    STEP: recreating EndpointSlices after they've been deleted 08/29/23 20:05:49.367
    Aug 29 20:05:49.396: INFO: EndpointSlice for Service endpointslice-1585/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:05:59.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-1585" for this suite. 08/29/23 20:05:59.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:05:59.418
Aug 29 20:05:59.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:05:59.419
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:59.436
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:59.439
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:05:59.458
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:05:59.901
STEP: Deploying the webhook pod 08/29/23 20:05:59.912
STEP: Wait for the deployment to be ready 08/29/23 20:05:59.926
Aug 29 20:05:59.935: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 29 20:06:01.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 5, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 5, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/29/23 20:06:03.955
STEP: Verifying the service has paired with the endpoint 08/29/23 20:06:03.97
Aug 29 20:06:04.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 08/29/23 20:06:05.067
STEP: Creating a configMap that should be mutated 08/29/23 20:06:05.084
STEP: Deleting the collection of validation webhooks 08/29/23 20:06:05.122
STEP: Creating a configMap that should not be mutated 08/29/23 20:06:05.187
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:05.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3855" for this suite. 08/29/23 20:06:05.255
STEP: Destroying namespace "webhook-3855-markers" for this suite. 08/29/23 20:06:05.268
------------------------------
• [SLOW TEST] [5.865 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:05:59.418
    Aug 29 20:05:59.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:05:59.419
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:05:59.436
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:05:59.439
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:05:59.458
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:05:59.901
    STEP: Deploying the webhook pod 08/29/23 20:05:59.912
    STEP: Wait for the deployment to be ready 08/29/23 20:05:59.926
    Aug 29 20:05:59.935: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 29 20:06:01.950: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 5, 59, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 5, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 5, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-865554f4d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/29/23 20:06:03.955
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:06:03.97
    Aug 29 20:06:04.971: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 08/29/23 20:06:05.067
    STEP: Creating a configMap that should be mutated 08/29/23 20:06:05.084
    STEP: Deleting the collection of validation webhooks 08/29/23 20:06:05.122
    STEP: Creating a configMap that should not be mutated 08/29/23 20:06:05.187
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:05.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3855" for this suite. 08/29/23 20:06:05.255
    STEP: Destroying namespace "webhook-3855-markers" for this suite. 08/29/23 20:06:05.268
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:05.285
Aug 29 20:06:05.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename certificates 08/29/23 20:06:05.286
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:05.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:05.302
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/29/23 20:06:05.967
STEP: getting /apis/certificates.k8s.io 08/29/23 20:06:05.97
STEP: getting /apis/certificates.k8s.io/v1 08/29/23 20:06:05.972
STEP: creating 08/29/23 20:06:05.973
STEP: getting 08/29/23 20:06:06.01
STEP: listing 08/29/23 20:06:06.018
STEP: watching 08/29/23 20:06:06.025
Aug 29 20:06:06.026: INFO: starting watch
STEP: patching 08/29/23 20:06:06.028
STEP: updating 08/29/23 20:06:06.05
Aug 29 20:06:06.060: INFO: waiting for watch events with expected annotations
Aug 29 20:06:06.060: INFO: saw patched and updated annotations
STEP: getting /approval 08/29/23 20:06:06.06
STEP: patching /approval 08/29/23 20:06:06.063
STEP: updating /approval 08/29/23 20:06:06.08
STEP: getting /status 08/29/23 20:06:06.091
STEP: patching /status 08/29/23 20:06:06.095
STEP: updating /status 08/29/23 20:06:06.11
STEP: deleting 08/29/23 20:06:06.121
STEP: deleting a collection 08/29/23 20:06:06.136
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:06.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-7403" for this suite. 08/29/23 20:06:06.162
------------------------------
• [0.886 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:05.285
    Aug 29 20:06:05.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename certificates 08/29/23 20:06:05.286
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:05.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:05.302
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/29/23 20:06:05.967
    STEP: getting /apis/certificates.k8s.io 08/29/23 20:06:05.97
    STEP: getting /apis/certificates.k8s.io/v1 08/29/23 20:06:05.972
    STEP: creating 08/29/23 20:06:05.973
    STEP: getting 08/29/23 20:06:06.01
    STEP: listing 08/29/23 20:06:06.018
    STEP: watching 08/29/23 20:06:06.025
    Aug 29 20:06:06.026: INFO: starting watch
    STEP: patching 08/29/23 20:06:06.028
    STEP: updating 08/29/23 20:06:06.05
    Aug 29 20:06:06.060: INFO: waiting for watch events with expected annotations
    Aug 29 20:06:06.060: INFO: saw patched and updated annotations
    STEP: getting /approval 08/29/23 20:06:06.06
    STEP: patching /approval 08/29/23 20:06:06.063
    STEP: updating /approval 08/29/23 20:06:06.08
    STEP: getting /status 08/29/23 20:06:06.091
    STEP: patching /status 08/29/23 20:06:06.095
    STEP: updating /status 08/29/23 20:06:06.11
    STEP: deleting 08/29/23 20:06:06.121
    STEP: deleting a collection 08/29/23 20:06:06.136
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:06.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-7403" for this suite. 08/29/23 20:06:06.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:06.171
Aug 29 20:06:06.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:06:06.173
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:06.188
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:06.191
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:06:06.209
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:06:06.725
STEP: Deploying the webhook pod 08/29/23 20:06:06.732
STEP: Wait for the deployment to be ready 08/29/23 20:06:06.747
Aug 29 20:06:06.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 20:06:08.769
STEP: Verifying the service has paired with the endpoint 08/29/23 20:06:08.785
Aug 29 20:06:09.786: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 08/29/23 20:06:09.791
STEP: Creating a custom resource definition that should be denied by the webhook 08/29/23 20:06:09.812
Aug 29 20:06:09.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:09.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9984" for this suite. 08/29/23 20:06:09.889
STEP: Destroying namespace "webhook-9984-markers" for this suite. 08/29/23 20:06:09.897
------------------------------
• [3.741 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:06.171
    Aug 29 20:06:06.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:06:06.173
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:06.188
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:06.191
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:06:06.209
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:06:06.725
    STEP: Deploying the webhook pod 08/29/23 20:06:06.732
    STEP: Wait for the deployment to be ready 08/29/23 20:06:06.747
    Aug 29 20:06:06.757: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 20:06:08.769
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:06:08.785
    Aug 29 20:06:09.786: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/29/23 20:06:09.791
    STEP: Creating a custom resource definition that should be denied by the webhook 08/29/23 20:06:09.812
    Aug 29 20:06:09.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:09.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9984" for this suite. 08/29/23 20:06:09.889
    STEP: Destroying namespace "webhook-9984-markers" for this suite. 08/29/23 20:06:09.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:09.913
Aug 29 20:06:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 20:06:09.914
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:09.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:09.94
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 08/29/23 20:06:09.946
STEP: Creating a ResourceQuota 08/29/23 20:06:14.952
STEP: Ensuring resource quota status is calculated 08/29/23 20:06:14.958
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:16.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9860" for this suite. 08/29/23 20:06:16.968
------------------------------
• [SLOW TEST] [7.066 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:09.913
    Aug 29 20:06:09.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 20:06:09.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:09.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:09.94
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 08/29/23 20:06:09.946
    STEP: Creating a ResourceQuota 08/29/23 20:06:14.952
    STEP: Ensuring resource quota status is calculated 08/29/23 20:06:14.958
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:16.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9860" for this suite. 08/29/23 20:06:16.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:16.98
Aug 29 20:06:16.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename daemonsets 08/29/23 20:06:16.981
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:17.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:17.006
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873
STEP: Creating simple DaemonSet "daemon-set" 08/29/23 20:06:17.043
STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 20:06:17.05
Aug 29 20:06:17.059: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:06:17.059: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:06:18.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:06:18.070: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:06:19.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Aug 29 20:06:19.068: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 20:06:20.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:06:20.070: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 20:06:21.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 20:06:21.069: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Getting /status 08/29/23 20:06:21.073
Aug 29 20:06:21.077: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/29/23 20:06:21.077
Aug 29 20:06:21.089: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/29/23 20:06:21.089
Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: ADDED
Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.092: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.092: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.092: INFO: Found daemon set daemon-set in namespace daemonsets-4578 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 20:06:21.092: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/29/23 20:06:21.092
STEP: watching for the daemon set status to be patched 08/29/23 20:06:21.103
Aug 29 20:06:21.105: INFO: Observed &DaemonSet event: ADDED
Aug 29 20:06:21.105: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.105: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.105: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.106: INFO: Observed daemon set daemon-set in namespace daemonsets-4578 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
Aug 29 20:06:21.106: INFO: Found daemon set daemon-set in namespace daemonsets-4578 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 29 20:06:21.106: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/29/23 20:06:21.113
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4578, will wait for the garbage collector to delete the pods 08/29/23 20:06:21.113
Aug 29 20:06:21.178: INFO: Deleting DaemonSet.extensions daemon-set took: 11.22653ms
Aug 29 20:06:21.279: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022048ms
Aug 29 20:06:23.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:06:23.285: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 20:06:23.289: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22421"},"items":null}

Aug 29 20:06:23.292: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22421"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:23.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-4578" for this suite. 08/29/23 20:06:23.318
------------------------------
• [SLOW TEST] [6.344 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:873

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:16.98
    Aug 29 20:06:16.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename daemonsets 08/29/23 20:06:16.981
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:17.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:17.006
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:873
    STEP: Creating simple DaemonSet "daemon-set" 08/29/23 20:06:17.043
    STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 20:06:17.05
    Aug 29 20:06:17.059: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:06:17.059: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:06:18.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:06:18.070: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:06:19.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Aug 29 20:06:19.068: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 20:06:20.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:06:20.070: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 20:06:21.068: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 20:06:21.069: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Getting /status 08/29/23 20:06:21.073
    Aug 29 20:06:21.077: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/29/23 20:06:21.077
    Aug 29 20:06:21.089: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/29/23 20:06:21.089
    Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: ADDED
    Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.091: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.092: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.092: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.092: INFO: Found daemon set daemon-set in namespace daemonsets-4578 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 29 20:06:21.092: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/29/23 20:06:21.092
    STEP: watching for the daemon set status to be patched 08/29/23 20:06:21.103
    Aug 29 20:06:21.105: INFO: Observed &DaemonSet event: ADDED
    Aug 29 20:06:21.105: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.105: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.105: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.106: INFO: Observed daemon set daemon-set in namespace daemonsets-4578 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 29 20:06:21.106: INFO: Observed &DaemonSet event: MODIFIED
    Aug 29 20:06:21.106: INFO: Found daemon set daemon-set in namespace daemonsets-4578 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 29 20:06:21.106: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/29/23 20:06:21.113
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4578, will wait for the garbage collector to delete the pods 08/29/23 20:06:21.113
    Aug 29 20:06:21.178: INFO: Deleting DaemonSet.extensions daemon-set took: 11.22653ms
    Aug 29 20:06:21.279: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.022048ms
    Aug 29 20:06:23.285: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:06:23.285: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 29 20:06:23.289: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22421"},"items":null}

    Aug 29 20:06:23.292: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22421"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:23.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-4578" for this suite. 08/29/23 20:06:23.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:23.325
Aug 29 20:06:23.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename dns 08/29/23 20:06:23.326
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:23.348
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:23.354
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/29/23 20:06:23.357
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7745.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7745.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/29/23 20:06:23.363
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7745.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7745.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/29/23 20:06:23.363
STEP: creating a pod to probe DNS 08/29/23 20:06:23.364
STEP: submitting the pod to kubernetes 08/29/23 20:06:23.364
Aug 29 20:06:23.377: INFO: Waiting up to 15m0s for pod "dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d" in namespace "dns-7745" to be "running"
Aug 29 20:06:23.382: INFO: Pod "dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096128ms
Aug 29 20:06:25.386: INFO: Pod "dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009138337s
Aug 29 20:06:25.386: INFO: Pod "dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d" satisfied condition "running"
STEP: retrieving the pod 08/29/23 20:06:25.386
STEP: looking for the results for each expected name from probers 08/29/23 20:06:25.39
Aug 29 20:06:25.410: INFO: DNS probes using dns-7745/dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d succeeded

STEP: deleting the pod 08/29/23 20:06:25.41
STEP: deleting the test headless service 08/29/23 20:06:25.425
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:25.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-7745" for this suite. 08/29/23 20:06:25.448
------------------------------
• [2.132 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:23.325
    Aug 29 20:06:23.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename dns 08/29/23 20:06:23.326
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:23.348
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:23.354
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/29/23 20:06:23.357
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7745.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7745.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/29/23 20:06:23.363
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7745.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7745.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/29/23 20:06:23.363
    STEP: creating a pod to probe DNS 08/29/23 20:06:23.364
    STEP: submitting the pod to kubernetes 08/29/23 20:06:23.364
    Aug 29 20:06:23.377: INFO: Waiting up to 15m0s for pod "dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d" in namespace "dns-7745" to be "running"
    Aug 29 20:06:23.382: INFO: Pod "dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.096128ms
    Aug 29 20:06:25.386: INFO: Pod "dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d": Phase="Running", Reason="", readiness=true. Elapsed: 2.009138337s
    Aug 29 20:06:25.386: INFO: Pod "dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 20:06:25.386
    STEP: looking for the results for each expected name from probers 08/29/23 20:06:25.39
    Aug 29 20:06:25.410: INFO: DNS probes using dns-7745/dns-test-d749a950-6dba-4c06-9bdd-12b84b82e71d succeeded

    STEP: deleting the pod 08/29/23 20:06:25.41
    STEP: deleting the test headless service 08/29/23 20:06:25.425
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:25.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-7745" for this suite. 08/29/23 20:06:25.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:25.458
Aug 29 20:06:25.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:06:25.459
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:25.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:25.48
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:25.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3764" for this suite. 08/29/23 20:06:25.535
------------------------------
• [0.084 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:25.458
    Aug 29 20:06:25.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:06:25.459
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:25.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:25.48
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:25.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3764" for this suite. 08/29/23 20:06:25.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:25.543
Aug 29 20:06:25.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:06:25.544
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:25.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:25.564
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:06:25.568
Aug 29 20:06:25.580: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24" in namespace "projected-5664" to be "Succeeded or Failed"
Aug 29 20:06:25.587: INFO: Pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24": Phase="Pending", Reason="", readiness=false. Elapsed: 7.079669ms
Aug 29 20:06:27.591: INFO: Pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011313052s
Aug 29 20:06:29.593: INFO: Pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013352282s
STEP: Saw pod success 08/29/23 20:06:29.593
Aug 29 20:06:29.593: INFO: Pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24" satisfied condition "Succeeded or Failed"
Aug 29 20:06:29.597: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24 container client-container: <nil>
STEP: delete the pod 08/29/23 20:06:29.606
Aug 29 20:06:29.626: INFO: Waiting for pod downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24 to disappear
Aug 29 20:06:29.628: INFO: Pod downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:29.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5664" for this suite. 08/29/23 20:06:29.634
------------------------------
• [4.099 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:25.543
    Aug 29 20:06:25.543: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:06:25.544
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:25.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:25.564
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:06:25.568
    Aug 29 20:06:25.580: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24" in namespace "projected-5664" to be "Succeeded or Failed"
    Aug 29 20:06:25.587: INFO: Pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24": Phase="Pending", Reason="", readiness=false. Elapsed: 7.079669ms
    Aug 29 20:06:27.591: INFO: Pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011313052s
    Aug 29 20:06:29.593: INFO: Pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013352282s
    STEP: Saw pod success 08/29/23 20:06:29.593
    Aug 29 20:06:29.593: INFO: Pod "downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24" satisfied condition "Succeeded or Failed"
    Aug 29 20:06:29.597: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24 container client-container: <nil>
    STEP: delete the pod 08/29/23 20:06:29.606
    Aug 29 20:06:29.626: INFO: Waiting for pod downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24 to disappear
    Aug 29 20:06:29.628: INFO: Pod downwardapi-volume-9142a1fe-ebbf-4685-8d4c-cd0eda64dd24 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:29.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5664" for this suite. 08/29/23 20:06:29.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:29.643
Aug 29 20:06:29.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename security-context 08/29/23 20:06:29.644
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:29.661
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:29.664
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/29/23 20:06:29.667
Aug 29 20:06:29.678: INFO: Waiting up to 5m0s for pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33" in namespace "security-context-1670" to be "Succeeded or Failed"
Aug 29 20:06:29.681: INFO: Pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.014758ms
Aug 29 20:06:31.687: INFO: Pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008778568s
Aug 29 20:06:33.685: INFO: Pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006939213s
STEP: Saw pod success 08/29/23 20:06:33.685
Aug 29 20:06:33.685: INFO: Pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33" satisfied condition "Succeeded or Failed"
Aug 29 20:06:33.689: INFO: Trying to get logs from node loki-15bd39-worker-1 pod security-context-12378a4d-c7aa-4681-a841-bc177a429d33 container test-container: <nil>
STEP: delete the pod 08/29/23 20:06:33.696
Aug 29 20:06:33.712: INFO: Waiting for pod security-context-12378a4d-c7aa-4681-a841-bc177a429d33 to disappear
Aug 29 20:06:33.716: INFO: Pod security-context-12378a4d-c7aa-4681-a841-bc177a429d33 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:33.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-1670" for this suite. 08/29/23 20:06:33.721
------------------------------
• [4.085 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:29.643
    Aug 29 20:06:29.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename security-context 08/29/23 20:06:29.644
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:29.661
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:29.664
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/29/23 20:06:29.667
    Aug 29 20:06:29.678: INFO: Waiting up to 5m0s for pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33" in namespace "security-context-1670" to be "Succeeded or Failed"
    Aug 29 20:06:29.681: INFO: Pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33": Phase="Pending", Reason="", readiness=false. Elapsed: 3.014758ms
    Aug 29 20:06:31.687: INFO: Pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008778568s
    Aug 29 20:06:33.685: INFO: Pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006939213s
    STEP: Saw pod success 08/29/23 20:06:33.685
    Aug 29 20:06:33.685: INFO: Pod "security-context-12378a4d-c7aa-4681-a841-bc177a429d33" satisfied condition "Succeeded or Failed"
    Aug 29 20:06:33.689: INFO: Trying to get logs from node loki-15bd39-worker-1 pod security-context-12378a4d-c7aa-4681-a841-bc177a429d33 container test-container: <nil>
    STEP: delete the pod 08/29/23 20:06:33.696
    Aug 29 20:06:33.712: INFO: Waiting for pod security-context-12378a4d-c7aa-4681-a841-bc177a429d33 to disappear
    Aug 29 20:06:33.716: INFO: Pod security-context-12378a4d-c7aa-4681-a841-bc177a429d33 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:33.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-1670" for this suite. 08/29/23 20:06:33.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:33.729
Aug 29 20:06:33.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename cronjob 08/29/23 20:06:33.73
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:33.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:33.752
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/29/23 20:06:33.755
STEP: creating 08/29/23 20:06:33.755
STEP: getting 08/29/23 20:06:33.764
STEP: listing 08/29/23 20:06:33.768
STEP: watching 08/29/23 20:06:33.772
Aug 29 20:06:33.772: INFO: starting watch
STEP: cluster-wide listing 08/29/23 20:06:33.773
STEP: cluster-wide watching 08/29/23 20:06:33.776
Aug 29 20:06:33.776: INFO: starting watch
STEP: patching 08/29/23 20:06:33.777
STEP: updating 08/29/23 20:06:33.786
Aug 29 20:06:33.795: INFO: waiting for watch events with expected annotations
Aug 29 20:06:33.795: INFO: saw patched and updated annotations
STEP: patching /status 08/29/23 20:06:33.795
STEP: updating /status 08/29/23 20:06:33.804
STEP: get /status 08/29/23 20:06:33.812
STEP: deleting 08/29/23 20:06:33.815
STEP: deleting a collection 08/29/23 20:06:33.834
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:33.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8027" for this suite. 08/29/23 20:06:33.85
------------------------------
• [0.128 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:33.729
    Aug 29 20:06:33.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename cronjob 08/29/23 20:06:33.73
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:33.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:33.752
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/29/23 20:06:33.755
    STEP: creating 08/29/23 20:06:33.755
    STEP: getting 08/29/23 20:06:33.764
    STEP: listing 08/29/23 20:06:33.768
    STEP: watching 08/29/23 20:06:33.772
    Aug 29 20:06:33.772: INFO: starting watch
    STEP: cluster-wide listing 08/29/23 20:06:33.773
    STEP: cluster-wide watching 08/29/23 20:06:33.776
    Aug 29 20:06:33.776: INFO: starting watch
    STEP: patching 08/29/23 20:06:33.777
    STEP: updating 08/29/23 20:06:33.786
    Aug 29 20:06:33.795: INFO: waiting for watch events with expected annotations
    Aug 29 20:06:33.795: INFO: saw patched and updated annotations
    STEP: patching /status 08/29/23 20:06:33.795
    STEP: updating /status 08/29/23 20:06:33.804
    STEP: get /status 08/29/23 20:06:33.812
    STEP: deleting 08/29/23 20:06:33.815
    STEP: deleting a collection 08/29/23 20:06:33.834
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:33.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8027" for this suite. 08/29/23 20:06:33.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:33.858
Aug 29 20:06:33.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:06:33.859
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:33.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:33.882
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/29/23 20:06:33.885
Aug 29 20:06:33.896: INFO: Waiting up to 5m0s for pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0" in namespace "emptydir-7260" to be "Succeeded or Failed"
Aug 29 20:06:33.900: INFO: Pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114243ms
Aug 29 20:06:35.905: INFO: Pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008981614s
Aug 29 20:06:37.906: INFO: Pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010055106s
STEP: Saw pod success 08/29/23 20:06:37.906
Aug 29 20:06:37.906: INFO: Pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0" satisfied condition "Succeeded or Failed"
Aug 29 20:06:37.910: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0 container test-container: <nil>
STEP: delete the pod 08/29/23 20:06:37.918
Aug 29 20:06:37.931: INFO: Waiting for pod pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0 to disappear
Aug 29 20:06:37.934: INFO: Pod pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:37.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7260" for this suite. 08/29/23 20:06:37.939
------------------------------
• [4.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:33.858
    Aug 29 20:06:33.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:06:33.859
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:33.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:33.882
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/29/23 20:06:33.885
    Aug 29 20:06:33.896: INFO: Waiting up to 5m0s for pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0" in namespace "emptydir-7260" to be "Succeeded or Failed"
    Aug 29 20:06:33.900: INFO: Pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.114243ms
    Aug 29 20:06:35.905: INFO: Pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008981614s
    Aug 29 20:06:37.906: INFO: Pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010055106s
    STEP: Saw pod success 08/29/23 20:06:37.906
    Aug 29 20:06:37.906: INFO: Pod "pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0" satisfied condition "Succeeded or Failed"
    Aug 29 20:06:37.910: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0 container test-container: <nil>
    STEP: delete the pod 08/29/23 20:06:37.918
    Aug 29 20:06:37.931: INFO: Waiting for pod pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0 to disappear
    Aug 29 20:06:37.934: INFO: Pod pod-dfd864b3-ef66-471f-9491-5a0e849e5bf0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:37.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7260" for this suite. 08/29/23 20:06:37.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:37.946
Aug 29 20:06:37.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename daemonsets 08/29/23 20:06:37.947
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:37.962
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:37.965
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305
STEP: Creating a simple DaemonSet "daemon-set" 08/29/23 20:06:37.997
STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 20:06:38.007
Aug 29 20:06:38.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:06:38.017: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:06:39.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:06:39.029: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:06:40.029: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:06:40.029: INFO: Node loki-15bd39-worker-0 is running 0 daemon pod, expected 1
Aug 29 20:06:41.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 20:06:41.028: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/29/23 20:06:41.031
Aug 29 20:06:41.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 20:06:41.057: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/29/23 20:06:41.057
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/29/23 20:06:42.074
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2485, will wait for the garbage collector to delete the pods 08/29/23 20:06:42.074
Aug 29 20:06:42.136: INFO: Deleting DaemonSet.extensions daemon-set took: 8.776066ms
Aug 29 20:06:42.236: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.62461ms
Aug 29 20:06:45.041: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:06:45.042: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 20:06:45.045: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22831"},"items":null}

Aug 29 20:06:45.049: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22831"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:45.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2485" for this suite. 08/29/23 20:06:45.073
------------------------------
• [SLOW TEST] [7.135 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:305

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:37.946
    Aug 29 20:06:37.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename daemonsets 08/29/23 20:06:37.947
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:37.962
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:37.965
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:305
    STEP: Creating a simple DaemonSet "daemon-set" 08/29/23 20:06:37.997
    STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 20:06:38.007
    Aug 29 20:06:38.017: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:06:38.017: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:06:39.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:06:39.029: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:06:40.029: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:06:40.029: INFO: Node loki-15bd39-worker-0 is running 0 daemon pod, expected 1
    Aug 29 20:06:41.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 20:06:41.028: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/29/23 20:06:41.031
    Aug 29 20:06:41.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 20:06:41.057: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/29/23 20:06:41.057
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/29/23 20:06:42.074
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2485, will wait for the garbage collector to delete the pods 08/29/23 20:06:42.074
    Aug 29 20:06:42.136: INFO: Deleting DaemonSet.extensions daemon-set took: 8.776066ms
    Aug 29 20:06:42.236: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.62461ms
    Aug 29 20:06:45.041: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:06:45.042: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 29 20:06:45.045: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22831"},"items":null}

    Aug 29 20:06:45.049: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22831"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:45.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2485" for this suite. 08/29/23 20:06:45.073
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:45.082
Aug 29 20:06:45.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:06:45.083
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:45.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:45.103
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Aug 29 20:06:45.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: creating the pod 08/29/23 20:06:45.106
STEP: submitting the pod to kubernetes 08/29/23 20:06:45.106
Aug 29 20:06:45.116: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3" in namespace "pods-6179" to be "running and ready"
Aug 29 20:06:45.121: INFO: Pod "pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01706ms
Aug 29 20:06:45.122: INFO: The phase of Pod pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:06:47.126: INFO: Pod "pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00992245s
Aug 29 20:06:47.126: INFO: The phase of Pod pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3 is Running (Ready = true)
Aug 29 20:06:47.126: INFO: Pod "pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6179" for this suite. 08/29/23 20:06:47.165
------------------------------
• [2.091 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:45.082
    Aug 29 20:06:45.082: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:06:45.083
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:45.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:45.103
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Aug 29 20:06:45.105: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: creating the pod 08/29/23 20:06:45.106
    STEP: submitting the pod to kubernetes 08/29/23 20:06:45.106
    Aug 29 20:06:45.116: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3" in namespace "pods-6179" to be "running and ready"
    Aug 29 20:06:45.121: INFO: Pod "pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.01706ms
    Aug 29 20:06:45.122: INFO: The phase of Pod pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:06:47.126: INFO: Pod "pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3": Phase="Running", Reason="", readiness=true. Elapsed: 2.00992245s
    Aug 29 20:06:47.126: INFO: The phase of Pod pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3 is Running (Ready = true)
    Aug 29 20:06:47.126: INFO: Pod "pod-logs-websocket-e90d1f38-b40a-46cc-b661-fa27fd44dfd3" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:47.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6179" for this suite. 08/29/23 20:06:47.165
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:47.173
Aug 29 20:06:47.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:06:47.175
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:47.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:47.201
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-1f03efe7-16f7-43bb-ad77-bf6fcad7c2fa 08/29/23 20:06:47.206
STEP: Creating a pod to test consume configMaps 08/29/23 20:06:47.211
Aug 29 20:06:47.220: INFO: Waiting up to 5m0s for pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b" in namespace "configmap-7556" to be "Succeeded or Failed"
Aug 29 20:06:47.223: INFO: Pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.345208ms
Aug 29 20:06:49.229: INFO: Pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008997687s
Aug 29 20:06:51.229: INFO: Pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00894803s
STEP: Saw pod success 08/29/23 20:06:51.229
Aug 29 20:06:51.229: INFO: Pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b" satisfied condition "Succeeded or Failed"
Aug 29 20:06:51.233: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:06:51.24
Aug 29 20:06:51.257: INFO: Waiting for pod pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b to disappear
Aug 29 20:06:51.260: INFO: Pod pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:51.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7556" for this suite. 08/29/23 20:06:51.265
------------------------------
• [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:47.173
    Aug 29 20:06:47.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:06:47.175
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:47.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:47.201
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-1f03efe7-16f7-43bb-ad77-bf6fcad7c2fa 08/29/23 20:06:47.206
    STEP: Creating a pod to test consume configMaps 08/29/23 20:06:47.211
    Aug 29 20:06:47.220: INFO: Waiting up to 5m0s for pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b" in namespace "configmap-7556" to be "Succeeded or Failed"
    Aug 29 20:06:47.223: INFO: Pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.345208ms
    Aug 29 20:06:49.229: INFO: Pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008997687s
    Aug 29 20:06:51.229: INFO: Pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00894803s
    STEP: Saw pod success 08/29/23 20:06:51.229
    Aug 29 20:06:51.229: INFO: Pod "pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b" satisfied condition "Succeeded or Failed"
    Aug 29 20:06:51.233: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:06:51.24
    Aug 29 20:06:51.257: INFO: Waiting for pod pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b to disappear
    Aug 29 20:06:51.260: INFO: Pod pod-configmaps-2add9081-eb98-479e-9735-8686fc68c94b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:51.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7556" for this suite. 08/29/23 20:06:51.265
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:51.272
Aug 29 20:06:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replicaset 08/29/23 20:06:51.273
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:51.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:51.292
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/29/23 20:06:51.295
STEP: Verify that the required pods have come up 08/29/23 20:06:51.3
Aug 29 20:06:51.302: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 29 20:06:56.308: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/29/23 20:06:56.308
Aug 29 20:06:56.311: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/29/23 20:06:56.311
STEP: DeleteCollection of the ReplicaSets 08/29/23 20:06:56.315
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/29/23 20:06:56.326
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:56.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2742" for this suite. 08/29/23 20:06:56.352
------------------------------
• [SLOW TEST] [5.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:51.272
    Aug 29 20:06:51.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replicaset 08/29/23 20:06:51.273
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:51.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:51.292
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/29/23 20:06:51.295
    STEP: Verify that the required pods have come up 08/29/23 20:06:51.3
    Aug 29 20:06:51.302: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 29 20:06:56.308: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/29/23 20:06:56.308
    Aug 29 20:06:56.311: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/29/23 20:06:56.311
    STEP: DeleteCollection of the ReplicaSets 08/29/23 20:06:56.315
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/29/23 20:06:56.326
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:56.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2742" for this suite. 08/29/23 20:06:56.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:56.373
Aug 29 20:06:56.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename events 08/29/23 20:06:56.374
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:56.393
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:56.396
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/29/23 20:06:56.399
Aug 29 20:06:56.406: INFO: created test-event-1
Aug 29 20:06:56.411: INFO: created test-event-2
Aug 29 20:06:56.416: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/29/23 20:06:56.416
STEP: delete collection of events 08/29/23 20:06:56.42
Aug 29 20:06:56.420: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/29/23 20:06:56.445
Aug 29 20:06:56.445: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Aug 29 20:06:56.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-8485" for this suite. 08/29/23 20:06:56.454
------------------------------
• [0.089 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:56.373
    Aug 29 20:06:56.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename events 08/29/23 20:06:56.374
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:56.393
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:56.396
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/29/23 20:06:56.399
    Aug 29 20:06:56.406: INFO: created test-event-1
    Aug 29 20:06:56.411: INFO: created test-event-2
    Aug 29 20:06:56.416: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/29/23 20:06:56.416
    STEP: delete collection of events 08/29/23 20:06:56.42
    Aug 29 20:06:56.420: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/29/23 20:06:56.445
    Aug 29 20:06:56.445: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:06:56.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-8485" for this suite. 08/29/23 20:06:56.454
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:06:56.463
Aug 29 20:06:56.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-webhook 08/29/23 20:06:56.464
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:56.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:56.483
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/29/23 20:06:56.487
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/29/23 20:06:56.864
STEP: Deploying the custom resource conversion webhook pod 08/29/23 20:06:56.875
STEP: Wait for the deployment to be ready 08/29/23 20:06:56.901
Aug 29 20:06:56.915: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 20:06:58.926
STEP: Verifying the service has paired with the endpoint 08/29/23 20:06:58.939
Aug 29 20:06:59.940: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 29 20:06:59.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Creating a v1 custom resource 08/29/23 20:07:02.555
STEP: Create a v2 custom resource 08/29/23 20:07:02.579
STEP: List CRs in v1 08/29/23 20:07:02.64
STEP: List CRs in v2 08/29/23 20:07:02.645
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:03.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-4886" for this suite. 08/29/23 20:07:03.222
------------------------------
• [SLOW TEST] [6.769 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:06:56.463
    Aug 29 20:06:56.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-webhook 08/29/23 20:06:56.464
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:06:56.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:06:56.483
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/29/23 20:06:56.487
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/29/23 20:06:56.864
    STEP: Deploying the custom resource conversion webhook pod 08/29/23 20:06:56.875
    STEP: Wait for the deployment to be ready 08/29/23 20:06:56.901
    Aug 29 20:06:56.915: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 20:06:58.926
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:06:58.939
    Aug 29 20:06:59.940: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 29 20:06:59.949: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Creating a v1 custom resource 08/29/23 20:07:02.555
    STEP: Create a v2 custom resource 08/29/23 20:07:02.579
    STEP: List CRs in v1 08/29/23 20:07:02.64
    STEP: List CRs in v2 08/29/23 20:07:02.645
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:03.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-4886" for this suite. 08/29/23 20:07:03.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:03.233
Aug 29 20:07:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename job 08/29/23 20:07:03.235
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:03.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:03.258
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 08/29/23 20:07:03.261
STEP: Ensuring active pods == parallelism 08/29/23 20:07:03.268
STEP: Orphaning one of the Job's Pods 08/29/23 20:07:05.275
Aug 29 20:07:05.796: INFO: Successfully updated pod "adopt-release-8nclb"
STEP: Checking that the Job readopts the Pod 08/29/23 20:07:05.796
Aug 29 20:07:05.796: INFO: Waiting up to 15m0s for pod "adopt-release-8nclb" in namespace "job-9397" to be "adopted"
Aug 29 20:07:05.802: INFO: Pod "adopt-release-8nclb": Phase="Running", Reason="", readiness=true. Elapsed: 6.241029ms
Aug 29 20:07:07.808: INFO: Pod "adopt-release-8nclb": Phase="Running", Reason="", readiness=true. Elapsed: 2.012402037s
Aug 29 20:07:07.808: INFO: Pod "adopt-release-8nclb" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/29/23 20:07:07.808
Aug 29 20:07:08.323: INFO: Successfully updated pod "adopt-release-8nclb"
STEP: Checking that the Job releases the Pod 08/29/23 20:07:08.323
Aug 29 20:07:08.323: INFO: Waiting up to 15m0s for pod "adopt-release-8nclb" in namespace "job-9397" to be "released"
Aug 29 20:07:08.327: INFO: Pod "adopt-release-8nclb": Phase="Running", Reason="", readiness=true. Elapsed: 3.38544ms
Aug 29 20:07:10.332: INFO: Pod "adopt-release-8nclb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008725197s
Aug 29 20:07:10.332: INFO: Pod "adopt-release-8nclb" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:10.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9397" for this suite. 08/29/23 20:07:10.341
------------------------------
• [SLOW TEST] [7.114 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:03.233
    Aug 29 20:07:03.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename job 08/29/23 20:07:03.235
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:03.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:03.258
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 08/29/23 20:07:03.261
    STEP: Ensuring active pods == parallelism 08/29/23 20:07:03.268
    STEP: Orphaning one of the Job's Pods 08/29/23 20:07:05.275
    Aug 29 20:07:05.796: INFO: Successfully updated pod "adopt-release-8nclb"
    STEP: Checking that the Job readopts the Pod 08/29/23 20:07:05.796
    Aug 29 20:07:05.796: INFO: Waiting up to 15m0s for pod "adopt-release-8nclb" in namespace "job-9397" to be "adopted"
    Aug 29 20:07:05.802: INFO: Pod "adopt-release-8nclb": Phase="Running", Reason="", readiness=true. Elapsed: 6.241029ms
    Aug 29 20:07:07.808: INFO: Pod "adopt-release-8nclb": Phase="Running", Reason="", readiness=true. Elapsed: 2.012402037s
    Aug 29 20:07:07.808: INFO: Pod "adopt-release-8nclb" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/29/23 20:07:07.808
    Aug 29 20:07:08.323: INFO: Successfully updated pod "adopt-release-8nclb"
    STEP: Checking that the Job releases the Pod 08/29/23 20:07:08.323
    Aug 29 20:07:08.323: INFO: Waiting up to 15m0s for pod "adopt-release-8nclb" in namespace "job-9397" to be "released"
    Aug 29 20:07:08.327: INFO: Pod "adopt-release-8nclb": Phase="Running", Reason="", readiness=true. Elapsed: 3.38544ms
    Aug 29 20:07:10.332: INFO: Pod "adopt-release-8nclb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008725197s
    Aug 29 20:07:10.332: INFO: Pod "adopt-release-8nclb" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:10.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9397" for this suite. 08/29/23 20:07:10.341
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:10.347
Aug 29 20:07:10.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:07:10.348
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:10.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:10.382
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 08/29/23 20:07:10.385
Aug 29 20:07:10.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: rename a version 08/29/23 20:07:16.135
STEP: check the new version name is served 08/29/23 20:07:16.152
STEP: check the old version name is removed 08/29/23 20:07:18.321
STEP: check the other version is not changed 08/29/23 20:07:19.117
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:23.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7684" for this suite. 08/29/23 20:07:23.467
------------------------------
• [SLOW TEST] [13.129 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:10.347
    Aug 29 20:07:10.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:07:10.348
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:10.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:10.382
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 08/29/23 20:07:10.385
    Aug 29 20:07:10.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: rename a version 08/29/23 20:07:16.135
    STEP: check the new version name is served 08/29/23 20:07:16.152
    STEP: check the old version name is removed 08/29/23 20:07:18.321
    STEP: check the other version is not changed 08/29/23 20:07:19.117
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:23.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7684" for this suite. 08/29/23 20:07:23.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:23.477
Aug 29 20:07:23.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename namespaces 08/29/23 20:07:23.479
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:23.503
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:23.506
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 08/29/23 20:07:23.509
STEP: patching the Namespace 08/29/23 20:07:23.528
STEP: get the Namespace and ensuring it has the label 08/29/23 20:07:23.539
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:23.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5956" for this suite. 08/29/23 20:07:23.548
STEP: Destroying namespace "nspatchtest-ae35cfea-7f0f-486e-b0f7-8450d460d22e-9259" for this suite. 08/29/23 20:07:23.557
------------------------------
• [0.088 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:23.477
    Aug 29 20:07:23.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename namespaces 08/29/23 20:07:23.479
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:23.503
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:23.506
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 08/29/23 20:07:23.509
    STEP: patching the Namespace 08/29/23 20:07:23.528
    STEP: get the Namespace and ensuring it has the label 08/29/23 20:07:23.539
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:23.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5956" for this suite. 08/29/23 20:07:23.548
    STEP: Destroying namespace "nspatchtest-ae35cfea-7f0f-486e-b0f7-8450d460d22e-9259" for this suite. 08/29/23 20:07:23.557
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:23.566
Aug 29 20:07:23.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:07:23.567
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:23.586
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:23.589
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/29/23 20:07:23.592
Aug 29 20:07:23.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:07:26.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:34.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5063" for this suite. 08/29/23 20:07:34.494
------------------------------
• [SLOW TEST] [10.938 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:23.566
    Aug 29 20:07:23.566: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:07:23.567
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:23.586
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:23.589
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/29/23 20:07:23.592
    Aug 29 20:07:23.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:07:26.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:34.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5063" for this suite. 08/29/23 20:07:34.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:34.506
Aug 29 20:07:34.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:07:34.507
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:34.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:34.53
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-44465ae9-72d3-42e4-9025-eac933515608 08/29/23 20:07:34.533
STEP: Creating a pod to test consume configMaps 08/29/23 20:07:34.54
Aug 29 20:07:34.551: INFO: Waiting up to 5m0s for pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e" in namespace "configmap-7237" to be "Succeeded or Failed"
Aug 29 20:07:34.557: INFO: Pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053576ms
Aug 29 20:07:36.561: INFO: Pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010288464s
Aug 29 20:07:38.563: INFO: Pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012012048s
STEP: Saw pod success 08/29/23 20:07:38.563
Aug 29 20:07:38.563: INFO: Pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e" satisfied condition "Succeeded or Failed"
Aug 29 20:07:38.567: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:07:38.575
Aug 29 20:07:38.589: INFO: Waiting for pod pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e to disappear
Aug 29 20:07:38.592: INFO: Pod pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:38.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7237" for this suite. 08/29/23 20:07:38.598
------------------------------
• [4.099 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:34.506
    Aug 29 20:07:34.506: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:07:34.507
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:34.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:34.53
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-44465ae9-72d3-42e4-9025-eac933515608 08/29/23 20:07:34.533
    STEP: Creating a pod to test consume configMaps 08/29/23 20:07:34.54
    Aug 29 20:07:34.551: INFO: Waiting up to 5m0s for pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e" in namespace "configmap-7237" to be "Succeeded or Failed"
    Aug 29 20:07:34.557: INFO: Pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053576ms
    Aug 29 20:07:36.561: INFO: Pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010288464s
    Aug 29 20:07:38.563: INFO: Pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012012048s
    STEP: Saw pod success 08/29/23 20:07:38.563
    Aug 29 20:07:38.563: INFO: Pod "pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e" satisfied condition "Succeeded or Failed"
    Aug 29 20:07:38.567: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:07:38.575
    Aug 29 20:07:38.589: INFO: Waiting for pod pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e to disappear
    Aug 29 20:07:38.592: INFO: Pod pod-configmaps-d59b8522-a4fd-467e-8a5c-4a39b7670d8e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:38.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7237" for this suite. 08/29/23 20:07:38.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:38.605
Aug 29 20:07:38.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:07:38.607
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:38.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:38.626
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-14b28321-0d36-4a55-b645-1e7c597cf3f9 08/29/23 20:07:38.629
STEP: Creating a pod to test consume configMaps 08/29/23 20:07:38.635
Aug 29 20:07:38.646: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4" in namespace "projected-2514" to be "Succeeded or Failed"
Aug 29 20:07:38.649: INFO: Pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.389199ms
Aug 29 20:07:40.655: INFO: Pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009021503s
Aug 29 20:07:42.654: INFO: Pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008085784s
STEP: Saw pod success 08/29/23 20:07:42.654
Aug 29 20:07:42.654: INFO: Pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4" satisfied condition "Succeeded or Failed"
Aug 29 20:07:42.658: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4 container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:07:42.669
Aug 29 20:07:42.682: INFO: Waiting for pod pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4 to disappear
Aug 29 20:07:42.686: INFO: Pod pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2514" for this suite. 08/29/23 20:07:42.694
------------------------------
• [4.095 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:38.605
    Aug 29 20:07:38.606: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:07:38.607
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:38.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:38.626
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-14b28321-0d36-4a55-b645-1e7c597cf3f9 08/29/23 20:07:38.629
    STEP: Creating a pod to test consume configMaps 08/29/23 20:07:38.635
    Aug 29 20:07:38.646: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4" in namespace "projected-2514" to be "Succeeded or Failed"
    Aug 29 20:07:38.649: INFO: Pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.389199ms
    Aug 29 20:07:40.655: INFO: Pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009021503s
    Aug 29 20:07:42.654: INFO: Pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008085784s
    STEP: Saw pod success 08/29/23 20:07:42.654
    Aug 29 20:07:42.654: INFO: Pod "pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4" satisfied condition "Succeeded or Failed"
    Aug 29 20:07:42.658: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4 container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:07:42.669
    Aug 29 20:07:42.682: INFO: Waiting for pod pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4 to disappear
    Aug 29 20:07:42.686: INFO: Pod pod-projected-configmaps-08d9568b-61b8-4e19-8013-65202bf414b4 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:42.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2514" for this suite. 08/29/23 20:07:42.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:42.704
Aug 29 20:07:42.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:07:42.705
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:42.724
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:42.727
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Aug 29 20:07:42.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/29/23 20:07:44.885
Aug 29 20:07:44.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 --namespace=crd-publish-openapi-6971 create -f -'
Aug 29 20:07:45.905: INFO: stderr: ""
Aug 29 20:07:45.905: INFO: stdout: "e2e-test-crd-publish-openapi-6259-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 29 20:07:45.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 --namespace=crd-publish-openapi-6971 delete e2e-test-crd-publish-openapi-6259-crds test-cr'
Aug 29 20:07:46.004: INFO: stderr: ""
Aug 29 20:07:46.004: INFO: stdout: "e2e-test-crd-publish-openapi-6259-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 29 20:07:46.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 --namespace=crd-publish-openapi-6971 apply -f -'
Aug 29 20:07:46.860: INFO: stderr: ""
Aug 29 20:07:46.860: INFO: stdout: "e2e-test-crd-publish-openapi-6259-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 29 20:07:46.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 --namespace=crd-publish-openapi-6971 delete e2e-test-crd-publish-openapi-6259-crds test-cr'
Aug 29 20:07:46.951: INFO: stderr: ""
Aug 29 20:07:46.951: INFO: stdout: "e2e-test-crd-publish-openapi-6259-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/29/23 20:07:46.951
Aug 29 20:07:46.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 explain e2e-test-crd-publish-openapi-6259-crds'
Aug 29 20:07:47.246: INFO: stderr: ""
Aug 29 20:07:47.246: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6259-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:49.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6971" for this suite. 08/29/23 20:07:49.391
------------------------------
• [SLOW TEST] [6.695 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:42.704
    Aug 29 20:07:42.704: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:07:42.705
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:42.724
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:42.727
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Aug 29 20:07:42.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/29/23 20:07:44.885
    Aug 29 20:07:44.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 --namespace=crd-publish-openapi-6971 create -f -'
    Aug 29 20:07:45.905: INFO: stderr: ""
    Aug 29 20:07:45.905: INFO: stdout: "e2e-test-crd-publish-openapi-6259-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 29 20:07:45.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 --namespace=crd-publish-openapi-6971 delete e2e-test-crd-publish-openapi-6259-crds test-cr'
    Aug 29 20:07:46.004: INFO: stderr: ""
    Aug 29 20:07:46.004: INFO: stdout: "e2e-test-crd-publish-openapi-6259-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 29 20:07:46.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 --namespace=crd-publish-openapi-6971 apply -f -'
    Aug 29 20:07:46.860: INFO: stderr: ""
    Aug 29 20:07:46.860: INFO: stdout: "e2e-test-crd-publish-openapi-6259-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 29 20:07:46.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 --namespace=crd-publish-openapi-6971 delete e2e-test-crd-publish-openapi-6259-crds test-cr'
    Aug 29 20:07:46.951: INFO: stderr: ""
    Aug 29 20:07:46.951: INFO: stdout: "e2e-test-crd-publish-openapi-6259-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/29/23 20:07:46.951
    Aug 29 20:07:46.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-6971 explain e2e-test-crd-publish-openapi-6259-crds'
    Aug 29 20:07:47.246: INFO: stderr: ""
    Aug 29 20:07:47.246: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6259-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:49.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6971" for this suite. 08/29/23 20:07:49.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:49.4
Aug 29 20:07:49.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:07:49.401
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:49.425
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:49.428
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 08/29/23 20:07:49.431
STEP: fetching the ConfigMap 08/29/23 20:07:49.436
STEP: patching the ConfigMap 08/29/23 20:07:49.44
STEP: listing all ConfigMaps in all namespaces with a label selector 08/29/23 20:07:49.447
STEP: deleting the ConfigMap by collection with a label selector 08/29/23 20:07:49.452
STEP: listing all ConfigMaps in test namespace 08/29/23 20:07:49.465
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:49.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6759" for this suite. 08/29/23 20:07:49.473
------------------------------
• [0.080 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:49.4
    Aug 29 20:07:49.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:07:49.401
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:49.425
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:49.428
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 08/29/23 20:07:49.431
    STEP: fetching the ConfigMap 08/29/23 20:07:49.436
    STEP: patching the ConfigMap 08/29/23 20:07:49.44
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/29/23 20:07:49.447
    STEP: deleting the ConfigMap by collection with a label selector 08/29/23 20:07:49.452
    STEP: listing all ConfigMaps in test namespace 08/29/23 20:07:49.465
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:49.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6759" for this suite. 08/29/23 20:07:49.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:49.482
Aug 29 20:07:49.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename csistoragecapacity 08/29/23 20:07:49.483
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:49.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:49.509
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/29/23 20:07:49.512
STEP: getting /apis/storage.k8s.io 08/29/23 20:07:49.515
STEP: getting /apis/storage.k8s.io/v1 08/29/23 20:07:49.517
STEP: creating 08/29/23 20:07:49.518
STEP: watching 08/29/23 20:07:49.537
Aug 29 20:07:49.537: INFO: starting watch
STEP: getting 08/29/23 20:07:49.544
STEP: listing in namespace 08/29/23 20:07:49.547
STEP: listing across namespaces 08/29/23 20:07:49.55
STEP: patching 08/29/23 20:07:49.554
STEP: updating 08/29/23 20:07:49.559
Aug 29 20:07:49.566: INFO: waiting for watch events with expected annotations in namespace
Aug 29 20:07:49.566: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/29/23 20:07:49.566
STEP: deleting a collection 08/29/23 20:07:49.578
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:49.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-6663" for this suite. 08/29/23 20:07:49.598
------------------------------
• [0.122 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:49.482
    Aug 29 20:07:49.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename csistoragecapacity 08/29/23 20:07:49.483
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:49.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:49.509
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/29/23 20:07:49.512
    STEP: getting /apis/storage.k8s.io 08/29/23 20:07:49.515
    STEP: getting /apis/storage.k8s.io/v1 08/29/23 20:07:49.517
    STEP: creating 08/29/23 20:07:49.518
    STEP: watching 08/29/23 20:07:49.537
    Aug 29 20:07:49.537: INFO: starting watch
    STEP: getting 08/29/23 20:07:49.544
    STEP: listing in namespace 08/29/23 20:07:49.547
    STEP: listing across namespaces 08/29/23 20:07:49.55
    STEP: patching 08/29/23 20:07:49.554
    STEP: updating 08/29/23 20:07:49.559
    Aug 29 20:07:49.566: INFO: waiting for watch events with expected annotations in namespace
    Aug 29 20:07:49.566: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/29/23 20:07:49.566
    STEP: deleting a collection 08/29/23 20:07:49.578
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:49.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-6663" for this suite. 08/29/23 20:07:49.598
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:49.606
Aug 29 20:07:49.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-lifecycle-hook 08/29/23 20:07:49.607
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:49.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:49.634
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/29/23 20:07:49.642
Aug 29 20:07:49.656: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9529" to be "running and ready"
Aug 29 20:07:49.664: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.276435ms
Aug 29 20:07:49.664: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:07:51.669: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013595005s
Aug 29 20:07:51.669: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 29 20:07:51.669: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 08/29/23 20:07:51.673
Aug 29 20:07:51.685: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9529" to be "running and ready"
Aug 29 20:07:51.692: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.951143ms
Aug 29 20:07:51.693: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:07:53.698: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012724127s
Aug 29 20:07:53.698: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 29 20:07:53.698: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/29/23 20:07:53.701
STEP: delete the pod with lifecycle hook 08/29/23 20:07:53.709
Aug 29 20:07:53.718: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 29 20:07:53.722: INFO: Pod pod-with-poststart-http-hook still exists
Aug 29 20:07:55.724: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 29 20:07:55.729: INFO: Pod pod-with-poststart-http-hook still exists
Aug 29 20:07:57.724: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 29 20:07:57.728: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:57.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9529" for this suite. 08/29/23 20:07:57.735
------------------------------
• [SLOW TEST] [8.137 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:49.606
    Aug 29 20:07:49.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/29/23 20:07:49.607
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:49.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:49.634
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/29/23 20:07:49.642
    Aug 29 20:07:49.656: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9529" to be "running and ready"
    Aug 29 20:07:49.664: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 8.276435ms
    Aug 29 20:07:49.664: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:07:51.669: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.013595005s
    Aug 29 20:07:51.669: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 29 20:07:51.669: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 08/29/23 20:07:51.673
    Aug 29 20:07:51.685: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-9529" to be "running and ready"
    Aug 29 20:07:51.692: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.951143ms
    Aug 29 20:07:51.693: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:07:53.698: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.012724127s
    Aug 29 20:07:53.698: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 29 20:07:53.698: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/29/23 20:07:53.701
    STEP: delete the pod with lifecycle hook 08/29/23 20:07:53.709
    Aug 29 20:07:53.718: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 29 20:07:53.722: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 29 20:07:55.724: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 29 20:07:55.729: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 29 20:07:57.724: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 29 20:07:57.728: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:57.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9529" for this suite. 08/29/23 20:07:57.735
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:57.743
Aug 29 20:07:57.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename ingress 08/29/23 20:07:57.745
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:57.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:57.768
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/29/23 20:07:57.771
STEP: getting /apis/networking.k8s.io 08/29/23 20:07:57.774
STEP: getting /apis/networking.k8s.iov1 08/29/23 20:07:57.776
STEP: creating 08/29/23 20:07:57.777
STEP: getting 08/29/23 20:07:57.809
STEP: listing 08/29/23 20:07:57.816
STEP: watching 08/29/23 20:07:57.82
Aug 29 20:07:57.820: INFO: starting watch
STEP: cluster-wide listing 08/29/23 20:07:57.821
STEP: cluster-wide watching 08/29/23 20:07:57.826
Aug 29 20:07:57.826: INFO: starting watch
STEP: patching 08/29/23 20:07:57.828
STEP: updating 08/29/23 20:07:57.835
Aug 29 20:07:57.845: INFO: waiting for watch events with expected annotations
Aug 29 20:07:57.845: INFO: saw patched and updated annotations
STEP: patching /status 08/29/23 20:07:57.845
STEP: updating /status 08/29/23 20:07:57.851
STEP: get /status 08/29/23 20:07:57.865
STEP: deleting 08/29/23 20:07:57.869
STEP: deleting a collection 08/29/23 20:07:57.885
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Aug 29 20:07:57.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-8004" for this suite. 08/29/23 20:07:57.906
------------------------------
• [0.170 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:57.743
    Aug 29 20:07:57.744: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename ingress 08/29/23 20:07:57.745
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:57.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:57.768
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/29/23 20:07:57.771
    STEP: getting /apis/networking.k8s.io 08/29/23 20:07:57.774
    STEP: getting /apis/networking.k8s.iov1 08/29/23 20:07:57.776
    STEP: creating 08/29/23 20:07:57.777
    STEP: getting 08/29/23 20:07:57.809
    STEP: listing 08/29/23 20:07:57.816
    STEP: watching 08/29/23 20:07:57.82
    Aug 29 20:07:57.820: INFO: starting watch
    STEP: cluster-wide listing 08/29/23 20:07:57.821
    STEP: cluster-wide watching 08/29/23 20:07:57.826
    Aug 29 20:07:57.826: INFO: starting watch
    STEP: patching 08/29/23 20:07:57.828
    STEP: updating 08/29/23 20:07:57.835
    Aug 29 20:07:57.845: INFO: waiting for watch events with expected annotations
    Aug 29 20:07:57.845: INFO: saw patched and updated annotations
    STEP: patching /status 08/29/23 20:07:57.845
    STEP: updating /status 08/29/23 20:07:57.851
    STEP: get /status 08/29/23 20:07:57.865
    STEP: deleting 08/29/23 20:07:57.869
    STEP: deleting a collection 08/29/23 20:07:57.885
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:07:57.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-8004" for this suite. 08/29/23 20:07:57.906
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:07:57.914
Aug 29 20:07:57.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:07:57.915
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:57.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:57.937
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-2951 08/29/23 20:07:57.941
STEP: creating replication controller nodeport-test in namespace services-2951 08/29/23 20:07:57.961
I0829 20:07:57.972826      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2951, replica count: 2
I0829 20:08:01.024889      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 20:08:01.024: INFO: Creating new exec pod
Aug 29 20:08:01.038: INFO: Waiting up to 5m0s for pod "execpod675fh" in namespace "services-2951" to be "running"
Aug 29 20:08:01.046: INFO: Pod "execpod675fh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.221733ms
Aug 29 20:08:03.051: INFO: Pod "execpod675fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.012621634s
Aug 29 20:08:03.051: INFO: Pod "execpod675fh" satisfied condition "running"
Aug 29 20:08:04.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2951 exec execpod675fh -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Aug 29 20:08:04.234: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 29 20:08:04.234: INFO: stdout: ""
Aug 29 20:08:04.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2951 exec execpod675fh -- /bin/sh -x -c nc -v -z -w 2 172.19.96.117 80'
Aug 29 20:08:04.392: INFO: stderr: "+ nc -v -z -w 2 172.19.96.117 80\nConnection to 172.19.96.117 80 port [tcp/http] succeeded!\n"
Aug 29 20:08:04.393: INFO: stdout: ""
Aug 29 20:08:04.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2951 exec execpod675fh -- /bin/sh -x -c nc -v -z -w 2 10.45.35.199 32117'
Aug 29 20:08:04.587: INFO: stderr: "+ nc -v -z -w 2 10.45.35.199 32117\nConnection to 10.45.35.199 32117 port [tcp/*] succeeded!\n"
Aug 29 20:08:04.587: INFO: stdout: ""
Aug 29 20:08:04.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2951 exec execpod675fh -- /bin/sh -x -c nc -v -z -w 2 10.45.35.202 32117'
Aug 29 20:08:04.767: INFO: stderr: "+ nc -v -z -w 2 10.45.35.202 32117\nConnection to 10.45.35.202 32117 port [tcp/*] succeeded!\n"
Aug 29 20:08:04.767: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:08:04.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2951" for this suite. 08/29/23 20:08:04.774
------------------------------
• [SLOW TEST] [6.868 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:07:57.914
    Aug 29 20:07:57.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:07:57.915
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:07:57.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:07:57.937
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-2951 08/29/23 20:07:57.941
    STEP: creating replication controller nodeport-test in namespace services-2951 08/29/23 20:07:57.961
    I0829 20:07:57.972826      19 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-2951, replica count: 2
    I0829 20:08:01.024889      19 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 20:08:01.024: INFO: Creating new exec pod
    Aug 29 20:08:01.038: INFO: Waiting up to 5m0s for pod "execpod675fh" in namespace "services-2951" to be "running"
    Aug 29 20:08:01.046: INFO: Pod "execpod675fh": Phase="Pending", Reason="", readiness=false. Elapsed: 8.221733ms
    Aug 29 20:08:03.051: INFO: Pod "execpod675fh": Phase="Running", Reason="", readiness=true. Elapsed: 2.012621634s
    Aug 29 20:08:03.051: INFO: Pod "execpod675fh" satisfied condition "running"
    Aug 29 20:08:04.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2951 exec execpod675fh -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Aug 29 20:08:04.234: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 29 20:08:04.234: INFO: stdout: ""
    Aug 29 20:08:04.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2951 exec execpod675fh -- /bin/sh -x -c nc -v -z -w 2 172.19.96.117 80'
    Aug 29 20:08:04.392: INFO: stderr: "+ nc -v -z -w 2 172.19.96.117 80\nConnection to 172.19.96.117 80 port [tcp/http] succeeded!\n"
    Aug 29 20:08:04.393: INFO: stdout: ""
    Aug 29 20:08:04.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2951 exec execpod675fh -- /bin/sh -x -c nc -v -z -w 2 10.45.35.199 32117'
    Aug 29 20:08:04.587: INFO: stderr: "+ nc -v -z -w 2 10.45.35.199 32117\nConnection to 10.45.35.199 32117 port [tcp/*] succeeded!\n"
    Aug 29 20:08:04.587: INFO: stdout: ""
    Aug 29 20:08:04.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2951 exec execpod675fh -- /bin/sh -x -c nc -v -z -w 2 10.45.35.202 32117'
    Aug 29 20:08:04.767: INFO: stderr: "+ nc -v -z -w 2 10.45.35.202 32117\nConnection to 10.45.35.202 32117 port [tcp/*] succeeded!\n"
    Aug 29 20:08:04.767: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:08:04.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2951" for this suite. 08/29/23 20:08:04.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:08:04.784
Aug 29 20:08:04.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:08:04.784
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:04.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:04.813
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-43964c61-6bba-4d45-80f3-110b4d086e38 08/29/23 20:08:04.816
STEP: Creating a pod to test consume configMaps 08/29/23 20:08:04.822
Aug 29 20:08:04.847: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624" in namespace "projected-2404" to be "Succeeded or Failed"
Aug 29 20:08:04.850: INFO: Pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624": Phase="Pending", Reason="", readiness=false. Elapsed: 3.179283ms
Aug 29 20:08:06.855: INFO: Pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008491175s
Aug 29 20:08:08.856: INFO: Pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008763479s
STEP: Saw pod success 08/29/23 20:08:08.856
Aug 29 20:08:08.856: INFO: Pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624" satisfied condition "Succeeded or Failed"
Aug 29 20:08:08.860: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624 container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/29/23 20:08:08.869
Aug 29 20:08:08.882: INFO: Waiting for pod pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624 to disappear
Aug 29 20:08:08.885: INFO: Pod pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:08:08.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2404" for this suite. 08/29/23 20:08:08.889
------------------------------
• [4.115 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:08:04.784
    Aug 29 20:08:04.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:08:04.784
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:04.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:04.813
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-43964c61-6bba-4d45-80f3-110b4d086e38 08/29/23 20:08:04.816
    STEP: Creating a pod to test consume configMaps 08/29/23 20:08:04.822
    Aug 29 20:08:04.847: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624" in namespace "projected-2404" to be "Succeeded or Failed"
    Aug 29 20:08:04.850: INFO: Pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624": Phase="Pending", Reason="", readiness=false. Elapsed: 3.179283ms
    Aug 29 20:08:06.855: INFO: Pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008491175s
    Aug 29 20:08:08.856: INFO: Pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008763479s
    STEP: Saw pod success 08/29/23 20:08:08.856
    Aug 29 20:08:08.856: INFO: Pod "pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624" satisfied condition "Succeeded or Failed"
    Aug 29 20:08:08.860: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:08:08.869
    Aug 29 20:08:08.882: INFO: Waiting for pod pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624 to disappear
    Aug 29 20:08:08.885: INFO: Pod pod-projected-configmaps-a9d80093-8da9-484f-879f-af6b21620624 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:08:08.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2404" for this suite. 08/29/23 20:08:08.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:08:08.899
Aug 29 20:08:08.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename job 08/29/23 20:08:08.901
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:08.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:08.928
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 08/29/23 20:08:08.931
STEP: Ensuring active pods == parallelism 08/29/23 20:08:08.941
STEP: delete a job 08/29/23 20:08:10.946
STEP: deleting Job.batch foo in namespace job-8460, will wait for the garbage collector to delete the pods 08/29/23 20:08:10.947
Aug 29 20:08:11.008: INFO: Deleting Job.batch foo took: 6.99113ms
Aug 29 20:08:11.109: INFO: Terminating Job.batch foo pods took: 101.162323ms
STEP: Ensuring job was deleted 08/29/23 20:08:43.81
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 29 20:08:43.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8460" for this suite. 08/29/23 20:08:43.819
------------------------------
• [SLOW TEST] [34.928 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:08:08.899
    Aug 29 20:08:08.899: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename job 08/29/23 20:08:08.901
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:08.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:08.928
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 08/29/23 20:08:08.931
    STEP: Ensuring active pods == parallelism 08/29/23 20:08:08.941
    STEP: delete a job 08/29/23 20:08:10.946
    STEP: deleting Job.batch foo in namespace job-8460, will wait for the garbage collector to delete the pods 08/29/23 20:08:10.947
    Aug 29 20:08:11.008: INFO: Deleting Job.batch foo took: 6.99113ms
    Aug 29 20:08:11.109: INFO: Terminating Job.batch foo pods took: 101.162323ms
    STEP: Ensuring job was deleted 08/29/23 20:08:43.81
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:08:43.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8460" for this suite. 08/29/23 20:08:43.819
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:08:43.828
Aug 29 20:08:43.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubelet-test 08/29/23 20:08:43.829
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:43.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:43.855
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:08:47.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2699" for this suite. 08/29/23 20:08:47.888
------------------------------
• [4.070 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:08:43.828
    Aug 29 20:08:43.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubelet-test 08/29/23 20:08:43.829
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:43.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:43.855
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:08:47.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2699" for this suite. 08/29/23 20:08:47.888
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:08:47.898
Aug 29 20:08:47.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:08:47.899
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:47.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:47.923
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:08:47.943
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:08:48.45
STEP: Deploying the webhook pod 08/29/23 20:08:48.464
STEP: Wait for the deployment to be ready 08/29/23 20:08:48.48
Aug 29 20:08:48.499: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 20:08:50.512
STEP: Verifying the service has paired with the endpoint 08/29/23 20:08:50.526
Aug 29 20:08:51.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 08/29/23 20:08:51.531
STEP: create a pod 08/29/23 20:08:51.55
Aug 29 20:08:51.561: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-841" to be "running"
Aug 29 20:08:51.565: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.257391ms
Aug 29 20:08:53.572: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010296577s
Aug 29 20:08:53.572: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/29/23 20:08:53.572
Aug 29 20:08:53.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=webhook-841 attach --namespace=webhook-841 to-be-attached-pod -i -c=container1'
Aug 29 20:08:53.679: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:08:53.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-841" for this suite. 08/29/23 20:08:53.744
STEP: Destroying namespace "webhook-841-markers" for this suite. 08/29/23 20:08:53.755
------------------------------
• [SLOW TEST] [5.868 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:08:47.898
    Aug 29 20:08:47.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:08:47.899
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:47.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:47.923
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:08:47.943
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:08:48.45
    STEP: Deploying the webhook pod 08/29/23 20:08:48.464
    STEP: Wait for the deployment to be ready 08/29/23 20:08:48.48
    Aug 29 20:08:48.499: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 20:08:50.512
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:08:50.526
    Aug 29 20:08:51.527: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 08/29/23 20:08:51.531
    STEP: create a pod 08/29/23 20:08:51.55
    Aug 29 20:08:51.561: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-841" to be "running"
    Aug 29 20:08:51.565: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.257391ms
    Aug 29 20:08:53.572: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010296577s
    Aug 29 20:08:53.572: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/29/23 20:08:53.572
    Aug 29 20:08:53.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=webhook-841 attach --namespace=webhook-841 to-be-attached-pod -i -c=container1'
    Aug 29 20:08:53.679: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:08:53.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-841" for this suite. 08/29/23 20:08:53.744
    STEP: Destroying namespace "webhook-841-markers" for this suite. 08/29/23 20:08:53.755
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:08:53.767
Aug 29 20:08:53.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename dns 08/29/23 20:08:53.768
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:53.787
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:53.79
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/29/23 20:08:53.795
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5555.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5555.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 182.208.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.208.182_udp@PTR;check="$$(dig +tcp +noall +answer +search 182.208.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.208.182_tcp@PTR;sleep 1; done
 08/29/23 20:08:53.817
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5555.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5555.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 182.208.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.208.182_udp@PTR;check="$$(dig +tcp +noall +answer +search 182.208.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.208.182_tcp@PTR;sleep 1; done
 08/29/23 20:08:53.817
STEP: creating a pod to probe DNS 08/29/23 20:08:53.817
STEP: submitting the pod to kubernetes 08/29/23 20:08:53.817
Aug 29 20:08:53.834: INFO: Waiting up to 15m0s for pod "dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e" in namespace "dns-5555" to be "running"
Aug 29 20:08:53.838: INFO: Pod "dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356675ms
Aug 29 20:08:55.845: INFO: Pod "dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011311974s
Aug 29 20:08:55.845: INFO: Pod "dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e" satisfied condition "running"
STEP: retrieving the pod 08/29/23 20:08:55.845
STEP: looking for the results for each expected name from probers 08/29/23 20:08:55.849
Aug 29 20:08:55.855: INFO: Unable to read wheezy_udp@dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
Aug 29 20:08:55.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
Aug 29 20:08:55.864: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
Aug 29 20:08:55.868: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
Aug 29 20:08:55.885: INFO: Unable to read jessie_udp@dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
Aug 29 20:08:55.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
Aug 29 20:08:55.892: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
Aug 29 20:08:55.896: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
Aug 29 20:08:55.912: INFO: Lookups using dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e failed for: [wheezy_udp@dns-test-service.dns-5555.svc.cluster.local wheezy_tcp@dns-test-service.dns-5555.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local jessie_udp@dns-test-service.dns-5555.svc.cluster.local jessie_tcp@dns-test-service.dns-5555.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local]

Aug 29 20:09:00.978: INFO: DNS probes using dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e succeeded

STEP: deleting the pod 08/29/23 20:09:00.978
STEP: deleting the test service 08/29/23 20:09:00.992
STEP: deleting the test headless service 08/29/23 20:09:01.024
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 29 20:09:01.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5555" for this suite. 08/29/23 20:09:01.043
------------------------------
• [SLOW TEST] [7.286 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:08:53.767
    Aug 29 20:08:53.767: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename dns 08/29/23 20:08:53.768
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:08:53.787
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:08:53.79
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/29/23 20:08:53.795
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5555.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5555.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 182.208.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.208.182_udp@PTR;check="$$(dig +tcp +noall +answer +search 182.208.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.208.182_tcp@PTR;sleep 1; done
     08/29/23 20:08:53.817
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5555.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5555.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5555.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5555.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5555.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 182.208.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.208.182_udp@PTR;check="$$(dig +tcp +noall +answer +search 182.208.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.208.182_tcp@PTR;sleep 1; done
     08/29/23 20:08:53.817
    STEP: creating a pod to probe DNS 08/29/23 20:08:53.817
    STEP: submitting the pod to kubernetes 08/29/23 20:08:53.817
    Aug 29 20:08:53.834: INFO: Waiting up to 15m0s for pod "dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e" in namespace "dns-5555" to be "running"
    Aug 29 20:08:53.838: INFO: Pod "dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.356675ms
    Aug 29 20:08:55.845: INFO: Pod "dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e": Phase="Running", Reason="", readiness=true. Elapsed: 2.011311974s
    Aug 29 20:08:55.845: INFO: Pod "dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 20:08:55.845
    STEP: looking for the results for each expected name from probers 08/29/23 20:08:55.849
    Aug 29 20:08:55.855: INFO: Unable to read wheezy_udp@dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
    Aug 29 20:08:55.859: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
    Aug 29 20:08:55.864: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
    Aug 29 20:08:55.868: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
    Aug 29 20:08:55.885: INFO: Unable to read jessie_udp@dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
    Aug 29 20:08:55.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
    Aug 29 20:08:55.892: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
    Aug 29 20:08:55.896: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local from pod dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e: the server could not find the requested resource (get pods dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e)
    Aug 29 20:08:55.912: INFO: Lookups using dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e failed for: [wheezy_udp@dns-test-service.dns-5555.svc.cluster.local wheezy_tcp@dns-test-service.dns-5555.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local jessie_udp@dns-test-service.dns-5555.svc.cluster.local jessie_tcp@dns-test-service.dns-5555.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5555.svc.cluster.local]

    Aug 29 20:09:00.978: INFO: DNS probes using dns-5555/dns-test-126d9f71-1b1c-408e-852a-c62f6d0f8a9e succeeded

    STEP: deleting the pod 08/29/23 20:09:00.978
    STEP: deleting the test service 08/29/23 20:09:00.992
    STEP: deleting the test headless service 08/29/23 20:09:01.024
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:09:01.038: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5555" for this suite. 08/29/23 20:09:01.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:09:01.055
Aug 29 20:09:01.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:09:01.055
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:01.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:01.077
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Aug 29 20:09:01.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5731 version'
Aug 29 20:09:01.163: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 29 20:09:01.163: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:09:01.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5731" for this suite. 08/29/23 20:09:01.171
------------------------------
• [0.125 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:09:01.055
    Aug 29 20:09:01.055: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:09:01.055
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:01.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:01.077
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Aug 29 20:09:01.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5731 version'
    Aug 29 20:09:01.163: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 29 20:09:01.163: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:50:44Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.8\", GitCommit:\"395f0a2fdc940aeb9ab88849e8fa4321decbf6e1\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:43:07Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:09:01.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5731" for this suite. 08/29/23 20:09:01.171
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:09:01.18
Aug 29 20:09:01.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:09:01.181
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:01.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:01.205
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-ef3b5999-c8bc-495a-a523-d265d5ca88e1 08/29/23 20:09:01.208
STEP: Creating a pod to test consume secrets 08/29/23 20:09:01.215
Aug 29 20:09:01.227: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4" in namespace "projected-2165" to be "Succeeded or Failed"
Aug 29 20:09:01.231: INFO: Pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.173391ms
Aug 29 20:09:03.236: INFO: Pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008836445s
Aug 29 20:09:05.237: INFO: Pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010525679s
STEP: Saw pod success 08/29/23 20:09:05.237
Aug 29 20:09:05.238: INFO: Pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4" satisfied condition "Succeeded or Failed"
Aug 29 20:09:05.241: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/29/23 20:09:05.249
Aug 29 20:09:05.266: INFO: Waiting for pod pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4 to disappear
Aug 29 20:09:05.269: INFO: Pod pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 29 20:09:05.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2165" for this suite. 08/29/23 20:09:05.274
------------------------------
• [4.105 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:09:01.18
    Aug 29 20:09:01.180: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:09:01.181
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:01.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:01.205
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-ef3b5999-c8bc-495a-a523-d265d5ca88e1 08/29/23 20:09:01.208
    STEP: Creating a pod to test consume secrets 08/29/23 20:09:01.215
    Aug 29 20:09:01.227: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4" in namespace "projected-2165" to be "Succeeded or Failed"
    Aug 29 20:09:01.231: INFO: Pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.173391ms
    Aug 29 20:09:03.236: INFO: Pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008836445s
    Aug 29 20:09:05.237: INFO: Pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010525679s
    STEP: Saw pod success 08/29/23 20:09:05.237
    Aug 29 20:09:05.238: INFO: Pod "pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4" satisfied condition "Succeeded or Failed"
    Aug 29 20:09:05.241: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:09:05.249
    Aug 29 20:09:05.266: INFO: Waiting for pod pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4 to disappear
    Aug 29 20:09:05.269: INFO: Pod pod-projected-secrets-830b8558-61cb-4314-a30a-a62543952be4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:09:05.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2165" for this suite. 08/29/23 20:09:05.274
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:09:05.285
Aug 29 20:09:05.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:09:05.287
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:05.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:05.313
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-493 08/29/23 20:09:05.315
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/29/23 20:09:05.336
STEP: creating service externalsvc in namespace services-493 08/29/23 20:09:05.336
STEP: creating replication controller externalsvc in namespace services-493 08/29/23 20:09:05.353
I0829 20:09:05.363093      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-493, replica count: 2
I0829 20:09:08.414271      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/29/23 20:09:08.418
Aug 29 20:09:08.442: INFO: Creating new exec pod
Aug 29 20:09:08.454: INFO: Waiting up to 5m0s for pod "execpodvqmg7" in namespace "services-493" to be "running"
Aug 29 20:09:08.459: INFO: Pod "execpodvqmg7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377418ms
Aug 29 20:09:10.463: INFO: Pod "execpodvqmg7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009795498s
Aug 29 20:09:10.464: INFO: Pod "execpodvqmg7" satisfied condition "running"
Aug 29 20:09:10.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-493 exec execpodvqmg7 -- /bin/sh -x -c nslookup nodeport-service.services-493.svc.cluster.local'
Aug 29 20:09:10.680: INFO: stderr: "+ nslookup nodeport-service.services-493.svc.cluster.local\n"
Aug 29 20:09:10.680: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nnodeport-service.services-493.svc.cluster.local\tcanonical name = externalsvc.services-493.svc.cluster.local.\nName:\texternalsvc.services-493.svc.cluster.local\nAddress: 172.19.127.117\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-493, will wait for the garbage collector to delete the pods 08/29/23 20:09:10.68
Aug 29 20:09:10.744: INFO: Deleting ReplicationController externalsvc took: 9.365101ms
Aug 29 20:09:10.845: INFO: Terminating ReplicationController externalsvc pods took: 100.739697ms
Aug 29 20:09:12.868: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:09:12.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-493" for this suite. 08/29/23 20:09:12.889
------------------------------
• [SLOW TEST] [7.614 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:09:05.285
    Aug 29 20:09:05.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:09:05.287
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:05.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:05.313
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-493 08/29/23 20:09:05.315
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/29/23 20:09:05.336
    STEP: creating service externalsvc in namespace services-493 08/29/23 20:09:05.336
    STEP: creating replication controller externalsvc in namespace services-493 08/29/23 20:09:05.353
    I0829 20:09:05.363093      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-493, replica count: 2
    I0829 20:09:08.414271      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/29/23 20:09:08.418
    Aug 29 20:09:08.442: INFO: Creating new exec pod
    Aug 29 20:09:08.454: INFO: Waiting up to 5m0s for pod "execpodvqmg7" in namespace "services-493" to be "running"
    Aug 29 20:09:08.459: INFO: Pod "execpodvqmg7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.377418ms
    Aug 29 20:09:10.463: INFO: Pod "execpodvqmg7": Phase="Running", Reason="", readiness=true. Elapsed: 2.009795498s
    Aug 29 20:09:10.464: INFO: Pod "execpodvqmg7" satisfied condition "running"
    Aug 29 20:09:10.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-493 exec execpodvqmg7 -- /bin/sh -x -c nslookup nodeport-service.services-493.svc.cluster.local'
    Aug 29 20:09:10.680: INFO: stderr: "+ nslookup nodeport-service.services-493.svc.cluster.local\n"
    Aug 29 20:09:10.680: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nnodeport-service.services-493.svc.cluster.local\tcanonical name = externalsvc.services-493.svc.cluster.local.\nName:\texternalsvc.services-493.svc.cluster.local\nAddress: 172.19.127.117\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-493, will wait for the garbage collector to delete the pods 08/29/23 20:09:10.68
    Aug 29 20:09:10.744: INFO: Deleting ReplicationController externalsvc took: 9.365101ms
    Aug 29 20:09:10.845: INFO: Terminating ReplicationController externalsvc pods took: 100.739697ms
    Aug 29 20:09:12.868: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:09:12.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-493" for this suite. 08/29/23 20:09:12.889
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:09:12.9
Aug 29 20:09:12.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replication-controller 08/29/23 20:09:12.901
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:12.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:12.924
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Aug 29 20:09:12.928: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/29/23 20:09:13.941
STEP: Checking rc "condition-test" has the desired failure condition set 08/29/23 20:09:13.947
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/29/23 20:09:14.955
Aug 29 20:09:14.966: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/29/23 20:09:14.966
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:09:15.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-1068" for this suite. 08/29/23 20:09:15.979
------------------------------
• [3.088 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:09:12.9
    Aug 29 20:09:12.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replication-controller 08/29/23 20:09:12.901
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:12.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:12.924
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Aug 29 20:09:12.928: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/29/23 20:09:13.941
    STEP: Checking rc "condition-test" has the desired failure condition set 08/29/23 20:09:13.947
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/29/23 20:09:14.955
    Aug 29 20:09:14.966: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/29/23 20:09:14.966
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:09:15.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-1068" for this suite. 08/29/23 20:09:15.979
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:09:15.989
Aug 29 20:09:15.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:09:15.991
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:16.012
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:16.015
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 08/29/23 20:09:16.018
Aug 29 20:09:16.018: INFO: namespace kubectl-2668
Aug 29 20:09:16.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2668 create -f -'
Aug 29 20:09:17.001: INFO: stderr: ""
Aug 29 20:09:17.001: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/29/23 20:09:17.001
Aug 29 20:09:18.009: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 20:09:18.009: INFO: Found 0 / 1
Aug 29 20:09:19.005: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 20:09:19.005: INFO: Found 1 / 1
Aug 29 20:09:19.005: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 29 20:09:19.009: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 20:09:19.009: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 20:09:19.009: INFO: wait on agnhost-primary startup in kubectl-2668 
Aug 29 20:09:19.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2668 logs agnhost-primary-6w6rk agnhost-primary'
Aug 29 20:09:19.117: INFO: stderr: ""
Aug 29 20:09:19.117: INFO: stdout: "Paused\n"
STEP: exposing RC 08/29/23 20:09:19.117
Aug 29 20:09:19.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2668 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 29 20:09:19.225: INFO: stderr: ""
Aug 29 20:09:19.225: INFO: stdout: "service/rm2 exposed\n"
Aug 29 20:09:19.231: INFO: Service rm2 in namespace kubectl-2668 found.
STEP: exposing service 08/29/23 20:09:21.239
Aug 29 20:09:21.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2668 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 29 20:09:21.374: INFO: stderr: ""
Aug 29 20:09:21.374: INFO: stdout: "service/rm3 exposed\n"
Aug 29 20:09:21.382: INFO: Service rm3 in namespace kubectl-2668 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:09:23.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2668" for this suite. 08/29/23 20:09:23.394
------------------------------
• [SLOW TEST] [7.412 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:09:15.989
    Aug 29 20:09:15.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:09:15.991
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:16.012
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:16.015
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 08/29/23 20:09:16.018
    Aug 29 20:09:16.018: INFO: namespace kubectl-2668
    Aug 29 20:09:16.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2668 create -f -'
    Aug 29 20:09:17.001: INFO: stderr: ""
    Aug 29 20:09:17.001: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/29/23 20:09:17.001
    Aug 29 20:09:18.009: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 20:09:18.009: INFO: Found 0 / 1
    Aug 29 20:09:19.005: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 20:09:19.005: INFO: Found 1 / 1
    Aug 29 20:09:19.005: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 29 20:09:19.009: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 20:09:19.009: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 29 20:09:19.009: INFO: wait on agnhost-primary startup in kubectl-2668 
    Aug 29 20:09:19.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2668 logs agnhost-primary-6w6rk agnhost-primary'
    Aug 29 20:09:19.117: INFO: stderr: ""
    Aug 29 20:09:19.117: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/29/23 20:09:19.117
    Aug 29 20:09:19.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2668 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 29 20:09:19.225: INFO: stderr: ""
    Aug 29 20:09:19.225: INFO: stdout: "service/rm2 exposed\n"
    Aug 29 20:09:19.231: INFO: Service rm2 in namespace kubectl-2668 found.
    STEP: exposing service 08/29/23 20:09:21.239
    Aug 29 20:09:21.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2668 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 29 20:09:21.374: INFO: stderr: ""
    Aug 29 20:09:21.374: INFO: stdout: "service/rm3 exposed\n"
    Aug 29 20:09:21.382: INFO: Service rm3 in namespace kubectl-2668 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:09:23.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2668" for this suite. 08/29/23 20:09:23.394
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:09:23.402
Aug 29 20:09:23.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename init-container 08/29/23 20:09:23.403
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:23.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:23.427
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 08/29/23 20:09:23.43
Aug 29 20:09:23.430: INFO: PodSpec: initContainers in spec.initContainers
Aug 29 20:10:07.019: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-94e288e0-7bb2-4b10-a032-7a2a10f1c6e9", GenerateName:"", Namespace:"init-container-7125", SelfLink:"", UID:"b9bb2197-4190-473c-9b88-df443ba3e535", ResourceVersion:"24756", Generation:0, CreationTimestamp:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"430687524"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"e74e3772c06d46e45737a736a5ff12087680e24516de191433bedf45b7c58791", "cni.projectcalico.org/podIP":"172.20.30.152/32", "cni.projectcalico.org/podIPs":"172.20.30.152/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014840f0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 29, 20, 9, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014841e0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 29, 20, 10, 7, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001484270), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-npnfg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001488000), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-npnfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-npnfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-npnfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00201c118), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"loki-15bd39-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc008d4c000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00201c190)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00201c1b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00201c1b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00201c1bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f94070), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.35.206", PodIP:"172.20.30.152", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.20.30.152"}}, StartTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008d4c0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008d4c150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://5a2183601faa88c5418d249967481f846a36a5dde4c3d4e73a403efe2843ec25", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0014880c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0014880a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00201c23f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:07.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-7125" for this suite. 08/29/23 20:10:07.025
------------------------------
• [SLOW TEST] [43.633 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:09:23.402
    Aug 29 20:09:23.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename init-container 08/29/23 20:09:23.403
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:09:23.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:09:23.427
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 08/29/23 20:09:23.43
    Aug 29 20:09:23.430: INFO: PodSpec: initContainers in spec.initContainers
    Aug 29 20:10:07.019: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-94e288e0-7bb2-4b10-a032-7a2a10f1c6e9", GenerateName:"", Namespace:"init-container-7125", SelfLink:"", UID:"b9bb2197-4190-473c-9b88-df443ba3e535", ResourceVersion:"24756", Generation:0, CreationTimestamp:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"430687524"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"e74e3772c06d46e45737a736a5ff12087680e24516de191433bedf45b7c58791", "cni.projectcalico.org/podIP":"172.20.30.152/32", "cni.projectcalico.org/podIPs":"172.20.30.152/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014840f0), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 29, 20, 9, 24, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0014841e0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 29, 20, 10, 7, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001484270), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-npnfg", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc001488000), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-npnfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-npnfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-npnfg", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00201c118), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"loki-15bd39-worker-1", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc008d4c000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00201c190)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00201c1b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00201c1b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00201c1bc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000f94070), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.45.35.206", PodIP:"172.20.30.152", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.20.30.152"}}, StartTime:time.Date(2023, time.August, 29, 20, 9, 23, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008d4c0e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc008d4c150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://5a2183601faa88c5418d249967481f846a36a5dde4c3d4e73a403efe2843ec25", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0014880c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0014880a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00201c23f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:07.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-7125" for this suite. 08/29/23 20:10:07.025
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:07.035
Aug 29 20:10:07.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:10:07.036
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:07.061
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:07.066
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:10:07.069
Aug 29 20:10:07.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2" in namespace "projected-8288" to be "Succeeded or Failed"
Aug 29 20:10:07.088: INFO: Pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.267029ms
Aug 29 20:10:09.093: INFO: Pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010441237s
Aug 29 20:10:11.094: INFO: Pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010709323s
STEP: Saw pod success 08/29/23 20:10:11.094
Aug 29 20:10:11.094: INFO: Pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2" satisfied condition "Succeeded or Failed"
Aug 29 20:10:11.097: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2 container client-container: <nil>
STEP: delete the pod 08/29/23 20:10:11.105
Aug 29 20:10:11.122: INFO: Waiting for pod downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2 to disappear
Aug 29 20:10:11.125: INFO: Pod downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:11.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8288" for this suite. 08/29/23 20:10:11.131
------------------------------
• [4.103 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:07.035
    Aug 29 20:10:07.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:10:07.036
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:07.061
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:07.066
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:10:07.069
    Aug 29 20:10:07.083: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2" in namespace "projected-8288" to be "Succeeded or Failed"
    Aug 29 20:10:07.088: INFO: Pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.267029ms
    Aug 29 20:10:09.093: INFO: Pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010441237s
    Aug 29 20:10:11.094: INFO: Pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010709323s
    STEP: Saw pod success 08/29/23 20:10:11.094
    Aug 29 20:10:11.094: INFO: Pod "downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2" satisfied condition "Succeeded or Failed"
    Aug 29 20:10:11.097: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2 container client-container: <nil>
    STEP: delete the pod 08/29/23 20:10:11.105
    Aug 29 20:10:11.122: INFO: Waiting for pod downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2 to disappear
    Aug 29 20:10:11.125: INFO: Pod downwardapi-volume-a5056369-0c7c-436a-9feb-5d74e634b4b2 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:11.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8288" for this suite. 08/29/23 20:10:11.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:11.139
Aug 29 20:10:11.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:10:11.14
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:11.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:11.168
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:10:11.171
Aug 29 20:10:11.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a" in namespace "projected-8143" to be "Succeeded or Failed"
Aug 29 20:10:11.187: INFO: Pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996128ms
Aug 29 20:10:13.197: INFO: Pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013497097s
Aug 29 20:10:15.191: INFO: Pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008241389s
STEP: Saw pod success 08/29/23 20:10:15.191
Aug 29 20:10:15.192: INFO: Pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a" satisfied condition "Succeeded or Failed"
Aug 29 20:10:15.197: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a container client-container: <nil>
STEP: delete the pod 08/29/23 20:10:15.21
Aug 29 20:10:15.227: INFO: Waiting for pod downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a to disappear
Aug 29 20:10:15.230: INFO: Pod downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:15.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8143" for this suite. 08/29/23 20:10:15.235
------------------------------
• [4.105 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:11.139
    Aug 29 20:10:11.140: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:10:11.14
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:11.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:11.168
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:10:11.171
    Aug 29 20:10:11.183: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a" in namespace "projected-8143" to be "Succeeded or Failed"
    Aug 29 20:10:11.187: INFO: Pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.996128ms
    Aug 29 20:10:13.197: INFO: Pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013497097s
    Aug 29 20:10:15.191: INFO: Pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008241389s
    STEP: Saw pod success 08/29/23 20:10:15.191
    Aug 29 20:10:15.192: INFO: Pod "downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a" satisfied condition "Succeeded or Failed"
    Aug 29 20:10:15.197: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a container client-container: <nil>
    STEP: delete the pod 08/29/23 20:10:15.21
    Aug 29 20:10:15.227: INFO: Waiting for pod downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a to disappear
    Aug 29 20:10:15.230: INFO: Pod downwardapi-volume-7f5fbf57-1292-42fd-92d9-e92e124a018a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:15.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8143" for this suite. 08/29/23 20:10:15.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:15.246
Aug 29 20:10:15.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:10:15.247
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:15.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:15.269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/29/23 20:10:15.272
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/29/23 20:10:15.273
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/29/23 20:10:15.273
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/29/23 20:10:15.273
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/29/23 20:10:15.275
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/29/23 20:10:15.275
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/29/23 20:10:15.276
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:15.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7554" for this suite. 08/29/23 20:10:15.281
------------------------------
• [0.042 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:15.246
    Aug 29 20:10:15.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:10:15.247
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:15.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:15.269
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/29/23 20:10:15.272
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/29/23 20:10:15.273
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/29/23 20:10:15.273
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/29/23 20:10:15.273
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/29/23 20:10:15.275
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/29/23 20:10:15.275
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/29/23 20:10:15.276
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:15.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7554" for this suite. 08/29/23 20:10:15.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:15.29
Aug 29 20:10:15.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:10:15.291
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:15.312
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:15.316
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:15.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-340" for this suite. 08/29/23 20:10:15.368
------------------------------
• [0.087 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:15.29
    Aug 29 20:10:15.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:10:15.291
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:15.312
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:15.316
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:15.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-340" for this suite. 08/29/23 20:10:15.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:15.377
Aug 29 20:10:15.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:10:15.378
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:15.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:15.404
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 08/29/23 20:10:15.406
Aug 29 20:10:15.416: INFO: Waiting up to 5m0s for pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a" in namespace "emptydir-2199" to be "Succeeded or Failed"
Aug 29 20:10:15.420: INFO: Pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301764ms
Aug 29 20:10:17.425: INFO: Pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009152533s
Aug 29 20:10:19.426: INFO: Pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010061682s
STEP: Saw pod success 08/29/23 20:10:19.426
Aug 29 20:10:19.426: INFO: Pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a" satisfied condition "Succeeded or Failed"
Aug 29 20:10:19.430: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-95a00629-60a1-47c5-a7c4-116a20e88c5a container test-container: <nil>
STEP: delete the pod 08/29/23 20:10:19.438
Aug 29 20:10:19.453: INFO: Waiting for pod pod-95a00629-60a1-47c5-a7c4-116a20e88c5a to disappear
Aug 29 20:10:19.456: INFO: Pod pod-95a00629-60a1-47c5-a7c4-116a20e88c5a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:19.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2199" for this suite. 08/29/23 20:10:19.462
------------------------------
• [4.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:15.377
    Aug 29 20:10:15.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:10:15.378
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:15.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:15.404
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/29/23 20:10:15.406
    Aug 29 20:10:15.416: INFO: Waiting up to 5m0s for pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a" in namespace "emptydir-2199" to be "Succeeded or Failed"
    Aug 29 20:10:15.420: INFO: Pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.301764ms
    Aug 29 20:10:17.425: INFO: Pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009152533s
    Aug 29 20:10:19.426: INFO: Pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010061682s
    STEP: Saw pod success 08/29/23 20:10:19.426
    Aug 29 20:10:19.426: INFO: Pod "pod-95a00629-60a1-47c5-a7c4-116a20e88c5a" satisfied condition "Succeeded or Failed"
    Aug 29 20:10:19.430: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-95a00629-60a1-47c5-a7c4-116a20e88c5a container test-container: <nil>
    STEP: delete the pod 08/29/23 20:10:19.438
    Aug 29 20:10:19.453: INFO: Waiting for pod pod-95a00629-60a1-47c5-a7c4-116a20e88c5a to disappear
    Aug 29 20:10:19.456: INFO: Pod pod-95a00629-60a1-47c5-a7c4-116a20e88c5a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:19.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2199" for this suite. 08/29/23 20:10:19.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:19.471
Aug 29 20:10:19.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:10:19.471
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:19.492
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:19.502
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Aug 29 20:10:19.515: INFO: Waiting up to 5m0s for pod "server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29" in namespace "pods-1684" to be "running and ready"
Aug 29 20:10:19.517: INFO: Pod "server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784507ms
Aug 29 20:10:19.517: INFO: The phase of Pod server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:10:21.522: INFO: Pod "server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29": Phase="Running", Reason="", readiness=true. Elapsed: 2.007499447s
Aug 29 20:10:21.522: INFO: The phase of Pod server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29 is Running (Ready = true)
Aug 29 20:10:21.522: INFO: Pod "server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29" satisfied condition "running and ready"
Aug 29 20:10:21.547: INFO: Waiting up to 5m0s for pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8" in namespace "pods-1684" to be "Succeeded or Failed"
Aug 29 20:10:21.555: INFO: Pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125133ms
Aug 29 20:10:23.559: INFO: Pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012432876s
Aug 29 20:10:25.560: INFO: Pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013208108s
STEP: Saw pod success 08/29/23 20:10:25.56
Aug 29 20:10:25.560: INFO: Pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8" satisfied condition "Succeeded or Failed"
Aug 29 20:10:25.564: INFO: Trying to get logs from node loki-15bd39-worker-1 pod client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8 container env3cont: <nil>
STEP: delete the pod 08/29/23 20:10:25.572
Aug 29 20:10:25.589: INFO: Waiting for pod client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8 to disappear
Aug 29 20:10:25.593: INFO: Pod client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8 no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:25.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1684" for this suite. 08/29/23 20:10:25.598
------------------------------
• [SLOW TEST] [6.134 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:19.471
    Aug 29 20:10:19.471: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:10:19.471
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:19.492
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:19.502
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Aug 29 20:10:19.515: INFO: Waiting up to 5m0s for pod "server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29" in namespace "pods-1684" to be "running and ready"
    Aug 29 20:10:19.517: INFO: Pod "server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29": Phase="Pending", Reason="", readiness=false. Elapsed: 2.784507ms
    Aug 29 20:10:19.517: INFO: The phase of Pod server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:10:21.522: INFO: Pod "server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29": Phase="Running", Reason="", readiness=true. Elapsed: 2.007499447s
    Aug 29 20:10:21.522: INFO: The phase of Pod server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29 is Running (Ready = true)
    Aug 29 20:10:21.522: INFO: Pod "server-envvars-8b5bb66b-f1a9-4c44-a73a-04a6372a3b29" satisfied condition "running and ready"
    Aug 29 20:10:21.547: INFO: Waiting up to 5m0s for pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8" in namespace "pods-1684" to be "Succeeded or Failed"
    Aug 29 20:10:21.555: INFO: Pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.125133ms
    Aug 29 20:10:23.559: INFO: Pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012432876s
    Aug 29 20:10:25.560: INFO: Pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013208108s
    STEP: Saw pod success 08/29/23 20:10:25.56
    Aug 29 20:10:25.560: INFO: Pod "client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8" satisfied condition "Succeeded or Failed"
    Aug 29 20:10:25.564: INFO: Trying to get logs from node loki-15bd39-worker-1 pod client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8 container env3cont: <nil>
    STEP: delete the pod 08/29/23 20:10:25.572
    Aug 29 20:10:25.589: INFO: Waiting for pod client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8 to disappear
    Aug 29 20:10:25.593: INFO: Pod client-envvars-03ffadc0-6f5e-444e-87a9-f18d7a29d3e8 no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:25.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1684" for this suite. 08/29/23 20:10:25.598
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:25.605
Aug 29 20:10:25.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:10:25.606
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:25.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:25.634
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 08/29/23 20:10:25.647
STEP: watching for Pod to be ready 08/29/23 20:10:25.658
Aug 29 20:10:25.660: INFO: observed Pod pod-test in namespace pods-355 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 29 20:10:25.666: INFO: observed Pod pod-test in namespace pods-355 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  }]
Aug 29 20:10:25.683: INFO: observed Pod pod-test in namespace pods-355 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  }]
Aug 29 20:10:26.293: INFO: observed Pod pod-test in namespace pods-355 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  }]
Aug 29 20:10:27.127: INFO: Found Pod pod-test in namespace pods-355 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/29/23 20:10:27.132
STEP: getting the Pod and ensuring that it's patched 08/29/23 20:10:27.144
STEP: replacing the Pod's status Ready condition to False 08/29/23 20:10:27.148
STEP: check the Pod again to ensure its Ready conditions are False 08/29/23 20:10:27.162
STEP: deleting the Pod via a Collection with a LabelSelector 08/29/23 20:10:27.162
STEP: watching for the Pod to be deleted 08/29/23 20:10:27.178
Aug 29 20:10:27.180: INFO: observed event type MODIFIED
Aug 29 20:10:29.136: INFO: observed event type MODIFIED
Aug 29 20:10:29.439: INFO: observed event type MODIFIED
Aug 29 20:10:30.151: INFO: observed event type MODIFIED
Aug 29 20:10:30.160: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-355" for this suite. 08/29/23 20:10:30.174
------------------------------
• [4.580 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:25.605
    Aug 29 20:10:25.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:10:25.606
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:25.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:25.634
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 08/29/23 20:10:25.647
    STEP: watching for Pod to be ready 08/29/23 20:10:25.658
    Aug 29 20:10:25.660: INFO: observed Pod pod-test in namespace pods-355 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 29 20:10:25.666: INFO: observed Pod pod-test in namespace pods-355 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  }]
    Aug 29 20:10:25.683: INFO: observed Pod pod-test in namespace pods-355 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  }]
    Aug 29 20:10:26.293: INFO: observed Pod pod-test in namespace pods-355 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  }]
    Aug 29 20:10:27.127: INFO: Found Pod pod-test in namespace pods-355 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:27 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:27 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:10:25 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/29/23 20:10:27.132
    STEP: getting the Pod and ensuring that it's patched 08/29/23 20:10:27.144
    STEP: replacing the Pod's status Ready condition to False 08/29/23 20:10:27.148
    STEP: check the Pod again to ensure its Ready conditions are False 08/29/23 20:10:27.162
    STEP: deleting the Pod via a Collection with a LabelSelector 08/29/23 20:10:27.162
    STEP: watching for the Pod to be deleted 08/29/23 20:10:27.178
    Aug 29 20:10:27.180: INFO: observed event type MODIFIED
    Aug 29 20:10:29.136: INFO: observed event type MODIFIED
    Aug 29 20:10:29.439: INFO: observed event type MODIFIED
    Aug 29 20:10:30.151: INFO: observed event type MODIFIED
    Aug 29 20:10:30.160: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:30.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-355" for this suite. 08/29/23 20:10:30.174
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:30.186
Aug 29 20:10:30.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 20:10:30.187
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:30.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:30.211
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 08/29/23 20:10:30.214
STEP: Creating a ResourceQuota 08/29/23 20:10:35.219
STEP: Ensuring resource quota status is calculated 08/29/23 20:10:35.227
STEP: Creating a Service 08/29/23 20:10:37.231
STEP: Creating a NodePort Service 08/29/23 20:10:37.254
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/29/23 20:10:37.285
STEP: Ensuring resource quota status captures service creation 08/29/23 20:10:37.314
STEP: Deleting Services 08/29/23 20:10:39.32
STEP: Ensuring resource quota status released usage 08/29/23 20:10:39.366
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:41.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7936" for this suite. 08/29/23 20:10:41.378
------------------------------
• [SLOW TEST] [11.203 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:30.186
    Aug 29 20:10:30.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 20:10:30.187
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:30.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:30.211
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 08/29/23 20:10:30.214
    STEP: Creating a ResourceQuota 08/29/23 20:10:35.219
    STEP: Ensuring resource quota status is calculated 08/29/23 20:10:35.227
    STEP: Creating a Service 08/29/23 20:10:37.231
    STEP: Creating a NodePort Service 08/29/23 20:10:37.254
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/29/23 20:10:37.285
    STEP: Ensuring resource quota status captures service creation 08/29/23 20:10:37.314
    STEP: Deleting Services 08/29/23 20:10:39.32
    STEP: Ensuring resource quota status released usage 08/29/23 20:10:39.366
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:41.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7936" for this suite. 08/29/23 20:10:41.378
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:41.389
Aug 29 20:10:41.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:10:41.39
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:41.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:41.413
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-1cc222fb-eb5a-49d2-b617-f2c18dce4f16 08/29/23 20:10:41.416
STEP: Creating a pod to test consume configMaps 08/29/23 20:10:41.423
Aug 29 20:10:41.436: INFO: Waiting up to 5m0s for pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b" in namespace "configmap-454" to be "Succeeded or Failed"
Aug 29 20:10:41.441: INFO: Pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114722ms
Aug 29 20:10:43.446: INFO: Pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010400723s
Aug 29 20:10:45.448: INFO: Pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011807136s
STEP: Saw pod success 08/29/23 20:10:45.448
Aug 29 20:10:45.448: INFO: Pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b" satisfied condition "Succeeded or Failed"
Aug 29 20:10:45.452: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b container configmap-volume-test: <nil>
STEP: delete the pod 08/29/23 20:10:45.46
Aug 29 20:10:45.473: INFO: Waiting for pod pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b to disappear
Aug 29 20:10:45.476: INFO: Pod pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:10:45.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-454" for this suite. 08/29/23 20:10:45.481
------------------------------
• [4.099 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:41.389
    Aug 29 20:10:41.389: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:10:41.39
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:41.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:41.413
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-1cc222fb-eb5a-49d2-b617-f2c18dce4f16 08/29/23 20:10:41.416
    STEP: Creating a pod to test consume configMaps 08/29/23 20:10:41.423
    Aug 29 20:10:41.436: INFO: Waiting up to 5m0s for pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b" in namespace "configmap-454" to be "Succeeded or Failed"
    Aug 29 20:10:41.441: INFO: Pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.114722ms
    Aug 29 20:10:43.446: INFO: Pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010400723s
    Aug 29 20:10:45.448: INFO: Pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011807136s
    STEP: Saw pod success 08/29/23 20:10:45.448
    Aug 29 20:10:45.448: INFO: Pod "pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b" satisfied condition "Succeeded or Failed"
    Aug 29 20:10:45.452: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b container configmap-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:10:45.46
    Aug 29 20:10:45.473: INFO: Waiting for pod pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b to disappear
    Aug 29 20:10:45.476: INFO: Pod pod-configmaps-59a59df4-d5d2-4ddc-8157-c9b4d1330e1b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:10:45.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-454" for this suite. 08/29/23 20:10:45.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:10:45.491
Aug 29 20:10:45.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename cronjob 08/29/23 20:10:45.491
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:45.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:45.512
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/29/23 20:10:45.515
STEP: Ensuring a job is scheduled 08/29/23 20:10:45.524
STEP: Ensuring exactly one is scheduled 08/29/23 20:11:01.529
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/29/23 20:11:01.534
STEP: Ensuring no more jobs are scheduled 08/29/23 20:11:01.537
STEP: Removing cronjob 08/29/23 20:16:01.545
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 29 20:16:01.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1115" for this suite. 08/29/23 20:16:01.559
------------------------------
• [SLOW TEST] [316.077 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:10:45.491
    Aug 29 20:10:45.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename cronjob 08/29/23 20:10:45.491
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:10:45.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:10:45.512
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/29/23 20:10:45.515
    STEP: Ensuring a job is scheduled 08/29/23 20:10:45.524
    STEP: Ensuring exactly one is scheduled 08/29/23 20:11:01.529
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/29/23 20:11:01.534
    STEP: Ensuring no more jobs are scheduled 08/29/23 20:11:01.537
    STEP: Removing cronjob 08/29/23 20:16:01.545
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:16:01.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1115" for this suite. 08/29/23 20:16:01.559
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:16:01.569
Aug 29 20:16:01.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:16:01.57
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:01.593
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:01.597
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2411 08/29/23 20:16:01.6
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/29/23 20:16:01.62
STEP: creating service externalsvc in namespace services-2411 08/29/23 20:16:01.62
STEP: creating replication controller externalsvc in namespace services-2411 08/29/23 20:16:01.636
I0829 20:16:01.644930      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2411, replica count: 2
I0829 20:16:04.696506      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/29/23 20:16:04.7
Aug 29 20:16:04.718: INFO: Creating new exec pod
Aug 29 20:16:04.729: INFO: Waiting up to 5m0s for pod "execpod7kx4w" in namespace "services-2411" to be "running"
Aug 29 20:16:04.733: INFO: Pod "execpod7kx4w": Phase="Pending", Reason="", readiness=false. Elapsed: 3.781997ms
Aug 29 20:16:06.740: INFO: Pod "execpod7kx4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.010471154s
Aug 29 20:16:06.740: INFO: Pod "execpod7kx4w" satisfied condition "running"
Aug 29 20:16:06.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2411 exec execpod7kx4w -- /bin/sh -x -c nslookup clusterip-service.services-2411.svc.cluster.local'
Aug 29 20:16:06.962: INFO: stderr: "+ nslookup clusterip-service.services-2411.svc.cluster.local\n"
Aug 29 20:16:06.962: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nclusterip-service.services-2411.svc.cluster.local\tcanonical name = externalsvc.services-2411.svc.cluster.local.\nName:\texternalsvc.services-2411.svc.cluster.local\nAddress: 172.19.216.23\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2411, will wait for the garbage collector to delete the pods 08/29/23 20:16:06.962
Aug 29 20:16:07.027: INFO: Deleting ReplicationController externalsvc took: 8.632102ms
Aug 29 20:16:07.128: INFO: Terminating ReplicationController externalsvc pods took: 101.029147ms
Aug 29 20:16:09.251: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:16:09.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2411" for this suite. 08/29/23 20:16:09.269
------------------------------
• [SLOW TEST] [7.707 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:16:01.569
    Aug 29 20:16:01.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:16:01.57
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:01.593
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:01.597
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2411 08/29/23 20:16:01.6
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/29/23 20:16:01.62
    STEP: creating service externalsvc in namespace services-2411 08/29/23 20:16:01.62
    STEP: creating replication controller externalsvc in namespace services-2411 08/29/23 20:16:01.636
    I0829 20:16:01.644930      19 runners.go:193] Created replication controller with name: externalsvc, namespace: services-2411, replica count: 2
    I0829 20:16:04.696506      19 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/29/23 20:16:04.7
    Aug 29 20:16:04.718: INFO: Creating new exec pod
    Aug 29 20:16:04.729: INFO: Waiting up to 5m0s for pod "execpod7kx4w" in namespace "services-2411" to be "running"
    Aug 29 20:16:04.733: INFO: Pod "execpod7kx4w": Phase="Pending", Reason="", readiness=false. Elapsed: 3.781997ms
    Aug 29 20:16:06.740: INFO: Pod "execpod7kx4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.010471154s
    Aug 29 20:16:06.740: INFO: Pod "execpod7kx4w" satisfied condition "running"
    Aug 29 20:16:06.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-2411 exec execpod7kx4w -- /bin/sh -x -c nslookup clusterip-service.services-2411.svc.cluster.local'
    Aug 29 20:16:06.962: INFO: stderr: "+ nslookup clusterip-service.services-2411.svc.cluster.local\n"
    Aug 29 20:16:06.962: INFO: stdout: "Server:\t\t172.19.0.10\nAddress:\t172.19.0.10#53\n\nclusterip-service.services-2411.svc.cluster.local\tcanonical name = externalsvc.services-2411.svc.cluster.local.\nName:\texternalsvc.services-2411.svc.cluster.local\nAddress: 172.19.216.23\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-2411, will wait for the garbage collector to delete the pods 08/29/23 20:16:06.962
    Aug 29 20:16:07.027: INFO: Deleting ReplicationController externalsvc took: 8.632102ms
    Aug 29 20:16:07.128: INFO: Terminating ReplicationController externalsvc pods took: 101.029147ms
    Aug 29 20:16:09.251: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:16:09.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2411" for this suite. 08/29/23 20:16:09.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:16:09.277
Aug 29 20:16:09.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename dns 08/29/23 20:16:09.278
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:09.301
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:09.304
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/29/23 20:16:09.307
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2157 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2157;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2157 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2157;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2157.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2157.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2157.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2157.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2157.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2157.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2157.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2157.svc;check="$$(dig +notcp +noall +answer +search 81.194.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.194.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.194.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.194.81_tcp@PTR;sleep 1; done
 08/29/23 20:16:09.328
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2157 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2157;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2157 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2157;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2157.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2157.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2157.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2157.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2157.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2157.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2157.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2157.svc;check="$$(dig +notcp +noall +answer +search 81.194.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.194.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.194.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.194.81_tcp@PTR;sleep 1; done
 08/29/23 20:16:09.328
STEP: creating a pod to probe DNS 08/29/23 20:16:09.328
STEP: submitting the pod to kubernetes 08/29/23 20:16:09.328
Aug 29 20:16:09.342: INFO: Waiting up to 15m0s for pod "dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb" in namespace "dns-2157" to be "running"
Aug 29 20:16:09.346: INFO: Pod "dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372888ms
Aug 29 20:16:11.351: INFO: Pod "dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008694491s
Aug 29 20:16:11.351: INFO: Pod "dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb" satisfied condition "running"
STEP: retrieving the pod 08/29/23 20:16:11.351
STEP: looking for the results for each expected name from probers 08/29/23 20:16:11.355
Aug 29 20:16:11.360: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.363: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.367: INFO: Unable to read wheezy_udp@dns-test-service.dns-2157 from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.371: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2157 from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.375: INFO: Unable to read wheezy_udp@dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.383: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.396: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.415: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.418: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.422: INFO: Unable to read jessie_udp@dns-test-service.dns-2157 from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.425: INFO: Unable to read jessie_tcp@dns-test-service.dns-2157 from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.429: INFO: Unable to read jessie_udp@dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.433: INFO: Unable to read jessie_tcp@dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.439: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.443: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
Aug 29 20:16:11.458: INFO: Lookups using dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2157 wheezy_tcp@dns-test-service.dns-2157 wheezy_udp@dns-test-service.dns-2157.svc wheezy_tcp@dns-test-service.dns-2157.svc wheezy_udp@_http._tcp.dns-test-service.dns-2157.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2157.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2157 jessie_tcp@dns-test-service.dns-2157 jessie_udp@dns-test-service.dns-2157.svc jessie_tcp@dns-test-service.dns-2157.svc jessie_udp@_http._tcp.dns-test-service.dns-2157.svc jessie_tcp@_http._tcp.dns-test-service.dns-2157.svc]

Aug 29 20:16:16.554: INFO: DNS probes using dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb succeeded

STEP: deleting the pod 08/29/23 20:16:16.554
STEP: deleting the test service 08/29/23 20:16:16.574
STEP: deleting the test headless service 08/29/23 20:16:16.605
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 29 20:16:16.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2157" for this suite. 08/29/23 20:16:16.624
------------------------------
• [SLOW TEST] [7.358 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:16:09.277
    Aug 29 20:16:09.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename dns 08/29/23 20:16:09.278
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:09.301
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:09.304
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/29/23 20:16:09.307
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2157 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2157;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2157 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2157;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2157.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2157.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2157.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2157.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2157.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2157.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2157.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2157.svc;check="$$(dig +notcp +noall +answer +search 81.194.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.194.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.194.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.194.81_tcp@PTR;sleep 1; done
     08/29/23 20:16:09.328
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2157 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2157;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2157 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2157;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2157.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2157.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2157.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2157.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2157.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2157.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2157.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2157.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2157.svc;check="$$(dig +notcp +noall +answer +search 81.194.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.194.81_udp@PTR;check="$$(dig +tcp +noall +answer +search 81.194.19.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.19.194.81_tcp@PTR;sleep 1; done
     08/29/23 20:16:09.328
    STEP: creating a pod to probe DNS 08/29/23 20:16:09.328
    STEP: submitting the pod to kubernetes 08/29/23 20:16:09.328
    Aug 29 20:16:09.342: INFO: Waiting up to 15m0s for pod "dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb" in namespace "dns-2157" to be "running"
    Aug 29 20:16:09.346: INFO: Pod "dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.372888ms
    Aug 29 20:16:11.351: INFO: Pod "dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.008694491s
    Aug 29 20:16:11.351: INFO: Pod "dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 20:16:11.351
    STEP: looking for the results for each expected name from probers 08/29/23 20:16:11.355
    Aug 29 20:16:11.360: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.363: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.367: INFO: Unable to read wheezy_udp@dns-test-service.dns-2157 from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.371: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2157 from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.375: INFO: Unable to read wheezy_udp@dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.380: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.383: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.396: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.415: INFO: Unable to read jessie_udp@dns-test-service from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.418: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.422: INFO: Unable to read jessie_udp@dns-test-service.dns-2157 from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.425: INFO: Unable to read jessie_tcp@dns-test-service.dns-2157 from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.429: INFO: Unable to read jessie_udp@dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.433: INFO: Unable to read jessie_tcp@dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.439: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.443: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2157.svc from pod dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb: the server could not find the requested resource (get pods dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb)
    Aug 29 20:16:11.458: INFO: Lookups using dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-2157 wheezy_tcp@dns-test-service.dns-2157 wheezy_udp@dns-test-service.dns-2157.svc wheezy_tcp@dns-test-service.dns-2157.svc wheezy_udp@_http._tcp.dns-test-service.dns-2157.svc wheezy_tcp@_http._tcp.dns-test-service.dns-2157.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-2157 jessie_tcp@dns-test-service.dns-2157 jessie_udp@dns-test-service.dns-2157.svc jessie_tcp@dns-test-service.dns-2157.svc jessie_udp@_http._tcp.dns-test-service.dns-2157.svc jessie_tcp@_http._tcp.dns-test-service.dns-2157.svc]

    Aug 29 20:16:16.554: INFO: DNS probes using dns-2157/dns-test-4ee8ba08-1d77-4b24-9b23-98dc42469bbb succeeded

    STEP: deleting the pod 08/29/23 20:16:16.554
    STEP: deleting the test service 08/29/23 20:16:16.574
    STEP: deleting the test headless service 08/29/23 20:16:16.605
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:16:16.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2157" for this suite. 08/29/23 20:16:16.624
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:16:16.635
Aug 29 20:16:16.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:16:16.636
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:16.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:16.662
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Aug 29 20:16:16.681: INFO: created pod
Aug 29 20:16:16.681: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2295" to be "Succeeded or Failed"
Aug 29 20:16:16.685: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520044ms
Aug 29 20:16:18.690: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008679792s
Aug 29 20:16:20.691: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009600792s
STEP: Saw pod success 08/29/23 20:16:20.691
Aug 29 20:16:20.691: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 29 20:16:50.692: INFO: polling logs
Aug 29 20:16:50.715: INFO: Pod logs: 
I0829 20:16:17.684677       1 log.go:198] OK: Got token
I0829 20:16:17.684739       1 log.go:198] validating with in-cluster discovery
I0829 20:16:17.685687       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
I0829 20:16:17.685748       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2295:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693340776, NotBefore:1693340176, IssuedAt:1693340176, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2295", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a549531b-5e3f-4f6d-ba27-32eeac4e18a6"}}}
I0829 20:16:17.704527       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0829 20:16:17.716840       1 log.go:198] OK: Validated signature on JWT
I0829 20:16:17.717018       1 log.go:198] OK: Got valid claims from token!
I0829 20:16:17.717087       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2295:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693340776, NotBefore:1693340176, IssuedAt:1693340176, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2295", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a549531b-5e3f-4f6d-ba27-32eeac4e18a6"}}}

Aug 29 20:16:50.715: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 29 20:16:50.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-2295" for this suite. 08/29/23 20:16:50.729
------------------------------
• [SLOW TEST] [34.101 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:16:16.635
    Aug 29 20:16:16.635: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:16:16.636
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:16.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:16.662
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Aug 29 20:16:16.681: INFO: created pod
    Aug 29 20:16:16.681: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2295" to be "Succeeded or Failed"
    Aug 29 20:16:16.685: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.520044ms
    Aug 29 20:16:18.690: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008679792s
    Aug 29 20:16:20.691: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009600792s
    STEP: Saw pod success 08/29/23 20:16:20.691
    Aug 29 20:16:20.691: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 29 20:16:50.692: INFO: polling logs
    Aug 29 20:16:50.715: INFO: Pod logs: 
    I0829 20:16:17.684677       1 log.go:198] OK: Got token
    I0829 20:16:17.684739       1 log.go:198] validating with in-cluster discovery
    I0829 20:16:17.685687       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0829 20:16:17.685748       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2295:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693340776, NotBefore:1693340176, IssuedAt:1693340176, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2295", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a549531b-5e3f-4f6d-ba27-32eeac4e18a6"}}}
    I0829 20:16:17.704527       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0829 20:16:17.716840       1 log.go:198] OK: Validated signature on JWT
    I0829 20:16:17.717018       1 log.go:198] OK: Got valid claims from token!
    I0829 20:16:17.717087       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-2295:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1693340776, NotBefore:1693340176, IssuedAt:1693340176, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2295", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a549531b-5e3f-4f6d-ba27-32eeac4e18a6"}}}

    Aug 29 20:16:50.715: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:16:50.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-2295" for this suite. 08/29/23 20:16:50.729
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:16:50.738
Aug 29 20:16:50.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:16:50.739
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:50.759
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:50.763
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 08/29/23 20:16:50.766
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/29/23 20:16:50.773
STEP: patching the secret 08/29/23 20:16:50.779
STEP: deleting the secret using a LabelSelector 08/29/23 20:16:50.79
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/29/23 20:16:50.8
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:16:50.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-310" for this suite. 08/29/23 20:16:50.809
------------------------------
• [0.078 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:16:50.738
    Aug 29 20:16:50.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:16:50.739
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:50.759
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:50.763
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 08/29/23 20:16:50.766
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/29/23 20:16:50.773
    STEP: patching the secret 08/29/23 20:16:50.779
    STEP: deleting the secret using a LabelSelector 08/29/23 20:16:50.79
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/29/23 20:16:50.8
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:16:50.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-310" for this suite. 08/29/23 20:16:50.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:16:50.816
Aug 29 20:16:50.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:16:50.817
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:50.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:50.838
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-8450 08/29/23 20:16:50.841
STEP: creating service affinity-nodeport-transition in namespace services-8450 08/29/23 20:16:50.841
STEP: creating replication controller affinity-nodeport-transition in namespace services-8450 08/29/23 20:16:50.864
I0829 20:16:50.875605      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8450, replica count: 3
I0829 20:16:53.926149      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 20:16:53.941: INFO: Creating new exec pod
Aug 29 20:16:53.950: INFO: Waiting up to 5m0s for pod "execpod-affinityj4h94" in namespace "services-8450" to be "running"
Aug 29 20:16:53.953: INFO: Pod "execpod-affinityj4h94": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302979ms
Aug 29 20:16:55.958: INFO: Pod "execpod-affinityj4h94": Phase="Running", Reason="", readiness=true. Elapsed: 2.007935218s
Aug 29 20:16:55.958: INFO: Pod "execpod-affinityj4h94" satisfied condition "running"
Aug 29 20:16:56.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Aug 29 20:16:57.129: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 29 20:16:57.129: INFO: stdout: ""
Aug 29 20:16:57.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c nc -v -z -w 2 172.19.74.217 80'
Aug 29 20:16:57.305: INFO: stderr: "+ nc -v -z -w 2 172.19.74.217 80\nConnection to 172.19.74.217 80 port [tcp/http] succeeded!\n"
Aug 29 20:16:57.305: INFO: stdout: ""
Aug 29 20:16:57.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.206 31440'
Aug 29 20:16:57.464: INFO: stderr: "+ nc -v -z -w 2 10.45.35.206 31440\nConnection to 10.45.35.206 31440 port [tcp/*] succeeded!\n"
Aug 29 20:16:57.464: INFO: stdout: ""
Aug 29 20:16:57.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.199 31440'
Aug 29 20:16:57.628: INFO: stderr: "+ nc -v -z -w 2 10.45.35.199 31440\nConnection to 10.45.35.199 31440 port [tcp/*] succeeded!\n"
Aug 29 20:16:57.628: INFO: stdout: ""
Aug 29 20:16:57.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:31440/ ; done'
Aug 29 20:16:57.911: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n"
Aug 29 20:16:57.911: INFO: stdout: "\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p"
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:27.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:31440/ ; done'
Aug 29 20:17:28.183: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n"
Aug 29 20:17:28.183: INFO: stdout: "\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9"
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-sfzj9
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-sfzj9
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-sfzj9
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-sfzj9
Aug 29 20:17:28.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:31440/ ; done'
Aug 29 20:17:28.481: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n"
Aug 29 20:17:28.481: INFO: stdout: "\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67"
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-sfzj9
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-sfzj9
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-sfzj9
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-sfzj9
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
Aug 29 20:17:58.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:31440/ ; done'
Aug 29 20:17:58.755: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n"
Aug 29 20:17:58.755: INFO: stdout: "\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p"
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
Aug 29 20:17:58.755: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8450, will wait for the garbage collector to delete the pods 08/29/23 20:17:58.776
Aug 29 20:17:58.840: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.523798ms
Aug 29 20:17:58.941: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.883641ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:01.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8450" for this suite. 08/29/23 20:18:01.384
------------------------------
• [SLOW TEST] [70.576 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:16:50.816
    Aug 29 20:16:50.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:16:50.817
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:16:50.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:16:50.838
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-8450 08/29/23 20:16:50.841
    STEP: creating service affinity-nodeport-transition in namespace services-8450 08/29/23 20:16:50.841
    STEP: creating replication controller affinity-nodeport-transition in namespace services-8450 08/29/23 20:16:50.864
    I0829 20:16:50.875605      19 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-8450, replica count: 3
    I0829 20:16:53.926149      19 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 20:16:53.941: INFO: Creating new exec pod
    Aug 29 20:16:53.950: INFO: Waiting up to 5m0s for pod "execpod-affinityj4h94" in namespace "services-8450" to be "running"
    Aug 29 20:16:53.953: INFO: Pod "execpod-affinityj4h94": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302979ms
    Aug 29 20:16:55.958: INFO: Pod "execpod-affinityj4h94": Phase="Running", Reason="", readiness=true. Elapsed: 2.007935218s
    Aug 29 20:16:55.958: INFO: Pod "execpod-affinityj4h94" satisfied condition "running"
    Aug 29 20:16:56.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Aug 29 20:16:57.129: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 29 20:16:57.129: INFO: stdout: ""
    Aug 29 20:16:57.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c nc -v -z -w 2 172.19.74.217 80'
    Aug 29 20:16:57.305: INFO: stderr: "+ nc -v -z -w 2 172.19.74.217 80\nConnection to 172.19.74.217 80 port [tcp/http] succeeded!\n"
    Aug 29 20:16:57.305: INFO: stdout: ""
    Aug 29 20:16:57.305: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.206 31440'
    Aug 29 20:16:57.464: INFO: stderr: "+ nc -v -z -w 2 10.45.35.206 31440\nConnection to 10.45.35.206 31440 port [tcp/*] succeeded!\n"
    Aug 29 20:16:57.464: INFO: stdout: ""
    Aug 29 20:16:57.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.199 31440'
    Aug 29 20:16:57.628: INFO: stderr: "+ nc -v -z -w 2 10.45.35.199 31440\nConnection to 10.45.35.199 31440 port [tcp/*] succeeded!\n"
    Aug 29 20:16:57.628: INFO: stdout: ""
    Aug 29 20:16:57.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:31440/ ; done'
    Aug 29 20:16:57.911: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n"
    Aug 29 20:16:57.911: INFO: stdout: "\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p"
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:16:57.911: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:27.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:31440/ ; done'
    Aug 29 20:17:28.183: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n"
    Aug 29 20:17:28.183: INFO: stdout: "\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9"
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-sfzj9
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-sfzj9
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-sfzj9
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.183: INFO: Received response from host: affinity-nodeport-transition-sfzj9
    Aug 29 20:17:28.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:31440/ ; done'
    Aug 29 20:17:28.481: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n"
    Aug 29 20:17:28.481: INFO: stdout: "\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-sfzj9\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67\naffinity-nodeport-transition-wxc67"
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-sfzj9
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-sfzj9
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-sfzj9
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-sfzj9
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:28.481: INFO: Received response from host: affinity-nodeport-transition-wxc67
    Aug 29 20:17:58.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8450 exec execpod-affinityj4h94 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.45.35.202:31440/ ; done'
    Aug 29 20:17:58.755: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.45.35.202:31440/\n"
    Aug 29 20:17:58.755: INFO: stdout: "\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p\naffinity-nodeport-transition-qv74p"
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Received response from host: affinity-nodeport-transition-qv74p
    Aug 29 20:17:58.755: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-8450, will wait for the garbage collector to delete the pods 08/29/23 20:17:58.776
    Aug 29 20:17:58.840: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.523798ms
    Aug 29 20:17:58.941: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.883641ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:01.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8450" for this suite. 08/29/23 20:18:01.384
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:01.393
Aug 29 20:18:01.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:18:01.394
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:01.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:01.416
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 08/29/23 20:18:01.419
Aug 29 20:18:01.430: INFO: Waiting up to 5m0s for pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b" in namespace "projected-8106" to be "running and ready"
Aug 29 20:18:01.434: INFO: Pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624362ms
Aug 29 20:18:01.434: INFO: The phase of Pod labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:18:03.440: INFO: Pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009749402s
Aug 29 20:18:03.440: INFO: The phase of Pod labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b is Running (Ready = true)
Aug 29 20:18:03.440: INFO: Pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b" satisfied condition "running and ready"
Aug 29 20:18:03.969: INFO: Successfully updated pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:07.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8106" for this suite. 08/29/23 20:18:08.002
------------------------------
• [SLOW TEST] [6.619 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:01.393
    Aug 29 20:18:01.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:18:01.394
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:01.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:01.416
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 08/29/23 20:18:01.419
    Aug 29 20:18:01.430: INFO: Waiting up to 5m0s for pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b" in namespace "projected-8106" to be "running and ready"
    Aug 29 20:18:01.434: INFO: Pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624362ms
    Aug 29 20:18:01.434: INFO: The phase of Pod labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:18:03.440: INFO: Pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009749402s
    Aug 29 20:18:03.440: INFO: The phase of Pod labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b is Running (Ready = true)
    Aug 29 20:18:03.440: INFO: Pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b" satisfied condition "running and ready"
    Aug 29 20:18:03.969: INFO: Successfully updated pod "labelsupdate05a6152f-e35b-49d7-b732-4fc4e3590e1b"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:07.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8106" for this suite. 08/29/23 20:18:08.002
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:08.013
Aug 29 20:18:08.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:18:08.014
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:08.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:08.039
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 08/29/23 20:18:08.042
Aug 29 20:18:08.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 create -f -'
Aug 29 20:18:08.953: INFO: stderr: ""
Aug 29 20:18:08.953: INFO: stdout: "pod/pause created\n"
Aug 29 20:18:08.953: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 29 20:18:08.953: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7759" to be "running and ready"
Aug 29 20:18:08.957: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83101ms
Aug 29 20:18:08.957: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'loki-15bd39-worker-1' to be 'Running' but was 'Pending'
Aug 29 20:18:10.962: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009038091s
Aug 29 20:18:10.962: INFO: Pod "pause" satisfied condition "running and ready"
Aug 29 20:18:10.962: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 08/29/23 20:18:10.962
Aug 29 20:18:10.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 label pods pause testing-label=testing-label-value'
Aug 29 20:18:11.072: INFO: stderr: ""
Aug 29 20:18:11.072: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/29/23 20:18:11.072
Aug 29 20:18:11.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 get pod pause -L testing-label'
Aug 29 20:18:11.158: INFO: stderr: ""
Aug 29 20:18:11.158: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/29/23 20:18:11.158
Aug 29 20:18:11.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 label pods pause testing-label-'
Aug 29 20:18:11.266: INFO: stderr: ""
Aug 29 20:18:11.266: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/29/23 20:18:11.266
Aug 29 20:18:11.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 get pod pause -L testing-label'
Aug 29 20:18:11.358: INFO: stderr: ""
Aug 29 20:18:11.358: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 08/29/23 20:18:11.358
Aug 29 20:18:11.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 delete --grace-period=0 --force -f -'
Aug 29 20:18:11.454: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 20:18:11.454: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 29 20:18:11.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 get rc,svc -l name=pause --no-headers'
Aug 29 20:18:11.547: INFO: stderr: "No resources found in kubectl-7759 namespace.\n"
Aug 29 20:18:11.547: INFO: stdout: ""
Aug 29 20:18:11.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 20:18:11.642: INFO: stderr: ""
Aug 29 20:18:11.642: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:11.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7759" for this suite. 08/29/23 20:18:11.648
------------------------------
• [3.643 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:08.013
    Aug 29 20:18:08.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:18:08.014
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:08.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:08.039
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 08/29/23 20:18:08.042
    Aug 29 20:18:08.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 create -f -'
    Aug 29 20:18:08.953: INFO: stderr: ""
    Aug 29 20:18:08.953: INFO: stdout: "pod/pause created\n"
    Aug 29 20:18:08.953: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 29 20:18:08.953: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-7759" to be "running and ready"
    Aug 29 20:18:08.957: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.83101ms
    Aug 29 20:18:08.957: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'loki-15bd39-worker-1' to be 'Running' but was 'Pending'
    Aug 29 20:18:10.962: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009038091s
    Aug 29 20:18:10.962: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 29 20:18:10.962: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 08/29/23 20:18:10.962
    Aug 29 20:18:10.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 label pods pause testing-label=testing-label-value'
    Aug 29 20:18:11.072: INFO: stderr: ""
    Aug 29 20:18:11.072: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/29/23 20:18:11.072
    Aug 29 20:18:11.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 get pod pause -L testing-label'
    Aug 29 20:18:11.158: INFO: stderr: ""
    Aug 29 20:18:11.158: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/29/23 20:18:11.158
    Aug 29 20:18:11.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 label pods pause testing-label-'
    Aug 29 20:18:11.266: INFO: stderr: ""
    Aug 29 20:18:11.266: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/29/23 20:18:11.266
    Aug 29 20:18:11.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 get pod pause -L testing-label'
    Aug 29 20:18:11.358: INFO: stderr: ""
    Aug 29 20:18:11.358: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 08/29/23 20:18:11.358
    Aug 29 20:18:11.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 delete --grace-period=0 --force -f -'
    Aug 29 20:18:11.454: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 20:18:11.454: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 29 20:18:11.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 get rc,svc -l name=pause --no-headers'
    Aug 29 20:18:11.547: INFO: stderr: "No resources found in kubectl-7759 namespace.\n"
    Aug 29 20:18:11.547: INFO: stdout: ""
    Aug 29 20:18:11.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-7759 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 29 20:18:11.642: INFO: stderr: ""
    Aug 29 20:18:11.642: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:11.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7759" for this suite. 08/29/23 20:18:11.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:11.656
Aug 29 20:18:11.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:18:11.657
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:11.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:11.681
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/29/23 20:18:11.684
Aug 29 20:18:11.694: INFO: Waiting up to 5m0s for pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857" in namespace "emptydir-9430" to be "Succeeded or Failed"
Aug 29 20:18:11.699: INFO: Pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857": Phase="Pending", Reason="", readiness=false. Elapsed: 4.287371ms
Aug 29 20:18:13.704: INFO: Pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009393356s
Aug 29 20:18:15.705: INFO: Pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01034077s
STEP: Saw pod success 08/29/23 20:18:15.705
Aug 29 20:18:15.705: INFO: Pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857" satisfied condition "Succeeded or Failed"
Aug 29 20:18:15.709: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-d5e0ab55-5399-4c9b-9a47-bed79f581857 container test-container: <nil>
STEP: delete the pod 08/29/23 20:18:15.716
Aug 29 20:18:15.732: INFO: Waiting for pod pod-d5e0ab55-5399-4c9b-9a47-bed79f581857 to disappear
Aug 29 20:18:15.735: INFO: Pod pod-d5e0ab55-5399-4c9b-9a47-bed79f581857 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:15.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9430" for this suite. 08/29/23 20:18:15.739
------------------------------
• [4.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:11.656
    Aug 29 20:18:11.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:18:11.657
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:11.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:11.681
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/29/23 20:18:11.684
    Aug 29 20:18:11.694: INFO: Waiting up to 5m0s for pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857" in namespace "emptydir-9430" to be "Succeeded or Failed"
    Aug 29 20:18:11.699: INFO: Pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857": Phase="Pending", Reason="", readiness=false. Elapsed: 4.287371ms
    Aug 29 20:18:13.704: INFO: Pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009393356s
    Aug 29 20:18:15.705: INFO: Pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01034077s
    STEP: Saw pod success 08/29/23 20:18:15.705
    Aug 29 20:18:15.705: INFO: Pod "pod-d5e0ab55-5399-4c9b-9a47-bed79f581857" satisfied condition "Succeeded or Failed"
    Aug 29 20:18:15.709: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-d5e0ab55-5399-4c9b-9a47-bed79f581857 container test-container: <nil>
    STEP: delete the pod 08/29/23 20:18:15.716
    Aug 29 20:18:15.732: INFO: Waiting for pod pod-d5e0ab55-5399-4c9b-9a47-bed79f581857 to disappear
    Aug 29 20:18:15.735: INFO: Pod pod-d5e0ab55-5399-4c9b-9a47-bed79f581857 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:15.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9430" for this suite. 08/29/23 20:18:15.739
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:15.748
Aug 29 20:18:15.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename gc 08/29/23 20:18:15.749
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:15.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:15.77
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/29/23 20:18:15.779
STEP: delete the rc 08/29/23 20:18:20.829
STEP: wait for the rc to be deleted 08/29/23 20:18:20.893
Aug 29 20:18:21.940: INFO: 84 pods remaining
Aug 29 20:18:21.940: INFO: 83 pods has nil DeletionTimestamp
Aug 29 20:18:21.940: INFO: 
Aug 29 20:18:22.926: INFO: 75 pods remaining
Aug 29 20:18:22.926: INFO: 75 pods has nil DeletionTimestamp
Aug 29 20:18:22.926: INFO: 
Aug 29 20:18:23.926: INFO: 60 pods remaining
Aug 29 20:18:23.926: INFO: 60 pods has nil DeletionTimestamp
Aug 29 20:18:23.926: INFO: 
Aug 29 20:18:24.919: INFO: 42 pods remaining
Aug 29 20:18:24.919: INFO: 42 pods has nil DeletionTimestamp
Aug 29 20:18:24.919: INFO: 
Aug 29 20:18:25.914: INFO: 36 pods remaining
Aug 29 20:18:25.914: INFO: 36 pods has nil DeletionTimestamp
Aug 29 20:18:25.914: INFO: 
Aug 29 20:18:26.911: INFO: 20 pods remaining
Aug 29 20:18:26.911: INFO: 20 pods has nil DeletionTimestamp
Aug 29 20:18:26.911: INFO: 
Aug 29 20:18:27.904: INFO: 0 pods remaining
Aug 29 20:18:27.904: INFO: 0 pods has nil DeletionTimestamp
Aug 29 20:18:27.904: INFO: 
STEP: Gathering metrics 08/29/23 20:18:28.91
W0829 20:18:28.920660      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 20:18:28.920: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:28.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1039" for this suite. 08/29/23 20:18:28.924
------------------------------
• [SLOW TEST] [13.187 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:15.748
    Aug 29 20:18:15.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename gc 08/29/23 20:18:15.749
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:15.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:15.77
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/29/23 20:18:15.779
    STEP: delete the rc 08/29/23 20:18:20.829
    STEP: wait for the rc to be deleted 08/29/23 20:18:20.893
    Aug 29 20:18:21.940: INFO: 84 pods remaining
    Aug 29 20:18:21.940: INFO: 83 pods has nil DeletionTimestamp
    Aug 29 20:18:21.940: INFO: 
    Aug 29 20:18:22.926: INFO: 75 pods remaining
    Aug 29 20:18:22.926: INFO: 75 pods has nil DeletionTimestamp
    Aug 29 20:18:22.926: INFO: 
    Aug 29 20:18:23.926: INFO: 60 pods remaining
    Aug 29 20:18:23.926: INFO: 60 pods has nil DeletionTimestamp
    Aug 29 20:18:23.926: INFO: 
    Aug 29 20:18:24.919: INFO: 42 pods remaining
    Aug 29 20:18:24.919: INFO: 42 pods has nil DeletionTimestamp
    Aug 29 20:18:24.919: INFO: 
    Aug 29 20:18:25.914: INFO: 36 pods remaining
    Aug 29 20:18:25.914: INFO: 36 pods has nil DeletionTimestamp
    Aug 29 20:18:25.914: INFO: 
    Aug 29 20:18:26.911: INFO: 20 pods remaining
    Aug 29 20:18:26.911: INFO: 20 pods has nil DeletionTimestamp
    Aug 29 20:18:26.911: INFO: 
    Aug 29 20:18:27.904: INFO: 0 pods remaining
    Aug 29 20:18:27.904: INFO: 0 pods has nil DeletionTimestamp
    Aug 29 20:18:27.904: INFO: 
    STEP: Gathering metrics 08/29/23 20:18:28.91
    W0829 20:18:28.920660      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 29 20:18:28.920: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:28.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1039" for this suite. 08/29/23 20:18:28.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:28.936
Aug 29 20:18:28.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:18:28.937
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:28.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:28.963
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 08/29/23 20:18:28.966
STEP: watching for the ServiceAccount to be added 08/29/23 20:18:28.978
STEP: patching the ServiceAccount 08/29/23 20:18:28.98
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/29/23 20:18:28.987
STEP: deleting the ServiceAccount 08/29/23 20:18:28.991
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:29.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3203" for this suite. 08/29/23 20:18:29.013
------------------------------
• [0.087 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:28.936
    Aug 29 20:18:28.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:18:28.937
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:28.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:28.963
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 08/29/23 20:18:28.966
    STEP: watching for the ServiceAccount to be added 08/29/23 20:18:28.978
    STEP: patching the ServiceAccount 08/29/23 20:18:28.98
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/29/23 20:18:28.987
    STEP: deleting the ServiceAccount 08/29/23 20:18:28.991
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:29.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3203" for this suite. 08/29/23 20:18:29.013
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:29.023
Aug 29 20:18:29.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename proxy 08/29/23 20:18:29.025
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:29.045
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:29.048
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 29 20:18:29.052: INFO: Creating pod...
Aug 29 20:18:29.064: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5157" to be "running"
Aug 29 20:18:29.068: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525603ms
Aug 29 20:18:31.073: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008800409s
Aug 29 20:18:33.073: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008900113s
Aug 29 20:18:35.073: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 6.008604761s
Aug 29 20:18:35.073: INFO: Pod "agnhost" satisfied condition "running"
Aug 29 20:18:35.073: INFO: Creating service...
Aug 29 20:18:35.086: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=DELETE
Aug 29 20:18:35.091: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 20:18:35.091: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=OPTIONS
Aug 29 20:18:35.099: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 20:18:35.099: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=PATCH
Aug 29 20:18:35.104: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 20:18:35.104: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=POST
Aug 29 20:18:35.107: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 20:18:35.107: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=PUT
Aug 29 20:18:35.113: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 29 20:18:35.113: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 29 20:18:35.119: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 29 20:18:35.119: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 29 20:18:35.128: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 29 20:18:35.128: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 29 20:18:35.135: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 29 20:18:35.135: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=POST
Aug 29 20:18:35.142: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 29 20:18:35.142: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=PUT
Aug 29 20:18:35.148: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 29 20:18:35.148: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=GET
Aug 29 20:18:35.150: INFO: http.Client request:GET StatusCode:301
Aug 29 20:18:35.150: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=GET
Aug 29 20:18:35.154: INFO: http.Client request:GET StatusCode:301
Aug 29 20:18:35.154: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=HEAD
Aug 29 20:18:35.156: INFO: http.Client request:HEAD StatusCode:301
Aug 29 20:18:35.156: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 29 20:18:35.161: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:35.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-5157" for this suite. 08/29/23 20:18:35.167
------------------------------
• [SLOW TEST] [6.152 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:29.023
    Aug 29 20:18:29.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename proxy 08/29/23 20:18:29.025
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:29.045
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:29.048
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 29 20:18:29.052: INFO: Creating pod...
    Aug 29 20:18:29.064: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-5157" to be "running"
    Aug 29 20:18:29.068: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525603ms
    Aug 29 20:18:31.073: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008800409s
    Aug 29 20:18:33.073: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008900113s
    Aug 29 20:18:35.073: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 6.008604761s
    Aug 29 20:18:35.073: INFO: Pod "agnhost" satisfied condition "running"
    Aug 29 20:18:35.073: INFO: Creating service...
    Aug 29 20:18:35.086: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=DELETE
    Aug 29 20:18:35.091: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 29 20:18:35.091: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=OPTIONS
    Aug 29 20:18:35.099: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 29 20:18:35.099: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=PATCH
    Aug 29 20:18:35.104: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 29 20:18:35.104: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=POST
    Aug 29 20:18:35.107: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 29 20:18:35.107: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=PUT
    Aug 29 20:18:35.113: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 29 20:18:35.113: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 29 20:18:35.119: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 29 20:18:35.119: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 29 20:18:35.128: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 29 20:18:35.128: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 29 20:18:35.135: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 29 20:18:35.135: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=POST
    Aug 29 20:18:35.142: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 29 20:18:35.142: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 29 20:18:35.148: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 29 20:18:35.148: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=GET
    Aug 29 20:18:35.150: INFO: http.Client request:GET StatusCode:301
    Aug 29 20:18:35.150: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=GET
    Aug 29 20:18:35.154: INFO: http.Client request:GET StatusCode:301
    Aug 29 20:18:35.154: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/pods/agnhost/proxy?method=HEAD
    Aug 29 20:18:35.156: INFO: http.Client request:HEAD StatusCode:301
    Aug 29 20:18:35.156: INFO: Starting http.Client for https://172.19.0.1:443/api/v1/namespaces/proxy-5157/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 29 20:18:35.161: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:35.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-5157" for this suite. 08/29/23 20:18:35.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:35.176
Aug 29 20:18:35.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir-wrapper 08/29/23 20:18:35.177
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:35.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:35.196
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 29 20:18:35.225: INFO: Waiting up to 5m0s for pod "pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb" in namespace "emptydir-wrapper-6631" to be "running and ready"
Aug 29 20:18:35.228: INFO: Pod "pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.47516ms
Aug 29 20:18:35.228: INFO: The phase of Pod pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:18:37.233: INFO: Pod "pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007887069s
Aug 29 20:18:37.233: INFO: The phase of Pod pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb is Running (Ready = true)
Aug 29 20:18:37.233: INFO: Pod "pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/29/23 20:18:37.235
STEP: Cleaning up the configmap 08/29/23 20:18:37.244
STEP: Cleaning up the pod 08/29/23 20:18:37.251
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:37.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-6631" for this suite. 08/29/23 20:18:37.273
------------------------------
• [2.106 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:35.176
    Aug 29 20:18:35.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir-wrapper 08/29/23 20:18:35.177
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:35.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:35.196
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 29 20:18:35.225: INFO: Waiting up to 5m0s for pod "pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb" in namespace "emptydir-wrapper-6631" to be "running and ready"
    Aug 29 20:18:35.228: INFO: Pod "pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.47516ms
    Aug 29 20:18:35.228: INFO: The phase of Pod pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:18:37.233: INFO: Pod "pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb": Phase="Running", Reason="", readiness=true. Elapsed: 2.007887069s
    Aug 29 20:18:37.233: INFO: The phase of Pod pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb is Running (Ready = true)
    Aug 29 20:18:37.233: INFO: Pod "pod-secrets-4c358679-5143-4163-a207-0d0f916c1fdb" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/29/23 20:18:37.235
    STEP: Cleaning up the configmap 08/29/23 20:18:37.244
    STEP: Cleaning up the pod 08/29/23 20:18:37.251
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:37.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-6631" for this suite. 08/29/23 20:18:37.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:37.282
Aug 29 20:18:37.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replicaset 08/29/23 20:18:37.283
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:37.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:37.31
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 29 20:18:37.328: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 20:18:42.332: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/29/23 20:18:42.332
STEP: Scaling up "test-rs" replicaset  08/29/23 20:18:42.332
Aug 29 20:18:42.342: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/29/23 20:18:42.342
W0829 20:18:42.355563      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 29 20:18:42.356: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 20:18:42.402: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 20:18:42.440: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 20:18:42.450: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 1, AvailableReplicas 1
Aug 29 20:18:43.549: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 2, AvailableReplicas 2
Aug 29 20:18:43.596: INFO: observed Replicaset test-rs in namespace replicaset-5462 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:43.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5462" for this suite. 08/29/23 20:18:43.602
------------------------------
• [SLOW TEST] [6.327 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:37.282
    Aug 29 20:18:37.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replicaset 08/29/23 20:18:37.283
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:37.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:37.31
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 29 20:18:37.328: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 29 20:18:42.332: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/29/23 20:18:42.332
    STEP: Scaling up "test-rs" replicaset  08/29/23 20:18:42.332
    Aug 29 20:18:42.342: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/29/23 20:18:42.342
    W0829 20:18:42.355563      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 29 20:18:42.356: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 1, AvailableReplicas 1
    Aug 29 20:18:42.402: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 1, AvailableReplicas 1
    Aug 29 20:18:42.440: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 1, AvailableReplicas 1
    Aug 29 20:18:42.450: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 1, AvailableReplicas 1
    Aug 29 20:18:43.549: INFO: observed ReplicaSet test-rs in namespace replicaset-5462 with ReadyReplicas 2, AvailableReplicas 2
    Aug 29 20:18:43.596: INFO: observed Replicaset test-rs in namespace replicaset-5462 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:43.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5462" for this suite. 08/29/23 20:18:43.602
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:43.609
Aug 29 20:18:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:18:43.611
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:43.63
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:43.633
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4691 08/29/23 20:18:43.637
STEP: changing the ExternalName service to type=NodePort 08/29/23 20:18:43.642
STEP: creating replication controller externalname-service in namespace services-4691 08/29/23 20:18:43.674
I0829 20:18:43.682145      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4691, replica count: 2
I0829 20:18:46.732859      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 20:18:46.732: INFO: Creating new exec pod
Aug 29 20:18:46.745: INFO: Waiting up to 5m0s for pod "execpod85pt7" in namespace "services-4691" to be "running"
Aug 29 20:18:46.749: INFO: Pod "execpod85pt7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354522ms
Aug 29 20:18:48.755: INFO: Pod "execpod85pt7": Phase="Running", Reason="", readiness=true. Elapsed: 2.0098534s
Aug 29 20:18:48.755: INFO: Pod "execpod85pt7" satisfied condition "running"
Aug 29 20:18:49.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-4691 exec execpod85pt7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Aug 29 20:18:49.940: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 29 20:18:49.941: INFO: stdout: ""
Aug 29 20:18:49.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-4691 exec execpod85pt7 -- /bin/sh -x -c nc -v -z -w 2 172.19.45.205 80'
Aug 29 20:18:50.119: INFO: stderr: "+ nc -v -z -w 2 172.19.45.205 80\nConnection to 172.19.45.205 80 port [tcp/http] succeeded!\n"
Aug 29 20:18:50.119: INFO: stdout: ""
Aug 29 20:18:50.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-4691 exec execpod85pt7 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.206 30014'
Aug 29 20:18:50.290: INFO: stderr: "+ nc -v -z -w 2 10.45.35.206 30014\nConnection to 10.45.35.206 30014 port [tcp/*] succeeded!\n"
Aug 29 20:18:50.290: INFO: stdout: ""
Aug 29 20:18:50.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-4691 exec execpod85pt7 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.202 30014'
Aug 29 20:18:50.495: INFO: stderr: "+ nc -v -z -w 2 10.45.35.202 30014\nConnection to 10.45.35.202 30014 port [tcp/*] succeeded!\n"
Aug 29 20:18:50.495: INFO: stdout: ""
Aug 29 20:18:50.495: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:50.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4691" for this suite. 08/29/23 20:18:50.527
------------------------------
• [SLOW TEST] [6.927 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:43.609
    Aug 29 20:18:43.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:18:43.611
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:43.63
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:43.633
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4691 08/29/23 20:18:43.637
    STEP: changing the ExternalName service to type=NodePort 08/29/23 20:18:43.642
    STEP: creating replication controller externalname-service in namespace services-4691 08/29/23 20:18:43.674
    I0829 20:18:43.682145      19 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4691, replica count: 2
    I0829 20:18:46.732859      19 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 20:18:46.732: INFO: Creating new exec pod
    Aug 29 20:18:46.745: INFO: Waiting up to 5m0s for pod "execpod85pt7" in namespace "services-4691" to be "running"
    Aug 29 20:18:46.749: INFO: Pod "execpod85pt7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.354522ms
    Aug 29 20:18:48.755: INFO: Pod "execpod85pt7": Phase="Running", Reason="", readiness=true. Elapsed: 2.0098534s
    Aug 29 20:18:48.755: INFO: Pod "execpod85pt7" satisfied condition "running"
    Aug 29 20:18:49.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-4691 exec execpod85pt7 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Aug 29 20:18:49.940: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 29 20:18:49.941: INFO: stdout: ""
    Aug 29 20:18:49.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-4691 exec execpod85pt7 -- /bin/sh -x -c nc -v -z -w 2 172.19.45.205 80'
    Aug 29 20:18:50.119: INFO: stderr: "+ nc -v -z -w 2 172.19.45.205 80\nConnection to 172.19.45.205 80 port [tcp/http] succeeded!\n"
    Aug 29 20:18:50.119: INFO: stdout: ""
    Aug 29 20:18:50.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-4691 exec execpod85pt7 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.206 30014'
    Aug 29 20:18:50.290: INFO: stderr: "+ nc -v -z -w 2 10.45.35.206 30014\nConnection to 10.45.35.206 30014 port [tcp/*] succeeded!\n"
    Aug 29 20:18:50.290: INFO: stdout: ""
    Aug 29 20:18:50.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-4691 exec execpod85pt7 -- /bin/sh -x -c nc -v -z -w 2 10.45.35.202 30014'
    Aug 29 20:18:50.495: INFO: stderr: "+ nc -v -z -w 2 10.45.35.202 30014\nConnection to 10.45.35.202 30014 port [tcp/*] succeeded!\n"
    Aug 29 20:18:50.495: INFO: stdout: ""
    Aug 29 20:18:50.495: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:50.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4691" for this suite. 08/29/23 20:18:50.527
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:50.537
Aug 29 20:18:50.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename deployment 08/29/23 20:18:50.538
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:50.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:50.561
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 29 20:18:50.565: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 29 20:18:50.579: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 20:18:55.584: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/29/23 20:18:55.584
Aug 29 20:18:55.584: INFO: Creating deployment "test-rolling-update-deployment"
Aug 29 20:18:55.593: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 29 20:18:55.600: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 29 20:18:57.612: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 29 20:18:57.616: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 20:18:57.630: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5820  857b2f31-caef-4170-ad5c-23de96c20e9a 29390 1 2023-08-29 20:18:55 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-29 20:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004599e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-29 20:18:55 +0000 UTC,LastTransitionTime:2023-08-29 20:18:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-29 20:18:57 +0000 UTC,LastTransitionTime:2023-08-29 20:18:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 20:18:57.634: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-5820  298e546a-4270-4cdb-9dde-6fd763e63b24 29377 1 2023-08-29 20:18:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 857b2f31-caef-4170-ad5c-23de96c20e9a 0xc0048e2327 0xc0048e2328}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"857b2f31-caef-4170-ad5c-23de96c20e9a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048e23d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 20:18:57.634: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 29 20:18:57.635: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5820  089cc1b8-0ef6-4385-86ad-13a029f6f03e 29389 2 2023-08-29 20:18:50 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 857b2f31-caef-4170-ad5c-23de96c20e9a 0xc0048e21f7 0xc0048e21f8}] [] [{e2e.test Update apps/v1 2023-08-29 20:18:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"857b2f31-caef-4170-ad5c-23de96c20e9a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0048e22b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 20:18:57.639: INFO: Pod "test-rolling-update-deployment-7549d9f46d-wmjv8" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-wmjv8 test-rolling-update-deployment-7549d9f46d- deployment-5820  0b06a2a3-32fb-412d-9cdc-dde8ab94dfb0 29376 0 2023-08-29 20:18:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:13c668d07df9101120da9bfc358d0ab09018fd5bdc0fb469b54c54f420db5320 cni.projectcalico.org/podIP:172.20.30.143/32 cni.projectcalico.org/podIPs:172.20.30.143/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 298e546a-4270-4cdb-9dde-6fd763e63b24 0xc0048e2857 0xc0048e2858}] [] [{kube-controller-manager Update v1 2023-08-29 20:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"298e546a-4270-4cdb-9dde-6fd763e63b24\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 20:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hqslt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hqslt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.143,StartTime:2023-08-29 20:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://521e7fa8ca8570d092d39e4aefe0ca190d1d7d9c2249db069db9cbd5de807431,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 29 20:18:57.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5820" for this suite. 08/29/23 20:18:57.644
------------------------------
• [SLOW TEST] [7.115 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:50.537
    Aug 29 20:18:50.537: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename deployment 08/29/23 20:18:50.538
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:50.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:50.561
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 29 20:18:50.565: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 29 20:18:50.579: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 29 20:18:55.584: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/29/23 20:18:55.584
    Aug 29 20:18:55.584: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 29 20:18:55.593: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 29 20:18:55.600: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 29 20:18:57.612: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 29 20:18:57.616: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 29 20:18:57.630: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-5820  857b2f31-caef-4170-ad5c-23de96c20e9a 29390 1 2023-08-29 20:18:55 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-29 20:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004599e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-29 20:18:55 +0000 UTC,LastTransitionTime:2023-08-29 20:18:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-08-29 20:18:57 +0000 UTC,LastTransitionTime:2023-08-29 20:18:55 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 29 20:18:57.634: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-5820  298e546a-4270-4cdb-9dde-6fd763e63b24 29377 1 2023-08-29 20:18:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 857b2f31-caef-4170-ad5c-23de96c20e9a 0xc0048e2327 0xc0048e2328}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"857b2f31-caef-4170-ad5c-23de96c20e9a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0048e23d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 20:18:57.634: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 29 20:18:57.635: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-5820  089cc1b8-0ef6-4385-86ad-13a029f6f03e 29389 2 2023-08-29 20:18:50 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 857b2f31-caef-4170-ad5c-23de96c20e9a 0xc0048e21f7 0xc0048e21f8}] [] [{e2e.test Update apps/v1 2023-08-29 20:18:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"857b2f31-caef-4170-ad5c-23de96c20e9a\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0048e22b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 20:18:57.639: INFO: Pod "test-rolling-update-deployment-7549d9f46d-wmjv8" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-wmjv8 test-rolling-update-deployment-7549d9f46d- deployment-5820  0b06a2a3-32fb-412d-9cdc-dde8ab94dfb0 29376 0 2023-08-29 20:18:55 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[cni.projectcalico.org/containerID:13c668d07df9101120da9bfc358d0ab09018fd5bdc0fb469b54c54f420db5320 cni.projectcalico.org/podIP:172.20.30.143/32 cni.projectcalico.org/podIPs:172.20.30.143/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 298e546a-4270-4cdb-9dde-6fd763e63b24 0xc0048e2857 0xc0048e2858}] [] [{kube-controller-manager Update v1 2023-08-29 20:18:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"298e546a-4270-4cdb-9dde-6fd763e63b24\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 20:18:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 20:18:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.143\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hqslt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hqslt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:18:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:18:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:18:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.143,StartTime:2023-08-29 20:18:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:18:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://521e7fa8ca8570d092d39e4aefe0ca190d1d7d9c2249db069db9cbd5de807431,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.143,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:18:57.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5820" for this suite. 08/29/23 20:18:57.644
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:18:57.652
Aug 29 20:18:57.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename subpath 08/29/23 20:18:57.654
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:57.671
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:57.675
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/29/23 20:18:57.678
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-q5k9 08/29/23 20:18:57.688
STEP: Creating a pod to test atomic-volume-subpath 08/29/23 20:18:57.688
Aug 29 20:18:57.698: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q5k9" in namespace "subpath-5635" to be "Succeeded or Failed"
Aug 29 20:18:57.702: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.460643ms
Aug 29 20:18:59.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007997454s
Aug 29 20:19:01.708: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009681658s
Aug 29 20:19:03.706: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 6.007501827s
Aug 29 20:19:05.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 8.008866595s
Aug 29 20:19:07.708: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009822354s
Aug 29 20:19:09.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 12.008217476s
Aug 29 20:19:11.709: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 14.009946436s
Aug 29 20:19:13.706: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 16.007822974s
Aug 29 20:19:15.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 18.008002056s
Aug 29 20:19:17.709: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 20.010059135s
Aug 29 20:19:19.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=false. Elapsed: 22.008162723s
Aug 29 20:19:21.714: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015475742s
STEP: Saw pod success 08/29/23 20:19:21.714
Aug 29 20:19:21.714: INFO: Pod "pod-subpath-test-projected-q5k9" satisfied condition "Succeeded or Failed"
Aug 29 20:19:21.718: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-projected-q5k9 container test-container-subpath-projected-q5k9: <nil>
STEP: delete the pod 08/29/23 20:19:21.73
Aug 29 20:19:21.748: INFO: Waiting for pod pod-subpath-test-projected-q5k9 to disappear
Aug 29 20:19:21.752: INFO: Pod pod-subpath-test-projected-q5k9 no longer exists
STEP: Deleting pod pod-subpath-test-projected-q5k9 08/29/23 20:19:21.752
Aug 29 20:19:21.753: INFO: Deleting pod "pod-subpath-test-projected-q5k9" in namespace "subpath-5635"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 29 20:19:21.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5635" for this suite. 08/29/23 20:19:21.764
------------------------------
• [SLOW TEST] [24.120 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:18:57.652
    Aug 29 20:18:57.653: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename subpath 08/29/23 20:18:57.654
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:18:57.671
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:18:57.675
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/29/23 20:18:57.678
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-q5k9 08/29/23 20:18:57.688
    STEP: Creating a pod to test atomic-volume-subpath 08/29/23 20:18:57.688
    Aug 29 20:18:57.698: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-q5k9" in namespace "subpath-5635" to be "Succeeded or Failed"
    Aug 29 20:18:57.702: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.460643ms
    Aug 29 20:18:59.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 2.007997454s
    Aug 29 20:19:01.708: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 4.009681658s
    Aug 29 20:19:03.706: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 6.007501827s
    Aug 29 20:19:05.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 8.008866595s
    Aug 29 20:19:07.708: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 10.009822354s
    Aug 29 20:19:09.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 12.008217476s
    Aug 29 20:19:11.709: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 14.009946436s
    Aug 29 20:19:13.706: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 16.007822974s
    Aug 29 20:19:15.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 18.008002056s
    Aug 29 20:19:17.709: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=true. Elapsed: 20.010059135s
    Aug 29 20:19:19.707: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Running", Reason="", readiness=false. Elapsed: 22.008162723s
    Aug 29 20:19:21.714: INFO: Pod "pod-subpath-test-projected-q5k9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.015475742s
    STEP: Saw pod success 08/29/23 20:19:21.714
    Aug 29 20:19:21.714: INFO: Pod "pod-subpath-test-projected-q5k9" satisfied condition "Succeeded or Failed"
    Aug 29 20:19:21.718: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-projected-q5k9 container test-container-subpath-projected-q5k9: <nil>
    STEP: delete the pod 08/29/23 20:19:21.73
    Aug 29 20:19:21.748: INFO: Waiting for pod pod-subpath-test-projected-q5k9 to disappear
    Aug 29 20:19:21.752: INFO: Pod pod-subpath-test-projected-q5k9 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-q5k9 08/29/23 20:19:21.752
    Aug 29 20:19:21.753: INFO: Deleting pod "pod-subpath-test-projected-q5k9" in namespace "subpath-5635"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:19:21.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5635" for this suite. 08/29/23 20:19:21.764
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:19:21.774
Aug 29 20:19:21.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:19:21.775
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:21.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:21.807
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-jwsvj"  08/29/23 20:19:21.81
Aug 29 20:19:21.817: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-jwsvj"  08/29/23 20:19:21.817
Aug 29 20:19:21.827: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 29 20:19:21.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1242" for this suite. 08/29/23 20:19:21.834
------------------------------
• [0.071 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:19:21.774
    Aug 29 20:19:21.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:19:21.775
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:21.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:21.807
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-jwsvj"  08/29/23 20:19:21.81
    Aug 29 20:19:21.817: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-jwsvj"  08/29/23 20:19:21.817
    Aug 29 20:19:21.827: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:19:21.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1242" for this suite. 08/29/23 20:19:21.834
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:19:21.845
Aug 29 20:19:21.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:19:21.846
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:21.869
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:21.873
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/29/23 20:19:21.876
Aug 29 20:19:21.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-8561 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 29 20:19:21.995: INFO: stderr: ""
Aug 29 20:19:21.995: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/29/23 20:19:21.995
Aug 29 20:19:21.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-8561 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Aug 29 20:19:22.308: INFO: stderr: ""
Aug 29 20:19:22.308: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/29/23 20:19:22.308
Aug 29 20:19:22.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-8561 delete pods e2e-test-httpd-pod'
Aug 29 20:19:24.145: INFO: stderr: ""
Aug 29 20:19:24.145: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:19:24.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8561" for this suite. 08/29/23 20:19:24.151
------------------------------
• [2.316 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:19:21.845
    Aug 29 20:19:21.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:19:21.846
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:21.869
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:21.873
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/29/23 20:19:21.876
    Aug 29 20:19:21.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-8561 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 29 20:19:21.995: INFO: stderr: ""
    Aug 29 20:19:21.995: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/29/23 20:19:21.995
    Aug 29 20:19:21.995: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-8561 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Aug 29 20:19:22.308: INFO: stderr: ""
    Aug 29 20:19:22.308: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 08/29/23 20:19:22.308
    Aug 29 20:19:22.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-8561 delete pods e2e-test-httpd-pod'
    Aug 29 20:19:24.145: INFO: stderr: ""
    Aug 29 20:19:24.145: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:19:24.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8561" for this suite. 08/29/23 20:19:24.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:19:24.162
Aug 29 20:19:24.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename gc 08/29/23 20:19:24.163
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:24.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:24.194
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 29 20:19:24.237: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1f46661c-2085-4c1a-ac2a-4238392ca18c", Controller:(*bool)(0xc000df2b9e), BlockOwnerDeletion:(*bool)(0xc000df2b9f)}}
Aug 29 20:19:24.248: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f3bfa4ca-f5bb-482f-ba51-0b86c5b2ab45", Controller:(*bool)(0xc007d09a8e), BlockOwnerDeletion:(*bool)(0xc007d09a8f)}}
Aug 29 20:19:24.256: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"138ea03b-2ecf-40db-b2d9-79388b04acbb", Controller:(*bool)(0xc0080071b6), BlockOwnerDeletion:(*bool)(0xc0080071b7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 29 20:19:29.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-3209" for this suite. 08/29/23 20:19:29.275
------------------------------
• [SLOW TEST] [5.121 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:19:24.162
    Aug 29 20:19:24.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename gc 08/29/23 20:19:24.163
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:24.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:24.194
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 29 20:19:24.237: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"1f46661c-2085-4c1a-ac2a-4238392ca18c", Controller:(*bool)(0xc000df2b9e), BlockOwnerDeletion:(*bool)(0xc000df2b9f)}}
    Aug 29 20:19:24.248: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f3bfa4ca-f5bb-482f-ba51-0b86c5b2ab45", Controller:(*bool)(0xc007d09a8e), BlockOwnerDeletion:(*bool)(0xc007d09a8f)}}
    Aug 29 20:19:24.256: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"138ea03b-2ecf-40db-b2d9-79388b04acbb", Controller:(*bool)(0xc0080071b6), BlockOwnerDeletion:(*bool)(0xc0080071b7)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:19:29.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-3209" for this suite. 08/29/23 20:19:29.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:19:29.285
Aug 29 20:19:29.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename var-expansion 08/29/23 20:19:29.286
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:29.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:29.307
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 08/29/23 20:19:29.31
Aug 29 20:19:29.320: INFO: Waiting up to 5m0s for pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1" in namespace "var-expansion-8341" to be "Succeeded or Failed"
Aug 29 20:19:29.323: INFO: Pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27152ms
Aug 29 20:19:31.329: INFO: Pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009316599s
Aug 29 20:19:33.329: INFO: Pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008939014s
STEP: Saw pod success 08/29/23 20:19:33.329
Aug 29 20:19:33.329: INFO: Pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1" satisfied condition "Succeeded or Failed"
Aug 29 20:19:33.333: INFO: Trying to get logs from node loki-15bd39-worker-1 pod var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1 container dapi-container: <nil>
STEP: delete the pod 08/29/23 20:19:33.34
Aug 29 20:19:33.357: INFO: Waiting for pod var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1 to disappear
Aug 29 20:19:33.361: INFO: Pod var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 29 20:19:33.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8341" for this suite. 08/29/23 20:19:33.366
------------------------------
• [4.090 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:19:29.285
    Aug 29 20:19:29.285: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename var-expansion 08/29/23 20:19:29.286
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:29.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:29.307
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 08/29/23 20:19:29.31
    Aug 29 20:19:29.320: INFO: Waiting up to 5m0s for pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1" in namespace "var-expansion-8341" to be "Succeeded or Failed"
    Aug 29 20:19:29.323: INFO: Pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27152ms
    Aug 29 20:19:31.329: INFO: Pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009316599s
    Aug 29 20:19:33.329: INFO: Pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008939014s
    STEP: Saw pod success 08/29/23 20:19:33.329
    Aug 29 20:19:33.329: INFO: Pod "var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1" satisfied condition "Succeeded or Failed"
    Aug 29 20:19:33.333: INFO: Trying to get logs from node loki-15bd39-worker-1 pod var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1 container dapi-container: <nil>
    STEP: delete the pod 08/29/23 20:19:33.34
    Aug 29 20:19:33.357: INFO: Waiting for pod var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1 to disappear
    Aug 29 20:19:33.361: INFO: Pod var-expansion-1d68d68c-6f3f-4ba4-b173-4107bf147db1 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:19:33.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8341" for this suite. 08/29/23 20:19:33.366
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:19:33.375
Aug 29 20:19:33.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 20:19:33.376
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:33.394
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:33.397
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:19:33.401
Aug 29 20:19:33.410: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88" in namespace "downward-api-8125" to be "Succeeded or Failed"
Aug 29 20:19:33.414: INFO: Pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.339491ms
Aug 29 20:19:35.419: INFO: Pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009203724s
Aug 29 20:19:37.418: INFO: Pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007540115s
STEP: Saw pod success 08/29/23 20:19:37.418
Aug 29 20:19:37.418: INFO: Pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88" satisfied condition "Succeeded or Failed"
Aug 29 20:19:37.422: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88 container client-container: <nil>
STEP: delete the pod 08/29/23 20:19:37.431
Aug 29 20:19:37.445: INFO: Waiting for pod downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88 to disappear
Aug 29 20:19:37.448: INFO: Pod downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 20:19:37.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8125" for this suite. 08/29/23 20:19:37.453
------------------------------
• [4.085 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:19:33.375
    Aug 29 20:19:33.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 20:19:33.376
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:33.394
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:33.397
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:19:33.401
    Aug 29 20:19:33.410: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88" in namespace "downward-api-8125" to be "Succeeded or Failed"
    Aug 29 20:19:33.414: INFO: Pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88": Phase="Pending", Reason="", readiness=false. Elapsed: 3.339491ms
    Aug 29 20:19:35.419: INFO: Pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009203724s
    Aug 29 20:19:37.418: INFO: Pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007540115s
    STEP: Saw pod success 08/29/23 20:19:37.418
    Aug 29 20:19:37.418: INFO: Pod "downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88" satisfied condition "Succeeded or Failed"
    Aug 29 20:19:37.422: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88 container client-container: <nil>
    STEP: delete the pod 08/29/23 20:19:37.431
    Aug 29 20:19:37.445: INFO: Waiting for pod downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88 to disappear
    Aug 29 20:19:37.448: INFO: Pod downwardapi-volume-2df59353-af0d-4403-9c0a-5baf85646a88 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:19:37.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8125" for this suite. 08/29/23 20:19:37.453
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:19:37.461
Aug 29 20:19:37.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename taint-multiple-pods 08/29/23 20:19:37.462
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:37.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:37.487
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Aug 29 20:19:37.490: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 20:20:37.540: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Aug 29 20:20:37.544: INFO: Starting informer...
STEP: Starting pods... 08/29/23 20:20:37.544
Aug 29 20:20:37.766: INFO: Pod1 is running on loki-15bd39-worker-1. Tainting Node
Aug 29 20:20:37.978: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8214" to be "running"
Aug 29 20:20:37.982: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.636836ms
Aug 29 20:20:39.987: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008867408s
Aug 29 20:20:39.987: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 29 20:20:39.987: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8214" to be "running"
Aug 29 20:20:39.991: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.063953ms
Aug 29 20:20:39.991: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 29 20:20:39.991: INFO: Pod2 is running on loki-15bd39-worker-1. Tainting Node
STEP: Trying to apply a taint on the Node 08/29/23 20:20:39.991
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/29/23 20:20:40.006
STEP: Waiting for Pod1 and Pod2 to be deleted 08/29/23 20:20:40.01
Aug 29 20:20:46.574: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 29 20:21:06.540: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/29/23 20:21:06.556
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:21:06.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-8214" for this suite. 08/29/23 20:21:06.565
------------------------------
• [SLOW TEST] [89.119 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:19:37.461
    Aug 29 20:19:37.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename taint-multiple-pods 08/29/23 20:19:37.462
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:19:37.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:19:37.487
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Aug 29 20:19:37.490: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 29 20:20:37.540: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Aug 29 20:20:37.544: INFO: Starting informer...
    STEP: Starting pods... 08/29/23 20:20:37.544
    Aug 29 20:20:37.766: INFO: Pod1 is running on loki-15bd39-worker-1. Tainting Node
    Aug 29 20:20:37.978: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-8214" to be "running"
    Aug 29 20:20:37.982: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.636836ms
    Aug 29 20:20:39.987: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008867408s
    Aug 29 20:20:39.987: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 29 20:20:39.987: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-8214" to be "running"
    Aug 29 20:20:39.991: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 4.063953ms
    Aug 29 20:20:39.991: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 29 20:20:39.991: INFO: Pod2 is running on loki-15bd39-worker-1. Tainting Node
    STEP: Trying to apply a taint on the Node 08/29/23 20:20:39.991
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/29/23 20:20:40.006
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/29/23 20:20:40.01
    Aug 29 20:20:46.574: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 29 20:21:06.540: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/29/23 20:21:06.556
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:21:06.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-8214" for this suite. 08/29/23 20:21:06.565
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:21:06.58
Aug 29 20:21:06.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:21:06.581
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:21:06.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:21:06.615
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 08/29/23 20:21:06.619
Aug 29 20:21:06.633: INFO: Waiting up to 5m0s for pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405" in namespace "emptydir-3135" to be "Succeeded or Failed"
Aug 29 20:21:06.637: INFO: Pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405": Phase="Pending", Reason="", readiness=false. Elapsed: 3.637343ms
Aug 29 20:21:08.643: INFO: Pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009199399s
Aug 29 20:21:10.643: INFO: Pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009895326s
STEP: Saw pod success 08/29/23 20:21:10.643
Aug 29 20:21:10.643: INFO: Pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405" satisfied condition "Succeeded or Failed"
Aug 29 20:21:10.647: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405 container test-container: <nil>
STEP: delete the pod 08/29/23 20:21:10.669
Aug 29 20:21:10.686: INFO: Waiting for pod pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405 to disappear
Aug 29 20:21:10.691: INFO: Pod pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:21:10.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3135" for this suite. 08/29/23 20:21:10.695
------------------------------
• [4.127 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:21:06.58
    Aug 29 20:21:06.580: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:21:06.581
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:21:06.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:21:06.615
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/29/23 20:21:06.619
    Aug 29 20:21:06.633: INFO: Waiting up to 5m0s for pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405" in namespace "emptydir-3135" to be "Succeeded or Failed"
    Aug 29 20:21:06.637: INFO: Pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405": Phase="Pending", Reason="", readiness=false. Elapsed: 3.637343ms
    Aug 29 20:21:08.643: INFO: Pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009199399s
    Aug 29 20:21:10.643: INFO: Pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009895326s
    STEP: Saw pod success 08/29/23 20:21:10.643
    Aug 29 20:21:10.643: INFO: Pod "pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405" satisfied condition "Succeeded or Failed"
    Aug 29 20:21:10.647: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405 container test-container: <nil>
    STEP: delete the pod 08/29/23 20:21:10.669
    Aug 29 20:21:10.686: INFO: Waiting for pod pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405 to disappear
    Aug 29 20:21:10.691: INFO: Pod pod-642b2ce4-b03e-4d33-9bd2-59fb5fc8d405 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:21:10.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3135" for this suite. 08/29/23 20:21:10.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:21:10.714
Aug 29 20:21:10.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename dns 08/29/23 20:21:10.715
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:21:10.745
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:21:10.753
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/29/23 20:21:10.756
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4860.svc.cluster.local;sleep 1; done
 08/29/23 20:21:10.764
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4860.svc.cluster.local;sleep 1; done
 08/29/23 20:21:10.764
STEP: creating a pod to probe DNS 08/29/23 20:21:10.764
STEP: submitting the pod to kubernetes 08/29/23 20:21:10.764
Aug 29 20:21:10.783: INFO: Waiting up to 15m0s for pod "dns-test-665a895b-e919-48af-9c53-27835f2ed66a" in namespace "dns-4860" to be "running"
Aug 29 20:21:10.788: INFO: Pod "dns-test-665a895b-e919-48af-9c53-27835f2ed66a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.139755ms
Aug 29 20:21:12.795: INFO: Pod "dns-test-665a895b-e919-48af-9c53-27835f2ed66a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011252305s
Aug 29 20:21:12.795: INFO: Pod "dns-test-665a895b-e919-48af-9c53-27835f2ed66a" satisfied condition "running"
STEP: retrieving the pod 08/29/23 20:21:12.795
STEP: looking for the results for each expected name from probers 08/29/23 20:21:12.798
Aug 29 20:21:12.804: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
Aug 29 20:21:12.807: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
Aug 29 20:21:12.810: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
Aug 29 20:21:12.814: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
Aug 29 20:21:12.818: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
Aug 29 20:21:12.821: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
Aug 29 20:21:12.825: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
Aug 29 20:21:12.829: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
Aug 29 20:21:12.829: INFO: Lookups using dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4860.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4860.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local jessie_udp@dns-test-service-2.dns-4860.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4860.svc.cluster.local]

Aug 29 20:21:17.863: INFO: DNS probes using dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a succeeded

STEP: deleting the pod 08/29/23 20:21:17.863
STEP: deleting the test headless service 08/29/23 20:21:17.88
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 29 20:21:17.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-4860" for this suite. 08/29/23 20:21:17.902
------------------------------
• [SLOW TEST] [7.199 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:21:10.714
    Aug 29 20:21:10.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename dns 08/29/23 20:21:10.715
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:21:10.745
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:21:10.753
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/29/23 20:21:10.756
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-4860.svc.cluster.local;sleep 1; done
     08/29/23 20:21:10.764
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-4860.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-4860.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-4860.svc.cluster.local;sleep 1; done
     08/29/23 20:21:10.764
    STEP: creating a pod to probe DNS 08/29/23 20:21:10.764
    STEP: submitting the pod to kubernetes 08/29/23 20:21:10.764
    Aug 29 20:21:10.783: INFO: Waiting up to 15m0s for pod "dns-test-665a895b-e919-48af-9c53-27835f2ed66a" in namespace "dns-4860" to be "running"
    Aug 29 20:21:10.788: INFO: Pod "dns-test-665a895b-e919-48af-9c53-27835f2ed66a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.139755ms
    Aug 29 20:21:12.795: INFO: Pod "dns-test-665a895b-e919-48af-9c53-27835f2ed66a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011252305s
    Aug 29 20:21:12.795: INFO: Pod "dns-test-665a895b-e919-48af-9c53-27835f2ed66a" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 20:21:12.795
    STEP: looking for the results for each expected name from probers 08/29/23 20:21:12.798
    Aug 29 20:21:12.804: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
    Aug 29 20:21:12.807: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
    Aug 29 20:21:12.810: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
    Aug 29 20:21:12.814: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
    Aug 29 20:21:12.818: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
    Aug 29 20:21:12.821: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
    Aug 29 20:21:12.825: INFO: Unable to read jessie_udp@dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
    Aug 29 20:21:12.829: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-4860.svc.cluster.local from pod dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a: the server could not find the requested resource (get pods dns-test-665a895b-e919-48af-9c53-27835f2ed66a)
    Aug 29 20:21:12.829: INFO: Lookups using dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local wheezy_udp@dns-test-service-2.dns-4860.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-4860.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-4860.svc.cluster.local jessie_udp@dns-test-service-2.dns-4860.svc.cluster.local jessie_tcp@dns-test-service-2.dns-4860.svc.cluster.local]

    Aug 29 20:21:17.863: INFO: DNS probes using dns-4860/dns-test-665a895b-e919-48af-9c53-27835f2ed66a succeeded

    STEP: deleting the pod 08/29/23 20:21:17.863
    STEP: deleting the test headless service 08/29/23 20:21:17.88
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:21:17.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-4860" for this suite. 08/29/23 20:21:17.902
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:21:17.913
Aug 29 20:21:17.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-runtime 08/29/23 20:21:17.914
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:21:17.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:21:17.94
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/29/23 20:21:17.955
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/29/23 20:21:35.056
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/29/23 20:21:35.061
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/29/23 20:21:35.067
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/29/23 20:21:35.068
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/29/23 20:21:35.092
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/29/23 20:21:38.109
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/29/23 20:21:40.121
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/29/23 20:21:40.128
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/29/23 20:21:40.128
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/29/23 20:21:40.155
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/29/23 20:21:41.163
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/29/23 20:21:44.185
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/29/23 20:21:44.192
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/29/23 20:21:44.192
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Aug 29 20:21:44.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6274" for this suite. 08/29/23 20:21:44.23
------------------------------
• [SLOW TEST] [26.327 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:21:17.913
    Aug 29 20:21:17.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-runtime 08/29/23 20:21:17.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:21:17.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:21:17.94
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/29/23 20:21:17.955
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/29/23 20:21:35.056
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/29/23 20:21:35.061
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/29/23 20:21:35.067
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/29/23 20:21:35.068
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/29/23 20:21:35.092
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/29/23 20:21:38.109
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/29/23 20:21:40.121
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/29/23 20:21:40.128
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/29/23 20:21:40.128
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/29/23 20:21:40.155
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/29/23 20:21:41.163
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/29/23 20:21:44.185
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/29/23 20:21:44.192
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/29/23 20:21:44.192
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:21:44.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6274" for this suite. 08/29/23 20:21:44.23
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:21:44.241
Aug 29 20:21:44.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename var-expansion 08/29/23 20:21:44.242
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:21:44.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:21:44.266
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 08/29/23 20:21:44.269
STEP: waiting for pod running 08/29/23 20:21:44.279
Aug 29 20:21:44.279: INFO: Waiting up to 2m0s for pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" in namespace "var-expansion-5077" to be "running"
Aug 29 20:21:44.284: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672": Phase="Pending", Reason="", readiness=false. Elapsed: 5.541121ms
Aug 29 20:21:46.290: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672": Phase="Running", Reason="", readiness=true. Elapsed: 2.010912341s
Aug 29 20:21:46.290: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" satisfied condition "running"
STEP: creating a file in subpath 08/29/23 20:21:46.29
Aug 29 20:21:46.294: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5077 PodName:var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:21:46.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:21:46.295: INFO: ExecWithOptions: Clientset creation
Aug 29 20:21:46.295: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/var-expansion-5077/pods/var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/29/23 20:21:46.374
Aug 29 20:21:46.378: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5077 PodName:var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:21:46.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:21:46.379: INFO: ExecWithOptions: Clientset creation
Aug 29 20:21:46.379: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/var-expansion-5077/pods/var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/29/23 20:21:46.449
Aug 29 20:21:46.966: INFO: Successfully updated pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672"
STEP: waiting for annotated pod running 08/29/23 20:21:46.966
Aug 29 20:21:46.967: INFO: Waiting up to 2m0s for pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" in namespace "var-expansion-5077" to be "running"
Aug 29 20:21:46.970: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672": Phase="Running", Reason="", readiness=true. Elapsed: 3.909524ms
Aug 29 20:21:46.970: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" satisfied condition "running"
STEP: deleting the pod gracefully 08/29/23 20:21:46.97
Aug 29 20:21:46.971: INFO: Deleting pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" in namespace "var-expansion-5077"
Aug 29 20:21:46.980: INFO: Wait up to 5m0s for pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 29 20:22:20.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-5077" for this suite. 08/29/23 20:22:20.992
------------------------------
• [SLOW TEST] [36.758 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:21:44.241
    Aug 29 20:21:44.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename var-expansion 08/29/23 20:21:44.242
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:21:44.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:21:44.266
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 08/29/23 20:21:44.269
    STEP: waiting for pod running 08/29/23 20:21:44.279
    Aug 29 20:21:44.279: INFO: Waiting up to 2m0s for pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" in namespace "var-expansion-5077" to be "running"
    Aug 29 20:21:44.284: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672": Phase="Pending", Reason="", readiness=false. Elapsed: 5.541121ms
    Aug 29 20:21:46.290: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672": Phase="Running", Reason="", readiness=true. Elapsed: 2.010912341s
    Aug 29 20:21:46.290: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" satisfied condition "running"
    STEP: creating a file in subpath 08/29/23 20:21:46.29
    Aug 29 20:21:46.294: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-5077 PodName:var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:21:46.294: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:21:46.295: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:21:46.295: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/var-expansion-5077/pods/var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/29/23 20:21:46.374
    Aug 29 20:21:46.378: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-5077 PodName:var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:21:46.378: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:21:46.379: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:21:46.379: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/var-expansion-5077/pods/var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/29/23 20:21:46.449
    Aug 29 20:21:46.966: INFO: Successfully updated pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672"
    STEP: waiting for annotated pod running 08/29/23 20:21:46.966
    Aug 29 20:21:46.967: INFO: Waiting up to 2m0s for pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" in namespace "var-expansion-5077" to be "running"
    Aug 29 20:21:46.970: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672": Phase="Running", Reason="", readiness=true. Elapsed: 3.909524ms
    Aug 29 20:21:46.970: INFO: Pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" satisfied condition "running"
    STEP: deleting the pod gracefully 08/29/23 20:21:46.97
    Aug 29 20:21:46.971: INFO: Deleting pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" in namespace "var-expansion-5077"
    Aug 29 20:21:46.980: INFO: Wait up to 5m0s for pod "var-expansion-197bc42e-2af4-41a8-b6dd-2524c6640672" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:22:20.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-5077" for this suite. 08/29/23 20:22:20.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:22:21.003
Aug 29 20:22:21.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename endpointslicemirroring 08/29/23 20:22:21.004
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:22:21.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:22:21.026
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/29/23 20:22:21.045
Aug 29 20:22:21.055: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/29/23 20:22:23.061
Aug 29 20:22:23.071: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 08/29/23 20:22:25.077
Aug 29 20:22:25.087: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Aug 29 20:22:27.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-269" for this suite. 08/29/23 20:22:27.116
------------------------------
• [SLOW TEST] [6.121 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:22:21.003
    Aug 29 20:22:21.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename endpointslicemirroring 08/29/23 20:22:21.004
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:22:21.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:22:21.026
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/29/23 20:22:21.045
    Aug 29 20:22:21.055: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/29/23 20:22:23.061
    Aug 29 20:22:23.071: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 08/29/23 20:22:25.077
    Aug 29 20:22:25.087: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:22:27.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-269" for this suite. 08/29/23 20:22:27.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:22:27.126
Aug 29 20:22:27.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename cronjob 08/29/23 20:22:27.127
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:22:27.148
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:22:27.151
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/29/23 20:22:27.154
STEP: Ensuring a job is scheduled 08/29/23 20:22:27.161
STEP: Ensuring exactly one is scheduled 08/29/23 20:23:01.167
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/29/23 20:23:01.17
STEP: Ensuring the job is replaced with a new one 08/29/23 20:23:01.174
STEP: Removing cronjob 08/29/23 20:24:01.179
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 29 20:24:01.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8987" for this suite. 08/29/23 20:24:01.196
------------------------------
• [SLOW TEST] [94.082 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:22:27.126
    Aug 29 20:22:27.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename cronjob 08/29/23 20:22:27.127
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:22:27.148
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:22:27.151
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/29/23 20:22:27.154
    STEP: Ensuring a job is scheduled 08/29/23 20:22:27.161
    STEP: Ensuring exactly one is scheduled 08/29/23 20:23:01.167
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/29/23 20:23:01.17
    STEP: Ensuring the job is replaced with a new one 08/29/23 20:23:01.174
    STEP: Removing cronjob 08/29/23 20:24:01.179
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:24:01.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8987" for this suite. 08/29/23 20:24:01.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:24:01.209
Aug 29 20:24:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:24:01.21
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:01.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:01.237
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-2fa5ad2b-94d8-4577-a9f3-08e7f64b74dd 08/29/23 20:24:01.241
STEP: Creating secret with name secret-projected-all-test-volume-8fb1671c-febe-40bb-b77c-1d834fd667ef 08/29/23 20:24:01.247
STEP: Creating a pod to test Check all projections for projected volume plugin 08/29/23 20:24:01.253
Aug 29 20:24:01.265: INFO: Waiting up to 5m0s for pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501" in namespace "projected-1955" to be "Succeeded or Failed"
Aug 29 20:24:01.269: INFO: Pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545835ms
Aug 29 20:24:03.275: INFO: Pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009487865s
Aug 29 20:24:05.283: INFO: Pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0177906s
STEP: Saw pod success 08/29/23 20:24:05.283
Aug 29 20:24:05.283: INFO: Pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501" satisfied condition "Succeeded or Failed"
Aug 29 20:24:05.288: INFO: Trying to get logs from node loki-15bd39-worker-1 pod projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501 container projected-all-volume-test: <nil>
STEP: delete the pod 08/29/23 20:24:05.31
Aug 29 20:24:05.326: INFO: Waiting for pod projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501 to disappear
Aug 29 20:24:05.329: INFO: Pod projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Aug 29 20:24:05.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1955" for this suite. 08/29/23 20:24:05.333
------------------------------
• [4.133 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:24:01.209
    Aug 29 20:24:01.209: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:24:01.21
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:01.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:01.237
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-2fa5ad2b-94d8-4577-a9f3-08e7f64b74dd 08/29/23 20:24:01.241
    STEP: Creating secret with name secret-projected-all-test-volume-8fb1671c-febe-40bb-b77c-1d834fd667ef 08/29/23 20:24:01.247
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/29/23 20:24:01.253
    Aug 29 20:24:01.265: INFO: Waiting up to 5m0s for pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501" in namespace "projected-1955" to be "Succeeded or Failed"
    Aug 29 20:24:01.269: INFO: Pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501": Phase="Pending", Reason="", readiness=false. Elapsed: 3.545835ms
    Aug 29 20:24:03.275: INFO: Pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009487865s
    Aug 29 20:24:05.283: INFO: Pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0177906s
    STEP: Saw pod success 08/29/23 20:24:05.283
    Aug 29 20:24:05.283: INFO: Pod "projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501" satisfied condition "Succeeded or Failed"
    Aug 29 20:24:05.288: INFO: Trying to get logs from node loki-15bd39-worker-1 pod projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501 container projected-all-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:24:05.31
    Aug 29 20:24:05.326: INFO: Waiting for pod projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501 to disappear
    Aug 29 20:24:05.329: INFO: Pod projected-volume-30148ee8-bf0e-4ea3-bb51-dbd6d6255501 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:24:05.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1955" for this suite. 08/29/23 20:24:05.333
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:24:05.343
Aug 29 20:24:05.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename runtimeclass 08/29/23 20:24:05.344
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:05.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:05.371
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 29 20:24:05.389: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1636 to be scheduled
Aug 29 20:24:05.392: INFO: 1 pods are not scheduled: [runtimeclass-1636/test-runtimeclass-runtimeclass-1636-preconfigured-handler-ck5vj(c84ef238-3c99-46a2-90e4-97d84581baac)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 29 20:24:07.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-1636" for this suite. 08/29/23 20:24:07.412
------------------------------
• [2.077 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:24:05.343
    Aug 29 20:24:05.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename runtimeclass 08/29/23 20:24:05.344
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:05.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:05.371
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 29 20:24:05.389: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-1636 to be scheduled
    Aug 29 20:24:05.392: INFO: 1 pods are not scheduled: [runtimeclass-1636/test-runtimeclass-runtimeclass-1636-preconfigured-handler-ck5vj(c84ef238-3c99-46a2-90e4-97d84581baac)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:24:07.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-1636" for this suite. 08/29/23 20:24:07.412
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:24:07.421
Aug 29 20:24:07.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:24:07.422
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:07.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:07.445
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 08/29/23 20:24:07.448
Aug 29 20:24:07.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5569 cluster-info'
Aug 29 20:24:07.542: INFO: stderr: ""
Aug 29 20:24:07.542: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:24:07.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5569" for this suite. 08/29/23 20:24:07.547
------------------------------
• [0.135 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:24:07.421
    Aug 29 20:24:07.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:24:07.422
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:07.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:07.445
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 08/29/23 20:24:07.448
    Aug 29 20:24:07.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5569 cluster-info'
    Aug 29 20:24:07.542: INFO: stderr: ""
    Aug 29 20:24:07.542: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.19.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:24:07.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5569" for this suite. 08/29/23 20:24:07.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:24:07.558
Aug 29 20:24:07.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:24:07.56
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:07.584
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:07.587
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 08/29/23 20:24:07.59
STEP: submitting the pod to kubernetes 08/29/23 20:24:07.59
Aug 29 20:24:07.599: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5" in namespace "pods-5743" to be "running and ready"
Aug 29 20:24:07.602: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416735ms
Aug 29 20:24:07.602: INFO: The phase of Pod pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:24:09.607: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007939502s
Aug 29 20:24:09.607: INFO: The phase of Pod pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5 is Running (Ready = true)
Aug 29 20:24:09.607: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/29/23 20:24:09.61
STEP: updating the pod 08/29/23 20:24:09.613
Aug 29 20:24:10.128: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5"
Aug 29 20:24:10.129: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5" in namespace "pods-5743" to be "terminated with reason DeadlineExceeded"
Aug 29 20:24:10.132: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Running", Reason="", readiness=true. Elapsed: 3.658577ms
Aug 29 20:24:12.139: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010093182s
Aug 29 20:24:14.138: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.008993664s
Aug 29 20:24:14.138: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:24:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5743" for this suite. 08/29/23 20:24:14.143
------------------------------
• [SLOW TEST] [6.595 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:24:07.558
    Aug 29 20:24:07.558: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:24:07.56
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:07.584
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:07.587
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 08/29/23 20:24:07.59
    STEP: submitting the pod to kubernetes 08/29/23 20:24:07.59
    Aug 29 20:24:07.599: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5" in namespace "pods-5743" to be "running and ready"
    Aug 29 20:24:07.602: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.416735ms
    Aug 29 20:24:07.602: INFO: The phase of Pod pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:24:09.607: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007939502s
    Aug 29 20:24:09.607: INFO: The phase of Pod pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5 is Running (Ready = true)
    Aug 29 20:24:09.607: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/29/23 20:24:09.61
    STEP: updating the pod 08/29/23 20:24:09.613
    Aug 29 20:24:10.128: INFO: Successfully updated pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5"
    Aug 29 20:24:10.129: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5" in namespace "pods-5743" to be "terminated with reason DeadlineExceeded"
    Aug 29 20:24:10.132: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Running", Reason="", readiness=true. Elapsed: 3.658577ms
    Aug 29 20:24:12.139: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010093182s
    Aug 29 20:24:14.138: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.008993664s
    Aug 29 20:24:14.138: INFO: Pod "pod-update-activedeadlineseconds-5ccb64ac-7b05-4a57-a17d-29a663da8df5" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:24:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5743" for this suite. 08/29/23 20:24:14.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:24:14.154
Aug 29 20:24:14.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename statefulset 08/29/23 20:24:14.155
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:14.175
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:14.178
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3273 08/29/23 20:24:14.181
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Aug 29 20:24:14.203: INFO: Found 0 stateful pods, waiting for 1
Aug 29 20:24:24.208: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/29/23 20:24:24.214
W0829 20:24:24.228393      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 29 20:24:24.238: INFO: Found 1 stateful pods, waiting for 2
Aug 29 20:24:34.243: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 20:24:34.244: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/29/23 20:24:34.251
STEP: Delete all of the StatefulSets 08/29/23 20:24:34.255
STEP: Verify that StatefulSets have been deleted 08/29/23 20:24:34.263
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 29 20:24:34.267: INFO: Deleting all statefulset in ns statefulset-3273
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:24:34.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3273" for this suite. 08/29/23 20:24:34.28
------------------------------
• [SLOW TEST] [20.137 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:24:14.154
    Aug 29 20:24:14.154: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename statefulset 08/29/23 20:24:14.155
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:14.175
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:14.178
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3273 08/29/23 20:24:14.181
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Aug 29 20:24:14.203: INFO: Found 0 stateful pods, waiting for 1
    Aug 29 20:24:24.208: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/29/23 20:24:24.214
    W0829 20:24:24.228393      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 29 20:24:24.238: INFO: Found 1 stateful pods, waiting for 2
    Aug 29 20:24:34.243: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 20:24:34.244: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/29/23 20:24:34.251
    STEP: Delete all of the StatefulSets 08/29/23 20:24:34.255
    STEP: Verify that StatefulSets have been deleted 08/29/23 20:24:34.263
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 29 20:24:34.267: INFO: Deleting all statefulset in ns statefulset-3273
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:24:34.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3273" for this suite. 08/29/23 20:24:34.28
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:24:34.291
Aug 29 20:24:34.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename subpath 08/29/23 20:24:34.292
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:34.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:34.329
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/29/23 20:24:34.331
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-8vd4 08/29/23 20:24:34.343
STEP: Creating a pod to test atomic-volume-subpath 08/29/23 20:24:34.343
Aug 29 20:24:34.356: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8vd4" in namespace "subpath-7294" to be "Succeeded or Failed"
Aug 29 20:24:34.359: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.780107ms
Aug 29 20:24:36.365: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008688555s
Aug 29 20:24:38.365: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 4.008228673s
Aug 29 20:24:40.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 6.007348558s
Aug 29 20:24:42.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 8.00743522s
Aug 29 20:24:44.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 10.008058738s
Aug 29 20:24:46.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 12.008103707s
Aug 29 20:24:48.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 14.00820412s
Aug 29 20:24:50.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 16.008206506s
Aug 29 20:24:52.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 18.007777016s
Aug 29 20:24:54.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 20.007662941s
Aug 29 20:24:56.366: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=false. Elapsed: 22.010205236s
Aug 29 20:24:58.363: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007111776s
STEP: Saw pod success 08/29/23 20:24:58.363
Aug 29 20:24:58.364: INFO: Pod "pod-subpath-test-configmap-8vd4" satisfied condition "Succeeded or Failed"
Aug 29 20:24:58.370: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-configmap-8vd4 container test-container-subpath-configmap-8vd4: <nil>
STEP: delete the pod 08/29/23 20:24:58.379
Aug 29 20:24:58.396: INFO: Waiting for pod pod-subpath-test-configmap-8vd4 to disappear
Aug 29 20:24:58.400: INFO: Pod pod-subpath-test-configmap-8vd4 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-8vd4 08/29/23 20:24:58.4
Aug 29 20:24:58.400: INFO: Deleting pod "pod-subpath-test-configmap-8vd4" in namespace "subpath-7294"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 29 20:24:58.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7294" for this suite. 08/29/23 20:24:58.408
------------------------------
• [SLOW TEST] [24.126 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:24:34.291
    Aug 29 20:24:34.292: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename subpath 08/29/23 20:24:34.292
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:34.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:34.329
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/29/23 20:24:34.331
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-8vd4 08/29/23 20:24:34.343
    STEP: Creating a pod to test atomic-volume-subpath 08/29/23 20:24:34.343
    Aug 29 20:24:34.356: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-8vd4" in namespace "subpath-7294" to be "Succeeded or Failed"
    Aug 29 20:24:34.359: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.780107ms
    Aug 29 20:24:36.365: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 2.008688555s
    Aug 29 20:24:38.365: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 4.008228673s
    Aug 29 20:24:40.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 6.007348558s
    Aug 29 20:24:42.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 8.00743522s
    Aug 29 20:24:44.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 10.008058738s
    Aug 29 20:24:46.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 12.008103707s
    Aug 29 20:24:48.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 14.00820412s
    Aug 29 20:24:50.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 16.008206506s
    Aug 29 20:24:52.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 18.007777016s
    Aug 29 20:24:54.364: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=true. Elapsed: 20.007662941s
    Aug 29 20:24:56.366: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Running", Reason="", readiness=false. Elapsed: 22.010205236s
    Aug 29 20:24:58.363: INFO: Pod "pod-subpath-test-configmap-8vd4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007111776s
    STEP: Saw pod success 08/29/23 20:24:58.363
    Aug 29 20:24:58.364: INFO: Pod "pod-subpath-test-configmap-8vd4" satisfied condition "Succeeded or Failed"
    Aug 29 20:24:58.370: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-configmap-8vd4 container test-container-subpath-configmap-8vd4: <nil>
    STEP: delete the pod 08/29/23 20:24:58.379
    Aug 29 20:24:58.396: INFO: Waiting for pod pod-subpath-test-configmap-8vd4 to disappear
    Aug 29 20:24:58.400: INFO: Pod pod-subpath-test-configmap-8vd4 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-8vd4 08/29/23 20:24:58.4
    Aug 29 20:24:58.400: INFO: Deleting pod "pod-subpath-test-configmap-8vd4" in namespace "subpath-7294"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:24:58.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7294" for this suite. 08/29/23 20:24:58.408
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:24:58.418
Aug 29 20:24:58.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:24:58.419
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:58.437
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:58.439
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-323 08/29/23 20:24:58.442
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[] 08/29/23 20:24:58.46
Aug 29 20:24:58.464: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Aug 29 20:24:59.475: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-323 08/29/23 20:24:59.475
Aug 29 20:24:59.485: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-323" to be "running and ready"
Aug 29 20:24:59.489: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.418156ms
Aug 29 20:24:59.489: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:25:01.494: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008480572s
Aug 29 20:25:01.494: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 29 20:25:01.494: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[pod1:[100]] 08/29/23 20:25:01.497
Aug 29 20:25:01.510: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-323 08/29/23 20:25:01.51
Aug 29 20:25:01.516: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-323" to be "running and ready"
Aug 29 20:25:01.519: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.317846ms
Aug 29 20:25:01.519: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:25:03.524: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007797397s
Aug 29 20:25:03.524: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 29 20:25:03.524: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[pod1:[100] pod2:[101]] 08/29/23 20:25:03.528
Aug 29 20:25:03.545: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/29/23 20:25:03.545
Aug 29 20:25:03.546: INFO: Creating new exec pod
Aug 29 20:25:03.551: INFO: Waiting up to 5m0s for pod "execpodd9tqt" in namespace "services-323" to be "running"
Aug 29 20:25:03.555: INFO: Pod "execpodd9tqt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133527ms
Aug 29 20:25:05.559: INFO: Pod "execpodd9tqt": Phase="Running", Reason="", readiness=true. Elapsed: 2.007153998s
Aug 29 20:25:05.559: INFO: Pod "execpodd9tqt" satisfied condition "running"
Aug 29 20:25:06.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-323 exec execpodd9tqt -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Aug 29 20:25:06.737: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 29 20:25:06.737: INFO: stdout: ""
Aug 29 20:25:06.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-323 exec execpodd9tqt -- /bin/sh -x -c nc -v -z -w 2 172.19.141.37 80'
Aug 29 20:25:06.907: INFO: stderr: "+ nc -v -z -w 2 172.19.141.37 80\nConnection to 172.19.141.37 80 port [tcp/http] succeeded!\n"
Aug 29 20:25:06.907: INFO: stdout: ""
Aug 29 20:25:06.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-323 exec execpodd9tqt -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Aug 29 20:25:07.082: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 29 20:25:07.082: INFO: stdout: ""
Aug 29 20:25:07.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-323 exec execpodd9tqt -- /bin/sh -x -c nc -v -z -w 2 172.19.141.37 81'
Aug 29 20:25:07.260: INFO: stderr: "+ nc -v -z -w 2 172.19.141.37 81\nConnection to 172.19.141.37 81 port [tcp/*] succeeded!\n"
Aug 29 20:25:07.260: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-323 08/29/23 20:25:07.26
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[pod2:[101]] 08/29/23 20:25:07.275
Aug 29 20:25:08.299: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-323 08/29/23 20:25:08.299
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[] 08/29/23 20:25:08.313
Aug 29 20:25:09.339: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:09.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-323" for this suite. 08/29/23 20:25:09.366
------------------------------
• [SLOW TEST] [10.956 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:24:58.418
    Aug 29 20:24:58.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:24:58.419
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:24:58.437
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:24:58.439
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-323 08/29/23 20:24:58.442
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[] 08/29/23 20:24:58.46
    Aug 29 20:24:58.464: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Aug 29 20:24:59.475: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-323 08/29/23 20:24:59.475
    Aug 29 20:24:59.485: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-323" to be "running and ready"
    Aug 29 20:24:59.489: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.418156ms
    Aug 29 20:24:59.489: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:25:01.494: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008480572s
    Aug 29 20:25:01.494: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 29 20:25:01.494: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[pod1:[100]] 08/29/23 20:25:01.497
    Aug 29 20:25:01.510: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-323 08/29/23 20:25:01.51
    Aug 29 20:25:01.516: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-323" to be "running and ready"
    Aug 29 20:25:01.519: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.317846ms
    Aug 29 20:25:01.519: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:25:03.524: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007797397s
    Aug 29 20:25:03.524: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 29 20:25:03.524: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[pod1:[100] pod2:[101]] 08/29/23 20:25:03.528
    Aug 29 20:25:03.545: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/29/23 20:25:03.545
    Aug 29 20:25:03.546: INFO: Creating new exec pod
    Aug 29 20:25:03.551: INFO: Waiting up to 5m0s for pod "execpodd9tqt" in namespace "services-323" to be "running"
    Aug 29 20:25:03.555: INFO: Pod "execpodd9tqt": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133527ms
    Aug 29 20:25:05.559: INFO: Pod "execpodd9tqt": Phase="Running", Reason="", readiness=true. Elapsed: 2.007153998s
    Aug 29 20:25:05.559: INFO: Pod "execpodd9tqt" satisfied condition "running"
    Aug 29 20:25:06.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-323 exec execpodd9tqt -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Aug 29 20:25:06.737: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 29 20:25:06.737: INFO: stdout: ""
    Aug 29 20:25:06.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-323 exec execpodd9tqt -- /bin/sh -x -c nc -v -z -w 2 172.19.141.37 80'
    Aug 29 20:25:06.907: INFO: stderr: "+ nc -v -z -w 2 172.19.141.37 80\nConnection to 172.19.141.37 80 port [tcp/http] succeeded!\n"
    Aug 29 20:25:06.907: INFO: stdout: ""
    Aug 29 20:25:06.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-323 exec execpodd9tqt -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Aug 29 20:25:07.082: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 29 20:25:07.082: INFO: stdout: ""
    Aug 29 20:25:07.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-323 exec execpodd9tqt -- /bin/sh -x -c nc -v -z -w 2 172.19.141.37 81'
    Aug 29 20:25:07.260: INFO: stderr: "+ nc -v -z -w 2 172.19.141.37 81\nConnection to 172.19.141.37 81 port [tcp/*] succeeded!\n"
    Aug 29 20:25:07.260: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-323 08/29/23 20:25:07.26
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[pod2:[101]] 08/29/23 20:25:07.275
    Aug 29 20:25:08.299: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-323 08/29/23 20:25:08.299
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-323 to expose endpoints map[] 08/29/23 20:25:08.313
    Aug 29 20:25:09.339: INFO: successfully validated that service multi-endpoint-test in namespace services-323 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:09.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-323" for this suite. 08/29/23 20:25:09.366
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:09.374
Aug 29 20:25:09.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename deployment 08/29/23 20:25:09.375
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:09.395
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:09.398
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/29/23 20:25:09.406
STEP: waiting for Deployment to be created 08/29/23 20:25:09.413
STEP: waiting for all Replicas to be Ready 08/29/23 20:25:09.415
Aug 29 20:25:09.416: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 20:25:09.416: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 20:25:09.434: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 20:25:09.434: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 20:25:09.453: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 20:25:09.453: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 20:25:09.485: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 20:25:09.485: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 29 20:25:10.730: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 29 20:25:10.730: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 29 20:25:11.356: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/29/23 20:25:11.356
W0829 20:25:11.369313      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 29 20:25:11.370: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/29/23 20:25:11.37
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:11.373: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:11.373: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:11.373: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:11.373: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:11.392: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:11.392: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:11.417: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:11.417: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:11.425: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:11.426: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:11.437: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:11.437: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:13.376: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:13.376: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:13.409: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
STEP: listing Deployments 08/29/23 20:25:13.409
Aug 29 20:25:13.414: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/29/23 20:25:13.414
Aug 29 20:25:13.430: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/29/23 20:25:13.43
Aug 29 20:25:13.437: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:13.450: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:13.477: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:13.495: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:13.512: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:14.713: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:15.391: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:15.435: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:15.451: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 29 20:25:16.760: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/29/23 20:25:16.794
STEP: fetching the DeploymentStatus 08/29/23 20:25:16.802
Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 3
Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 3
STEP: deleting the Deployment 08/29/23 20:25:16.809
Aug 29 20:25:16.819: INFO: observed event type MODIFIED
Aug 29 20:25:16.819: INFO: observed event type MODIFIED
Aug 29 20:25:16.819: INFO: observed event type MODIFIED
Aug 29 20:25:16.820: INFO: observed event type MODIFIED
Aug 29 20:25:16.820: INFO: observed event type MODIFIED
Aug 29 20:25:16.820: INFO: observed event type MODIFIED
Aug 29 20:25:16.820: INFO: observed event type MODIFIED
Aug 29 20:25:16.820: INFO: observed event type MODIFIED
Aug 29 20:25:16.820: INFO: observed event type MODIFIED
Aug 29 20:25:16.820: INFO: observed event type MODIFIED
Aug 29 20:25:16.820: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 20:25:16.823: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 29 20:25:16.826: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-1888  665c9a5e-8bc6-4fe0-b687-a170bcf5d85d 31834 2 2023-08-29 20:25:13 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b 0xc003dcab97 0xc003dcab98}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:25:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dcac40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Aug 29 20:25:16.834: INFO: pod: "test-deployment-7b7876f9d6-dpkzn":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-dpkzn test-deployment-7b7876f9d6- deployment-1888  7315abc1-3e8c-4a5d-8093-d5982dcffef2 31833 0 2023-08-29 20:25:15 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:526db37332911f29ab9f0324c612494fab83d1fbdd9aa6af4a4b690e9ce8a1bc cni.projectcalico.org/podIP:172.20.143.224/32 cni.projectcalico.org/podIPs:172.20.143.224/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 665c9a5e-8bc6-4fe0-b687-a170bcf5d85d 0xc003dcb0d7 0xc003dcb0d8}] [] [{calico Update v1 2023-08-29 20:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-29 20:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665c9a5e-8bc6-4fe0-b687-a170bcf5d85d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-29 20:25:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2bfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2bfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.198,PodIP:172.20.143.224,StartTime:2023-08-29 20:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:25:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://50be10534246967f525e636b0945d92a69e96775f407a244a1a25e65f76757ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.143.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 29 20:25:16.834: INFO: pod: "test-deployment-7b7876f9d6-ll79k":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-ll79k test-deployment-7b7876f9d6- deployment-1888  f44706cd-5c94-4e7d-bdde-f28af8a39c28 31796 0 2023-08-29 20:25:13 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:c0c56ade7be9c6947c1c9365469f51a3b8f69b59d4718cae89e0346d27a23325 cni.projectcalico.org/podIP:172.20.30.165/32 cni.projectcalico.org/podIPs:172.20.30.165/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 665c9a5e-8bc6-4fe0-b687-a170bcf5d85d 0xc003dcb2f7 0xc003dcb2f8}] [] [{kube-controller-manager Update v1 2023-08-29 20:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665c9a5e-8bc6-4fe0-b687-a170bcf5d85d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 20:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 20:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhs57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhs57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.165,StartTime:2023-08-29 20:25:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:25:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ee4341c3fa9f46b4803a812687810915f30347f9ff48b094e2eb3b897efade05,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 29 20:25:16.835: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-1888  31a7b9f0-23f4-4ce7-8991-39559ff62e5b 31841 4 2023-08-29 20:25:11 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b 0xc003dcaca7 0xc003dcaca8}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:25:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dcad30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 29 20:25:16.838: INFO: pod: "test-deployment-7df74c55ff-8w27h":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-8w27h test-deployment-7df74c55ff- deployment-1888  d1e15178-a96b-438f-b83e-5a99d25fec6e 31838 0 2023-08-29 20:25:13 +0000 UTC 2023-08-29 20:25:17 +0000 UTC 0xc006410e98 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:9dff048167678a2946c0f96f285fa49cfd31007138b2d77aa9e9da1788c9f747 cni.projectcalico.org/podIP:172.20.84.159/32 cni.projectcalico.org/podIPs:172.20.84.159/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 31a7b9f0-23f4-4ce7-8991-39559ff62e5b 0xc006410ee7 0xc006410ee8}] [] [{kube-controller-manager Update v1 2023-08-29 20:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31a7b9f0-23f4-4ce7-8991-39559ff62e5b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 20:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 20:25:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97lcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97lcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.159,StartTime:2023-08-29 20:25:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:25:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://0001be768aa1ad3022434ff735d600106183561de7850b2fcb82da424865a8d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Aug 29 20:25:16.839: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-1888  032cb053-29a8-46b9-8e16-a640057a748c 31709 3 2023-08-29 20:25:09 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b 0xc003dcad97 0xc003dcad98}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:25:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dcae20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:16.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-1888" for this suite. 08/29/23 20:25:16.85
------------------------------
• [SLOW TEST] [7.490 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:09.374
    Aug 29 20:25:09.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename deployment 08/29/23 20:25:09.375
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:09.395
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:09.398
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/29/23 20:25:09.406
    STEP: waiting for Deployment to be created 08/29/23 20:25:09.413
    STEP: waiting for all Replicas to be Ready 08/29/23 20:25:09.415
    Aug 29 20:25:09.416: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 29 20:25:09.416: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 29 20:25:09.434: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 29 20:25:09.434: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 29 20:25:09.453: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 29 20:25:09.453: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 29 20:25:09.485: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 29 20:25:09.485: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 29 20:25:10.730: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 29 20:25:10.730: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 29 20:25:11.356: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/29/23 20:25:11.356
    W0829 20:25:11.369313      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 29 20:25:11.370: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/29/23 20:25:11.37
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 0
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:11.372: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:11.373: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:11.373: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:11.373: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:11.373: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:11.392: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:11.392: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:11.417: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:11.417: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:11.425: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:11.426: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:11.437: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:11.437: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:13.376: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:13.376: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:13.409: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    STEP: listing Deployments 08/29/23 20:25:13.409
    Aug 29 20:25:13.414: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/29/23 20:25:13.414
    Aug 29 20:25:13.430: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/29/23 20:25:13.43
    Aug 29 20:25:13.437: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:13.450: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:13.477: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:13.495: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:13.512: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:14.713: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:15.391: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:15.435: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:15.451: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 29 20:25:16.760: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/29/23 20:25:16.794
    STEP: fetching the DeploymentStatus 08/29/23 20:25:16.802
    Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:16.808: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 1
    Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 3
    Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 2
    Aug 29 20:25:16.809: INFO: observed Deployment test-deployment in namespace deployment-1888 with ReadyReplicas 3
    STEP: deleting the Deployment 08/29/23 20:25:16.809
    Aug 29 20:25:16.819: INFO: observed event type MODIFIED
    Aug 29 20:25:16.819: INFO: observed event type MODIFIED
    Aug 29 20:25:16.819: INFO: observed event type MODIFIED
    Aug 29 20:25:16.820: INFO: observed event type MODIFIED
    Aug 29 20:25:16.820: INFO: observed event type MODIFIED
    Aug 29 20:25:16.820: INFO: observed event type MODIFIED
    Aug 29 20:25:16.820: INFO: observed event type MODIFIED
    Aug 29 20:25:16.820: INFO: observed event type MODIFIED
    Aug 29 20:25:16.820: INFO: observed event type MODIFIED
    Aug 29 20:25:16.820: INFO: observed event type MODIFIED
    Aug 29 20:25:16.820: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 29 20:25:16.823: INFO: Log out all the ReplicaSets if there is no deployment created
    Aug 29 20:25:16.826: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-1888  665c9a5e-8bc6-4fe0-b687-a170bcf5d85d 31834 2 2023-08-29 20:25:13 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b 0xc003dcab97 0xc003dcab98}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:25:16 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dcac40 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Aug 29 20:25:16.834: INFO: pod: "test-deployment-7b7876f9d6-dpkzn":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-dpkzn test-deployment-7b7876f9d6- deployment-1888  7315abc1-3e8c-4a5d-8093-d5982dcffef2 31833 0 2023-08-29 20:25:15 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:526db37332911f29ab9f0324c612494fab83d1fbdd9aa6af4a4b690e9ce8a1bc cni.projectcalico.org/podIP:172.20.143.224/32 cni.projectcalico.org/podIPs:172.20.143.224/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 665c9a5e-8bc6-4fe0-b687-a170bcf5d85d 0xc003dcb0d7 0xc003dcb0d8}] [] [{calico Update v1 2023-08-29 20:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-08-29 20:25:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665c9a5e-8bc6-4fe0-b687-a170bcf5d85d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-29 20:25:16 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.143.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2bfs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2bfs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-0,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.198,PodIP:172.20.143.224,StartTime:2023-08-29 20:25:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:25:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://50be10534246967f525e636b0945d92a69e96775f407a244a1a25e65f76757ce,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.143.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 29 20:25:16.834: INFO: pod: "test-deployment-7b7876f9d6-ll79k":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-ll79k test-deployment-7b7876f9d6- deployment-1888  f44706cd-5c94-4e7d-bdde-f28af8a39c28 31796 0 2023-08-29 20:25:13 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[cni.projectcalico.org/containerID:c0c56ade7be9c6947c1c9365469f51a3b8f69b59d4718cae89e0346d27a23325 cni.projectcalico.org/podIP:172.20.30.165/32 cni.projectcalico.org/podIPs:172.20.30.165/32] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 665c9a5e-8bc6-4fe0-b687-a170bcf5d85d 0xc003dcb2f7 0xc003dcb2f8}] [] [{kube-controller-manager Update v1 2023-08-29 20:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"665c9a5e-8bc6-4fe0-b687-a170bcf5d85d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 20:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 20:25:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qhs57,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qhs57,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.165,StartTime:2023-08-29 20:25:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:25:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ee4341c3fa9f46b4803a812687810915f30347f9ff48b094e2eb3b897efade05,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.165,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 29 20:25:16.835: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-1888  31a7b9f0-23f4-4ce7-8991-39559ff62e5b 31841 4 2023-08-29 20:25:11 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b 0xc003dcaca7 0xc003dcaca8}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:25:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:25:16 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dcad30 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 29 20:25:16.838: INFO: pod: "test-deployment-7df74c55ff-8w27h":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-8w27h test-deployment-7df74c55ff- deployment-1888  d1e15178-a96b-438f-b83e-5a99d25fec6e 31838 0 2023-08-29 20:25:13 +0000 UTC 2023-08-29 20:25:17 +0000 UTC 0xc006410e98 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[cni.projectcalico.org/containerID:9dff048167678a2946c0f96f285fa49cfd31007138b2d77aa9e9da1788c9f747 cni.projectcalico.org/podIP:172.20.84.159/32 cni.projectcalico.org/podIPs:172.20.84.159/32] [{apps/v1 ReplicaSet test-deployment-7df74c55ff 31a7b9f0-23f4-4ce7-8991-39559ff62e5b 0xc006410ee7 0xc006410ee8}] [] [{kube-controller-manager Update v1 2023-08-29 20:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"31a7b9f0-23f4-4ce7-8991-39559ff62e5b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 20:25:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 20:25:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.84.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-97lcv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-97lcv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:25:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.199,PodIP:172.20.84.159,StartTime:2023-08-29 20:25:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:25:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://0001be768aa1ad3022434ff735d600106183561de7850b2fcb82da424865a8d0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.84.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Aug 29 20:25:16.839: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-1888  032cb053-29a8-46b9-8e16-a640057a748c 31709 3 2023-08-29 20:25:09 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b 0xc003dcad97 0xc003dcad98}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:25:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"04f787b7-fc97-4a9a-ba5f-f0ffb0b8533b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:25:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003dcae20 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:16.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-1888" for this suite. 08/29/23 20:25:16.85
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:16.865
Aug 29 20:25:16.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename podtemplate 08/29/23 20:25:16.866
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:16.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:16.891
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:16.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-2753" for this suite. 08/29/23 20:25:16.933
------------------------------
• [0.076 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:16.865
    Aug 29 20:25:16.865: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename podtemplate 08/29/23 20:25:16.866
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:16.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:16.891
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:16.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-2753" for this suite. 08/29/23 20:25:16.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:16.943
Aug 29 20:25:16.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename runtimeclass 08/29/23 20:25:16.944
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:16.964
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:16.967
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 29 20:25:16.990: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9800 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:17.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-9800" for this suite. 08/29/23 20:25:17.027
------------------------------
• [0.093 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:16.943
    Aug 29 20:25:16.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename runtimeclass 08/29/23 20:25:16.944
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:16.964
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:16.967
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 29 20:25:16.990: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-9800 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:17.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-9800" for this suite. 08/29/23 20:25:17.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:17.039
Aug 29 20:25:17.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-lifecycle-hook 08/29/23 20:25:17.04
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:17.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:17.061
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/29/23 20:25:17.069
Aug 29 20:25:17.080: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4444" to be "running and ready"
Aug 29 20:25:17.084: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562006ms
Aug 29 20:25:17.084: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:25:19.089: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008926421s
Aug 29 20:25:19.089: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 29 20:25:19.089: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 08/29/23 20:25:19.093
Aug 29 20:25:19.102: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4444" to be "running and ready"
Aug 29 20:25:19.105: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632073ms
Aug 29 20:25:19.105: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:25:21.111: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00947672s
Aug 29 20:25:21.111: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 29 20:25:21.111: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/29/23 20:25:21.114
Aug 29 20:25:21.124: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 20:25:21.130: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 29 20:25:23.131: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 20:25:23.136: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 29 20:25:25.132: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 29 20:25:25.137: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/29/23 20:25:25.138
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:25.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4444" for this suite. 08/29/23 20:25:25.173
------------------------------
• [SLOW TEST] [8.144 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:17.039
    Aug 29 20:25:17.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/29/23 20:25:17.04
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:17.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:17.061
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/29/23 20:25:17.069
    Aug 29 20:25:17.080: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4444" to be "running and ready"
    Aug 29 20:25:17.084: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.562006ms
    Aug 29 20:25:17.084: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:25:19.089: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008926421s
    Aug 29 20:25:19.089: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 29 20:25:19.089: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 08/29/23 20:25:19.093
    Aug 29 20:25:19.102: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4444" to be "running and ready"
    Aug 29 20:25:19.105: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.632073ms
    Aug 29 20:25:19.105: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:25:21.111: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00947672s
    Aug 29 20:25:21.111: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 29 20:25:21.111: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/29/23 20:25:21.114
    Aug 29 20:25:21.124: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 29 20:25:21.130: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 29 20:25:23.131: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 29 20:25:23.136: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 29 20:25:25.132: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 29 20:25:25.137: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/29/23 20:25:25.138
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:25.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4444" for this suite. 08/29/23 20:25:25.173
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:25.183
Aug 29 20:25:25.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:25:25.184
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:25.209
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:25.213
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-d29241de-2f11-412b-ab71-5880b61abd0d 08/29/23 20:25:25.216
STEP: Creating a pod to test consume configMaps 08/29/23 20:25:25.221
Aug 29 20:25:25.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c" in namespace "projected-273" to be "Succeeded or Failed"
Aug 29 20:25:25.238: INFO: Pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.627387ms
Aug 29 20:25:27.243: INFO: Pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009251683s
Aug 29 20:25:29.243: INFO: Pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009142987s
STEP: Saw pod success 08/29/23 20:25:29.243
Aug 29 20:25:29.243: INFO: Pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c" satisfied condition "Succeeded or Failed"
Aug 29 20:25:29.247: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:25:29.254
Aug 29 20:25:29.267: INFO: Waiting for pod pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c to disappear
Aug 29 20:25:29.270: INFO: Pod pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:29.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-273" for this suite. 08/29/23 20:25:29.275
------------------------------
• [4.100 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:25.183
    Aug 29 20:25:25.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:25:25.184
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:25.209
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:25.213
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-d29241de-2f11-412b-ab71-5880b61abd0d 08/29/23 20:25:25.216
    STEP: Creating a pod to test consume configMaps 08/29/23 20:25:25.221
    Aug 29 20:25:25.234: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c" in namespace "projected-273" to be "Succeeded or Failed"
    Aug 29 20:25:25.238: INFO: Pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.627387ms
    Aug 29 20:25:27.243: INFO: Pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009251683s
    Aug 29 20:25:29.243: INFO: Pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009142987s
    STEP: Saw pod success 08/29/23 20:25:29.243
    Aug 29 20:25:29.243: INFO: Pod "pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c" satisfied condition "Succeeded or Failed"
    Aug 29 20:25:29.247: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:25:29.254
    Aug 29 20:25:29.267: INFO: Waiting for pod pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c to disappear
    Aug 29 20:25:29.270: INFO: Pod pod-projected-configmaps-be30b135-7ea6-49aa-82c1-04243604436c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:29.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-273" for this suite. 08/29/23 20:25:29.275
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:29.283
Aug 29 20:25:29.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename endpointslice 08/29/23 20:25:29.284
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:29.306
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:29.309
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Aug 29 20:25:29.322: INFO: Endpoints addresses: [10.45.35.202 10.45.35.204] , ports: [443]
Aug 29 20:25:29.322: INFO: EndpointSlices addresses: [10.45.35.202 10.45.35.204] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:29.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-4223" for this suite. 08/29/23 20:25:29.327
------------------------------
• [0.053 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:29.283
    Aug 29 20:25:29.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename endpointslice 08/29/23 20:25:29.284
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:29.306
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:29.309
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Aug 29 20:25:29.322: INFO: Endpoints addresses: [10.45.35.202 10.45.35.204] , ports: [443]
    Aug 29 20:25:29.322: INFO: EndpointSlices addresses: [10.45.35.202 10.45.35.204] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:29.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-4223" for this suite. 08/29/23 20:25:29.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:29.338
Aug 29 20:25:29.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:25:29.339
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:29.358
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:29.361
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 08/29/23 20:25:29.364
Aug 29 20:25:29.364: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 29 20:25:29.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
Aug 29 20:25:29.724: INFO: stderr: ""
Aug 29 20:25:29.724: INFO: stdout: "service/agnhost-replica created\n"
Aug 29 20:25:29.725: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 29 20:25:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
Aug 29 20:25:30.068: INFO: stderr: ""
Aug 29 20:25:30.068: INFO: stdout: "service/agnhost-primary created\n"
Aug 29 20:25:30.068: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 29 20:25:30.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
Aug 29 20:25:30.437: INFO: stderr: ""
Aug 29 20:25:30.437: INFO: stdout: "service/frontend created\n"
Aug 29 20:25:30.437: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 29 20:25:30.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
Aug 29 20:25:30.803: INFO: stderr: ""
Aug 29 20:25:30.803: INFO: stdout: "deployment.apps/frontend created\n"
Aug 29 20:25:30.803: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 29 20:25:30.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
Aug 29 20:25:31.198: INFO: stderr: ""
Aug 29 20:25:31.198: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 29 20:25:31.198: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 29 20:25:31.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
Aug 29 20:25:31.611: INFO: stderr: ""
Aug 29 20:25:31.611: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/29/23 20:25:31.611
Aug 29 20:25:31.611: INFO: Waiting for all frontend pods to be Running.
Aug 29 20:25:36.662: INFO: Waiting for frontend to serve content.
Aug 29 20:25:36.674: INFO: Trying to add a new entry to the guestbook.
Aug 29 20:25:36.687: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/29/23 20:25:36.698
Aug 29 20:25:36.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
Aug 29 20:25:36.816: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 20:25:36.816: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/29/23 20:25:36.816
Aug 29 20:25:36.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
Aug 29 20:25:36.925: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 20:25:36.925: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/29/23 20:25:36.926
Aug 29 20:25:36.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
Aug 29 20:25:37.036: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 20:25:37.036: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/29/23 20:25:37.037
Aug 29 20:25:37.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
Aug 29 20:25:37.154: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 20:25:37.154: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/29/23 20:25:37.154
Aug 29 20:25:37.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
Aug 29 20:25:37.270: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 20:25:37.270: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/29/23 20:25:37.27
Aug 29 20:25:37.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
Aug 29 20:25:37.377: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 20:25:37.377: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:37.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6590" for this suite. 08/29/23 20:25:37.383
------------------------------
• [SLOW TEST] [8.053 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:29.338
    Aug 29 20:25:29.338: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:25:29.339
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:29.358
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:29.361
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 08/29/23 20:25:29.364
    Aug 29 20:25:29.364: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 29 20:25:29.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
    Aug 29 20:25:29.724: INFO: stderr: ""
    Aug 29 20:25:29.724: INFO: stdout: "service/agnhost-replica created\n"
    Aug 29 20:25:29.725: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 29 20:25:29.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
    Aug 29 20:25:30.068: INFO: stderr: ""
    Aug 29 20:25:30.068: INFO: stdout: "service/agnhost-primary created\n"
    Aug 29 20:25:30.068: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 29 20:25:30.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
    Aug 29 20:25:30.437: INFO: stderr: ""
    Aug 29 20:25:30.437: INFO: stdout: "service/frontend created\n"
    Aug 29 20:25:30.437: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 29 20:25:30.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
    Aug 29 20:25:30.803: INFO: stderr: ""
    Aug 29 20:25:30.803: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 29 20:25:30.803: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 29 20:25:30.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
    Aug 29 20:25:31.198: INFO: stderr: ""
    Aug 29 20:25:31.198: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 29 20:25:31.198: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 29 20:25:31.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 create -f -'
    Aug 29 20:25:31.611: INFO: stderr: ""
    Aug 29 20:25:31.611: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/29/23 20:25:31.611
    Aug 29 20:25:31.611: INFO: Waiting for all frontend pods to be Running.
    Aug 29 20:25:36.662: INFO: Waiting for frontend to serve content.
    Aug 29 20:25:36.674: INFO: Trying to add a new entry to the guestbook.
    Aug 29 20:25:36.687: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/29/23 20:25:36.698
    Aug 29 20:25:36.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
    Aug 29 20:25:36.816: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 20:25:36.816: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/29/23 20:25:36.816
    Aug 29 20:25:36.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
    Aug 29 20:25:36.925: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 20:25:36.925: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/29/23 20:25:36.926
    Aug 29 20:25:36.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
    Aug 29 20:25:37.036: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 20:25:37.036: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/29/23 20:25:37.037
    Aug 29 20:25:37.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
    Aug 29 20:25:37.154: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 20:25:37.154: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/29/23 20:25:37.154
    Aug 29 20:25:37.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
    Aug 29 20:25:37.270: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 20:25:37.270: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/29/23 20:25:37.27
    Aug 29 20:25:37.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-6590 delete --grace-period=0 --force -f -'
    Aug 29 20:25:37.377: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 20:25:37.377: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:37.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6590" for this suite. 08/29/23 20:25:37.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:37.393
Aug 29 20:25:37.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:25:37.394
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:37.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:37.423
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-0c5d6d44-6d12-4986-8541-13b67cb93f38 08/29/23 20:25:37.426
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:25:37.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-638" for this suite. 08/29/23 20:25:37.434
------------------------------
• [0.052 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:37.393
    Aug 29 20:25:37.393: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:25:37.394
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:37.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:37.423
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-0c5d6d44-6d12-4986-8541-13b67cb93f38 08/29/23 20:25:37.426
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:25:37.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-638" for this suite. 08/29/23 20:25:37.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:25:37.446
Aug 29 20:25:37.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename gc 08/29/23 20:25:37.447
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:37.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:37.475
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/29/23 20:25:37.486
STEP: delete the rc 08/29/23 20:25:42.504
STEP: wait for the rc to be deleted 08/29/23 20:25:42.518
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/29/23 20:25:47.523
STEP: Gathering metrics 08/29/23 20:26:17.541
W0829 20:26:17.551424      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 20:26:17.551: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 29 20:26:17.551: INFO: Deleting pod "simpletest.rc-278ck" in namespace "gc-1667"
Aug 29 20:26:17.564: INFO: Deleting pod "simpletest.rc-2gdjh" in namespace "gc-1667"
Aug 29 20:26:17.585: INFO: Deleting pod "simpletest.rc-2h5zz" in namespace "gc-1667"
Aug 29 20:26:17.604: INFO: Deleting pod "simpletest.rc-2jz55" in namespace "gc-1667"
Aug 29 20:26:17.625: INFO: Deleting pod "simpletest.rc-2sllj" in namespace "gc-1667"
Aug 29 20:26:17.644: INFO: Deleting pod "simpletest.rc-2wkkx" in namespace "gc-1667"
Aug 29 20:26:17.664: INFO: Deleting pod "simpletest.rc-458ps" in namespace "gc-1667"
Aug 29 20:26:17.685: INFO: Deleting pod "simpletest.rc-4cl52" in namespace "gc-1667"
Aug 29 20:26:17.709: INFO: Deleting pod "simpletest.rc-4hfbx" in namespace "gc-1667"
Aug 29 20:26:17.729: INFO: Deleting pod "simpletest.rc-4jmkg" in namespace "gc-1667"
Aug 29 20:26:17.751: INFO: Deleting pod "simpletest.rc-4nlpt" in namespace "gc-1667"
Aug 29 20:26:17.775: INFO: Deleting pod "simpletest.rc-4z2r8" in namespace "gc-1667"
Aug 29 20:26:17.798: INFO: Deleting pod "simpletest.rc-57bk2" in namespace "gc-1667"
Aug 29 20:26:17.815: INFO: Deleting pod "simpletest.rc-586bn" in namespace "gc-1667"
Aug 29 20:26:17.832: INFO: Deleting pod "simpletest.rc-58bv5" in namespace "gc-1667"
Aug 29 20:26:17.873: INFO: Deleting pod "simpletest.rc-5fcmz" in namespace "gc-1667"
Aug 29 20:26:17.907: INFO: Deleting pod "simpletest.rc-5g2sw" in namespace "gc-1667"
Aug 29 20:26:17.940: INFO: Deleting pod "simpletest.rc-5mth9" in namespace "gc-1667"
Aug 29 20:26:17.968: INFO: Deleting pod "simpletest.rc-5trtx" in namespace "gc-1667"
Aug 29 20:26:17.997: INFO: Deleting pod "simpletest.rc-5wt7j" in namespace "gc-1667"
Aug 29 20:26:18.022: INFO: Deleting pod "simpletest.rc-5zgfc" in namespace "gc-1667"
Aug 29 20:26:18.050: INFO: Deleting pod "simpletest.rc-6hffp" in namespace "gc-1667"
Aug 29 20:26:18.099: INFO: Deleting pod "simpletest.rc-6m6lg" in namespace "gc-1667"
Aug 29 20:26:18.132: INFO: Deleting pod "simpletest.rc-6mvv2" in namespace "gc-1667"
Aug 29 20:26:18.153: INFO: Deleting pod "simpletest.rc-78hrx" in namespace "gc-1667"
Aug 29 20:26:18.177: INFO: Deleting pod "simpletest.rc-82zlp" in namespace "gc-1667"
Aug 29 20:26:18.198: INFO: Deleting pod "simpletest.rc-8hl96" in namespace "gc-1667"
Aug 29 20:26:18.223: INFO: Deleting pod "simpletest.rc-8mvwl" in namespace "gc-1667"
Aug 29 20:26:18.242: INFO: Deleting pod "simpletest.rc-8rgv6" in namespace "gc-1667"
Aug 29 20:26:18.271: INFO: Deleting pod "simpletest.rc-8rnhr" in namespace "gc-1667"
Aug 29 20:26:18.303: INFO: Deleting pod "simpletest.rc-8s7hp" in namespace "gc-1667"
Aug 29 20:26:18.321: INFO: Deleting pod "simpletest.rc-9hjvl" in namespace "gc-1667"
Aug 29 20:26:18.343: INFO: Deleting pod "simpletest.rc-9jxzh" in namespace "gc-1667"
Aug 29 20:26:18.366: INFO: Deleting pod "simpletest.rc-bh9kx" in namespace "gc-1667"
Aug 29 20:26:18.394: INFO: Deleting pod "simpletest.rc-bhdrk" in namespace "gc-1667"
Aug 29 20:26:18.426: INFO: Deleting pod "simpletest.rc-bqmb2" in namespace "gc-1667"
Aug 29 20:26:18.488: INFO: Deleting pod "simpletest.rc-c2wpp" in namespace "gc-1667"
Aug 29 20:26:18.521: INFO: Deleting pod "simpletest.rc-c8zds" in namespace "gc-1667"
Aug 29 20:26:18.592: INFO: Deleting pod "simpletest.rc-chdv6" in namespace "gc-1667"
Aug 29 20:26:18.648: INFO: Deleting pod "simpletest.rc-d5pk5" in namespace "gc-1667"
Aug 29 20:26:18.677: INFO: Deleting pod "simpletest.rc-dhdfv" in namespace "gc-1667"
Aug 29 20:26:18.722: INFO: Deleting pod "simpletest.rc-dhpkh" in namespace "gc-1667"
Aug 29 20:26:18.746: INFO: Deleting pod "simpletest.rc-djddg" in namespace "gc-1667"
Aug 29 20:26:18.775: INFO: Deleting pod "simpletest.rc-dlq9x" in namespace "gc-1667"
Aug 29 20:26:18.814: INFO: Deleting pod "simpletest.rc-f287h" in namespace "gc-1667"
Aug 29 20:26:18.841: INFO: Deleting pod "simpletest.rc-f2g8c" in namespace "gc-1667"
Aug 29 20:26:18.862: INFO: Deleting pod "simpletest.rc-f9cbd" in namespace "gc-1667"
Aug 29 20:26:18.910: INFO: Deleting pod "simpletest.rc-fgwsv" in namespace "gc-1667"
Aug 29 20:26:18.959: INFO: Deleting pod "simpletest.rc-frvv5" in namespace "gc-1667"
Aug 29 20:26:18.985: INFO: Deleting pod "simpletest.rc-fwznf" in namespace "gc-1667"
Aug 29 20:26:19.013: INFO: Deleting pod "simpletest.rc-fzzft" in namespace "gc-1667"
Aug 29 20:26:19.055: INFO: Deleting pod "simpletest.rc-g6qkt" in namespace "gc-1667"
Aug 29 20:26:19.090: INFO: Deleting pod "simpletest.rc-gddb4" in namespace "gc-1667"
Aug 29 20:26:19.117: INFO: Deleting pod "simpletest.rc-glmsf" in namespace "gc-1667"
Aug 29 20:26:19.153: INFO: Deleting pod "simpletest.rc-gvb47" in namespace "gc-1667"
Aug 29 20:26:19.182: INFO: Deleting pod "simpletest.rc-gvxf6" in namespace "gc-1667"
Aug 29 20:26:19.225: INFO: Deleting pod "simpletest.rc-gw6cc" in namespace "gc-1667"
Aug 29 20:26:19.259: INFO: Deleting pod "simpletest.rc-h9ktn" in namespace "gc-1667"
Aug 29 20:26:19.283: INFO: Deleting pod "simpletest.rc-hbwvl" in namespace "gc-1667"
Aug 29 20:26:19.310: INFO: Deleting pod "simpletest.rc-hhl4t" in namespace "gc-1667"
Aug 29 20:26:19.331: INFO: Deleting pod "simpletest.rc-jfgxb" in namespace "gc-1667"
Aug 29 20:26:19.350: INFO: Deleting pod "simpletest.rc-jhgrf" in namespace "gc-1667"
Aug 29 20:26:19.376: INFO: Deleting pod "simpletest.rc-jm9n5" in namespace "gc-1667"
Aug 29 20:26:19.398: INFO: Deleting pod "simpletest.rc-jns7p" in namespace "gc-1667"
Aug 29 20:26:19.423: INFO: Deleting pod "simpletest.rc-kf82r" in namespace "gc-1667"
Aug 29 20:26:19.446: INFO: Deleting pod "simpletest.rc-kx2x8" in namespace "gc-1667"
Aug 29 20:26:19.470: INFO: Deleting pod "simpletest.rc-l4ppz" in namespace "gc-1667"
Aug 29 20:26:19.493: INFO: Deleting pod "simpletest.rc-lpr84" in namespace "gc-1667"
Aug 29 20:26:19.532: INFO: Deleting pod "simpletest.rc-lqdds" in namespace "gc-1667"
Aug 29 20:26:19.581: INFO: Deleting pod "simpletest.rc-lwbnp" in namespace "gc-1667"
Aug 29 20:26:19.608: INFO: Deleting pod "simpletest.rc-lxdwg" in namespace "gc-1667"
Aug 29 20:26:19.631: INFO: Deleting pod "simpletest.rc-mbnhw" in namespace "gc-1667"
Aug 29 20:26:19.663: INFO: Deleting pod "simpletest.rc-mqmq7" in namespace "gc-1667"
Aug 29 20:26:19.696: INFO: Deleting pod "simpletest.rc-ns7bx" in namespace "gc-1667"
Aug 29 20:26:19.732: INFO: Deleting pod "simpletest.rc-nvtw7" in namespace "gc-1667"
Aug 29 20:26:19.774: INFO: Deleting pod "simpletest.rc-pt77k" in namespace "gc-1667"
Aug 29 20:26:19.830: INFO: Deleting pod "simpletest.rc-pv89s" in namespace "gc-1667"
Aug 29 20:26:19.874: INFO: Deleting pod "simpletest.rc-q8jcl" in namespace "gc-1667"
Aug 29 20:26:19.928: INFO: Deleting pod "simpletest.rc-qqhv9" in namespace "gc-1667"
Aug 29 20:26:19.994: INFO: Deleting pod "simpletest.rc-qqs6w" in namespace "gc-1667"
Aug 29 20:26:20.033: INFO: Deleting pod "simpletest.rc-rlc9x" in namespace "gc-1667"
Aug 29 20:26:20.077: INFO: Deleting pod "simpletest.rc-rtx4b" in namespace "gc-1667"
Aug 29 20:26:20.122: INFO: Deleting pod "simpletest.rc-s6jwq" in namespace "gc-1667"
Aug 29 20:26:20.163: INFO: Deleting pod "simpletest.rc-s87n4" in namespace "gc-1667"
Aug 29 20:26:20.202: INFO: Deleting pod "simpletest.rc-sd2n2" in namespace "gc-1667"
Aug 29 20:26:20.235: INFO: Deleting pod "simpletest.rc-sdmgd" in namespace "gc-1667"
Aug 29 20:26:20.295: INFO: Deleting pod "simpletest.rc-sp7s6" in namespace "gc-1667"
Aug 29 20:26:20.337: INFO: Deleting pod "simpletest.rc-sp94f" in namespace "gc-1667"
Aug 29 20:26:20.369: INFO: Deleting pod "simpletest.rc-t8l5k" in namespace "gc-1667"
Aug 29 20:26:20.430: INFO: Deleting pod "simpletest.rc-trc8g" in namespace "gc-1667"
Aug 29 20:26:20.484: INFO: Deleting pod "simpletest.rc-vhqmh" in namespace "gc-1667"
Aug 29 20:26:20.530: INFO: Deleting pod "simpletest.rc-vn7rz" in namespace "gc-1667"
Aug 29 20:26:20.575: INFO: Deleting pod "simpletest.rc-vnkfd" in namespace "gc-1667"
Aug 29 20:26:20.613: INFO: Deleting pod "simpletest.rc-vv8xt" in namespace "gc-1667"
Aug 29 20:26:20.639: INFO: Deleting pod "simpletest.rc-wbtrv" in namespace "gc-1667"
Aug 29 20:26:20.668: INFO: Deleting pod "simpletest.rc-wn7kg" in namespace "gc-1667"
Aug 29 20:26:20.701: INFO: Deleting pod "simpletest.rc-wqhhl" in namespace "gc-1667"
Aug 29 20:26:20.727: INFO: Deleting pod "simpletest.rc-xhqqx" in namespace "gc-1667"
Aug 29 20:26:20.749: INFO: Deleting pod "simpletest.rc-xvt9l" in namespace "gc-1667"
Aug 29 20:26:20.781: INFO: Deleting pod "simpletest.rc-zrzkq" in namespace "gc-1667"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 29 20:26:20.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1667" for this suite. 08/29/23 20:26:20.823
------------------------------
• [SLOW TEST] [43.387 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:25:37.446
    Aug 29 20:25:37.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename gc 08/29/23 20:25:37.447
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:25:37.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:25:37.475
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/29/23 20:25:37.486
    STEP: delete the rc 08/29/23 20:25:42.504
    STEP: wait for the rc to be deleted 08/29/23 20:25:42.518
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/29/23 20:25:47.523
    STEP: Gathering metrics 08/29/23 20:26:17.541
    W0829 20:26:17.551424      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 29 20:26:17.551: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 29 20:26:17.551: INFO: Deleting pod "simpletest.rc-278ck" in namespace "gc-1667"
    Aug 29 20:26:17.564: INFO: Deleting pod "simpletest.rc-2gdjh" in namespace "gc-1667"
    Aug 29 20:26:17.585: INFO: Deleting pod "simpletest.rc-2h5zz" in namespace "gc-1667"
    Aug 29 20:26:17.604: INFO: Deleting pod "simpletest.rc-2jz55" in namespace "gc-1667"
    Aug 29 20:26:17.625: INFO: Deleting pod "simpletest.rc-2sllj" in namespace "gc-1667"
    Aug 29 20:26:17.644: INFO: Deleting pod "simpletest.rc-2wkkx" in namespace "gc-1667"
    Aug 29 20:26:17.664: INFO: Deleting pod "simpletest.rc-458ps" in namespace "gc-1667"
    Aug 29 20:26:17.685: INFO: Deleting pod "simpletest.rc-4cl52" in namespace "gc-1667"
    Aug 29 20:26:17.709: INFO: Deleting pod "simpletest.rc-4hfbx" in namespace "gc-1667"
    Aug 29 20:26:17.729: INFO: Deleting pod "simpletest.rc-4jmkg" in namespace "gc-1667"
    Aug 29 20:26:17.751: INFO: Deleting pod "simpletest.rc-4nlpt" in namespace "gc-1667"
    Aug 29 20:26:17.775: INFO: Deleting pod "simpletest.rc-4z2r8" in namespace "gc-1667"
    Aug 29 20:26:17.798: INFO: Deleting pod "simpletest.rc-57bk2" in namespace "gc-1667"
    Aug 29 20:26:17.815: INFO: Deleting pod "simpletest.rc-586bn" in namespace "gc-1667"
    Aug 29 20:26:17.832: INFO: Deleting pod "simpletest.rc-58bv5" in namespace "gc-1667"
    Aug 29 20:26:17.873: INFO: Deleting pod "simpletest.rc-5fcmz" in namespace "gc-1667"
    Aug 29 20:26:17.907: INFO: Deleting pod "simpletest.rc-5g2sw" in namespace "gc-1667"
    Aug 29 20:26:17.940: INFO: Deleting pod "simpletest.rc-5mth9" in namespace "gc-1667"
    Aug 29 20:26:17.968: INFO: Deleting pod "simpletest.rc-5trtx" in namespace "gc-1667"
    Aug 29 20:26:17.997: INFO: Deleting pod "simpletest.rc-5wt7j" in namespace "gc-1667"
    Aug 29 20:26:18.022: INFO: Deleting pod "simpletest.rc-5zgfc" in namespace "gc-1667"
    Aug 29 20:26:18.050: INFO: Deleting pod "simpletest.rc-6hffp" in namespace "gc-1667"
    Aug 29 20:26:18.099: INFO: Deleting pod "simpletest.rc-6m6lg" in namespace "gc-1667"
    Aug 29 20:26:18.132: INFO: Deleting pod "simpletest.rc-6mvv2" in namespace "gc-1667"
    Aug 29 20:26:18.153: INFO: Deleting pod "simpletest.rc-78hrx" in namespace "gc-1667"
    Aug 29 20:26:18.177: INFO: Deleting pod "simpletest.rc-82zlp" in namespace "gc-1667"
    Aug 29 20:26:18.198: INFO: Deleting pod "simpletest.rc-8hl96" in namespace "gc-1667"
    Aug 29 20:26:18.223: INFO: Deleting pod "simpletest.rc-8mvwl" in namespace "gc-1667"
    Aug 29 20:26:18.242: INFO: Deleting pod "simpletest.rc-8rgv6" in namespace "gc-1667"
    Aug 29 20:26:18.271: INFO: Deleting pod "simpletest.rc-8rnhr" in namespace "gc-1667"
    Aug 29 20:26:18.303: INFO: Deleting pod "simpletest.rc-8s7hp" in namespace "gc-1667"
    Aug 29 20:26:18.321: INFO: Deleting pod "simpletest.rc-9hjvl" in namespace "gc-1667"
    Aug 29 20:26:18.343: INFO: Deleting pod "simpletest.rc-9jxzh" in namespace "gc-1667"
    Aug 29 20:26:18.366: INFO: Deleting pod "simpletest.rc-bh9kx" in namespace "gc-1667"
    Aug 29 20:26:18.394: INFO: Deleting pod "simpletest.rc-bhdrk" in namespace "gc-1667"
    Aug 29 20:26:18.426: INFO: Deleting pod "simpletest.rc-bqmb2" in namespace "gc-1667"
    Aug 29 20:26:18.488: INFO: Deleting pod "simpletest.rc-c2wpp" in namespace "gc-1667"
    Aug 29 20:26:18.521: INFO: Deleting pod "simpletest.rc-c8zds" in namespace "gc-1667"
    Aug 29 20:26:18.592: INFO: Deleting pod "simpletest.rc-chdv6" in namespace "gc-1667"
    Aug 29 20:26:18.648: INFO: Deleting pod "simpletest.rc-d5pk5" in namespace "gc-1667"
    Aug 29 20:26:18.677: INFO: Deleting pod "simpletest.rc-dhdfv" in namespace "gc-1667"
    Aug 29 20:26:18.722: INFO: Deleting pod "simpletest.rc-dhpkh" in namespace "gc-1667"
    Aug 29 20:26:18.746: INFO: Deleting pod "simpletest.rc-djddg" in namespace "gc-1667"
    Aug 29 20:26:18.775: INFO: Deleting pod "simpletest.rc-dlq9x" in namespace "gc-1667"
    Aug 29 20:26:18.814: INFO: Deleting pod "simpletest.rc-f287h" in namespace "gc-1667"
    Aug 29 20:26:18.841: INFO: Deleting pod "simpletest.rc-f2g8c" in namespace "gc-1667"
    Aug 29 20:26:18.862: INFO: Deleting pod "simpletest.rc-f9cbd" in namespace "gc-1667"
    Aug 29 20:26:18.910: INFO: Deleting pod "simpletest.rc-fgwsv" in namespace "gc-1667"
    Aug 29 20:26:18.959: INFO: Deleting pod "simpletest.rc-frvv5" in namespace "gc-1667"
    Aug 29 20:26:18.985: INFO: Deleting pod "simpletest.rc-fwznf" in namespace "gc-1667"
    Aug 29 20:26:19.013: INFO: Deleting pod "simpletest.rc-fzzft" in namespace "gc-1667"
    Aug 29 20:26:19.055: INFO: Deleting pod "simpletest.rc-g6qkt" in namespace "gc-1667"
    Aug 29 20:26:19.090: INFO: Deleting pod "simpletest.rc-gddb4" in namespace "gc-1667"
    Aug 29 20:26:19.117: INFO: Deleting pod "simpletest.rc-glmsf" in namespace "gc-1667"
    Aug 29 20:26:19.153: INFO: Deleting pod "simpletest.rc-gvb47" in namespace "gc-1667"
    Aug 29 20:26:19.182: INFO: Deleting pod "simpletest.rc-gvxf6" in namespace "gc-1667"
    Aug 29 20:26:19.225: INFO: Deleting pod "simpletest.rc-gw6cc" in namespace "gc-1667"
    Aug 29 20:26:19.259: INFO: Deleting pod "simpletest.rc-h9ktn" in namespace "gc-1667"
    Aug 29 20:26:19.283: INFO: Deleting pod "simpletest.rc-hbwvl" in namespace "gc-1667"
    Aug 29 20:26:19.310: INFO: Deleting pod "simpletest.rc-hhl4t" in namespace "gc-1667"
    Aug 29 20:26:19.331: INFO: Deleting pod "simpletest.rc-jfgxb" in namespace "gc-1667"
    Aug 29 20:26:19.350: INFO: Deleting pod "simpletest.rc-jhgrf" in namespace "gc-1667"
    Aug 29 20:26:19.376: INFO: Deleting pod "simpletest.rc-jm9n5" in namespace "gc-1667"
    Aug 29 20:26:19.398: INFO: Deleting pod "simpletest.rc-jns7p" in namespace "gc-1667"
    Aug 29 20:26:19.423: INFO: Deleting pod "simpletest.rc-kf82r" in namespace "gc-1667"
    Aug 29 20:26:19.446: INFO: Deleting pod "simpletest.rc-kx2x8" in namespace "gc-1667"
    Aug 29 20:26:19.470: INFO: Deleting pod "simpletest.rc-l4ppz" in namespace "gc-1667"
    Aug 29 20:26:19.493: INFO: Deleting pod "simpletest.rc-lpr84" in namespace "gc-1667"
    Aug 29 20:26:19.532: INFO: Deleting pod "simpletest.rc-lqdds" in namespace "gc-1667"
    Aug 29 20:26:19.581: INFO: Deleting pod "simpletest.rc-lwbnp" in namespace "gc-1667"
    Aug 29 20:26:19.608: INFO: Deleting pod "simpletest.rc-lxdwg" in namespace "gc-1667"
    Aug 29 20:26:19.631: INFO: Deleting pod "simpletest.rc-mbnhw" in namespace "gc-1667"
    Aug 29 20:26:19.663: INFO: Deleting pod "simpletest.rc-mqmq7" in namespace "gc-1667"
    Aug 29 20:26:19.696: INFO: Deleting pod "simpletest.rc-ns7bx" in namespace "gc-1667"
    Aug 29 20:26:19.732: INFO: Deleting pod "simpletest.rc-nvtw7" in namespace "gc-1667"
    Aug 29 20:26:19.774: INFO: Deleting pod "simpletest.rc-pt77k" in namespace "gc-1667"
    Aug 29 20:26:19.830: INFO: Deleting pod "simpletest.rc-pv89s" in namespace "gc-1667"
    Aug 29 20:26:19.874: INFO: Deleting pod "simpletest.rc-q8jcl" in namespace "gc-1667"
    Aug 29 20:26:19.928: INFO: Deleting pod "simpletest.rc-qqhv9" in namespace "gc-1667"
    Aug 29 20:26:19.994: INFO: Deleting pod "simpletest.rc-qqs6w" in namespace "gc-1667"
    Aug 29 20:26:20.033: INFO: Deleting pod "simpletest.rc-rlc9x" in namespace "gc-1667"
    Aug 29 20:26:20.077: INFO: Deleting pod "simpletest.rc-rtx4b" in namespace "gc-1667"
    Aug 29 20:26:20.122: INFO: Deleting pod "simpletest.rc-s6jwq" in namespace "gc-1667"
    Aug 29 20:26:20.163: INFO: Deleting pod "simpletest.rc-s87n4" in namespace "gc-1667"
    Aug 29 20:26:20.202: INFO: Deleting pod "simpletest.rc-sd2n2" in namespace "gc-1667"
    Aug 29 20:26:20.235: INFO: Deleting pod "simpletest.rc-sdmgd" in namespace "gc-1667"
    Aug 29 20:26:20.295: INFO: Deleting pod "simpletest.rc-sp7s6" in namespace "gc-1667"
    Aug 29 20:26:20.337: INFO: Deleting pod "simpletest.rc-sp94f" in namespace "gc-1667"
    Aug 29 20:26:20.369: INFO: Deleting pod "simpletest.rc-t8l5k" in namespace "gc-1667"
    Aug 29 20:26:20.430: INFO: Deleting pod "simpletest.rc-trc8g" in namespace "gc-1667"
    Aug 29 20:26:20.484: INFO: Deleting pod "simpletest.rc-vhqmh" in namespace "gc-1667"
    Aug 29 20:26:20.530: INFO: Deleting pod "simpletest.rc-vn7rz" in namespace "gc-1667"
    Aug 29 20:26:20.575: INFO: Deleting pod "simpletest.rc-vnkfd" in namespace "gc-1667"
    Aug 29 20:26:20.613: INFO: Deleting pod "simpletest.rc-vv8xt" in namespace "gc-1667"
    Aug 29 20:26:20.639: INFO: Deleting pod "simpletest.rc-wbtrv" in namespace "gc-1667"
    Aug 29 20:26:20.668: INFO: Deleting pod "simpletest.rc-wn7kg" in namespace "gc-1667"
    Aug 29 20:26:20.701: INFO: Deleting pod "simpletest.rc-wqhhl" in namespace "gc-1667"
    Aug 29 20:26:20.727: INFO: Deleting pod "simpletest.rc-xhqqx" in namespace "gc-1667"
    Aug 29 20:26:20.749: INFO: Deleting pod "simpletest.rc-xvt9l" in namespace "gc-1667"
    Aug 29 20:26:20.781: INFO: Deleting pod "simpletest.rc-zrzkq" in namespace "gc-1667"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:26:20.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1667" for this suite. 08/29/23 20:26:20.823
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:26:20.834
Aug 29 20:26:20.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-watch 08/29/23 20:26:20.835
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:26:20.887
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:26:20.893
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 29 20:26:20.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Creating first CR  08/29/23 20:26:23.473
Aug 29 20:26:23.479: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:23Z]] name:name1 resourceVersion:34341 uid:0a71ad51-39ae-4945-8a7c-3207a453d7e8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/29/23 20:26:33.48
Aug 29 20:26:33.489: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:33Z]] name:name2 resourceVersion:34883 uid:102613bb-ef5f-4fa9-94ae-c92594e2e146] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/29/23 20:26:43.489
Aug 29 20:26:43.498: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:43Z]] name:name1 resourceVersion:34911 uid:0a71ad51-39ae-4945-8a7c-3207a453d7e8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/29/23 20:26:53.499
Aug 29 20:26:53.509: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:53Z]] name:name2 resourceVersion:34939 uid:102613bb-ef5f-4fa9-94ae-c92594e2e146] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/29/23 20:27:03.509
Aug 29 20:27:03.518: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:43Z]] name:name1 resourceVersion:34967 uid:0a71ad51-39ae-4945-8a7c-3207a453d7e8] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/29/23 20:27:13.519
Aug 29 20:27:13.528: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:53Z]] name:name2 resourceVersion:34995 uid:102613bb-ef5f-4fa9-94ae-c92594e2e146] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:27:24.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-7666" for this suite. 08/29/23 20:27:24.049
------------------------------
• [SLOW TEST] [63.223 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:26:20.834
    Aug 29 20:26:20.834: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-watch 08/29/23 20:26:20.835
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:26:20.887
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:26:20.893
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 29 20:26:20.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Creating first CR  08/29/23 20:26:23.473
    Aug 29 20:26:23.479: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:23Z]] name:name1 resourceVersion:34341 uid:0a71ad51-39ae-4945-8a7c-3207a453d7e8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/29/23 20:26:33.48
    Aug 29 20:26:33.489: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:33Z]] name:name2 resourceVersion:34883 uid:102613bb-ef5f-4fa9-94ae-c92594e2e146] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/29/23 20:26:43.489
    Aug 29 20:26:43.498: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:43Z]] name:name1 resourceVersion:34911 uid:0a71ad51-39ae-4945-8a7c-3207a453d7e8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/29/23 20:26:53.499
    Aug 29 20:26:53.509: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:53Z]] name:name2 resourceVersion:34939 uid:102613bb-ef5f-4fa9-94ae-c92594e2e146] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/29/23 20:27:03.509
    Aug 29 20:27:03.518: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:43Z]] name:name1 resourceVersion:34967 uid:0a71ad51-39ae-4945-8a7c-3207a453d7e8] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/29/23 20:27:13.519
    Aug 29 20:27:13.528: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-29T20:26:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-29T20:26:53Z]] name:name2 resourceVersion:34995 uid:102613bb-ef5f-4fa9-94ae-c92594e2e146] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:27:24.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-7666" for this suite. 08/29/23 20:27:24.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:27:24.058
Aug 29 20:27:24.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:27:24.059
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:24.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:24.084
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-e8431aef-2957-4231-b431-ebdcbe514a44 08/29/23 20:27:24.087
STEP: Creating a pod to test consume configMaps 08/29/23 20:27:24.093
Aug 29 20:27:24.104: INFO: Waiting up to 5m0s for pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87" in namespace "configmap-6247" to be "Succeeded or Failed"
Aug 29 20:27:24.107: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297893ms
Aug 29 20:27:26.111: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87": Phase="Running", Reason="", readiness=true. Elapsed: 2.007123074s
Aug 29 20:27:28.113: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87": Phase="Running", Reason="", readiness=false. Elapsed: 4.009276998s
Aug 29 20:27:30.112: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008450603s
STEP: Saw pod success 08/29/23 20:27:30.112
Aug 29 20:27:30.113: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87" satisfied condition "Succeeded or Failed"
Aug 29 20:27:30.117: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87 container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:27:30.134
Aug 29 20:27:30.150: INFO: Waiting for pod pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87 to disappear
Aug 29 20:27:30.153: INFO: Pod pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:27:30.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6247" for this suite. 08/29/23 20:27:30.158
------------------------------
• [SLOW TEST] [6.107 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:27:24.058
    Aug 29 20:27:24.058: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:27:24.059
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:24.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:24.084
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-e8431aef-2957-4231-b431-ebdcbe514a44 08/29/23 20:27:24.087
    STEP: Creating a pod to test consume configMaps 08/29/23 20:27:24.093
    Aug 29 20:27:24.104: INFO: Waiting up to 5m0s for pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87" in namespace "configmap-6247" to be "Succeeded or Failed"
    Aug 29 20:27:24.107: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297893ms
    Aug 29 20:27:26.111: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87": Phase="Running", Reason="", readiness=true. Elapsed: 2.007123074s
    Aug 29 20:27:28.113: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87": Phase="Running", Reason="", readiness=false. Elapsed: 4.009276998s
    Aug 29 20:27:30.112: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008450603s
    STEP: Saw pod success 08/29/23 20:27:30.112
    Aug 29 20:27:30.113: INFO: Pod "pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87" satisfied condition "Succeeded or Failed"
    Aug 29 20:27:30.117: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87 container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:27:30.134
    Aug 29 20:27:30.150: INFO: Waiting for pod pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87 to disappear
    Aug 29 20:27:30.153: INFO: Pod pod-configmaps-71b271f7-5686-4b5b-979a-08137babae87 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:27:30.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6247" for this suite. 08/29/23 20:27:30.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:27:30.165
Aug 29 20:27:30.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:27:30.167
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:30.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:30.192
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 08/29/23 20:27:30.195
Aug 29 20:27:30.207: INFO: Waiting up to 5m0s for pod "pod-tpjgg" in namespace "pods-4084" to be "running"
Aug 29 20:27:30.210: INFO: Pod "pod-tpjgg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.945803ms
Aug 29 20:27:32.215: INFO: Pod "pod-tpjgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.008156949s
Aug 29 20:27:32.215: INFO: Pod "pod-tpjgg" satisfied condition "running"
STEP: patching /status 08/29/23 20:27:32.215
Aug 29 20:27:32.226: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:27:32.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4084" for this suite. 08/29/23 20:27:32.232
------------------------------
• [2.075 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:27:30.165
    Aug 29 20:27:30.166: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:27:30.167
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:30.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:30.192
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 08/29/23 20:27:30.195
    Aug 29 20:27:30.207: INFO: Waiting up to 5m0s for pod "pod-tpjgg" in namespace "pods-4084" to be "running"
    Aug 29 20:27:30.210: INFO: Pod "pod-tpjgg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.945803ms
    Aug 29 20:27:32.215: INFO: Pod "pod-tpjgg": Phase="Running", Reason="", readiness=true. Elapsed: 2.008156949s
    Aug 29 20:27:32.215: INFO: Pod "pod-tpjgg" satisfied condition "running"
    STEP: patching /status 08/29/23 20:27:32.215
    Aug 29 20:27:32.226: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:27:32.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4084" for this suite. 08/29/23 20:27:32.232
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:27:32.241
Aug 29 20:27:32.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename runtimeclass 08/29/23 20:27:32.242
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:32.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:32.268
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 29 20:27:32.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-449" for this suite. 08/29/23 20:27:32.285
------------------------------
• [0.051 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:27:32.241
    Aug 29 20:27:32.241: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename runtimeclass 08/29/23 20:27:32.242
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:32.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:32.268
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:27:32.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-449" for this suite. 08/29/23 20:27:32.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:27:32.293
Aug 29 20:27:32.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:27:32.294
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:32.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:32.32
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 29 20:27:32.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:27:33.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3916" for this suite. 08/29/23 20:27:33.359
------------------------------
• [1.074 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:27:32.293
    Aug 29 20:27:32.293: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:27:32.294
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:32.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:32.32
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 29 20:27:32.323: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:27:33.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3916" for this suite. 08/29/23 20:27:33.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:27:33.368
Aug 29 20:27:33.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename svc-latency 08/29/23 20:27:33.369
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:33.392
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:33.395
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 29 20:27:33.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7554 08/29/23 20:27:33.399
I0829 20:27:33.405516      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7554, replica count: 1
I0829 20:27:34.456035      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0829 20:27:35.456743      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 20:27:35.575: INFO: Created: latency-svc-ptxzr
Aug 29 20:27:35.585: INFO: Got endpoints: latency-svc-ptxzr [28.522985ms]
Aug 29 20:27:35.602: INFO: Created: latency-svc-m8cm2
Aug 29 20:27:35.612: INFO: Got endpoints: latency-svc-m8cm2 [26.651256ms]
Aug 29 20:27:35.615: INFO: Created: latency-svc-84nlp
Aug 29 20:27:35.625: INFO: Got endpoints: latency-svc-84nlp [38.546283ms]
Aug 29 20:27:35.632: INFO: Created: latency-svc-jrcrc
Aug 29 20:27:35.643: INFO: Got endpoints: latency-svc-jrcrc [57.410746ms]
Aug 29 20:27:35.644: INFO: Created: latency-svc-qwr5h
Aug 29 20:27:35.653: INFO: Got endpoints: latency-svc-qwr5h [67.285602ms]
Aug 29 20:27:35.659: INFO: Created: latency-svc-4878b
Aug 29 20:27:35.669: INFO: Got endpoints: latency-svc-4878b [83.285162ms]
Aug 29 20:27:35.677: INFO: Created: latency-svc-xcszl
Aug 29 20:27:35.687: INFO: Created: latency-svc-ztdzp
Aug 29 20:27:35.689: INFO: Got endpoints: latency-svc-xcszl [103.388017ms]
Aug 29 20:27:35.698: INFO: Got endpoints: latency-svc-ztdzp [111.968225ms]
Aug 29 20:27:35.700: INFO: Created: latency-svc-49964
Aug 29 20:27:35.714: INFO: Got endpoints: latency-svc-49964 [127.830361ms]
Aug 29 20:27:35.830: INFO: Created: latency-svc-s2x4j
Aug 29 20:27:35.830: INFO: Created: latency-svc-nvkdz
Aug 29 20:27:35.831: INFO: Created: latency-svc-289kg
Aug 29 20:27:35.831: INFO: Created: latency-svc-hqhxw
Aug 29 20:27:35.832: INFO: Created: latency-svc-8s2lk
Aug 29 20:27:35.832: INFO: Created: latency-svc-fjz7f
Aug 29 20:27:35.834: INFO: Created: latency-svc-28nls
Aug 29 20:27:35.834: INFO: Created: latency-svc-7xzd7
Aug 29 20:27:35.834: INFO: Created: latency-svc-9zrnl
Aug 29 20:27:35.835: INFO: Created: latency-svc-dbnfn
Aug 29 20:27:35.835: INFO: Created: latency-svc-l47kw
Aug 29 20:27:35.835: INFO: Created: latency-svc-s5bdn
Aug 29 20:27:35.835: INFO: Created: latency-svc-99wsg
Aug 29 20:27:35.836: INFO: Created: latency-svc-qzm9t
Aug 29 20:27:35.836: INFO: Created: latency-svc-bz5wm
Aug 29 20:27:35.849: INFO: Got endpoints: latency-svc-s2x4j [159.524026ms]
Aug 29 20:27:35.850: INFO: Got endpoints: latency-svc-289kg [264.050529ms]
Aug 29 20:27:35.852: INFO: Got endpoints: latency-svc-nvkdz [266.294778ms]
Aug 29 20:27:35.856: INFO: Got endpoints: latency-svc-l47kw [213.155225ms]
Aug 29 20:27:35.859: INFO: Got endpoints: latency-svc-dbnfn [190.09591ms]
Aug 29 20:27:35.862: INFO: Got endpoints: latency-svc-8s2lk [148.196116ms]
Aug 29 20:27:35.863: INFO: Got endpoints: latency-svc-fjz7f [276.671884ms]
Aug 29 20:27:35.872: INFO: Got endpoints: latency-svc-bz5wm [259.643094ms]
Aug 29 20:27:35.873: INFO: Got endpoints: latency-svc-hqhxw [219.639349ms]
Aug 29 20:27:35.876: INFO: Got endpoints: latency-svc-28nls [250.954665ms]
Aug 29 20:27:35.880: INFO: Created: latency-svc-xljcf
Aug 29 20:27:35.881: INFO: Got endpoints: latency-svc-7xzd7 [294.861088ms]
Aug 29 20:27:35.881: INFO: Got endpoints: latency-svc-99wsg [294.992913ms]
Aug 29 20:27:35.881: INFO: Got endpoints: latency-svc-9zrnl [294.761534ms]
Aug 29 20:27:35.887: INFO: Got endpoints: latency-svc-qzm9t [189.306057ms]
Aug 29 20:27:35.888: INFO: Got endpoints: latency-svc-s5bdn [302.474583ms]
Aug 29 20:27:35.893: INFO: Got endpoints: latency-svc-xljcf [43.22125ms]
Aug 29 20:27:35.897: INFO: Created: latency-svc-xwh5n
Aug 29 20:27:35.908: INFO: Got endpoints: latency-svc-xwh5n [59.40133ms]
Aug 29 20:27:35.915: INFO: Created: latency-svc-phm7d
Aug 29 20:27:35.917: INFO: Created: latency-svc-q85vn
Aug 29 20:27:35.925: INFO: Got endpoints: latency-svc-phm7d [72.842211ms]
Aug 29 20:27:35.930: INFO: Created: latency-svc-v948z
Aug 29 20:27:35.930: INFO: Got endpoints: latency-svc-q85vn [73.736251ms]
Aug 29 20:27:35.944: INFO: Got endpoints: latency-svc-v948z [84.753766ms]
Aug 29 20:27:35.948: INFO: Created: latency-svc-m4mml
Aug 29 20:27:35.949: INFO: Created: latency-svc-rzxdb
Aug 29 20:27:35.958: INFO: Got endpoints: latency-svc-rzxdb [95.905102ms]
Aug 29 20:27:35.960: INFO: Created: latency-svc-8bb79
Aug 29 20:27:35.963: INFO: Got endpoints: latency-svc-m4mml [99.896237ms]
Aug 29 20:27:35.969: INFO: Got endpoints: latency-svc-8bb79 [96.742902ms]
Aug 29 20:27:35.972: INFO: Created: latency-svc-rkxmw
Aug 29 20:27:35.981: INFO: Created: latency-svc-fcqml
Aug 29 20:27:35.984: INFO: Got endpoints: latency-svc-rkxmw [111.636411ms]
Aug 29 20:27:35.996: INFO: Got endpoints: latency-svc-fcqml [120.488433ms]
Aug 29 20:27:36.000: INFO: Created: latency-svc-6gnln
Aug 29 20:27:36.008: INFO: Got endpoints: latency-svc-6gnln [127.250082ms]
Aug 29 20:27:36.015: INFO: Created: latency-svc-lzj5f
Aug 29 20:27:36.023: INFO: Got endpoints: latency-svc-lzj5f [141.840375ms]
Aug 29 20:27:36.029: INFO: Created: latency-svc-x4khq
Aug 29 20:27:36.041: INFO: Created: latency-svc-vfsjl
Aug 29 20:27:36.044: INFO: Got endpoints: latency-svc-x4khq [163.198432ms]
Aug 29 20:27:36.055: INFO: Got endpoints: latency-svc-vfsjl [167.307888ms]
Aug 29 20:27:36.061: INFO: Created: latency-svc-ldw6z
Aug 29 20:27:36.063: INFO: Created: latency-svc-ps7gt
Aug 29 20:27:36.075: INFO: Created: latency-svc-pq94q
Aug 29 20:27:36.076: INFO: Got endpoints: latency-svc-ldw6z [187.546528ms]
Aug 29 20:27:36.082: INFO: Got endpoints: latency-svc-ps7gt [189.422243ms]
Aug 29 20:27:36.090: INFO: Created: latency-svc-dhqsm
Aug 29 20:27:36.095: INFO: Created: latency-svc-z8266
Aug 29 20:27:36.104: INFO: Created: latency-svc-l6pxp
Aug 29 20:27:36.116: INFO: Created: latency-svc-cprxw
Aug 29 20:27:36.125: INFO: Created: latency-svc-jp5q6
Aug 29 20:27:36.137: INFO: Got endpoints: latency-svc-pq94q [228.809809ms]
Aug 29 20:27:36.140: INFO: Created: latency-svc-gz75r
Aug 29 20:27:36.151: INFO: Created: latency-svc-t52jx
Aug 29 20:27:36.160: INFO: Created: latency-svc-7hg7w
Aug 29 20:27:36.174: INFO: Created: latency-svc-c57j6
Aug 29 20:27:36.191: INFO: Got endpoints: latency-svc-dhqsm [265.770052ms]
Aug 29 20:27:36.191: INFO: Created: latency-svc-wfqbv
Aug 29 20:27:36.199: INFO: Created: latency-svc-59bmb
Aug 29 20:27:36.215: INFO: Created: latency-svc-npd9r
Aug 29 20:27:36.227: INFO: Created: latency-svc-8bmtb
Aug 29 20:27:36.236: INFO: Got endpoints: latency-svc-z8266 [306.205478ms]
Aug 29 20:27:36.245: INFO: Created: latency-svc-645kv
Aug 29 20:27:36.255: INFO: Created: latency-svc-nhccd
Aug 29 20:27:36.271: INFO: Created: latency-svc-2tvht
Aug 29 20:27:36.273: INFO: Created: latency-svc-4vr6n
Aug 29 20:27:36.285: INFO: Got endpoints: latency-svc-l6pxp [341.321874ms]
Aug 29 20:27:36.303: INFO: Created: latency-svc-wng6j
Aug 29 20:27:36.335: INFO: Got endpoints: latency-svc-cprxw [376.928685ms]
Aug 29 20:27:36.349: INFO: Created: latency-svc-knxnh
Aug 29 20:27:36.385: INFO: Got endpoints: latency-svc-jp5q6 [422.251057ms]
Aug 29 20:27:36.400: INFO: Created: latency-svc-lsklq
Aug 29 20:27:36.436: INFO: Got endpoints: latency-svc-gz75r [466.967533ms]
Aug 29 20:27:36.454: INFO: Created: latency-svc-sjwkj
Aug 29 20:27:36.483: INFO: Got endpoints: latency-svc-t52jx [498.927634ms]
Aug 29 20:27:36.499: INFO: Created: latency-svc-5tlcz
Aug 29 20:27:36.535: INFO: Got endpoints: latency-svc-7hg7w [538.853996ms]
Aug 29 20:27:36.549: INFO: Created: latency-svc-kk69c
Aug 29 20:27:36.585: INFO: Got endpoints: latency-svc-c57j6 [576.60324ms]
Aug 29 20:27:36.600: INFO: Created: latency-svc-67nkk
Aug 29 20:27:36.635: INFO: Got endpoints: latency-svc-wfqbv [611.828572ms]
Aug 29 20:27:36.652: INFO: Created: latency-svc-gkl44
Aug 29 20:27:36.683: INFO: Got endpoints: latency-svc-59bmb [638.75294ms]
Aug 29 20:27:36.698: INFO: Created: latency-svc-mv695
Aug 29 20:27:36.734: INFO: Got endpoints: latency-svc-npd9r [679.739028ms]
Aug 29 20:27:36.755: INFO: Created: latency-svc-g9t5s
Aug 29 20:27:36.784: INFO: Got endpoints: latency-svc-8bmtb [708.208594ms]
Aug 29 20:27:36.798: INFO: Created: latency-svc-t5mjg
Aug 29 20:27:36.837: INFO: Got endpoints: latency-svc-645kv [754.589484ms]
Aug 29 20:27:36.851: INFO: Created: latency-svc-zg5h2
Aug 29 20:27:36.885: INFO: Got endpoints: latency-svc-nhccd [747.190109ms]
Aug 29 20:27:36.903: INFO: Created: latency-svc-xhp6l
Aug 29 20:27:36.933: INFO: Got endpoints: latency-svc-2tvht [741.593744ms]
Aug 29 20:27:36.947: INFO: Created: latency-svc-nn5cj
Aug 29 20:27:36.984: INFO: Got endpoints: latency-svc-4vr6n [747.760724ms]
Aug 29 20:27:36.999: INFO: Created: latency-svc-6j8gm
Aug 29 20:27:37.036: INFO: Got endpoints: latency-svc-wng6j [751.128961ms]
Aug 29 20:27:37.054: INFO: Created: latency-svc-52d7d
Aug 29 20:27:37.085: INFO: Got endpoints: latency-svc-knxnh [750.276687ms]
Aug 29 20:27:37.102: INFO: Created: latency-svc-vbrdf
Aug 29 20:27:37.134: INFO: Got endpoints: latency-svc-lsklq [749.332511ms]
Aug 29 20:27:37.151: INFO: Created: latency-svc-np4gw
Aug 29 20:27:37.183: INFO: Got endpoints: latency-svc-sjwkj [746.272919ms]
Aug 29 20:27:37.200: INFO: Created: latency-svc-g7d2f
Aug 29 20:27:37.234: INFO: Got endpoints: latency-svc-5tlcz [751.129816ms]
Aug 29 20:27:37.251: INFO: Created: latency-svc-25x8t
Aug 29 20:27:37.284: INFO: Got endpoints: latency-svc-kk69c [748.815137ms]
Aug 29 20:27:37.297: INFO: Created: latency-svc-hzd25
Aug 29 20:27:37.337: INFO: Got endpoints: latency-svc-67nkk [752.082544ms]
Aug 29 20:27:37.352: INFO: Created: latency-svc-7ntb5
Aug 29 20:27:37.390: INFO: Got endpoints: latency-svc-gkl44 [754.967406ms]
Aug 29 20:27:37.408: INFO: Created: latency-svc-thxtp
Aug 29 20:27:37.438: INFO: Got endpoints: latency-svc-mv695 [755.345076ms]
Aug 29 20:27:37.454: INFO: Created: latency-svc-wfdxq
Aug 29 20:27:37.487: INFO: Got endpoints: latency-svc-g9t5s [749.500539ms]
Aug 29 20:27:37.502: INFO: Created: latency-svc-29b9v
Aug 29 20:27:37.533: INFO: Got endpoints: latency-svc-t5mjg [748.694109ms]
Aug 29 20:27:37.551: INFO: Created: latency-svc-kq2sd
Aug 29 20:27:37.585: INFO: Got endpoints: latency-svc-zg5h2 [747.908607ms]
Aug 29 20:27:37.599: INFO: Created: latency-svc-969zv
Aug 29 20:27:37.638: INFO: Got endpoints: latency-svc-xhp6l [753.269424ms]
Aug 29 20:27:37.651: INFO: Created: latency-svc-xzfmp
Aug 29 20:27:37.686: INFO: Got endpoints: latency-svc-nn5cj [753.64285ms]
Aug 29 20:27:37.700: INFO: Created: latency-svc-8r9z7
Aug 29 20:27:37.738: INFO: Got endpoints: latency-svc-6j8gm [753.405396ms]
Aug 29 20:27:37.755: INFO: Created: latency-svc-kpds9
Aug 29 20:27:37.783: INFO: Got endpoints: latency-svc-52d7d [746.896077ms]
Aug 29 20:27:37.798: INFO: Created: latency-svc-pmswk
Aug 29 20:27:37.837: INFO: Got endpoints: latency-svc-vbrdf [752.018023ms]
Aug 29 20:27:37.854: INFO: Created: latency-svc-shtp6
Aug 29 20:27:37.885: INFO: Got endpoints: latency-svc-np4gw [750.488166ms]
Aug 29 20:27:37.899: INFO: Created: latency-svc-4xfl9
Aug 29 20:27:37.934: INFO: Got endpoints: latency-svc-g7d2f [751.397081ms]
Aug 29 20:27:37.950: INFO: Created: latency-svc-nqkdj
Aug 29 20:27:37.984: INFO: Got endpoints: latency-svc-25x8t [749.763177ms]
Aug 29 20:27:37.998: INFO: Created: latency-svc-75d8d
Aug 29 20:27:38.031: INFO: Got endpoints: latency-svc-hzd25 [747.076793ms]
Aug 29 20:27:38.044: INFO: Created: latency-svc-6cjvl
Aug 29 20:27:38.083: INFO: Got endpoints: latency-svc-7ntb5 [746.388924ms]
Aug 29 20:27:38.097: INFO: Created: latency-svc-bxk4m
Aug 29 20:27:38.132: INFO: Got endpoints: latency-svc-thxtp [741.852795ms]
Aug 29 20:27:38.150: INFO: Created: latency-svc-ndd4n
Aug 29 20:27:38.181: INFO: Got endpoints: latency-svc-wfdxq [743.046146ms]
Aug 29 20:27:38.197: INFO: Created: latency-svc-vtmt4
Aug 29 20:27:38.236: INFO: Got endpoints: latency-svc-29b9v [748.707647ms]
Aug 29 20:27:38.253: INFO: Created: latency-svc-8v4fk
Aug 29 20:27:38.283: INFO: Got endpoints: latency-svc-kq2sd [750.067664ms]
Aug 29 20:27:38.301: INFO: Created: latency-svc-8wrqn
Aug 29 20:27:38.336: INFO: Got endpoints: latency-svc-969zv [750.447539ms]
Aug 29 20:27:38.350: INFO: Created: latency-svc-dsljm
Aug 29 20:27:38.386: INFO: Got endpoints: latency-svc-xzfmp [747.840991ms]
Aug 29 20:27:38.404: INFO: Created: latency-svc-5z6jc
Aug 29 20:27:38.432: INFO: Got endpoints: latency-svc-8r9z7 [746.034624ms]
Aug 29 20:27:38.450: INFO: Created: latency-svc-7cg7n
Aug 29 20:27:38.483: INFO: Got endpoints: latency-svc-kpds9 [744.82281ms]
Aug 29 20:27:38.498: INFO: Created: latency-svc-qn6qr
Aug 29 20:27:38.535: INFO: Got endpoints: latency-svc-pmswk [751.88641ms]
Aug 29 20:27:38.552: INFO: Created: latency-svc-pxwhj
Aug 29 20:27:38.585: INFO: Got endpoints: latency-svc-shtp6 [747.38383ms]
Aug 29 20:27:38.601: INFO: Created: latency-svc-pfkhd
Aug 29 20:27:38.639: INFO: Got endpoints: latency-svc-4xfl9 [753.566077ms]
Aug 29 20:27:38.652: INFO: Created: latency-svc-7mtb5
Aug 29 20:27:38.684: INFO: Got endpoints: latency-svc-nqkdj [749.582585ms]
Aug 29 20:27:38.698: INFO: Created: latency-svc-sq7zq
Aug 29 20:27:38.735: INFO: Got endpoints: latency-svc-75d8d [751.134092ms]
Aug 29 20:27:38.750: INFO: Created: latency-svc-ck29r
Aug 29 20:27:38.783: INFO: Got endpoints: latency-svc-6cjvl [752.3107ms]
Aug 29 20:27:38.798: INFO: Created: latency-svc-jh58f
Aug 29 20:27:38.833: INFO: Got endpoints: latency-svc-bxk4m [749.508601ms]
Aug 29 20:27:38.848: INFO: Created: latency-svc-ngzq9
Aug 29 20:27:38.884: INFO: Got endpoints: latency-svc-ndd4n [751.659825ms]
Aug 29 20:27:38.904: INFO: Created: latency-svc-5jnsg
Aug 29 20:27:38.935: INFO: Got endpoints: latency-svc-vtmt4 [753.977317ms]
Aug 29 20:27:38.950: INFO: Created: latency-svc-bzvrd
Aug 29 20:27:38.985: INFO: Got endpoints: latency-svc-8v4fk [749.124208ms]
Aug 29 20:27:39.001: INFO: Created: latency-svc-p428n
Aug 29 20:27:39.036: INFO: Got endpoints: latency-svc-8wrqn [752.468048ms]
Aug 29 20:27:39.059: INFO: Created: latency-svc-g6fwk
Aug 29 20:27:39.083: INFO: Got endpoints: latency-svc-dsljm [747.570991ms]
Aug 29 20:27:39.097: INFO: Created: latency-svc-sd2md
Aug 29 20:27:39.131: INFO: Got endpoints: latency-svc-5z6jc [744.954276ms]
Aug 29 20:27:39.144: INFO: Created: latency-svc-tp6bf
Aug 29 20:27:39.184: INFO: Got endpoints: latency-svc-7cg7n [751.382439ms]
Aug 29 20:27:39.202: INFO: Created: latency-svc-zcp97
Aug 29 20:27:39.236: INFO: Got endpoints: latency-svc-qn6qr [753.549854ms]
Aug 29 20:27:39.252: INFO: Created: latency-svc-skksd
Aug 29 20:27:39.289: INFO: Got endpoints: latency-svc-pxwhj [753.774488ms]
Aug 29 20:27:39.304: INFO: Created: latency-svc-4t6bp
Aug 29 20:27:39.335: INFO: Got endpoints: latency-svc-pfkhd [750.647217ms]
Aug 29 20:27:39.355: INFO: Created: latency-svc-qhwjx
Aug 29 20:27:39.383: INFO: Got endpoints: latency-svc-7mtb5 [744.106974ms]
Aug 29 20:27:39.397: INFO: Created: latency-svc-qsj6x
Aug 29 20:27:39.433: INFO: Got endpoints: latency-svc-sq7zq [748.930569ms]
Aug 29 20:27:39.449: INFO: Created: latency-svc-gjccw
Aug 29 20:27:39.481: INFO: Got endpoints: latency-svc-ck29r [745.957326ms]
Aug 29 20:27:39.498: INFO: Created: latency-svc-qc7j6
Aug 29 20:27:39.534: INFO: Got endpoints: latency-svc-jh58f [750.933749ms]
Aug 29 20:27:39.550: INFO: Created: latency-svc-dwn7b
Aug 29 20:27:39.585: INFO: Got endpoints: latency-svc-ngzq9 [752.095851ms]
Aug 29 20:27:39.604: INFO: Created: latency-svc-8kp7z
Aug 29 20:27:39.639: INFO: Got endpoints: latency-svc-5jnsg [755.068307ms]
Aug 29 20:27:39.655: INFO: Created: latency-svc-dpqmj
Aug 29 20:27:39.691: INFO: Got endpoints: latency-svc-bzvrd [755.755133ms]
Aug 29 20:27:39.705: INFO: Created: latency-svc-msc2v
Aug 29 20:27:39.734: INFO: Got endpoints: latency-svc-p428n [748.8456ms]
Aug 29 20:27:39.749: INFO: Created: latency-svc-jp54g
Aug 29 20:27:39.785: INFO: Got endpoints: latency-svc-g6fwk [749.282822ms]
Aug 29 20:27:39.802: INFO: Created: latency-svc-z66kl
Aug 29 20:27:39.834: INFO: Got endpoints: latency-svc-sd2md [750.856303ms]
Aug 29 20:27:39.852: INFO: Created: latency-svc-4t5rf
Aug 29 20:27:39.885: INFO: Got endpoints: latency-svc-tp6bf [753.801129ms]
Aug 29 20:27:39.903: INFO: Created: latency-svc-k9sgf
Aug 29 20:27:39.932: INFO: Got endpoints: latency-svc-zcp97 [748.193271ms]
Aug 29 20:27:39.948: INFO: Created: latency-svc-fl54z
Aug 29 20:27:39.985: INFO: Got endpoints: latency-svc-skksd [748.634313ms]
Aug 29 20:27:39.999: INFO: Created: latency-svc-6b6wt
Aug 29 20:27:40.039: INFO: Got endpoints: latency-svc-4t6bp [749.856384ms]
Aug 29 20:27:40.055: INFO: Created: latency-svc-vsbkx
Aug 29 20:27:40.084: INFO: Got endpoints: latency-svc-qhwjx [748.25479ms]
Aug 29 20:27:40.102: INFO: Created: latency-svc-9sfxn
Aug 29 20:27:40.135: INFO: Got endpoints: latency-svc-qsj6x [752.268601ms]
Aug 29 20:27:40.152: INFO: Created: latency-svc-cn96w
Aug 29 20:27:40.189: INFO: Got endpoints: latency-svc-gjccw [756.539843ms]
Aug 29 20:27:40.205: INFO: Created: latency-svc-97grh
Aug 29 20:27:40.235: INFO: Got endpoints: latency-svc-qc7j6 [753.714985ms]
Aug 29 20:27:40.254: INFO: Created: latency-svc-x8rtg
Aug 29 20:27:40.286: INFO: Got endpoints: latency-svc-dwn7b [751.815437ms]
Aug 29 20:27:40.300: INFO: Created: latency-svc-tj4kx
Aug 29 20:27:40.333: INFO: Got endpoints: latency-svc-8kp7z [747.942398ms]
Aug 29 20:27:40.347: INFO: Created: latency-svc-5v7hz
Aug 29 20:27:40.385: INFO: Got endpoints: latency-svc-dpqmj [746.228723ms]
Aug 29 20:27:40.403: INFO: Created: latency-svc-2wspl
Aug 29 20:27:40.435: INFO: Got endpoints: latency-svc-msc2v [744.006759ms]
Aug 29 20:27:40.449: INFO: Created: latency-svc-k8sqf
Aug 29 20:27:40.484: INFO: Got endpoints: latency-svc-jp54g [750.178255ms]
Aug 29 20:27:40.504: INFO: Created: latency-svc-psxc8
Aug 29 20:27:40.535: INFO: Got endpoints: latency-svc-z66kl [750.294813ms]
Aug 29 20:27:40.552: INFO: Created: latency-svc-fs4fg
Aug 29 20:27:40.587: INFO: Got endpoints: latency-svc-4t5rf [752.593282ms]
Aug 29 20:27:40.602: INFO: Created: latency-svc-sjbtl
Aug 29 20:27:40.639: INFO: Got endpoints: latency-svc-k9sgf [754.184138ms]
Aug 29 20:27:40.652: INFO: Created: latency-svc-hkpqc
Aug 29 20:27:40.685: INFO: Got endpoints: latency-svc-fl54z [752.326002ms]
Aug 29 20:27:40.701: INFO: Created: latency-svc-k525c
Aug 29 20:27:40.735: INFO: Got endpoints: latency-svc-6b6wt [750.01571ms]
Aug 29 20:27:40.750: INFO: Created: latency-svc-dthjf
Aug 29 20:27:40.785: INFO: Got endpoints: latency-svc-vsbkx [745.833306ms]
Aug 29 20:27:40.801: INFO: Created: latency-svc-rg76f
Aug 29 20:27:40.835: INFO: Got endpoints: latency-svc-9sfxn [750.776003ms]
Aug 29 20:27:40.849: INFO: Created: latency-svc-2xdq5
Aug 29 20:27:40.887: INFO: Got endpoints: latency-svc-cn96w [751.984729ms]
Aug 29 20:27:40.900: INFO: Created: latency-svc-vsb74
Aug 29 20:27:40.935: INFO: Got endpoints: latency-svc-97grh [745.254193ms]
Aug 29 20:27:40.948: INFO: Created: latency-svc-xlh45
Aug 29 20:27:40.991: INFO: Got endpoints: latency-svc-x8rtg [755.529824ms]
Aug 29 20:27:41.007: INFO: Created: latency-svc-98xxg
Aug 29 20:27:41.036: INFO: Got endpoints: latency-svc-tj4kx [749.424078ms]
Aug 29 20:27:41.058: INFO: Created: latency-svc-8rj7h
Aug 29 20:27:41.085: INFO: Got endpoints: latency-svc-5v7hz [751.455993ms]
Aug 29 20:27:41.100: INFO: Created: latency-svc-nc2b4
Aug 29 20:27:41.135: INFO: Got endpoints: latency-svc-2wspl [749.901683ms]
Aug 29 20:27:41.150: INFO: Created: latency-svc-bsk9d
Aug 29 20:27:41.183: INFO: Got endpoints: latency-svc-k8sqf [747.847986ms]
Aug 29 20:27:41.201: INFO: Created: latency-svc-j6d59
Aug 29 20:27:41.239: INFO: Got endpoints: latency-svc-psxc8 [754.702788ms]
Aug 29 20:27:41.253: INFO: Created: latency-svc-snqf5
Aug 29 20:27:41.285: INFO: Got endpoints: latency-svc-fs4fg [749.344598ms]
Aug 29 20:27:41.298: INFO: Created: latency-svc-5mdxk
Aug 29 20:27:41.339: INFO: Got endpoints: latency-svc-sjbtl [752.192043ms]
Aug 29 20:27:41.355: INFO: Created: latency-svc-l4sjh
Aug 29 20:27:41.388: INFO: Got endpoints: latency-svc-hkpqc [749.149495ms]
Aug 29 20:27:41.405: INFO: Created: latency-svc-mjww6
Aug 29 20:27:41.434: INFO: Got endpoints: latency-svc-k525c [749.410177ms]
Aug 29 20:27:41.449: INFO: Created: latency-svc-mcvx8
Aug 29 20:27:41.485: INFO: Got endpoints: latency-svc-dthjf [749.774523ms]
Aug 29 20:27:41.499: INFO: Created: latency-svc-4ndfv
Aug 29 20:27:41.534: INFO: Got endpoints: latency-svc-rg76f [748.674984ms]
Aug 29 20:27:41.549: INFO: Created: latency-svc-bt7sg
Aug 29 20:27:41.585: INFO: Got endpoints: latency-svc-2xdq5 [750.059887ms]
Aug 29 20:27:41.603: INFO: Created: latency-svc-dlcbn
Aug 29 20:27:41.633: INFO: Got endpoints: latency-svc-vsb74 [746.266384ms]
Aug 29 20:27:41.649: INFO: Created: latency-svc-c5lfn
Aug 29 20:27:41.686: INFO: Got endpoints: latency-svc-xlh45 [750.80277ms]
Aug 29 20:27:41.702: INFO: Created: latency-svc-krsgw
Aug 29 20:27:41.741: INFO: Got endpoints: latency-svc-98xxg [750.111338ms]
Aug 29 20:27:41.758: INFO: Created: latency-svc-84g9s
Aug 29 20:27:41.787: INFO: Got endpoints: latency-svc-8rj7h [751.011251ms]
Aug 29 20:27:41.806: INFO: Created: latency-svc-twp9k
Aug 29 20:27:41.835: INFO: Got endpoints: latency-svc-nc2b4 [750.689726ms]
Aug 29 20:27:41.854: INFO: Created: latency-svc-qr758
Aug 29 20:27:41.885: INFO: Got endpoints: latency-svc-bsk9d [749.683147ms]
Aug 29 20:27:41.900: INFO: Created: latency-svc-sq2kq
Aug 29 20:27:41.936: INFO: Got endpoints: latency-svc-j6d59 [753.230297ms]
Aug 29 20:27:41.950: INFO: Created: latency-svc-zj9v2
Aug 29 20:27:41.987: INFO: Got endpoints: latency-svc-snqf5 [747.750745ms]
Aug 29 20:27:42.001: INFO: Created: latency-svc-tv4r6
Aug 29 20:27:42.037: INFO: Got endpoints: latency-svc-5mdxk [751.884114ms]
Aug 29 20:27:42.051: INFO: Created: latency-svc-zjz89
Aug 29 20:27:42.082: INFO: Got endpoints: latency-svc-l4sjh [743.128012ms]
Aug 29 20:27:42.098: INFO: Created: latency-svc-tfg9q
Aug 29 20:27:42.132: INFO: Got endpoints: latency-svc-mjww6 [743.566018ms]
Aug 29 20:27:42.151: INFO: Created: latency-svc-lsbpw
Aug 29 20:27:42.188: INFO: Got endpoints: latency-svc-mcvx8 [753.950364ms]
Aug 29 20:27:42.203: INFO: Created: latency-svc-xn7s4
Aug 29 20:27:42.236: INFO: Got endpoints: latency-svc-4ndfv [750.729623ms]
Aug 29 20:27:42.249: INFO: Created: latency-svc-49jjq
Aug 29 20:27:42.285: INFO: Got endpoints: latency-svc-bt7sg [750.693625ms]
Aug 29 20:27:42.297: INFO: Created: latency-svc-47n6l
Aug 29 20:27:42.335: INFO: Got endpoints: latency-svc-dlcbn [750.395957ms]
Aug 29 20:27:42.350: INFO: Created: latency-svc-55xwb
Aug 29 20:27:42.384: INFO: Got endpoints: latency-svc-c5lfn [750.134375ms]
Aug 29 20:27:42.397: INFO: Created: latency-svc-4cfcv
Aug 29 20:27:42.437: INFO: Got endpoints: latency-svc-krsgw [750.828798ms]
Aug 29 20:27:42.455: INFO: Created: latency-svc-c2dhd
Aug 29 20:27:42.486: INFO: Got endpoints: latency-svc-84g9s [745.671349ms]
Aug 29 20:27:42.503: INFO: Created: latency-svc-j9cdd
Aug 29 20:27:42.536: INFO: Got endpoints: latency-svc-twp9k [749.654108ms]
Aug 29 20:27:42.552: INFO: Created: latency-svc-6dmsc
Aug 29 20:27:42.584: INFO: Got endpoints: latency-svc-qr758 [748.897387ms]
Aug 29 20:27:42.599: INFO: Created: latency-svc-dppqp
Aug 29 20:27:42.634: INFO: Got endpoints: latency-svc-sq2kq [749.343643ms]
Aug 29 20:27:42.652: INFO: Created: latency-svc-tps6b
Aug 29 20:27:42.684: INFO: Got endpoints: latency-svc-zj9v2 [746.671542ms]
Aug 29 20:27:42.697: INFO: Created: latency-svc-jsdm5
Aug 29 20:27:42.734: INFO: Got endpoints: latency-svc-tv4r6 [747.227034ms]
Aug 29 20:27:42.751: INFO: Created: latency-svc-zx49z
Aug 29 20:27:42.785: INFO: Got endpoints: latency-svc-zjz89 [748.539835ms]
Aug 29 20:27:42.801: INFO: Created: latency-svc-n8w86
Aug 29 20:27:42.834: INFO: Got endpoints: latency-svc-tfg9q [751.75762ms]
Aug 29 20:27:42.848: INFO: Created: latency-svc-pdjj7
Aug 29 20:27:42.885: INFO: Got endpoints: latency-svc-lsbpw [753.619621ms]
Aug 29 20:27:42.901: INFO: Created: latency-svc-7qvkk
Aug 29 20:27:42.934: INFO: Got endpoints: latency-svc-xn7s4 [745.424239ms]
Aug 29 20:27:42.949: INFO: Created: latency-svc-gn9dr
Aug 29 20:27:42.985: INFO: Got endpoints: latency-svc-49jjq [749.389789ms]
Aug 29 20:27:43.000: INFO: Created: latency-svc-k4vxj
Aug 29 20:27:43.034: INFO: Got endpoints: latency-svc-47n6l [749.610924ms]
Aug 29 20:27:43.064: INFO: Created: latency-svc-ksgfj
Aug 29 20:27:43.083: INFO: Got endpoints: latency-svc-55xwb [748.035062ms]
Aug 29 20:27:43.100: INFO: Created: latency-svc-9tf6l
Aug 29 20:27:43.135: INFO: Got endpoints: latency-svc-4cfcv [751.333362ms]
Aug 29 20:27:43.149: INFO: Created: latency-svc-cs8w4
Aug 29 20:27:43.184: INFO: Got endpoints: latency-svc-c2dhd [747.314875ms]
Aug 29 20:27:43.200: INFO: Created: latency-svc-k9txm
Aug 29 20:27:43.233: INFO: Got endpoints: latency-svc-j9cdd [746.715815ms]
Aug 29 20:27:43.251: INFO: Created: latency-svc-2nslw
Aug 29 20:27:43.289: INFO: Got endpoints: latency-svc-6dmsc [752.056855ms]
Aug 29 20:27:43.304: INFO: Created: latency-svc-4g7th
Aug 29 20:27:43.334: INFO: Got endpoints: latency-svc-dppqp [749.862988ms]
Aug 29 20:27:43.350: INFO: Created: latency-svc-2p46m
Aug 29 20:27:43.397: INFO: Got endpoints: latency-svc-tps6b [762.985999ms]
Aug 29 20:27:43.412: INFO: Created: latency-svc-668kz
Aug 29 20:27:43.434: INFO: Got endpoints: latency-svc-jsdm5 [750.372167ms]
Aug 29 20:27:43.484: INFO: Got endpoints: latency-svc-zx49z [750.411736ms]
Aug 29 20:27:43.536: INFO: Got endpoints: latency-svc-n8w86 [750.62405ms]
Aug 29 20:27:43.584: INFO: Got endpoints: latency-svc-pdjj7 [750.104301ms]
Aug 29 20:27:43.635: INFO: Got endpoints: latency-svc-7qvkk [749.694479ms]
Aug 29 20:27:43.685: INFO: Got endpoints: latency-svc-gn9dr [751.644925ms]
Aug 29 20:27:43.750: INFO: Got endpoints: latency-svc-k4vxj [765.273768ms]
Aug 29 20:27:43.785: INFO: Got endpoints: latency-svc-ksgfj [750.187485ms]
Aug 29 20:27:43.836: INFO: Got endpoints: latency-svc-9tf6l [752.290783ms]
Aug 29 20:27:43.886: INFO: Got endpoints: latency-svc-cs8w4 [750.836881ms]
Aug 29 20:27:43.934: INFO: Got endpoints: latency-svc-k9txm [750.479342ms]
Aug 29 20:27:43.984: INFO: Got endpoints: latency-svc-2nslw [750.524451ms]
Aug 29 20:27:44.033: INFO: Got endpoints: latency-svc-4g7th [744.473358ms]
Aug 29 20:27:44.084: INFO: Got endpoints: latency-svc-2p46m [749.963746ms]
Aug 29 20:27:44.135: INFO: Got endpoints: latency-svc-668kz [737.745346ms]
Aug 29 20:27:44.135: INFO: Latencies: [26.651256ms 38.546283ms 43.22125ms 57.410746ms 59.40133ms 67.285602ms 72.842211ms 73.736251ms 83.285162ms 84.753766ms 95.905102ms 96.742902ms 99.896237ms 103.388017ms 111.636411ms 111.968225ms 120.488433ms 127.250082ms 127.830361ms 141.840375ms 148.196116ms 159.524026ms 163.198432ms 167.307888ms 187.546528ms 189.306057ms 189.422243ms 190.09591ms 213.155225ms 219.639349ms 228.809809ms 250.954665ms 259.643094ms 264.050529ms 265.770052ms 266.294778ms 276.671884ms 294.761534ms 294.861088ms 294.992913ms 302.474583ms 306.205478ms 341.321874ms 376.928685ms 422.251057ms 466.967533ms 498.927634ms 538.853996ms 576.60324ms 611.828572ms 638.75294ms 679.739028ms 708.208594ms 737.745346ms 741.593744ms 741.852795ms 743.046146ms 743.128012ms 743.566018ms 744.006759ms 744.106974ms 744.473358ms 744.82281ms 744.954276ms 745.254193ms 745.424239ms 745.671349ms 745.833306ms 745.957326ms 746.034624ms 746.228723ms 746.266384ms 746.272919ms 746.388924ms 746.671542ms 746.715815ms 746.896077ms 747.076793ms 747.190109ms 747.227034ms 747.314875ms 747.38383ms 747.570991ms 747.750745ms 747.760724ms 747.840991ms 747.847986ms 747.908607ms 747.942398ms 748.035062ms 748.193271ms 748.25479ms 748.539835ms 748.634313ms 748.674984ms 748.694109ms 748.707647ms 748.815137ms 748.8456ms 748.897387ms 748.930569ms 749.124208ms 749.149495ms 749.282822ms 749.332511ms 749.343643ms 749.344598ms 749.389789ms 749.410177ms 749.424078ms 749.500539ms 749.508601ms 749.582585ms 749.610924ms 749.654108ms 749.683147ms 749.694479ms 749.763177ms 749.774523ms 749.856384ms 749.862988ms 749.901683ms 749.963746ms 750.01571ms 750.059887ms 750.067664ms 750.104301ms 750.111338ms 750.134375ms 750.178255ms 750.187485ms 750.276687ms 750.294813ms 750.372167ms 750.395957ms 750.411736ms 750.447539ms 750.479342ms 750.488166ms 750.524451ms 750.62405ms 750.647217ms 750.689726ms 750.693625ms 750.729623ms 750.776003ms 750.80277ms 750.828798ms 750.836881ms 750.856303ms 750.933749ms 751.011251ms 751.128961ms 751.129816ms 751.134092ms 751.333362ms 751.382439ms 751.397081ms 751.455993ms 751.644925ms 751.659825ms 751.75762ms 751.815437ms 751.884114ms 751.88641ms 751.984729ms 752.018023ms 752.056855ms 752.082544ms 752.095851ms 752.192043ms 752.268601ms 752.290783ms 752.3107ms 752.326002ms 752.468048ms 752.593282ms 753.230297ms 753.269424ms 753.405396ms 753.549854ms 753.566077ms 753.619621ms 753.64285ms 753.714985ms 753.774488ms 753.801129ms 753.950364ms 753.977317ms 754.184138ms 754.589484ms 754.702788ms 754.967406ms 755.068307ms 755.345076ms 755.529824ms 755.755133ms 756.539843ms 762.985999ms 765.273768ms]
Aug 29 20:27:44.135: INFO: 50 %ile: 748.930569ms
Aug 29 20:27:44.135: INFO: 90 %ile: 753.549854ms
Aug 29 20:27:44.135: INFO: 99 %ile: 762.985999ms
Aug 29 20:27:44.135: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Aug 29 20:27:44.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-7554" for this suite. 08/29/23 20:27:44.143
------------------------------
• [SLOW TEST] [10.784 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:27:33.368
    Aug 29 20:27:33.368: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename svc-latency 08/29/23 20:27:33.369
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:33.392
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:33.395
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 29 20:27:33.398: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-7554 08/29/23 20:27:33.399
    I0829 20:27:33.405516      19 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7554, replica count: 1
    I0829 20:27:34.456035      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0829 20:27:35.456743      19 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 20:27:35.575: INFO: Created: latency-svc-ptxzr
    Aug 29 20:27:35.585: INFO: Got endpoints: latency-svc-ptxzr [28.522985ms]
    Aug 29 20:27:35.602: INFO: Created: latency-svc-m8cm2
    Aug 29 20:27:35.612: INFO: Got endpoints: latency-svc-m8cm2 [26.651256ms]
    Aug 29 20:27:35.615: INFO: Created: latency-svc-84nlp
    Aug 29 20:27:35.625: INFO: Got endpoints: latency-svc-84nlp [38.546283ms]
    Aug 29 20:27:35.632: INFO: Created: latency-svc-jrcrc
    Aug 29 20:27:35.643: INFO: Got endpoints: latency-svc-jrcrc [57.410746ms]
    Aug 29 20:27:35.644: INFO: Created: latency-svc-qwr5h
    Aug 29 20:27:35.653: INFO: Got endpoints: latency-svc-qwr5h [67.285602ms]
    Aug 29 20:27:35.659: INFO: Created: latency-svc-4878b
    Aug 29 20:27:35.669: INFO: Got endpoints: latency-svc-4878b [83.285162ms]
    Aug 29 20:27:35.677: INFO: Created: latency-svc-xcszl
    Aug 29 20:27:35.687: INFO: Created: latency-svc-ztdzp
    Aug 29 20:27:35.689: INFO: Got endpoints: latency-svc-xcszl [103.388017ms]
    Aug 29 20:27:35.698: INFO: Got endpoints: latency-svc-ztdzp [111.968225ms]
    Aug 29 20:27:35.700: INFO: Created: latency-svc-49964
    Aug 29 20:27:35.714: INFO: Got endpoints: latency-svc-49964 [127.830361ms]
    Aug 29 20:27:35.830: INFO: Created: latency-svc-s2x4j
    Aug 29 20:27:35.830: INFO: Created: latency-svc-nvkdz
    Aug 29 20:27:35.831: INFO: Created: latency-svc-289kg
    Aug 29 20:27:35.831: INFO: Created: latency-svc-hqhxw
    Aug 29 20:27:35.832: INFO: Created: latency-svc-8s2lk
    Aug 29 20:27:35.832: INFO: Created: latency-svc-fjz7f
    Aug 29 20:27:35.834: INFO: Created: latency-svc-28nls
    Aug 29 20:27:35.834: INFO: Created: latency-svc-7xzd7
    Aug 29 20:27:35.834: INFO: Created: latency-svc-9zrnl
    Aug 29 20:27:35.835: INFO: Created: latency-svc-dbnfn
    Aug 29 20:27:35.835: INFO: Created: latency-svc-l47kw
    Aug 29 20:27:35.835: INFO: Created: latency-svc-s5bdn
    Aug 29 20:27:35.835: INFO: Created: latency-svc-99wsg
    Aug 29 20:27:35.836: INFO: Created: latency-svc-qzm9t
    Aug 29 20:27:35.836: INFO: Created: latency-svc-bz5wm
    Aug 29 20:27:35.849: INFO: Got endpoints: latency-svc-s2x4j [159.524026ms]
    Aug 29 20:27:35.850: INFO: Got endpoints: latency-svc-289kg [264.050529ms]
    Aug 29 20:27:35.852: INFO: Got endpoints: latency-svc-nvkdz [266.294778ms]
    Aug 29 20:27:35.856: INFO: Got endpoints: latency-svc-l47kw [213.155225ms]
    Aug 29 20:27:35.859: INFO: Got endpoints: latency-svc-dbnfn [190.09591ms]
    Aug 29 20:27:35.862: INFO: Got endpoints: latency-svc-8s2lk [148.196116ms]
    Aug 29 20:27:35.863: INFO: Got endpoints: latency-svc-fjz7f [276.671884ms]
    Aug 29 20:27:35.872: INFO: Got endpoints: latency-svc-bz5wm [259.643094ms]
    Aug 29 20:27:35.873: INFO: Got endpoints: latency-svc-hqhxw [219.639349ms]
    Aug 29 20:27:35.876: INFO: Got endpoints: latency-svc-28nls [250.954665ms]
    Aug 29 20:27:35.880: INFO: Created: latency-svc-xljcf
    Aug 29 20:27:35.881: INFO: Got endpoints: latency-svc-7xzd7 [294.861088ms]
    Aug 29 20:27:35.881: INFO: Got endpoints: latency-svc-99wsg [294.992913ms]
    Aug 29 20:27:35.881: INFO: Got endpoints: latency-svc-9zrnl [294.761534ms]
    Aug 29 20:27:35.887: INFO: Got endpoints: latency-svc-qzm9t [189.306057ms]
    Aug 29 20:27:35.888: INFO: Got endpoints: latency-svc-s5bdn [302.474583ms]
    Aug 29 20:27:35.893: INFO: Got endpoints: latency-svc-xljcf [43.22125ms]
    Aug 29 20:27:35.897: INFO: Created: latency-svc-xwh5n
    Aug 29 20:27:35.908: INFO: Got endpoints: latency-svc-xwh5n [59.40133ms]
    Aug 29 20:27:35.915: INFO: Created: latency-svc-phm7d
    Aug 29 20:27:35.917: INFO: Created: latency-svc-q85vn
    Aug 29 20:27:35.925: INFO: Got endpoints: latency-svc-phm7d [72.842211ms]
    Aug 29 20:27:35.930: INFO: Created: latency-svc-v948z
    Aug 29 20:27:35.930: INFO: Got endpoints: latency-svc-q85vn [73.736251ms]
    Aug 29 20:27:35.944: INFO: Got endpoints: latency-svc-v948z [84.753766ms]
    Aug 29 20:27:35.948: INFO: Created: latency-svc-m4mml
    Aug 29 20:27:35.949: INFO: Created: latency-svc-rzxdb
    Aug 29 20:27:35.958: INFO: Got endpoints: latency-svc-rzxdb [95.905102ms]
    Aug 29 20:27:35.960: INFO: Created: latency-svc-8bb79
    Aug 29 20:27:35.963: INFO: Got endpoints: latency-svc-m4mml [99.896237ms]
    Aug 29 20:27:35.969: INFO: Got endpoints: latency-svc-8bb79 [96.742902ms]
    Aug 29 20:27:35.972: INFO: Created: latency-svc-rkxmw
    Aug 29 20:27:35.981: INFO: Created: latency-svc-fcqml
    Aug 29 20:27:35.984: INFO: Got endpoints: latency-svc-rkxmw [111.636411ms]
    Aug 29 20:27:35.996: INFO: Got endpoints: latency-svc-fcqml [120.488433ms]
    Aug 29 20:27:36.000: INFO: Created: latency-svc-6gnln
    Aug 29 20:27:36.008: INFO: Got endpoints: latency-svc-6gnln [127.250082ms]
    Aug 29 20:27:36.015: INFO: Created: latency-svc-lzj5f
    Aug 29 20:27:36.023: INFO: Got endpoints: latency-svc-lzj5f [141.840375ms]
    Aug 29 20:27:36.029: INFO: Created: latency-svc-x4khq
    Aug 29 20:27:36.041: INFO: Created: latency-svc-vfsjl
    Aug 29 20:27:36.044: INFO: Got endpoints: latency-svc-x4khq [163.198432ms]
    Aug 29 20:27:36.055: INFO: Got endpoints: latency-svc-vfsjl [167.307888ms]
    Aug 29 20:27:36.061: INFO: Created: latency-svc-ldw6z
    Aug 29 20:27:36.063: INFO: Created: latency-svc-ps7gt
    Aug 29 20:27:36.075: INFO: Created: latency-svc-pq94q
    Aug 29 20:27:36.076: INFO: Got endpoints: latency-svc-ldw6z [187.546528ms]
    Aug 29 20:27:36.082: INFO: Got endpoints: latency-svc-ps7gt [189.422243ms]
    Aug 29 20:27:36.090: INFO: Created: latency-svc-dhqsm
    Aug 29 20:27:36.095: INFO: Created: latency-svc-z8266
    Aug 29 20:27:36.104: INFO: Created: latency-svc-l6pxp
    Aug 29 20:27:36.116: INFO: Created: latency-svc-cprxw
    Aug 29 20:27:36.125: INFO: Created: latency-svc-jp5q6
    Aug 29 20:27:36.137: INFO: Got endpoints: latency-svc-pq94q [228.809809ms]
    Aug 29 20:27:36.140: INFO: Created: latency-svc-gz75r
    Aug 29 20:27:36.151: INFO: Created: latency-svc-t52jx
    Aug 29 20:27:36.160: INFO: Created: latency-svc-7hg7w
    Aug 29 20:27:36.174: INFO: Created: latency-svc-c57j6
    Aug 29 20:27:36.191: INFO: Got endpoints: latency-svc-dhqsm [265.770052ms]
    Aug 29 20:27:36.191: INFO: Created: latency-svc-wfqbv
    Aug 29 20:27:36.199: INFO: Created: latency-svc-59bmb
    Aug 29 20:27:36.215: INFO: Created: latency-svc-npd9r
    Aug 29 20:27:36.227: INFO: Created: latency-svc-8bmtb
    Aug 29 20:27:36.236: INFO: Got endpoints: latency-svc-z8266 [306.205478ms]
    Aug 29 20:27:36.245: INFO: Created: latency-svc-645kv
    Aug 29 20:27:36.255: INFO: Created: latency-svc-nhccd
    Aug 29 20:27:36.271: INFO: Created: latency-svc-2tvht
    Aug 29 20:27:36.273: INFO: Created: latency-svc-4vr6n
    Aug 29 20:27:36.285: INFO: Got endpoints: latency-svc-l6pxp [341.321874ms]
    Aug 29 20:27:36.303: INFO: Created: latency-svc-wng6j
    Aug 29 20:27:36.335: INFO: Got endpoints: latency-svc-cprxw [376.928685ms]
    Aug 29 20:27:36.349: INFO: Created: latency-svc-knxnh
    Aug 29 20:27:36.385: INFO: Got endpoints: latency-svc-jp5q6 [422.251057ms]
    Aug 29 20:27:36.400: INFO: Created: latency-svc-lsklq
    Aug 29 20:27:36.436: INFO: Got endpoints: latency-svc-gz75r [466.967533ms]
    Aug 29 20:27:36.454: INFO: Created: latency-svc-sjwkj
    Aug 29 20:27:36.483: INFO: Got endpoints: latency-svc-t52jx [498.927634ms]
    Aug 29 20:27:36.499: INFO: Created: latency-svc-5tlcz
    Aug 29 20:27:36.535: INFO: Got endpoints: latency-svc-7hg7w [538.853996ms]
    Aug 29 20:27:36.549: INFO: Created: latency-svc-kk69c
    Aug 29 20:27:36.585: INFO: Got endpoints: latency-svc-c57j6 [576.60324ms]
    Aug 29 20:27:36.600: INFO: Created: latency-svc-67nkk
    Aug 29 20:27:36.635: INFO: Got endpoints: latency-svc-wfqbv [611.828572ms]
    Aug 29 20:27:36.652: INFO: Created: latency-svc-gkl44
    Aug 29 20:27:36.683: INFO: Got endpoints: latency-svc-59bmb [638.75294ms]
    Aug 29 20:27:36.698: INFO: Created: latency-svc-mv695
    Aug 29 20:27:36.734: INFO: Got endpoints: latency-svc-npd9r [679.739028ms]
    Aug 29 20:27:36.755: INFO: Created: latency-svc-g9t5s
    Aug 29 20:27:36.784: INFO: Got endpoints: latency-svc-8bmtb [708.208594ms]
    Aug 29 20:27:36.798: INFO: Created: latency-svc-t5mjg
    Aug 29 20:27:36.837: INFO: Got endpoints: latency-svc-645kv [754.589484ms]
    Aug 29 20:27:36.851: INFO: Created: latency-svc-zg5h2
    Aug 29 20:27:36.885: INFO: Got endpoints: latency-svc-nhccd [747.190109ms]
    Aug 29 20:27:36.903: INFO: Created: latency-svc-xhp6l
    Aug 29 20:27:36.933: INFO: Got endpoints: latency-svc-2tvht [741.593744ms]
    Aug 29 20:27:36.947: INFO: Created: latency-svc-nn5cj
    Aug 29 20:27:36.984: INFO: Got endpoints: latency-svc-4vr6n [747.760724ms]
    Aug 29 20:27:36.999: INFO: Created: latency-svc-6j8gm
    Aug 29 20:27:37.036: INFO: Got endpoints: latency-svc-wng6j [751.128961ms]
    Aug 29 20:27:37.054: INFO: Created: latency-svc-52d7d
    Aug 29 20:27:37.085: INFO: Got endpoints: latency-svc-knxnh [750.276687ms]
    Aug 29 20:27:37.102: INFO: Created: latency-svc-vbrdf
    Aug 29 20:27:37.134: INFO: Got endpoints: latency-svc-lsklq [749.332511ms]
    Aug 29 20:27:37.151: INFO: Created: latency-svc-np4gw
    Aug 29 20:27:37.183: INFO: Got endpoints: latency-svc-sjwkj [746.272919ms]
    Aug 29 20:27:37.200: INFO: Created: latency-svc-g7d2f
    Aug 29 20:27:37.234: INFO: Got endpoints: latency-svc-5tlcz [751.129816ms]
    Aug 29 20:27:37.251: INFO: Created: latency-svc-25x8t
    Aug 29 20:27:37.284: INFO: Got endpoints: latency-svc-kk69c [748.815137ms]
    Aug 29 20:27:37.297: INFO: Created: latency-svc-hzd25
    Aug 29 20:27:37.337: INFO: Got endpoints: latency-svc-67nkk [752.082544ms]
    Aug 29 20:27:37.352: INFO: Created: latency-svc-7ntb5
    Aug 29 20:27:37.390: INFO: Got endpoints: latency-svc-gkl44 [754.967406ms]
    Aug 29 20:27:37.408: INFO: Created: latency-svc-thxtp
    Aug 29 20:27:37.438: INFO: Got endpoints: latency-svc-mv695 [755.345076ms]
    Aug 29 20:27:37.454: INFO: Created: latency-svc-wfdxq
    Aug 29 20:27:37.487: INFO: Got endpoints: latency-svc-g9t5s [749.500539ms]
    Aug 29 20:27:37.502: INFO: Created: latency-svc-29b9v
    Aug 29 20:27:37.533: INFO: Got endpoints: latency-svc-t5mjg [748.694109ms]
    Aug 29 20:27:37.551: INFO: Created: latency-svc-kq2sd
    Aug 29 20:27:37.585: INFO: Got endpoints: latency-svc-zg5h2 [747.908607ms]
    Aug 29 20:27:37.599: INFO: Created: latency-svc-969zv
    Aug 29 20:27:37.638: INFO: Got endpoints: latency-svc-xhp6l [753.269424ms]
    Aug 29 20:27:37.651: INFO: Created: latency-svc-xzfmp
    Aug 29 20:27:37.686: INFO: Got endpoints: latency-svc-nn5cj [753.64285ms]
    Aug 29 20:27:37.700: INFO: Created: latency-svc-8r9z7
    Aug 29 20:27:37.738: INFO: Got endpoints: latency-svc-6j8gm [753.405396ms]
    Aug 29 20:27:37.755: INFO: Created: latency-svc-kpds9
    Aug 29 20:27:37.783: INFO: Got endpoints: latency-svc-52d7d [746.896077ms]
    Aug 29 20:27:37.798: INFO: Created: latency-svc-pmswk
    Aug 29 20:27:37.837: INFO: Got endpoints: latency-svc-vbrdf [752.018023ms]
    Aug 29 20:27:37.854: INFO: Created: latency-svc-shtp6
    Aug 29 20:27:37.885: INFO: Got endpoints: latency-svc-np4gw [750.488166ms]
    Aug 29 20:27:37.899: INFO: Created: latency-svc-4xfl9
    Aug 29 20:27:37.934: INFO: Got endpoints: latency-svc-g7d2f [751.397081ms]
    Aug 29 20:27:37.950: INFO: Created: latency-svc-nqkdj
    Aug 29 20:27:37.984: INFO: Got endpoints: latency-svc-25x8t [749.763177ms]
    Aug 29 20:27:37.998: INFO: Created: latency-svc-75d8d
    Aug 29 20:27:38.031: INFO: Got endpoints: latency-svc-hzd25 [747.076793ms]
    Aug 29 20:27:38.044: INFO: Created: latency-svc-6cjvl
    Aug 29 20:27:38.083: INFO: Got endpoints: latency-svc-7ntb5 [746.388924ms]
    Aug 29 20:27:38.097: INFO: Created: latency-svc-bxk4m
    Aug 29 20:27:38.132: INFO: Got endpoints: latency-svc-thxtp [741.852795ms]
    Aug 29 20:27:38.150: INFO: Created: latency-svc-ndd4n
    Aug 29 20:27:38.181: INFO: Got endpoints: latency-svc-wfdxq [743.046146ms]
    Aug 29 20:27:38.197: INFO: Created: latency-svc-vtmt4
    Aug 29 20:27:38.236: INFO: Got endpoints: latency-svc-29b9v [748.707647ms]
    Aug 29 20:27:38.253: INFO: Created: latency-svc-8v4fk
    Aug 29 20:27:38.283: INFO: Got endpoints: latency-svc-kq2sd [750.067664ms]
    Aug 29 20:27:38.301: INFO: Created: latency-svc-8wrqn
    Aug 29 20:27:38.336: INFO: Got endpoints: latency-svc-969zv [750.447539ms]
    Aug 29 20:27:38.350: INFO: Created: latency-svc-dsljm
    Aug 29 20:27:38.386: INFO: Got endpoints: latency-svc-xzfmp [747.840991ms]
    Aug 29 20:27:38.404: INFO: Created: latency-svc-5z6jc
    Aug 29 20:27:38.432: INFO: Got endpoints: latency-svc-8r9z7 [746.034624ms]
    Aug 29 20:27:38.450: INFO: Created: latency-svc-7cg7n
    Aug 29 20:27:38.483: INFO: Got endpoints: latency-svc-kpds9 [744.82281ms]
    Aug 29 20:27:38.498: INFO: Created: latency-svc-qn6qr
    Aug 29 20:27:38.535: INFO: Got endpoints: latency-svc-pmswk [751.88641ms]
    Aug 29 20:27:38.552: INFO: Created: latency-svc-pxwhj
    Aug 29 20:27:38.585: INFO: Got endpoints: latency-svc-shtp6 [747.38383ms]
    Aug 29 20:27:38.601: INFO: Created: latency-svc-pfkhd
    Aug 29 20:27:38.639: INFO: Got endpoints: latency-svc-4xfl9 [753.566077ms]
    Aug 29 20:27:38.652: INFO: Created: latency-svc-7mtb5
    Aug 29 20:27:38.684: INFO: Got endpoints: latency-svc-nqkdj [749.582585ms]
    Aug 29 20:27:38.698: INFO: Created: latency-svc-sq7zq
    Aug 29 20:27:38.735: INFO: Got endpoints: latency-svc-75d8d [751.134092ms]
    Aug 29 20:27:38.750: INFO: Created: latency-svc-ck29r
    Aug 29 20:27:38.783: INFO: Got endpoints: latency-svc-6cjvl [752.3107ms]
    Aug 29 20:27:38.798: INFO: Created: latency-svc-jh58f
    Aug 29 20:27:38.833: INFO: Got endpoints: latency-svc-bxk4m [749.508601ms]
    Aug 29 20:27:38.848: INFO: Created: latency-svc-ngzq9
    Aug 29 20:27:38.884: INFO: Got endpoints: latency-svc-ndd4n [751.659825ms]
    Aug 29 20:27:38.904: INFO: Created: latency-svc-5jnsg
    Aug 29 20:27:38.935: INFO: Got endpoints: latency-svc-vtmt4 [753.977317ms]
    Aug 29 20:27:38.950: INFO: Created: latency-svc-bzvrd
    Aug 29 20:27:38.985: INFO: Got endpoints: latency-svc-8v4fk [749.124208ms]
    Aug 29 20:27:39.001: INFO: Created: latency-svc-p428n
    Aug 29 20:27:39.036: INFO: Got endpoints: latency-svc-8wrqn [752.468048ms]
    Aug 29 20:27:39.059: INFO: Created: latency-svc-g6fwk
    Aug 29 20:27:39.083: INFO: Got endpoints: latency-svc-dsljm [747.570991ms]
    Aug 29 20:27:39.097: INFO: Created: latency-svc-sd2md
    Aug 29 20:27:39.131: INFO: Got endpoints: latency-svc-5z6jc [744.954276ms]
    Aug 29 20:27:39.144: INFO: Created: latency-svc-tp6bf
    Aug 29 20:27:39.184: INFO: Got endpoints: latency-svc-7cg7n [751.382439ms]
    Aug 29 20:27:39.202: INFO: Created: latency-svc-zcp97
    Aug 29 20:27:39.236: INFO: Got endpoints: latency-svc-qn6qr [753.549854ms]
    Aug 29 20:27:39.252: INFO: Created: latency-svc-skksd
    Aug 29 20:27:39.289: INFO: Got endpoints: latency-svc-pxwhj [753.774488ms]
    Aug 29 20:27:39.304: INFO: Created: latency-svc-4t6bp
    Aug 29 20:27:39.335: INFO: Got endpoints: latency-svc-pfkhd [750.647217ms]
    Aug 29 20:27:39.355: INFO: Created: latency-svc-qhwjx
    Aug 29 20:27:39.383: INFO: Got endpoints: latency-svc-7mtb5 [744.106974ms]
    Aug 29 20:27:39.397: INFO: Created: latency-svc-qsj6x
    Aug 29 20:27:39.433: INFO: Got endpoints: latency-svc-sq7zq [748.930569ms]
    Aug 29 20:27:39.449: INFO: Created: latency-svc-gjccw
    Aug 29 20:27:39.481: INFO: Got endpoints: latency-svc-ck29r [745.957326ms]
    Aug 29 20:27:39.498: INFO: Created: latency-svc-qc7j6
    Aug 29 20:27:39.534: INFO: Got endpoints: latency-svc-jh58f [750.933749ms]
    Aug 29 20:27:39.550: INFO: Created: latency-svc-dwn7b
    Aug 29 20:27:39.585: INFO: Got endpoints: latency-svc-ngzq9 [752.095851ms]
    Aug 29 20:27:39.604: INFO: Created: latency-svc-8kp7z
    Aug 29 20:27:39.639: INFO: Got endpoints: latency-svc-5jnsg [755.068307ms]
    Aug 29 20:27:39.655: INFO: Created: latency-svc-dpqmj
    Aug 29 20:27:39.691: INFO: Got endpoints: latency-svc-bzvrd [755.755133ms]
    Aug 29 20:27:39.705: INFO: Created: latency-svc-msc2v
    Aug 29 20:27:39.734: INFO: Got endpoints: latency-svc-p428n [748.8456ms]
    Aug 29 20:27:39.749: INFO: Created: latency-svc-jp54g
    Aug 29 20:27:39.785: INFO: Got endpoints: latency-svc-g6fwk [749.282822ms]
    Aug 29 20:27:39.802: INFO: Created: latency-svc-z66kl
    Aug 29 20:27:39.834: INFO: Got endpoints: latency-svc-sd2md [750.856303ms]
    Aug 29 20:27:39.852: INFO: Created: latency-svc-4t5rf
    Aug 29 20:27:39.885: INFO: Got endpoints: latency-svc-tp6bf [753.801129ms]
    Aug 29 20:27:39.903: INFO: Created: latency-svc-k9sgf
    Aug 29 20:27:39.932: INFO: Got endpoints: latency-svc-zcp97 [748.193271ms]
    Aug 29 20:27:39.948: INFO: Created: latency-svc-fl54z
    Aug 29 20:27:39.985: INFO: Got endpoints: latency-svc-skksd [748.634313ms]
    Aug 29 20:27:39.999: INFO: Created: latency-svc-6b6wt
    Aug 29 20:27:40.039: INFO: Got endpoints: latency-svc-4t6bp [749.856384ms]
    Aug 29 20:27:40.055: INFO: Created: latency-svc-vsbkx
    Aug 29 20:27:40.084: INFO: Got endpoints: latency-svc-qhwjx [748.25479ms]
    Aug 29 20:27:40.102: INFO: Created: latency-svc-9sfxn
    Aug 29 20:27:40.135: INFO: Got endpoints: latency-svc-qsj6x [752.268601ms]
    Aug 29 20:27:40.152: INFO: Created: latency-svc-cn96w
    Aug 29 20:27:40.189: INFO: Got endpoints: latency-svc-gjccw [756.539843ms]
    Aug 29 20:27:40.205: INFO: Created: latency-svc-97grh
    Aug 29 20:27:40.235: INFO: Got endpoints: latency-svc-qc7j6 [753.714985ms]
    Aug 29 20:27:40.254: INFO: Created: latency-svc-x8rtg
    Aug 29 20:27:40.286: INFO: Got endpoints: latency-svc-dwn7b [751.815437ms]
    Aug 29 20:27:40.300: INFO: Created: latency-svc-tj4kx
    Aug 29 20:27:40.333: INFO: Got endpoints: latency-svc-8kp7z [747.942398ms]
    Aug 29 20:27:40.347: INFO: Created: latency-svc-5v7hz
    Aug 29 20:27:40.385: INFO: Got endpoints: latency-svc-dpqmj [746.228723ms]
    Aug 29 20:27:40.403: INFO: Created: latency-svc-2wspl
    Aug 29 20:27:40.435: INFO: Got endpoints: latency-svc-msc2v [744.006759ms]
    Aug 29 20:27:40.449: INFO: Created: latency-svc-k8sqf
    Aug 29 20:27:40.484: INFO: Got endpoints: latency-svc-jp54g [750.178255ms]
    Aug 29 20:27:40.504: INFO: Created: latency-svc-psxc8
    Aug 29 20:27:40.535: INFO: Got endpoints: latency-svc-z66kl [750.294813ms]
    Aug 29 20:27:40.552: INFO: Created: latency-svc-fs4fg
    Aug 29 20:27:40.587: INFO: Got endpoints: latency-svc-4t5rf [752.593282ms]
    Aug 29 20:27:40.602: INFO: Created: latency-svc-sjbtl
    Aug 29 20:27:40.639: INFO: Got endpoints: latency-svc-k9sgf [754.184138ms]
    Aug 29 20:27:40.652: INFO: Created: latency-svc-hkpqc
    Aug 29 20:27:40.685: INFO: Got endpoints: latency-svc-fl54z [752.326002ms]
    Aug 29 20:27:40.701: INFO: Created: latency-svc-k525c
    Aug 29 20:27:40.735: INFO: Got endpoints: latency-svc-6b6wt [750.01571ms]
    Aug 29 20:27:40.750: INFO: Created: latency-svc-dthjf
    Aug 29 20:27:40.785: INFO: Got endpoints: latency-svc-vsbkx [745.833306ms]
    Aug 29 20:27:40.801: INFO: Created: latency-svc-rg76f
    Aug 29 20:27:40.835: INFO: Got endpoints: latency-svc-9sfxn [750.776003ms]
    Aug 29 20:27:40.849: INFO: Created: latency-svc-2xdq5
    Aug 29 20:27:40.887: INFO: Got endpoints: latency-svc-cn96w [751.984729ms]
    Aug 29 20:27:40.900: INFO: Created: latency-svc-vsb74
    Aug 29 20:27:40.935: INFO: Got endpoints: latency-svc-97grh [745.254193ms]
    Aug 29 20:27:40.948: INFO: Created: latency-svc-xlh45
    Aug 29 20:27:40.991: INFO: Got endpoints: latency-svc-x8rtg [755.529824ms]
    Aug 29 20:27:41.007: INFO: Created: latency-svc-98xxg
    Aug 29 20:27:41.036: INFO: Got endpoints: latency-svc-tj4kx [749.424078ms]
    Aug 29 20:27:41.058: INFO: Created: latency-svc-8rj7h
    Aug 29 20:27:41.085: INFO: Got endpoints: latency-svc-5v7hz [751.455993ms]
    Aug 29 20:27:41.100: INFO: Created: latency-svc-nc2b4
    Aug 29 20:27:41.135: INFO: Got endpoints: latency-svc-2wspl [749.901683ms]
    Aug 29 20:27:41.150: INFO: Created: latency-svc-bsk9d
    Aug 29 20:27:41.183: INFO: Got endpoints: latency-svc-k8sqf [747.847986ms]
    Aug 29 20:27:41.201: INFO: Created: latency-svc-j6d59
    Aug 29 20:27:41.239: INFO: Got endpoints: latency-svc-psxc8 [754.702788ms]
    Aug 29 20:27:41.253: INFO: Created: latency-svc-snqf5
    Aug 29 20:27:41.285: INFO: Got endpoints: latency-svc-fs4fg [749.344598ms]
    Aug 29 20:27:41.298: INFO: Created: latency-svc-5mdxk
    Aug 29 20:27:41.339: INFO: Got endpoints: latency-svc-sjbtl [752.192043ms]
    Aug 29 20:27:41.355: INFO: Created: latency-svc-l4sjh
    Aug 29 20:27:41.388: INFO: Got endpoints: latency-svc-hkpqc [749.149495ms]
    Aug 29 20:27:41.405: INFO: Created: latency-svc-mjww6
    Aug 29 20:27:41.434: INFO: Got endpoints: latency-svc-k525c [749.410177ms]
    Aug 29 20:27:41.449: INFO: Created: latency-svc-mcvx8
    Aug 29 20:27:41.485: INFO: Got endpoints: latency-svc-dthjf [749.774523ms]
    Aug 29 20:27:41.499: INFO: Created: latency-svc-4ndfv
    Aug 29 20:27:41.534: INFO: Got endpoints: latency-svc-rg76f [748.674984ms]
    Aug 29 20:27:41.549: INFO: Created: latency-svc-bt7sg
    Aug 29 20:27:41.585: INFO: Got endpoints: latency-svc-2xdq5 [750.059887ms]
    Aug 29 20:27:41.603: INFO: Created: latency-svc-dlcbn
    Aug 29 20:27:41.633: INFO: Got endpoints: latency-svc-vsb74 [746.266384ms]
    Aug 29 20:27:41.649: INFO: Created: latency-svc-c5lfn
    Aug 29 20:27:41.686: INFO: Got endpoints: latency-svc-xlh45 [750.80277ms]
    Aug 29 20:27:41.702: INFO: Created: latency-svc-krsgw
    Aug 29 20:27:41.741: INFO: Got endpoints: latency-svc-98xxg [750.111338ms]
    Aug 29 20:27:41.758: INFO: Created: latency-svc-84g9s
    Aug 29 20:27:41.787: INFO: Got endpoints: latency-svc-8rj7h [751.011251ms]
    Aug 29 20:27:41.806: INFO: Created: latency-svc-twp9k
    Aug 29 20:27:41.835: INFO: Got endpoints: latency-svc-nc2b4 [750.689726ms]
    Aug 29 20:27:41.854: INFO: Created: latency-svc-qr758
    Aug 29 20:27:41.885: INFO: Got endpoints: latency-svc-bsk9d [749.683147ms]
    Aug 29 20:27:41.900: INFO: Created: latency-svc-sq2kq
    Aug 29 20:27:41.936: INFO: Got endpoints: latency-svc-j6d59 [753.230297ms]
    Aug 29 20:27:41.950: INFO: Created: latency-svc-zj9v2
    Aug 29 20:27:41.987: INFO: Got endpoints: latency-svc-snqf5 [747.750745ms]
    Aug 29 20:27:42.001: INFO: Created: latency-svc-tv4r6
    Aug 29 20:27:42.037: INFO: Got endpoints: latency-svc-5mdxk [751.884114ms]
    Aug 29 20:27:42.051: INFO: Created: latency-svc-zjz89
    Aug 29 20:27:42.082: INFO: Got endpoints: latency-svc-l4sjh [743.128012ms]
    Aug 29 20:27:42.098: INFO: Created: latency-svc-tfg9q
    Aug 29 20:27:42.132: INFO: Got endpoints: latency-svc-mjww6 [743.566018ms]
    Aug 29 20:27:42.151: INFO: Created: latency-svc-lsbpw
    Aug 29 20:27:42.188: INFO: Got endpoints: latency-svc-mcvx8 [753.950364ms]
    Aug 29 20:27:42.203: INFO: Created: latency-svc-xn7s4
    Aug 29 20:27:42.236: INFO: Got endpoints: latency-svc-4ndfv [750.729623ms]
    Aug 29 20:27:42.249: INFO: Created: latency-svc-49jjq
    Aug 29 20:27:42.285: INFO: Got endpoints: latency-svc-bt7sg [750.693625ms]
    Aug 29 20:27:42.297: INFO: Created: latency-svc-47n6l
    Aug 29 20:27:42.335: INFO: Got endpoints: latency-svc-dlcbn [750.395957ms]
    Aug 29 20:27:42.350: INFO: Created: latency-svc-55xwb
    Aug 29 20:27:42.384: INFO: Got endpoints: latency-svc-c5lfn [750.134375ms]
    Aug 29 20:27:42.397: INFO: Created: latency-svc-4cfcv
    Aug 29 20:27:42.437: INFO: Got endpoints: latency-svc-krsgw [750.828798ms]
    Aug 29 20:27:42.455: INFO: Created: latency-svc-c2dhd
    Aug 29 20:27:42.486: INFO: Got endpoints: latency-svc-84g9s [745.671349ms]
    Aug 29 20:27:42.503: INFO: Created: latency-svc-j9cdd
    Aug 29 20:27:42.536: INFO: Got endpoints: latency-svc-twp9k [749.654108ms]
    Aug 29 20:27:42.552: INFO: Created: latency-svc-6dmsc
    Aug 29 20:27:42.584: INFO: Got endpoints: latency-svc-qr758 [748.897387ms]
    Aug 29 20:27:42.599: INFO: Created: latency-svc-dppqp
    Aug 29 20:27:42.634: INFO: Got endpoints: latency-svc-sq2kq [749.343643ms]
    Aug 29 20:27:42.652: INFO: Created: latency-svc-tps6b
    Aug 29 20:27:42.684: INFO: Got endpoints: latency-svc-zj9v2 [746.671542ms]
    Aug 29 20:27:42.697: INFO: Created: latency-svc-jsdm5
    Aug 29 20:27:42.734: INFO: Got endpoints: latency-svc-tv4r6 [747.227034ms]
    Aug 29 20:27:42.751: INFO: Created: latency-svc-zx49z
    Aug 29 20:27:42.785: INFO: Got endpoints: latency-svc-zjz89 [748.539835ms]
    Aug 29 20:27:42.801: INFO: Created: latency-svc-n8w86
    Aug 29 20:27:42.834: INFO: Got endpoints: latency-svc-tfg9q [751.75762ms]
    Aug 29 20:27:42.848: INFO: Created: latency-svc-pdjj7
    Aug 29 20:27:42.885: INFO: Got endpoints: latency-svc-lsbpw [753.619621ms]
    Aug 29 20:27:42.901: INFO: Created: latency-svc-7qvkk
    Aug 29 20:27:42.934: INFO: Got endpoints: latency-svc-xn7s4 [745.424239ms]
    Aug 29 20:27:42.949: INFO: Created: latency-svc-gn9dr
    Aug 29 20:27:42.985: INFO: Got endpoints: latency-svc-49jjq [749.389789ms]
    Aug 29 20:27:43.000: INFO: Created: latency-svc-k4vxj
    Aug 29 20:27:43.034: INFO: Got endpoints: latency-svc-47n6l [749.610924ms]
    Aug 29 20:27:43.064: INFO: Created: latency-svc-ksgfj
    Aug 29 20:27:43.083: INFO: Got endpoints: latency-svc-55xwb [748.035062ms]
    Aug 29 20:27:43.100: INFO: Created: latency-svc-9tf6l
    Aug 29 20:27:43.135: INFO: Got endpoints: latency-svc-4cfcv [751.333362ms]
    Aug 29 20:27:43.149: INFO: Created: latency-svc-cs8w4
    Aug 29 20:27:43.184: INFO: Got endpoints: latency-svc-c2dhd [747.314875ms]
    Aug 29 20:27:43.200: INFO: Created: latency-svc-k9txm
    Aug 29 20:27:43.233: INFO: Got endpoints: latency-svc-j9cdd [746.715815ms]
    Aug 29 20:27:43.251: INFO: Created: latency-svc-2nslw
    Aug 29 20:27:43.289: INFO: Got endpoints: latency-svc-6dmsc [752.056855ms]
    Aug 29 20:27:43.304: INFO: Created: latency-svc-4g7th
    Aug 29 20:27:43.334: INFO: Got endpoints: latency-svc-dppqp [749.862988ms]
    Aug 29 20:27:43.350: INFO: Created: latency-svc-2p46m
    Aug 29 20:27:43.397: INFO: Got endpoints: latency-svc-tps6b [762.985999ms]
    Aug 29 20:27:43.412: INFO: Created: latency-svc-668kz
    Aug 29 20:27:43.434: INFO: Got endpoints: latency-svc-jsdm5 [750.372167ms]
    Aug 29 20:27:43.484: INFO: Got endpoints: latency-svc-zx49z [750.411736ms]
    Aug 29 20:27:43.536: INFO: Got endpoints: latency-svc-n8w86 [750.62405ms]
    Aug 29 20:27:43.584: INFO: Got endpoints: latency-svc-pdjj7 [750.104301ms]
    Aug 29 20:27:43.635: INFO: Got endpoints: latency-svc-7qvkk [749.694479ms]
    Aug 29 20:27:43.685: INFO: Got endpoints: latency-svc-gn9dr [751.644925ms]
    Aug 29 20:27:43.750: INFO: Got endpoints: latency-svc-k4vxj [765.273768ms]
    Aug 29 20:27:43.785: INFO: Got endpoints: latency-svc-ksgfj [750.187485ms]
    Aug 29 20:27:43.836: INFO: Got endpoints: latency-svc-9tf6l [752.290783ms]
    Aug 29 20:27:43.886: INFO: Got endpoints: latency-svc-cs8w4 [750.836881ms]
    Aug 29 20:27:43.934: INFO: Got endpoints: latency-svc-k9txm [750.479342ms]
    Aug 29 20:27:43.984: INFO: Got endpoints: latency-svc-2nslw [750.524451ms]
    Aug 29 20:27:44.033: INFO: Got endpoints: latency-svc-4g7th [744.473358ms]
    Aug 29 20:27:44.084: INFO: Got endpoints: latency-svc-2p46m [749.963746ms]
    Aug 29 20:27:44.135: INFO: Got endpoints: latency-svc-668kz [737.745346ms]
    Aug 29 20:27:44.135: INFO: Latencies: [26.651256ms 38.546283ms 43.22125ms 57.410746ms 59.40133ms 67.285602ms 72.842211ms 73.736251ms 83.285162ms 84.753766ms 95.905102ms 96.742902ms 99.896237ms 103.388017ms 111.636411ms 111.968225ms 120.488433ms 127.250082ms 127.830361ms 141.840375ms 148.196116ms 159.524026ms 163.198432ms 167.307888ms 187.546528ms 189.306057ms 189.422243ms 190.09591ms 213.155225ms 219.639349ms 228.809809ms 250.954665ms 259.643094ms 264.050529ms 265.770052ms 266.294778ms 276.671884ms 294.761534ms 294.861088ms 294.992913ms 302.474583ms 306.205478ms 341.321874ms 376.928685ms 422.251057ms 466.967533ms 498.927634ms 538.853996ms 576.60324ms 611.828572ms 638.75294ms 679.739028ms 708.208594ms 737.745346ms 741.593744ms 741.852795ms 743.046146ms 743.128012ms 743.566018ms 744.006759ms 744.106974ms 744.473358ms 744.82281ms 744.954276ms 745.254193ms 745.424239ms 745.671349ms 745.833306ms 745.957326ms 746.034624ms 746.228723ms 746.266384ms 746.272919ms 746.388924ms 746.671542ms 746.715815ms 746.896077ms 747.076793ms 747.190109ms 747.227034ms 747.314875ms 747.38383ms 747.570991ms 747.750745ms 747.760724ms 747.840991ms 747.847986ms 747.908607ms 747.942398ms 748.035062ms 748.193271ms 748.25479ms 748.539835ms 748.634313ms 748.674984ms 748.694109ms 748.707647ms 748.815137ms 748.8456ms 748.897387ms 748.930569ms 749.124208ms 749.149495ms 749.282822ms 749.332511ms 749.343643ms 749.344598ms 749.389789ms 749.410177ms 749.424078ms 749.500539ms 749.508601ms 749.582585ms 749.610924ms 749.654108ms 749.683147ms 749.694479ms 749.763177ms 749.774523ms 749.856384ms 749.862988ms 749.901683ms 749.963746ms 750.01571ms 750.059887ms 750.067664ms 750.104301ms 750.111338ms 750.134375ms 750.178255ms 750.187485ms 750.276687ms 750.294813ms 750.372167ms 750.395957ms 750.411736ms 750.447539ms 750.479342ms 750.488166ms 750.524451ms 750.62405ms 750.647217ms 750.689726ms 750.693625ms 750.729623ms 750.776003ms 750.80277ms 750.828798ms 750.836881ms 750.856303ms 750.933749ms 751.011251ms 751.128961ms 751.129816ms 751.134092ms 751.333362ms 751.382439ms 751.397081ms 751.455993ms 751.644925ms 751.659825ms 751.75762ms 751.815437ms 751.884114ms 751.88641ms 751.984729ms 752.018023ms 752.056855ms 752.082544ms 752.095851ms 752.192043ms 752.268601ms 752.290783ms 752.3107ms 752.326002ms 752.468048ms 752.593282ms 753.230297ms 753.269424ms 753.405396ms 753.549854ms 753.566077ms 753.619621ms 753.64285ms 753.714985ms 753.774488ms 753.801129ms 753.950364ms 753.977317ms 754.184138ms 754.589484ms 754.702788ms 754.967406ms 755.068307ms 755.345076ms 755.529824ms 755.755133ms 756.539843ms 762.985999ms 765.273768ms]
    Aug 29 20:27:44.135: INFO: 50 %ile: 748.930569ms
    Aug 29 20:27:44.135: INFO: 90 %ile: 753.549854ms
    Aug 29 20:27:44.135: INFO: 99 %ile: 762.985999ms
    Aug 29 20:27:44.135: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:27:44.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-7554" for this suite. 08/29/23 20:27:44.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:27:44.153
Aug 29 20:27:44.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:27:44.154
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:44.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:44.182
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 08/29/23 20:27:44.185
Aug 29 20:27:44.185: INFO: Creating e2e-svc-a-cnb7h
Aug 29 20:27:44.197: INFO: Creating e2e-svc-b-b9mdl
Aug 29 20:27:44.215: INFO: Creating e2e-svc-c-x2qvt
STEP: deleting service collection 08/29/23 20:27:44.249
Aug 29 20:27:44.296: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:27:44.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9787" for this suite. 08/29/23 20:27:44.302
------------------------------
• [0.157 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:27:44.153
    Aug 29 20:27:44.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:27:44.154
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:44.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:44.182
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 08/29/23 20:27:44.185
    Aug 29 20:27:44.185: INFO: Creating e2e-svc-a-cnb7h
    Aug 29 20:27:44.197: INFO: Creating e2e-svc-b-b9mdl
    Aug 29 20:27:44.215: INFO: Creating e2e-svc-c-x2qvt
    STEP: deleting service collection 08/29/23 20:27:44.249
    Aug 29 20:27:44.296: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:27:44.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9787" for this suite. 08/29/23 20:27:44.302
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:27:44.311
Aug 29 20:27:44.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename dns 08/29/23 20:27:44.312
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:44.334
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:44.337
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/29/23 20:27:44.34
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
 08/29/23 20:27:44.346
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
 08/29/23 20:27:44.346
STEP: creating a pod to probe DNS 08/29/23 20:27:44.346
STEP: submitting the pod to kubernetes 08/29/23 20:27:44.347
Aug 29 20:27:44.357: INFO: Waiting up to 15m0s for pod "dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402" in namespace "dns-1139" to be "running"
Aug 29 20:27:44.362: INFO: Pod "dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402": Phase="Pending", Reason="", readiness=false. Elapsed: 4.188352ms
Aug 29 20:27:46.368: INFO: Pod "dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402": Phase="Running", Reason="", readiness=true. Elapsed: 2.011022686s
Aug 29 20:27:46.368: INFO: Pod "dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402" satisfied condition "running"
STEP: retrieving the pod 08/29/23 20:27:46.368
STEP: looking for the results for each expected name from probers 08/29/23 20:27:46.373
Aug 29 20:27:46.382: INFO: DNS probes using dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402 succeeded

STEP: deleting the pod 08/29/23 20:27:46.382
STEP: changing the externalName to bar.example.com 08/29/23 20:27:46.396
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
 08/29/23 20:27:46.406
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
 08/29/23 20:27:46.406
STEP: creating a second pod to probe DNS 08/29/23 20:27:46.406
STEP: submitting the pod to kubernetes 08/29/23 20:27:46.407
Aug 29 20:27:46.415: INFO: Waiting up to 15m0s for pod "dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de" in namespace "dns-1139" to be "running"
Aug 29 20:27:46.418: INFO: Pod "dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457738ms
Aug 29 20:27:48.425: INFO: Pod "dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de": Phase="Running", Reason="", readiness=true. Elapsed: 2.010055078s
Aug 29 20:27:48.425: INFO: Pod "dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de" satisfied condition "running"
STEP: retrieving the pod 08/29/23 20:27:48.425
STEP: looking for the results for each expected name from probers 08/29/23 20:27:48.429
Aug 29 20:27:48.439: INFO: DNS probes using dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de succeeded

STEP: deleting the pod 08/29/23 20:27:48.439
STEP: changing the service to type=ClusterIP 08/29/23 20:27:48.458
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
 08/29/23 20:27:48.476
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
 08/29/23 20:27:48.477
STEP: creating a third pod to probe DNS 08/29/23 20:27:48.477
STEP: submitting the pod to kubernetes 08/29/23 20:27:48.481
Aug 29 20:27:48.489: INFO: Waiting up to 15m0s for pod "dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5" in namespace "dns-1139" to be "running"
Aug 29 20:27:48.493: INFO: Pod "dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.925586ms
Aug 29 20:27:50.498: INFO: Pod "dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008498597s
Aug 29 20:27:50.498: INFO: Pod "dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5" satisfied condition "running"
STEP: retrieving the pod 08/29/23 20:27:50.498
STEP: looking for the results for each expected name from probers 08/29/23 20:27:50.501
Aug 29 20:27:50.511: INFO: File wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local from pod  dns-1139/dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5 contains '' instead of '172.19.40.168'
Aug 29 20:27:50.515: INFO: File jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local from pod  dns-1139/dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5 contains '' instead of '172.19.40.168'
Aug 29 20:27:50.515: INFO: Lookups using dns-1139/dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5 failed for: [wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local]

Aug 29 20:27:55.526: INFO: DNS probes using dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5 succeeded

STEP: deleting the pod 08/29/23 20:27:55.526
STEP: deleting the test externalName service 08/29/23 20:27:55.543
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 29 20:27:55.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1139" for this suite. 08/29/23 20:27:55.569
------------------------------
• [SLOW TEST] [11.266 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:27:44.311
    Aug 29 20:27:44.311: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename dns 08/29/23 20:27:44.312
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:44.334
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:44.337
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/29/23 20:27:44.34
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
     08/29/23 20:27:44.346
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
     08/29/23 20:27:44.346
    STEP: creating a pod to probe DNS 08/29/23 20:27:44.346
    STEP: submitting the pod to kubernetes 08/29/23 20:27:44.347
    Aug 29 20:27:44.357: INFO: Waiting up to 15m0s for pod "dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402" in namespace "dns-1139" to be "running"
    Aug 29 20:27:44.362: INFO: Pod "dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402": Phase="Pending", Reason="", readiness=false. Elapsed: 4.188352ms
    Aug 29 20:27:46.368: INFO: Pod "dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402": Phase="Running", Reason="", readiness=true. Elapsed: 2.011022686s
    Aug 29 20:27:46.368: INFO: Pod "dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 20:27:46.368
    STEP: looking for the results for each expected name from probers 08/29/23 20:27:46.373
    Aug 29 20:27:46.382: INFO: DNS probes using dns-test-d85e35bb-56c4-48c8-a336-0d10f7722402 succeeded

    STEP: deleting the pod 08/29/23 20:27:46.382
    STEP: changing the externalName to bar.example.com 08/29/23 20:27:46.396
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
     08/29/23 20:27:46.406
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
     08/29/23 20:27:46.406
    STEP: creating a second pod to probe DNS 08/29/23 20:27:46.406
    STEP: submitting the pod to kubernetes 08/29/23 20:27:46.407
    Aug 29 20:27:46.415: INFO: Waiting up to 15m0s for pod "dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de" in namespace "dns-1139" to be "running"
    Aug 29 20:27:46.418: INFO: Pod "dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de": Phase="Pending", Reason="", readiness=false. Elapsed: 3.457738ms
    Aug 29 20:27:48.425: INFO: Pod "dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de": Phase="Running", Reason="", readiness=true. Elapsed: 2.010055078s
    Aug 29 20:27:48.425: INFO: Pod "dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 20:27:48.425
    STEP: looking for the results for each expected name from probers 08/29/23 20:27:48.429
    Aug 29 20:27:48.439: INFO: DNS probes using dns-test-f9bc9fe7-0857-4009-89fa-507662fab1de succeeded

    STEP: deleting the pod 08/29/23 20:27:48.439
    STEP: changing the service to type=ClusterIP 08/29/23 20:27:48.458
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
     08/29/23 20:27:48.476
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1139.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local; sleep 1; done
     08/29/23 20:27:48.477
    STEP: creating a third pod to probe DNS 08/29/23 20:27:48.477
    STEP: submitting the pod to kubernetes 08/29/23 20:27:48.481
    Aug 29 20:27:48.489: INFO: Waiting up to 15m0s for pod "dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5" in namespace "dns-1139" to be "running"
    Aug 29 20:27:48.493: INFO: Pod "dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.925586ms
    Aug 29 20:27:50.498: INFO: Pod "dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5": Phase="Running", Reason="", readiness=true. Elapsed: 2.008498597s
    Aug 29 20:27:50.498: INFO: Pod "dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 20:27:50.498
    STEP: looking for the results for each expected name from probers 08/29/23 20:27:50.501
    Aug 29 20:27:50.511: INFO: File wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local from pod  dns-1139/dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5 contains '' instead of '172.19.40.168'
    Aug 29 20:27:50.515: INFO: File jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local from pod  dns-1139/dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5 contains '' instead of '172.19.40.168'
    Aug 29 20:27:50.515: INFO: Lookups using dns-1139/dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5 failed for: [wheezy_udp@dns-test-service-3.dns-1139.svc.cluster.local jessie_udp@dns-test-service-3.dns-1139.svc.cluster.local]

    Aug 29 20:27:55.526: INFO: DNS probes using dns-test-1ca92a57-954a-43ea-8b64-2437fc72e4f5 succeeded

    STEP: deleting the pod 08/29/23 20:27:55.526
    STEP: deleting the test externalName service 08/29/23 20:27:55.543
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:27:55.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1139" for this suite. 08/29/23 20:27:55.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:27:55.578
Aug 29 20:27:55.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-lifecycle-hook 08/29/23 20:27:55.58
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:55.596
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:55.6
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/29/23 20:27:55.607
Aug 29 20:27:55.617: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9997" to be "running and ready"
Aug 29 20:27:55.622: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568455ms
Aug 29 20:27:55.622: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:27:57.626: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008546541s
Aug 29 20:27:57.626: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 29 20:27:57.626: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 08/29/23 20:27:57.629
Aug 29 20:27:57.635: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9997" to be "running and ready"
Aug 29 20:27:57.639: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428693ms
Aug 29 20:27:57.639: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:27:59.643: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007586044s
Aug 29 20:27:59.643: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 29 20:27:59.643: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/29/23 20:27:59.647
STEP: delete the pod with lifecycle hook 08/29/23 20:27:59.67
Aug 29 20:27:59.680: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 20:27:59.684: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 29 20:28:01.685: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 20:28:01.690: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 29 20:28:03.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 29 20:28:03.690: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 29 20:28:03.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9997" for this suite. 08/29/23 20:28:03.696
------------------------------
• [SLOW TEST] [8.127 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:27:55.578
    Aug 29 20:27:55.578: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/29/23 20:27:55.58
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:27:55.596
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:27:55.6
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/29/23 20:27:55.607
    Aug 29 20:27:55.617: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9997" to be "running and ready"
    Aug 29 20:27:55.622: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568455ms
    Aug 29 20:27:55.622: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:27:57.626: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008546541s
    Aug 29 20:27:57.626: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 29 20:27:57.626: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 08/29/23 20:27:57.629
    Aug 29 20:27:57.635: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-9997" to be "running and ready"
    Aug 29 20:27:57.639: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.428693ms
    Aug 29 20:27:57.639: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:27:59.643: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007586044s
    Aug 29 20:27:59.643: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 29 20:27:59.643: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/29/23 20:27:59.647
    STEP: delete the pod with lifecycle hook 08/29/23 20:27:59.67
    Aug 29 20:27:59.680: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 29 20:27:59.684: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 29 20:28:01.685: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 29 20:28:01.690: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 29 20:28:03.684: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 29 20:28:03.690: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:28:03.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9997" for this suite. 08/29/23 20:28:03.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:28:03.706
Aug 29 20:28:03.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename watch 08/29/23 20:28:03.707
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:03.728
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:03.731
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/29/23 20:28:03.734
STEP: creating a new configmap 08/29/23 20:28:03.736
STEP: modifying the configmap once 08/29/23 20:28:03.744
STEP: changing the label value of the configmap 08/29/23 20:28:03.753
STEP: Expecting to observe a delete notification for the watched object 08/29/23 20:28:03.761
Aug 29 20:28:03.761: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37227 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:28:03.761: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37228 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:28:03.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37229 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/29/23 20:28:03.761
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/29/23 20:28:03.77
STEP: changing the label value of the configmap back 08/29/23 20:28:13.771
STEP: modifying the configmap a third time 08/29/23 20:28:13.783
STEP: deleting the configmap 08/29/23 20:28:13.791
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/29/23 20:28:13.799
Aug 29 20:28:13.799: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37280 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:28:13.799: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37281 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:28:13.799: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37282 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 29 20:28:13.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5916" for this suite. 08/29/23 20:28:13.804
------------------------------
• [SLOW TEST] [10.106 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:28:03.706
    Aug 29 20:28:03.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename watch 08/29/23 20:28:03.707
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:03.728
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:03.731
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/29/23 20:28:03.734
    STEP: creating a new configmap 08/29/23 20:28:03.736
    STEP: modifying the configmap once 08/29/23 20:28:03.744
    STEP: changing the label value of the configmap 08/29/23 20:28:03.753
    STEP: Expecting to observe a delete notification for the watched object 08/29/23 20:28:03.761
    Aug 29 20:28:03.761: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37227 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:28:03.761: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37228 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:28:03.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37229 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/29/23 20:28:03.761
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/29/23 20:28:03.77
    STEP: changing the label value of the configmap back 08/29/23 20:28:13.771
    STEP: modifying the configmap a third time 08/29/23 20:28:13.783
    STEP: deleting the configmap 08/29/23 20:28:13.791
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/29/23 20:28:13.799
    Aug 29 20:28:13.799: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37280 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:28:13.799: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37281 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:28:13.799: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-5916  97baddfe-5436-46e3-8a0e-9ae9f6d8102c 37282 0 2023-08-29 20:28:03 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-29 20:28:13 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:28:13.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5916" for this suite. 08/29/23 20:28:13.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:28:13.813
Aug 29 20:28:13.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename hostport 08/29/23 20:28:13.813
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:13.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:13.841
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/29/23 20:28:13.849
Aug 29 20:28:13.860: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9822" to be "running and ready"
Aug 29 20:28:13.863: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.615944ms
Aug 29 20:28:13.863: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:28:15.867: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007514934s
Aug 29 20:28:15.867: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 29 20:28:15.867: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.45.35.198 on the node which pod1 resides and expect scheduled 08/29/23 20:28:15.867
Aug 29 20:28:15.876: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9822" to be "running and ready"
Aug 29 20:28:15.879: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994356ms
Aug 29 20:28:15.879: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:28:17.885: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008742832s
Aug 29 20:28:17.885: INFO: The phase of Pod pod2 is Running (Ready = false)
Aug 29 20:28:19.885: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008378035s
Aug 29 20:28:19.885: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 29 20:28:19.885: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.45.35.198 but use UDP protocol on the node which pod2 resides 08/29/23 20:28:19.885
Aug 29 20:28:19.893: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9822" to be "running and ready"
Aug 29 20:28:19.896: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140273ms
Aug 29 20:28:19.896: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:28:21.902: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008986194s
Aug 29 20:28:21.902: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 29 20:28:21.902: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 29 20:28:21.910: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9822" to be "running and ready"
Aug 29 20:28:21.914: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313616ms
Aug 29 20:28:21.914: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:28:23.920: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009371403s
Aug 29 20:28:23.920: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 29 20:28:23.920: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/29/23 20:28:23.923
Aug 29 20:28:23.923: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.45.35.198 http://127.0.0.1:54323/hostname] Namespace:hostport-9822 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:28:23.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:28:23.924: INFO: ExecWithOptions: Clientset creation
Aug 29 20:28:23.924: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-9822/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.45.35.198+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.45.35.198, port: 54323 08/29/23 20:28:23.997
Aug 29 20:28:23.997: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.45.35.198:54323/hostname] Namespace:hostport-9822 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:28:23.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:28:23.998: INFO: ExecWithOptions: Clientset creation
Aug 29 20:28:23.998: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-9822/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.45.35.198%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.45.35.198, port: 54323 UDP 08/29/23 20:28:24.073
Aug 29 20:28:24.073: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.45.35.198 54323] Namespace:hostport-9822 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:28:24.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:28:24.074: INFO: ExecWithOptions: Clientset creation
Aug 29 20:28:24.074: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-9822/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.45.35.198+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Aug 29 20:28:29.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-9822" for this suite. 08/29/23 20:28:29.156
------------------------------
• [SLOW TEST] [15.350 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:28:13.813
    Aug 29 20:28:13.813: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename hostport 08/29/23 20:28:13.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:13.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:13.841
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/29/23 20:28:13.849
    Aug 29 20:28:13.860: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-9822" to be "running and ready"
    Aug 29 20:28:13.863: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.615944ms
    Aug 29 20:28:13.863: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:28:15.867: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007514934s
    Aug 29 20:28:15.867: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 29 20:28:15.867: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.45.35.198 on the node which pod1 resides and expect scheduled 08/29/23 20:28:15.867
    Aug 29 20:28:15.876: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-9822" to be "running and ready"
    Aug 29 20:28:15.879: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.994356ms
    Aug 29 20:28:15.879: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:28:17.885: INFO: Pod "pod2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008742832s
    Aug 29 20:28:17.885: INFO: The phase of Pod pod2 is Running (Ready = false)
    Aug 29 20:28:19.885: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.008378035s
    Aug 29 20:28:19.885: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 29 20:28:19.885: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.45.35.198 but use UDP protocol on the node which pod2 resides 08/29/23 20:28:19.885
    Aug 29 20:28:19.893: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-9822" to be "running and ready"
    Aug 29 20:28:19.896: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140273ms
    Aug 29 20:28:19.896: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:28:21.902: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.008986194s
    Aug 29 20:28:21.902: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 29 20:28:21.902: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 29 20:28:21.910: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-9822" to be "running and ready"
    Aug 29 20:28:21.914: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.313616ms
    Aug 29 20:28:21.914: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:28:23.920: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.009371403s
    Aug 29 20:28:23.920: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 29 20:28:23.920: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/29/23 20:28:23.923
    Aug 29 20:28:23.923: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.45.35.198 http://127.0.0.1:54323/hostname] Namespace:hostport-9822 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:28:23.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:28:23.924: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:28:23.924: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-9822/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.45.35.198+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.45.35.198, port: 54323 08/29/23 20:28:23.997
    Aug 29 20:28:23.997: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.45.35.198:54323/hostname] Namespace:hostport-9822 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:28:23.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:28:23.998: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:28:23.998: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-9822/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.45.35.198%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.45.35.198, port: 54323 UDP 08/29/23 20:28:24.073
    Aug 29 20:28:24.073: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.45.35.198 54323] Namespace:hostport-9822 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:28:24.073: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:28:24.074: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:28:24.074: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/hostport-9822/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.45.35.198+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:28:29.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-9822" for this suite. 08/29/23 20:28:29.156
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:28:29.163
Aug 29 20:28:29.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:28:29.165
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:29.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:29.187
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-e5b91e56-f7f7-4e42-9ebf-a443008cf2c0 08/29/23 20:28:29.19
STEP: Creating a pod to test consume configMaps 08/29/23 20:28:29.196
Aug 29 20:28:29.207: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346" in namespace "projected-9128" to be "Succeeded or Failed"
Aug 29 20:28:29.210: INFO: Pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346": Phase="Pending", Reason="", readiness=false. Elapsed: 3.210351ms
Aug 29 20:28:31.227: INFO: Pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019510274s
Aug 29 20:28:33.215: INFO: Pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008199132s
STEP: Saw pod success 08/29/23 20:28:33.215
Aug 29 20:28:33.216: INFO: Pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346" satisfied condition "Succeeded or Failed"
Aug 29 20:28:33.219: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346 container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:28:33.227
Aug 29 20:28:33.247: INFO: Waiting for pod pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346 to disappear
Aug 29 20:28:33.250: INFO: Pod pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:28:33.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9128" for this suite. 08/29/23 20:28:33.255
------------------------------
• [4.102 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:28:29.163
    Aug 29 20:28:29.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:28:29.165
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:29.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:29.187
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-e5b91e56-f7f7-4e42-9ebf-a443008cf2c0 08/29/23 20:28:29.19
    STEP: Creating a pod to test consume configMaps 08/29/23 20:28:29.196
    Aug 29 20:28:29.207: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346" in namespace "projected-9128" to be "Succeeded or Failed"
    Aug 29 20:28:29.210: INFO: Pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346": Phase="Pending", Reason="", readiness=false. Elapsed: 3.210351ms
    Aug 29 20:28:31.227: INFO: Pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019510274s
    Aug 29 20:28:33.215: INFO: Pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008199132s
    STEP: Saw pod success 08/29/23 20:28:33.215
    Aug 29 20:28:33.216: INFO: Pod "pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346" satisfied condition "Succeeded or Failed"
    Aug 29 20:28:33.219: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346 container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:28:33.227
    Aug 29 20:28:33.247: INFO: Waiting for pod pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346 to disappear
    Aug 29 20:28:33.250: INFO: Pod pod-projected-configmaps-f53d9bd7-fd72-481d-ad45-72c9e6e9c346 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:28:33.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9128" for this suite. 08/29/23 20:28:33.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:28:33.267
Aug 29 20:28:33.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 20:28:33.268
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:33.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:33.292
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 08/29/23 20:28:33.295
Aug 29 20:28:33.307: INFO: Waiting up to 5m0s for pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a" in namespace "downward-api-8948" to be "Succeeded or Failed"
Aug 29 20:28:33.311: INFO: Pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.698388ms
Aug 29 20:28:35.316: INFO: Pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008675186s
Aug 29 20:28:37.315: INFO: Pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007488855s
STEP: Saw pod success 08/29/23 20:28:37.315
Aug 29 20:28:37.315: INFO: Pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a" satisfied condition "Succeeded or Failed"
Aug 29 20:28:37.319: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downward-api-5617daaf-7865-4eae-b8ac-300a7392900a container dapi-container: <nil>
STEP: delete the pod 08/29/23 20:28:37.329
Aug 29 20:28:37.344: INFO: Waiting for pod downward-api-5617daaf-7865-4eae-b8ac-300a7392900a to disappear
Aug 29 20:28:37.346: INFO: Pod downward-api-5617daaf-7865-4eae-b8ac-300a7392900a no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 29 20:28:37.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8948" for this suite. 08/29/23 20:28:37.352
------------------------------
• [4.095 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:28:33.267
    Aug 29 20:28:33.267: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 20:28:33.268
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:33.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:33.292
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 08/29/23 20:28:33.295
    Aug 29 20:28:33.307: INFO: Waiting up to 5m0s for pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a" in namespace "downward-api-8948" to be "Succeeded or Failed"
    Aug 29 20:28:33.311: INFO: Pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.698388ms
    Aug 29 20:28:35.316: INFO: Pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008675186s
    Aug 29 20:28:37.315: INFO: Pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007488855s
    STEP: Saw pod success 08/29/23 20:28:37.315
    Aug 29 20:28:37.315: INFO: Pod "downward-api-5617daaf-7865-4eae-b8ac-300a7392900a" satisfied condition "Succeeded or Failed"
    Aug 29 20:28:37.319: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downward-api-5617daaf-7865-4eae-b8ac-300a7392900a container dapi-container: <nil>
    STEP: delete the pod 08/29/23 20:28:37.329
    Aug 29 20:28:37.344: INFO: Waiting for pod downward-api-5617daaf-7865-4eae-b8ac-300a7392900a to disappear
    Aug 29 20:28:37.346: INFO: Pod downward-api-5617daaf-7865-4eae-b8ac-300a7392900a no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:28:37.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8948" for this suite. 08/29/23 20:28:37.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:28:37.362
Aug 29 20:28:37.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename subpath 08/29/23 20:28:37.363
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:37.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:37.389
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/29/23 20:28:37.392
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-vmhq 08/29/23 20:28:37.404
STEP: Creating a pod to test atomic-volume-subpath 08/29/23 20:28:37.404
Aug 29 20:28:37.413: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-vmhq" in namespace "subpath-1579" to be "Succeeded or Failed"
Aug 29 20:28:37.417: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.873591ms
Aug 29 20:28:39.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 2.010029623s
Aug 29 20:28:41.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 4.010424131s
Aug 29 20:28:43.422: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 6.009103842s
Aug 29 20:28:45.421: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 8.008510138s
Aug 29 20:28:47.421: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 10.008178804s
Aug 29 20:28:49.422: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 12.009239497s
Aug 29 20:28:51.424: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 14.010875758s
Aug 29 20:28:53.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 16.010049103s
Aug 29 20:28:55.422: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 18.008877859s
Aug 29 20:28:57.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 20.010493009s
Aug 29 20:28:59.422: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 22.008758307s
Aug 29 20:29:01.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=false. Elapsed: 24.010021244s
Aug 29 20:29:03.424: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010695917s
STEP: Saw pod success 08/29/23 20:29:03.424
Aug 29 20:29:03.424: INFO: Pod "pod-subpath-test-secret-vmhq" satisfied condition "Succeeded or Failed"
Aug 29 20:29:03.428: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-secret-vmhq container test-container-subpath-secret-vmhq: <nil>
STEP: delete the pod 08/29/23 20:29:03.438
Aug 29 20:29:03.454: INFO: Waiting for pod pod-subpath-test-secret-vmhq to disappear
Aug 29 20:29:03.456: INFO: Pod pod-subpath-test-secret-vmhq no longer exists
STEP: Deleting pod pod-subpath-test-secret-vmhq 08/29/23 20:29:03.456
Aug 29 20:29:03.457: INFO: Deleting pod "pod-subpath-test-secret-vmhq" in namespace "subpath-1579"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Aug 29 20:29:03.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-1579" for this suite. 08/29/23 20:29:03.464
------------------------------
• [SLOW TEST] [26.109 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:28:37.362
    Aug 29 20:28:37.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename subpath 08/29/23 20:28:37.363
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:28:37.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:28:37.389
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/29/23 20:28:37.392
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-vmhq 08/29/23 20:28:37.404
    STEP: Creating a pod to test atomic-volume-subpath 08/29/23 20:28:37.404
    Aug 29 20:28:37.413: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-vmhq" in namespace "subpath-1579" to be "Succeeded or Failed"
    Aug 29 20:28:37.417: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.873591ms
    Aug 29 20:28:39.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 2.010029623s
    Aug 29 20:28:41.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 4.010424131s
    Aug 29 20:28:43.422: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 6.009103842s
    Aug 29 20:28:45.421: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 8.008510138s
    Aug 29 20:28:47.421: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 10.008178804s
    Aug 29 20:28:49.422: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 12.009239497s
    Aug 29 20:28:51.424: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 14.010875758s
    Aug 29 20:28:53.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 16.010049103s
    Aug 29 20:28:55.422: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 18.008877859s
    Aug 29 20:28:57.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 20.010493009s
    Aug 29 20:28:59.422: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=true. Elapsed: 22.008758307s
    Aug 29 20:29:01.423: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Running", Reason="", readiness=false. Elapsed: 24.010021244s
    Aug 29 20:29:03.424: INFO: Pod "pod-subpath-test-secret-vmhq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.010695917s
    STEP: Saw pod success 08/29/23 20:29:03.424
    Aug 29 20:29:03.424: INFO: Pod "pod-subpath-test-secret-vmhq" satisfied condition "Succeeded or Failed"
    Aug 29 20:29:03.428: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-subpath-test-secret-vmhq container test-container-subpath-secret-vmhq: <nil>
    STEP: delete the pod 08/29/23 20:29:03.438
    Aug 29 20:29:03.454: INFO: Waiting for pod pod-subpath-test-secret-vmhq to disappear
    Aug 29 20:29:03.456: INFO: Pod pod-subpath-test-secret-vmhq no longer exists
    STEP: Deleting pod pod-subpath-test-secret-vmhq 08/29/23 20:29:03.456
    Aug 29 20:29:03.457: INFO: Deleting pod "pod-subpath-test-secret-vmhq" in namespace "subpath-1579"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:29:03.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-1579" for this suite. 08/29/23 20:29:03.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:29:03.471
Aug 29 20:29:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sysctl 08/29/23 20:29:03.472
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:03.495
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:03.498
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/29/23 20:29:03.501
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:29:03.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-133" for this suite. 08/29/23 20:29:03.511
------------------------------
• [0.047 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:29:03.471
    Aug 29 20:29:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sysctl 08/29/23 20:29:03.472
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:03.495
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:03.498
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/29/23 20:29:03.501
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:29:03.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-133" for this suite. 08/29/23 20:29:03.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:29:03.519
Aug 29 20:29:03.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 20:29:03.521
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:03.538
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:03.541
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 08/29/23 20:29:03.544
STEP: Creating a ResourceQuota 08/29/23 20:29:08.548
STEP: Ensuring resource quota status is calculated 08/29/23 20:29:08.557
STEP: Creating a ReplicationController 08/29/23 20:29:10.562
STEP: Ensuring resource quota status captures replication controller creation 08/29/23 20:29:10.579
STEP: Deleting a ReplicationController 08/29/23 20:29:12.585
STEP: Ensuring resource quota status released usage 08/29/23 20:29:12.592
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 20:29:14.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-8049" for this suite. 08/29/23 20:29:14.601
------------------------------
• [SLOW TEST] [11.090 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:29:03.519
    Aug 29 20:29:03.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 20:29:03.521
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:03.538
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:03.541
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 08/29/23 20:29:03.544
    STEP: Creating a ResourceQuota 08/29/23 20:29:08.548
    STEP: Ensuring resource quota status is calculated 08/29/23 20:29:08.557
    STEP: Creating a ReplicationController 08/29/23 20:29:10.562
    STEP: Ensuring resource quota status captures replication controller creation 08/29/23 20:29:10.579
    STEP: Deleting a ReplicationController 08/29/23 20:29:12.585
    STEP: Ensuring resource quota status released usage 08/29/23 20:29:12.592
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:29:14.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-8049" for this suite. 08/29/23 20:29:14.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:29:14.612
Aug 29 20:29:14.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename disruption 08/29/23 20:29:14.613
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:14.634
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:14.637
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:29:14.639
Aug 29 20:29:14.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename disruption-2 08/29/23 20:29:14.64
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:14.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:14.671
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 08/29/23 20:29:14.696
STEP: Waiting for the pdb to be processed 08/29/23 20:29:16.711
STEP: Waiting for the pdb to be processed 08/29/23 20:29:18.726
STEP: listing a collection of PDBs across all namespaces 08/29/23 20:29:20.734
STEP: listing a collection of PDBs in namespace disruption-4868 08/29/23 20:29:20.738
STEP: deleting a collection of PDBs 08/29/23 20:29:20.742
STEP: Waiting for the PDB collection to be deleted 08/29/23 20:29:20.757
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Aug 29 20:29:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:29:20.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-3024" for this suite. 08/29/23 20:29:20.769
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4868" for this suite. 08/29/23 20:29:20.776
------------------------------
• [SLOW TEST] [6.178 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:29:14.612
    Aug 29 20:29:14.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename disruption 08/29/23 20:29:14.613
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:14.634
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:14.637
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:29:14.639
    Aug 29 20:29:14.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename disruption-2 08/29/23 20:29:14.64
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:14.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:14.671
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 08/29/23 20:29:14.696
    STEP: Waiting for the pdb to be processed 08/29/23 20:29:16.711
    STEP: Waiting for the pdb to be processed 08/29/23 20:29:18.726
    STEP: listing a collection of PDBs across all namespaces 08/29/23 20:29:20.734
    STEP: listing a collection of PDBs in namespace disruption-4868 08/29/23 20:29:20.738
    STEP: deleting a collection of PDBs 08/29/23 20:29:20.742
    STEP: Waiting for the PDB collection to be deleted 08/29/23 20:29:20.757
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:29:20.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:29:20.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-3024" for this suite. 08/29/23 20:29:20.769
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4868" for this suite. 08/29/23 20:29:20.776
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:29:20.79
Aug 29 20:29:20.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename gc 08/29/23 20:29:20.792
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:20.81
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:20.814
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/29/23 20:29:20.817
STEP: delete the rc 08/29/23 20:29:25.83
STEP: wait for all pods to be garbage collected 08/29/23 20:29:25.837
STEP: Gathering metrics 08/29/23 20:29:30.845
W0829 20:29:30.854891      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 20:29:30.854: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 29 20:29:30.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7379" for this suite. 08/29/23 20:29:30.86
------------------------------
• [SLOW TEST] [10.081 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:29:20.79
    Aug 29 20:29:20.791: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename gc 08/29/23 20:29:20.792
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:20.81
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:20.814
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/29/23 20:29:20.817
    STEP: delete the rc 08/29/23 20:29:25.83
    STEP: wait for all pods to be garbage collected 08/29/23 20:29:25.837
    STEP: Gathering metrics 08/29/23 20:29:30.845
    W0829 20:29:30.854891      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 29 20:29:30.854: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:29:30.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7379" for this suite. 08/29/23 20:29:30.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:29:30.872
Aug 29 20:29:30.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename cronjob 08/29/23 20:29:30.873
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:30.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:30.9
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/29/23 20:29:30.903
STEP: Ensuring more than one job is running at a time 08/29/23 20:29:30.91
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/29/23 20:31:00.914
STEP: Removing cronjob 08/29/23 20:31:00.917
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 29 20:31:00.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-8610" for this suite. 08/29/23 20:31:00.939
------------------------------
• [SLOW TEST] [90.074 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:29:30.872
    Aug 29 20:29:30.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename cronjob 08/29/23 20:29:30.873
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:29:30.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:29:30.9
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/29/23 20:29:30.903
    STEP: Ensuring more than one job is running at a time 08/29/23 20:29:30.91
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/29/23 20:31:00.914
    STEP: Removing cronjob 08/29/23 20:31:00.917
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:31:00.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-8610" for this suite. 08/29/23 20:31:00.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:31:00.947
Aug 29 20:31:00.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename aggregator 08/29/23 20:31:00.948
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:00.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:00.974
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 29 20:31:00.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/29/23 20:31:00.978
Aug 29 20:31:01.581: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 29 20:31:03.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:05.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:07.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:09.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:11.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:13.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:15.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:17.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:19.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:21.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:23.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:25.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:31:27.784: INFO: Waited 127.44067ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/29/23 20:31:27.84
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/29/23 20:31:27.844
STEP: List APIServices 08/29/23 20:31:27.852
Aug 29 20:31:27.859: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Aug 29 20:31:28.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-2369" for this suite. 08/29/23 20:31:28.169
------------------------------
• [SLOW TEST] [27.290 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:31:00.947
    Aug 29 20:31:00.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename aggregator 08/29/23 20:31:00.948
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:00.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:00.974
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 29 20:31:00.977: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/29/23 20:31:00.978
    Aug 29 20:31:01.581: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Aug 29 20:31:03.635: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:05.653: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:07.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:09.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:11.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:13.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:15.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:17.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:19.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:21.641: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:23.640: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:25.642: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 31, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:31:27.784: INFO: Waited 127.44067ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/29/23 20:31:27.84
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/29/23 20:31:27.844
    STEP: List APIServices 08/29/23 20:31:27.852
    Aug 29 20:31:27.859: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:31:28.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-2369" for this suite. 08/29/23 20:31:28.169
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:31:28.238
Aug 29 20:31:28.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename init-container 08/29/23 20:31:28.24
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:28.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:28.281
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 08/29/23 20:31:28.285
Aug 29 20:31:28.286: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:31:33.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-585" for this suite. 08/29/23 20:31:33.008
------------------------------
• [4.779 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:31:28.238
    Aug 29 20:31:28.238: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename init-container 08/29/23 20:31:28.24
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:28.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:28.281
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 08/29/23 20:31:28.285
    Aug 29 20:31:28.286: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:31:33.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-585" for this suite. 08/29/23 20:31:33.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:31:33.017
Aug 29 20:31:33.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename disruption 08/29/23 20:31:33.018
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:33.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:33.04
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 08/29/23 20:31:33.043
STEP: Waiting for the pdb to be processed 08/29/23 20:31:33.048
STEP: First trying to evict a pod which shouldn't be evictable 08/29/23 20:31:35.061
STEP: Waiting for all pods to be running 08/29/23 20:31:35.061
Aug 29 20:31:35.065: INFO: pods: 0 < 3
STEP: locating a running pod 08/29/23 20:31:37.07
STEP: Updating the pdb to allow a pod to be evicted 08/29/23 20:31:37.082
STEP: Waiting for the pdb to be processed 08/29/23 20:31:37.092
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/29/23 20:31:39.1
STEP: Waiting for all pods to be running 08/29/23 20:31:39.1
STEP: Waiting for the pdb to observed all healthy pods 08/29/23 20:31:39.104
STEP: Patching the pdb to disallow a pod to be evicted 08/29/23 20:31:39.135
STEP: Waiting for the pdb to be processed 08/29/23 20:31:39.155
STEP: Waiting for all pods to be running 08/29/23 20:31:41.165
STEP: locating a running pod 08/29/23 20:31:41.169
STEP: Deleting the pdb to allow a pod to be evicted 08/29/23 20:31:41.18
STEP: Waiting for the pdb to be deleted 08/29/23 20:31:41.186
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/29/23 20:31:41.19
STEP: Waiting for all pods to be running 08/29/23 20:31:41.19
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:31:41.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-561" for this suite. 08/29/23 20:31:41.219
------------------------------
• [SLOW TEST] [8.211 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:31:33.017
    Aug 29 20:31:33.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename disruption 08/29/23 20:31:33.018
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:33.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:33.04
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 08/29/23 20:31:33.043
    STEP: Waiting for the pdb to be processed 08/29/23 20:31:33.048
    STEP: First trying to evict a pod which shouldn't be evictable 08/29/23 20:31:35.061
    STEP: Waiting for all pods to be running 08/29/23 20:31:35.061
    Aug 29 20:31:35.065: INFO: pods: 0 < 3
    STEP: locating a running pod 08/29/23 20:31:37.07
    STEP: Updating the pdb to allow a pod to be evicted 08/29/23 20:31:37.082
    STEP: Waiting for the pdb to be processed 08/29/23 20:31:37.092
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/29/23 20:31:39.1
    STEP: Waiting for all pods to be running 08/29/23 20:31:39.1
    STEP: Waiting for the pdb to observed all healthy pods 08/29/23 20:31:39.104
    STEP: Patching the pdb to disallow a pod to be evicted 08/29/23 20:31:39.135
    STEP: Waiting for the pdb to be processed 08/29/23 20:31:39.155
    STEP: Waiting for all pods to be running 08/29/23 20:31:41.165
    STEP: locating a running pod 08/29/23 20:31:41.169
    STEP: Deleting the pdb to allow a pod to be evicted 08/29/23 20:31:41.18
    STEP: Waiting for the pdb to be deleted 08/29/23 20:31:41.186
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/29/23 20:31:41.19
    STEP: Waiting for all pods to be running 08/29/23 20:31:41.19
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:31:41.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-561" for this suite. 08/29/23 20:31:41.219
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:31:41.229
Aug 29 20:31:41.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:31:41.23
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:41.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:41.267
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/29/23 20:31:41.27
Aug 29 20:31:41.281: INFO: Waiting up to 5m0s for pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1" in namespace "emptydir-3638" to be "Succeeded or Failed"
Aug 29 20:31:41.284: INFO: Pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899737ms
Aug 29 20:31:43.290: INFO: Pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009050016s
Aug 29 20:31:45.289: INFO: Pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008345985s
STEP: Saw pod success 08/29/23 20:31:45.289
Aug 29 20:31:45.290: INFO: Pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1" satisfied condition "Succeeded or Failed"
Aug 29 20:31:45.293: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1 container test-container: <nil>
STEP: delete the pod 08/29/23 20:31:45.316
Aug 29 20:31:45.334: INFO: Waiting for pod pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1 to disappear
Aug 29 20:31:45.338: INFO: Pod pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:31:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3638" for this suite. 08/29/23 20:31:45.342
------------------------------
• [4.121 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:31:41.229
    Aug 29 20:31:41.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:31:41.23
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:41.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:41.267
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/29/23 20:31:41.27
    Aug 29 20:31:41.281: INFO: Waiting up to 5m0s for pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1" in namespace "emptydir-3638" to be "Succeeded or Failed"
    Aug 29 20:31:41.284: INFO: Pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.899737ms
    Aug 29 20:31:43.290: INFO: Pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009050016s
    Aug 29 20:31:45.289: INFO: Pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008345985s
    STEP: Saw pod success 08/29/23 20:31:45.289
    Aug 29 20:31:45.290: INFO: Pod "pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1" satisfied condition "Succeeded or Failed"
    Aug 29 20:31:45.293: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1 container test-container: <nil>
    STEP: delete the pod 08/29/23 20:31:45.316
    Aug 29 20:31:45.334: INFO: Waiting for pod pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1 to disappear
    Aug 29 20:31:45.338: INFO: Pod pod-8527a4b2-c40b-4b36-b1c8-c36a91dca4a1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:31:45.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3638" for this suite. 08/29/23 20:31:45.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:31:45.35
Aug 29 20:31:45.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename disruption 08/29/23 20:31:45.351
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:45.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:45.376
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 08/29/23 20:31:45.379
STEP: Waiting for the pdb to be processed 08/29/23 20:31:45.385
STEP: updating the pdb 08/29/23 20:31:47.394
STEP: Waiting for the pdb to be processed 08/29/23 20:31:47.409
STEP: patching the pdb 08/29/23 20:31:49.42
STEP: Waiting for the pdb to be processed 08/29/23 20:31:49.433
STEP: Waiting for the pdb to be deleted 08/29/23 20:31:51.451
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:31:51.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4142" for this suite. 08/29/23 20:31:51.46
------------------------------
• [SLOW TEST] [6.118 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:31:45.35
    Aug 29 20:31:45.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename disruption 08/29/23 20:31:45.351
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:45.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:45.376
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 08/29/23 20:31:45.379
    STEP: Waiting for the pdb to be processed 08/29/23 20:31:45.385
    STEP: updating the pdb 08/29/23 20:31:47.394
    STEP: Waiting for the pdb to be processed 08/29/23 20:31:47.409
    STEP: patching the pdb 08/29/23 20:31:49.42
    STEP: Waiting for the pdb to be processed 08/29/23 20:31:49.433
    STEP: Waiting for the pdb to be deleted 08/29/23 20:31:51.451
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:31:51.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4142" for this suite. 08/29/23 20:31:51.46
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:31:51.469
Aug 29 20:31:51.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename namespaces 08/29/23 20:31:51.471
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:51.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:51.493
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-4295" 08/29/23 20:31:51.496
Aug 29 20:31:51.506: INFO: Namespace "namespaces-4295" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"359ac39d-857c-4768-9030-64ce090d54e9", "kubernetes.io/metadata.name":"namespaces-4295", "namespaces-4295":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:31:51.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-4295" for this suite. 08/29/23 20:31:51.512
------------------------------
• [0.050 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:31:51.469
    Aug 29 20:31:51.469: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename namespaces 08/29/23 20:31:51.471
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:51.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:51.493
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-4295" 08/29/23 20:31:51.496
    Aug 29 20:31:51.506: INFO: Namespace "namespaces-4295" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"359ac39d-857c-4768-9030-64ce090d54e9", "kubernetes.io/metadata.name":"namespaces-4295", "namespaces-4295":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:31:51.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-4295" for this suite. 08/29/23 20:31:51.512
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:31:51.519
Aug 29 20:31:51.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 20:31:51.521
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:51.546
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:51.55
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 08/29/23 20:31:51.553
Aug 29 20:31:51.563: INFO: Waiting up to 5m0s for pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141" in namespace "downward-api-722" to be "Succeeded or Failed"
Aug 29 20:31:51.567: INFO: Pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588499ms
Aug 29 20:31:53.573: INFO: Pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009247245s
Aug 29 20:31:55.573: INFO: Pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009262989s
STEP: Saw pod success 08/29/23 20:31:55.573
Aug 29 20:31:55.573: INFO: Pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141" satisfied condition "Succeeded or Failed"
Aug 29 20:31:55.576: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downward-api-944d67a7-027b-4f36-bc90-f2595502b141 container dapi-container: <nil>
STEP: delete the pod 08/29/23 20:31:55.586
Aug 29 20:31:55.602: INFO: Waiting for pod downward-api-944d67a7-027b-4f36-bc90-f2595502b141 to disappear
Aug 29 20:31:55.605: INFO: Pod downward-api-944d67a7-027b-4f36-bc90-f2595502b141 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Aug 29 20:31:55.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-722" for this suite. 08/29/23 20:31:55.61
------------------------------
• [4.098 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:31:51.519
    Aug 29 20:31:51.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 20:31:51.521
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:51.546
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:51.55
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 08/29/23 20:31:51.553
    Aug 29 20:31:51.563: INFO: Waiting up to 5m0s for pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141" in namespace "downward-api-722" to be "Succeeded or Failed"
    Aug 29 20:31:51.567: INFO: Pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588499ms
    Aug 29 20:31:53.573: INFO: Pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009247245s
    Aug 29 20:31:55.573: INFO: Pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009262989s
    STEP: Saw pod success 08/29/23 20:31:55.573
    Aug 29 20:31:55.573: INFO: Pod "downward-api-944d67a7-027b-4f36-bc90-f2595502b141" satisfied condition "Succeeded or Failed"
    Aug 29 20:31:55.576: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downward-api-944d67a7-027b-4f36-bc90-f2595502b141 container dapi-container: <nil>
    STEP: delete the pod 08/29/23 20:31:55.586
    Aug 29 20:31:55.602: INFO: Waiting for pod downward-api-944d67a7-027b-4f36-bc90-f2595502b141 to disappear
    Aug 29 20:31:55.605: INFO: Pod downward-api-944d67a7-027b-4f36-bc90-f2595502b141 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:31:55.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-722" for this suite. 08/29/23 20:31:55.61
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:31:55.617
Aug 29 20:31:55.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:31:55.619
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:55.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:55.646
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Aug 29 20:31:55.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/29/23 20:31:57.939
Aug 29 20:31:57.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 create -f -'
Aug 29 20:31:58.947: INFO: stderr: ""
Aug 29 20:31:58.947: INFO: stdout: "e2e-test-crd-publish-openapi-4518-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 29 20:31:58.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 delete e2e-test-crd-publish-openapi-4518-crds test-foo'
Aug 29 20:31:59.055: INFO: stderr: ""
Aug 29 20:31:59.055: INFO: stdout: "e2e-test-crd-publish-openapi-4518-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 29 20:31:59.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 apply -f -'
Aug 29 20:31:59.375: INFO: stderr: ""
Aug 29 20:31:59.375: INFO: stdout: "e2e-test-crd-publish-openapi-4518-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 29 20:31:59.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 delete e2e-test-crd-publish-openapi-4518-crds test-foo'
Aug 29 20:31:59.480: INFO: stderr: ""
Aug 29 20:31:59.480: INFO: stdout: "e2e-test-crd-publish-openapi-4518-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/29/23 20:31:59.48
Aug 29 20:31:59.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 create -f -'
Aug 29 20:32:00.311: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/29/23 20:32:00.311
Aug 29 20:32:00.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 create -f -'
Aug 29 20:32:00.664: INFO: rc: 1
Aug 29 20:32:00.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 apply -f -'
Aug 29 20:32:01.036: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/29/23 20:32:01.036
Aug 29 20:32:01.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 create -f -'
Aug 29 20:32:01.386: INFO: rc: 1
Aug 29 20:32:01.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 apply -f -'
Aug 29 20:32:01.768: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/29/23 20:32:01.768
Aug 29 20:32:01.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds'
Aug 29 20:32:02.111: INFO: stderr: ""
Aug 29 20:32:02.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4518-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/29/23 20:32:02.111
Aug 29 20:32:02.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds.metadata'
Aug 29 20:32:02.469: INFO: stderr: ""
Aug 29 20:32:02.469: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4518-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 29 20:32:02.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds.spec'
Aug 29 20:32:02.812: INFO: stderr: ""
Aug 29 20:32:02.812: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4518-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 29 20:32:02.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds.spec.bars'
Aug 29 20:32:03.157: INFO: stderr: ""
Aug 29 20:32:03.157: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4518-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/29/23 20:32:03.157
Aug 29 20:32:03.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds.spec.bars2'
Aug 29 20:32:03.458: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:32:06.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2121" for this suite. 08/29/23 20:32:06.146
------------------------------
• [SLOW TEST] [10.537 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:31:55.617
    Aug 29 20:31:55.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:31:55.619
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:31:55.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:31:55.646
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Aug 29 20:31:55.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/29/23 20:31:57.939
    Aug 29 20:31:57.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 create -f -'
    Aug 29 20:31:58.947: INFO: stderr: ""
    Aug 29 20:31:58.947: INFO: stdout: "e2e-test-crd-publish-openapi-4518-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 29 20:31:58.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 delete e2e-test-crd-publish-openapi-4518-crds test-foo'
    Aug 29 20:31:59.055: INFO: stderr: ""
    Aug 29 20:31:59.055: INFO: stdout: "e2e-test-crd-publish-openapi-4518-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 29 20:31:59.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 apply -f -'
    Aug 29 20:31:59.375: INFO: stderr: ""
    Aug 29 20:31:59.375: INFO: stdout: "e2e-test-crd-publish-openapi-4518-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 29 20:31:59.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 delete e2e-test-crd-publish-openapi-4518-crds test-foo'
    Aug 29 20:31:59.480: INFO: stderr: ""
    Aug 29 20:31:59.480: INFO: stdout: "e2e-test-crd-publish-openapi-4518-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/29/23 20:31:59.48
    Aug 29 20:31:59.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 create -f -'
    Aug 29 20:32:00.311: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/29/23 20:32:00.311
    Aug 29 20:32:00.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 create -f -'
    Aug 29 20:32:00.664: INFO: rc: 1
    Aug 29 20:32:00.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 apply -f -'
    Aug 29 20:32:01.036: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/29/23 20:32:01.036
    Aug 29 20:32:01.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 create -f -'
    Aug 29 20:32:01.386: INFO: rc: 1
    Aug 29 20:32:01.386: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 --namespace=crd-publish-openapi-2121 apply -f -'
    Aug 29 20:32:01.768: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/29/23 20:32:01.768
    Aug 29 20:32:01.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds'
    Aug 29 20:32:02.111: INFO: stderr: ""
    Aug 29 20:32:02.111: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4518-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/29/23 20:32:02.111
    Aug 29 20:32:02.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds.metadata'
    Aug 29 20:32:02.469: INFO: stderr: ""
    Aug 29 20:32:02.469: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4518-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 29 20:32:02.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds.spec'
    Aug 29 20:32:02.812: INFO: stderr: ""
    Aug 29 20:32:02.812: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4518-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 29 20:32:02.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds.spec.bars'
    Aug 29 20:32:03.157: INFO: stderr: ""
    Aug 29 20:32:03.157: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4518-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/29/23 20:32:03.157
    Aug 29 20:32:03.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=crd-publish-openapi-2121 explain e2e-test-crd-publish-openapi-4518-crds.spec.bars2'
    Aug 29 20:32:03.458: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:32:06.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2121" for this suite. 08/29/23 20:32:06.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:32:06.155
Aug 29 20:32:06.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename var-expansion 08/29/23 20:32:06.156
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:06.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:06.176
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Aug 29 20:32:06.190: INFO: Waiting up to 2m0s for pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487" in namespace "var-expansion-3161" to be "container 0 failed with reason CreateContainerConfigError"
Aug 29 20:32:06.193: INFO: Pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648696ms
Aug 29 20:32:08.199: INFO: Pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009496241s
Aug 29 20:32:08.199: INFO: Pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 29 20:32:08.199: INFO: Deleting pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487" in namespace "var-expansion-3161"
Aug 29 20:32:08.209: INFO: Wait up to 5m0s for pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 29 20:32:10.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-3161" for this suite. 08/29/23 20:32:10.222
------------------------------
• [4.075 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:32:06.155
    Aug 29 20:32:06.155: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename var-expansion 08/29/23 20:32:06.156
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:06.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:06.176
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Aug 29 20:32:06.190: INFO: Waiting up to 2m0s for pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487" in namespace "var-expansion-3161" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 29 20:32:06.193: INFO: Pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487": Phase="Pending", Reason="", readiness=false. Elapsed: 3.648696ms
    Aug 29 20:32:08.199: INFO: Pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009496241s
    Aug 29 20:32:08.199: INFO: Pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 29 20:32:08.199: INFO: Deleting pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487" in namespace "var-expansion-3161"
    Aug 29 20:32:08.209: INFO: Wait up to 5m0s for pod "var-expansion-014815e8-552b-496e-9d6a-9c1517d6f487" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:32:10.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-3161" for this suite. 08/29/23 20:32:10.222
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:32:10.232
Aug 29 20:32:10.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-probe 08/29/23 20:32:10.233
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:10.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:10.257
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff in namespace container-probe-1177 08/29/23 20:32:10.262
Aug 29 20:32:10.275: INFO: Waiting up to 5m0s for pod "liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff" in namespace "container-probe-1177" to be "not pending"
Aug 29 20:32:10.278: INFO: Pod "liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831186ms
Aug 29 20:32:12.282: INFO: Pod "liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.006693302s
Aug 29 20:32:12.282: INFO: Pod "liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff" satisfied condition "not pending"
Aug 29 20:32:12.282: INFO: Started pod liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff in namespace container-probe-1177
STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 20:32:12.282
Aug 29 20:32:12.284: INFO: Initial restart count of pod liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff is 0
Aug 29 20:32:32.341: INFO: Restart count of pod container-probe-1177/liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff is now 1 (20.056984561s elapsed)
STEP: deleting the pod 08/29/23 20:32:32.341
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 29 20:32:32.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1177" for this suite. 08/29/23 20:32:32.367
------------------------------
• [SLOW TEST] [22.145 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:32:10.232
    Aug 29 20:32:10.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-probe 08/29/23 20:32:10.233
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:10.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:10.257
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff in namespace container-probe-1177 08/29/23 20:32:10.262
    Aug 29 20:32:10.275: INFO: Waiting up to 5m0s for pod "liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff" in namespace "container-probe-1177" to be "not pending"
    Aug 29 20:32:10.278: INFO: Pod "liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.831186ms
    Aug 29 20:32:12.282: INFO: Pod "liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.006693302s
    Aug 29 20:32:12.282: INFO: Pod "liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff" satisfied condition "not pending"
    Aug 29 20:32:12.282: INFO: Started pod liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff in namespace container-probe-1177
    STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 20:32:12.282
    Aug 29 20:32:12.284: INFO: Initial restart count of pod liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff is 0
    Aug 29 20:32:32.341: INFO: Restart count of pod container-probe-1177/liveness-aac2a5be-b799-4d62-a54e-3c3df7b921ff is now 1 (20.056984561s elapsed)
    STEP: deleting the pod 08/29/23 20:32:32.341
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:32:32.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1177" for this suite. 08/29/23 20:32:32.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:32:32.377
Aug 29 20:32:32.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:32:32.378
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:32.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:32.406
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-37b32ec4-75f9-4297-b18c-b83a26894f36 08/29/23 20:32:32.417
STEP: Creating the pod 08/29/23 20:32:32.424
Aug 29 20:32:32.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61" in namespace "configmap-7043" to be "running and ready"
Aug 29 20:32:32.437: INFO: Pod "pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.153139ms
Aug 29 20:32:32.437: INFO: The phase of Pod pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:32:34.442: INFO: Pod "pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61": Phase="Running", Reason="", readiness=true. Elapsed: 2.008735069s
Aug 29 20:32:34.442: INFO: The phase of Pod pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61 is Running (Ready = true)
Aug 29 20:32:34.442: INFO: Pod "pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-37b32ec4-75f9-4297-b18c-b83a26894f36 08/29/23 20:32:34.468
STEP: waiting to observe update in volume 08/29/23 20:32:34.474
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:32:36.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7043" for this suite. 08/29/23 20:32:36.497
------------------------------
• [4.127 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:32:32.377
    Aug 29 20:32:32.377: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:32:32.378
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:32.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:32.406
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-37b32ec4-75f9-4297-b18c-b83a26894f36 08/29/23 20:32:32.417
    STEP: Creating the pod 08/29/23 20:32:32.424
    Aug 29 20:32:32.433: INFO: Waiting up to 5m0s for pod "pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61" in namespace "configmap-7043" to be "running and ready"
    Aug 29 20:32:32.437: INFO: Pod "pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61": Phase="Pending", Reason="", readiness=false. Elapsed: 4.153139ms
    Aug 29 20:32:32.437: INFO: The phase of Pod pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:32:34.442: INFO: Pod "pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61": Phase="Running", Reason="", readiness=true. Elapsed: 2.008735069s
    Aug 29 20:32:34.442: INFO: The phase of Pod pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61 is Running (Ready = true)
    Aug 29 20:32:34.442: INFO: Pod "pod-configmaps-131b08ab-2123-4943-bf49-1dabce12cc61" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-37b32ec4-75f9-4297-b18c-b83a26894f36 08/29/23 20:32:34.468
    STEP: waiting to observe update in volume 08/29/23 20:32:34.474
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:32:36.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7043" for this suite. 08/29/23 20:32:36.497
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:32:36.505
Aug 29 20:32:36.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubelet-test 08/29/23 20:32:36.507
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:36.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:36.528
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 29 20:32:36.539: INFO: Waiting up to 5m0s for pod "busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56" in namespace "kubelet-test-8569" to be "running and ready"
Aug 29 20:32:36.542: INFO: Pod "busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56": Phase="Pending", Reason="", readiness=false. Elapsed: 3.611811ms
Aug 29 20:32:36.542: INFO: The phase of Pod busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:32:38.549: INFO: Pod "busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56": Phase="Running", Reason="", readiness=true. Elapsed: 2.009910177s
Aug 29 20:32:38.549: INFO: The phase of Pod busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56 is Running (Ready = true)
Aug 29 20:32:38.549: INFO: Pod "busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:32:38.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8569" for this suite. 08/29/23 20:32:38.585
------------------------------
• [2.088 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:32:36.505
    Aug 29 20:32:36.505: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubelet-test 08/29/23 20:32:36.507
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:36.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:36.528
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 29 20:32:36.539: INFO: Waiting up to 5m0s for pod "busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56" in namespace "kubelet-test-8569" to be "running and ready"
    Aug 29 20:32:36.542: INFO: Pod "busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56": Phase="Pending", Reason="", readiness=false. Elapsed: 3.611811ms
    Aug 29 20:32:36.542: INFO: The phase of Pod busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:32:38.549: INFO: Pod "busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56": Phase="Running", Reason="", readiness=true. Elapsed: 2.009910177s
    Aug 29 20:32:38.549: INFO: The phase of Pod busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56 is Running (Ready = true)
    Aug 29 20:32:38.549: INFO: Pod "busybox-scheduling-57fc7e34-7a2e-470c-96b2-434155cbff56" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:32:38.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8569" for this suite. 08/29/23 20:32:38.585
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:32:38.594
Aug 29 20:32:38.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:32:38.596
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:38.614
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:38.618
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:32:38.635
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:32:39.014
STEP: Deploying the webhook pod 08/29/23 20:32:39.024
STEP: Wait for the deployment to be ready 08/29/23 20:32:39.039
Aug 29 20:32:39.050: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 20:32:41.063
STEP: Verifying the service has paired with the endpoint 08/29/23 20:32:41.076
Aug 29 20:32:42.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 08/29/23 20:32:42.082
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/29/23 20:32:42.084
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/29/23 20:32:42.084
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/29/23 20:32:42.084
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/29/23 20:32:42.086
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/29/23 20:32:42.086
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/29/23 20:32:42.087
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:32:42.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-900" for this suite. 08/29/23 20:32:42.144
STEP: Destroying namespace "webhook-900-markers" for this suite. 08/29/23 20:32:42.154
------------------------------
• [3.568 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:32:38.594
    Aug 29 20:32:38.594: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:32:38.596
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:38.614
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:38.618
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:32:38.635
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:32:39.014
    STEP: Deploying the webhook pod 08/29/23 20:32:39.024
    STEP: Wait for the deployment to be ready 08/29/23 20:32:39.039
    Aug 29 20:32:39.050: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 20:32:41.063
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:32:41.076
    Aug 29 20:32:42.076: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 08/29/23 20:32:42.082
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/29/23 20:32:42.084
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/29/23 20:32:42.084
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/29/23 20:32:42.084
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/29/23 20:32:42.086
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/29/23 20:32:42.086
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/29/23 20:32:42.087
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:32:42.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-900" for this suite. 08/29/23 20:32:42.144
    STEP: Destroying namespace "webhook-900-markers" for this suite. 08/29/23 20:32:42.154
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:32:42.163
Aug 29 20:32:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename lease-test 08/29/23 20:32:42.164
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:42.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:42.184
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Aug 29 20:32:42.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-9908" for this suite. 08/29/23 20:32:42.255
------------------------------
• [0.100 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:32:42.163
    Aug 29 20:32:42.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename lease-test 08/29/23 20:32:42.164
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:42.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:42.184
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:32:42.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-9908" for this suite. 08/29/23 20:32:42.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:32:42.264
Aug 29 20:32:42.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename job 08/29/23 20:32:42.265
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:42.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:42.29
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 08/29/23 20:32:42.292
STEP: Ensuring job reaches completions 08/29/23 20:32:42.299
STEP: Ensuring pods with index for job exist 08/29/23 20:32:52.303
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Aug 29 20:32:52.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-7325" for this suite. 08/29/23 20:32:52.314
------------------------------
• [SLOW TEST] [10.060 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:32:42.264
    Aug 29 20:32:42.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename job 08/29/23 20:32:42.265
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:42.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:42.29
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 08/29/23 20:32:42.292
    STEP: Ensuring job reaches completions 08/29/23 20:32:42.299
    STEP: Ensuring pods with index for job exist 08/29/23 20:32:52.303
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:32:52.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-7325" for this suite. 08/29/23 20:32:52.314
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:32:52.327
Aug 29 20:32:52.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 20:32:52.328
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:52.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:52.347
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 08/29/23 20:32:52.398
STEP: Creating a ResourceQuota 08/29/23 20:32:57.402
STEP: Ensuring resource quota status is calculated 08/29/23 20:32:57.41
STEP: Creating a Pod that fits quota 08/29/23 20:32:59.415
STEP: Ensuring ResourceQuota status captures the pod usage 08/29/23 20:32:59.432
STEP: Not allowing a pod to be created that exceeds remaining quota 08/29/23 20:33:01.439
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/29/23 20:33:01.442
STEP: Ensuring a pod cannot update its resource requirements 08/29/23 20:33:01.444
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/29/23 20:33:01.449
STEP: Deleting the pod 08/29/23 20:33:03.455
STEP: Ensuring resource quota status released the pod usage 08/29/23 20:33:03.474
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 20:33:05.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2325" for this suite. 08/29/23 20:33:05.483
------------------------------
• [SLOW TEST] [13.166 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:32:52.327
    Aug 29 20:32:52.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 20:32:52.328
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:32:52.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:32:52.347
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 08/29/23 20:32:52.398
    STEP: Creating a ResourceQuota 08/29/23 20:32:57.402
    STEP: Ensuring resource quota status is calculated 08/29/23 20:32:57.41
    STEP: Creating a Pod that fits quota 08/29/23 20:32:59.415
    STEP: Ensuring ResourceQuota status captures the pod usage 08/29/23 20:32:59.432
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/29/23 20:33:01.439
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/29/23 20:33:01.442
    STEP: Ensuring a pod cannot update its resource requirements 08/29/23 20:33:01.444
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/29/23 20:33:01.449
    STEP: Deleting the pod 08/29/23 20:33:03.455
    STEP: Ensuring resource quota status released the pod usage 08/29/23 20:33:03.474
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:33:05.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2325" for this suite. 08/29/23 20:33:05.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:33:05.493
Aug 29 20:33:05.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:33:05.494
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:05.527
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:05.53
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-dc57ef36-c7f8-4d64-8e8c-9f4aad9c8db4 08/29/23 20:33:05.532
STEP: Creating a pod to test consume configMaps 08/29/23 20:33:05.538
Aug 29 20:33:05.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2" in namespace "projected-705" to be "Succeeded or Failed"
Aug 29 20:33:05.551: INFO: Pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.930111ms
Aug 29 20:33:07.556: INFO: Pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008068326s
Aug 29 20:33:09.555: INFO: Pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007012715s
STEP: Saw pod success 08/29/23 20:33:09.555
Aug 29 20:33:09.555: INFO: Pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2" satisfied condition "Succeeded or Failed"
Aug 29 20:33:09.559: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2 container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:33:09.568
Aug 29 20:33:09.587: INFO: Waiting for pod pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2 to disappear
Aug 29 20:33:09.590: INFO: Pod pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:33:09.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-705" for this suite. 08/29/23 20:33:09.595
------------------------------
• [4.111 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:33:05.493
    Aug 29 20:33:05.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:33:05.494
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:05.527
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:05.53
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-dc57ef36-c7f8-4d64-8e8c-9f4aad9c8db4 08/29/23 20:33:05.532
    STEP: Creating a pod to test consume configMaps 08/29/23 20:33:05.538
    Aug 29 20:33:05.548: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2" in namespace "projected-705" to be "Succeeded or Failed"
    Aug 29 20:33:05.551: INFO: Pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.930111ms
    Aug 29 20:33:07.556: INFO: Pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008068326s
    Aug 29 20:33:09.555: INFO: Pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007012715s
    STEP: Saw pod success 08/29/23 20:33:09.555
    Aug 29 20:33:09.555: INFO: Pod "pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2" satisfied condition "Succeeded or Failed"
    Aug 29 20:33:09.559: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2 container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:33:09.568
    Aug 29 20:33:09.587: INFO: Waiting for pod pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2 to disappear
    Aug 29 20:33:09.590: INFO: Pod pod-projected-configmaps-78c54704-8dc6-4fa0-900b-fe3a117d05e2 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:33:09.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-705" for this suite. 08/29/23 20:33:09.595
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:33:09.605
Aug 29 20:33:09.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename var-expansion 08/29/23 20:33:09.606
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:09.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:09.628
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 08/29/23 20:33:09.631
Aug 29 20:33:09.642: INFO: Waiting up to 5m0s for pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769" in namespace "var-expansion-1484" to be "Succeeded or Failed"
Aug 29 20:33:09.646: INFO: Pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408771ms
Aug 29 20:33:11.650: INFO: Pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008124251s
Aug 29 20:33:13.651: INFO: Pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0085217s
STEP: Saw pod success 08/29/23 20:33:13.651
Aug 29 20:33:13.651: INFO: Pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769" satisfied condition "Succeeded or Failed"
Aug 29 20:33:13.655: INFO: Trying to get logs from node loki-15bd39-worker-1 pod var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769 container dapi-container: <nil>
STEP: delete the pod 08/29/23 20:33:13.662
Aug 29 20:33:13.673: INFO: Waiting for pod var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769 to disappear
Aug 29 20:33:13.676: INFO: Pod var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 29 20:33:13.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1484" for this suite. 08/29/23 20:33:13.681
------------------------------
• [4.083 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:33:09.605
    Aug 29 20:33:09.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename var-expansion 08/29/23 20:33:09.606
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:09.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:09.628
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 08/29/23 20:33:09.631
    Aug 29 20:33:09.642: INFO: Waiting up to 5m0s for pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769" in namespace "var-expansion-1484" to be "Succeeded or Failed"
    Aug 29 20:33:09.646: INFO: Pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769": Phase="Pending", Reason="", readiness=false. Elapsed: 3.408771ms
    Aug 29 20:33:11.650: INFO: Pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008124251s
    Aug 29 20:33:13.651: INFO: Pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0085217s
    STEP: Saw pod success 08/29/23 20:33:13.651
    Aug 29 20:33:13.651: INFO: Pod "var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769" satisfied condition "Succeeded or Failed"
    Aug 29 20:33:13.655: INFO: Trying to get logs from node loki-15bd39-worker-1 pod var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769 container dapi-container: <nil>
    STEP: delete the pod 08/29/23 20:33:13.662
    Aug 29 20:33:13.673: INFO: Waiting for pod var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769 to disappear
    Aug 29 20:33:13.676: INFO: Pod var-expansion-69c348a6-c4d0-47b9-be79-ab561c5db769 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:33:13.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1484" for this suite. 08/29/23 20:33:13.681
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:33:13.688
Aug 29 20:33:13.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename csiinlinevolumes 08/29/23 20:33:13.689
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:13.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:13.711
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 08/29/23 20:33:13.714
STEP: getting 08/29/23 20:33:13.735
STEP: listing 08/29/23 20:33:13.741
STEP: deleting 08/29/23 20:33:13.744
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:33:13.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2107" for this suite. 08/29/23 20:33:13.768
------------------------------
• [0.088 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:33:13.688
    Aug 29 20:33:13.688: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename csiinlinevolumes 08/29/23 20:33:13.689
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:13.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:13.711
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 08/29/23 20:33:13.714
    STEP: getting 08/29/23 20:33:13.735
    STEP: listing 08/29/23 20:33:13.741
    STEP: deleting 08/29/23 20:33:13.744
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:33:13.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2107" for this suite. 08/29/23 20:33:13.768
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:33:13.776
Aug 29 20:33:13.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:33:13.777
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:13.793
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:13.796
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:33:13.813
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:33:14.579
STEP: Deploying the webhook pod 08/29/23 20:33:14.588
STEP: Wait for the deployment to be ready 08/29/23 20:33:14.605
Aug 29 20:33:14.617: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/29/23 20:33:16.63
STEP: Verifying the service has paired with the endpoint 08/29/23 20:33:16.646
Aug 29 20:33:17.647: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/29/23 20:33:17.651
STEP: Registering slow webhook via the AdmissionRegistration API 08/29/23 20:33:17.651
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/29/23 20:33:17.67
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/29/23 20:33:18.683
STEP: Registering slow webhook via the AdmissionRegistration API 08/29/23 20:33:18.683
STEP: Having no error when timeout is longer than webhook latency 08/29/23 20:33:19.718
STEP: Registering slow webhook via the AdmissionRegistration API 08/29/23 20:33:19.718
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/29/23 20:33:24.763
STEP: Registering slow webhook via the AdmissionRegistration API 08/29/23 20:33:24.763
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:33:29.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6981" for this suite. 08/29/23 20:33:29.871
STEP: Destroying namespace "webhook-6981-markers" for this suite. 08/29/23 20:33:29.878
------------------------------
• [SLOW TEST] [16.113 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:33:13.776
    Aug 29 20:33:13.776: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:33:13.777
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:13.793
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:13.796
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:33:13.813
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:33:14.579
    STEP: Deploying the webhook pod 08/29/23 20:33:14.588
    STEP: Wait for the deployment to be ready 08/29/23 20:33:14.605
    Aug 29 20:33:14.617: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/29/23 20:33:16.63
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:33:16.646
    Aug 29 20:33:17.647: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/29/23 20:33:17.651
    STEP: Registering slow webhook via the AdmissionRegistration API 08/29/23 20:33:17.651
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/29/23 20:33:17.67
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/29/23 20:33:18.683
    STEP: Registering slow webhook via the AdmissionRegistration API 08/29/23 20:33:18.683
    STEP: Having no error when timeout is longer than webhook latency 08/29/23 20:33:19.718
    STEP: Registering slow webhook via the AdmissionRegistration API 08/29/23 20:33:19.718
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/29/23 20:33:24.763
    STEP: Registering slow webhook via the AdmissionRegistration API 08/29/23 20:33:24.763
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:33:29.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6981" for this suite. 08/29/23 20:33:29.871
    STEP: Destroying namespace "webhook-6981-markers" for this suite. 08/29/23 20:33:29.878
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:33:29.892
Aug 29 20:33:29.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 20:33:29.894
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:29.913
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:29.917
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 08/29/23 20:33:46.924
STEP: Creating a ResourceQuota 08/29/23 20:33:51.928
STEP: Ensuring resource quota status is calculated 08/29/23 20:33:51.938
STEP: Creating a ConfigMap 08/29/23 20:33:53.943
STEP: Ensuring resource quota status captures configMap creation 08/29/23 20:33:53.961
STEP: Deleting a ConfigMap 08/29/23 20:33:55.967
STEP: Ensuring resource quota status released usage 08/29/23 20:33:55.974
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 20:33:57.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-685" for this suite. 08/29/23 20:33:57.985
------------------------------
• [SLOW TEST] [28.100 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:33:29.892
    Aug 29 20:33:29.892: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 20:33:29.894
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:29.913
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:29.917
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 08/29/23 20:33:46.924
    STEP: Creating a ResourceQuota 08/29/23 20:33:51.928
    STEP: Ensuring resource quota status is calculated 08/29/23 20:33:51.938
    STEP: Creating a ConfigMap 08/29/23 20:33:53.943
    STEP: Ensuring resource quota status captures configMap creation 08/29/23 20:33:53.961
    STEP: Deleting a ConfigMap 08/29/23 20:33:55.967
    STEP: Ensuring resource quota status released usage 08/29/23 20:33:55.974
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:33:57.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-685" for this suite. 08/29/23 20:33:57.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:33:58.003
Aug 29 20:33:58.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:33:58.004
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:58.023
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:58.026
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/29/23 20:33:58.03
Aug 29 20:33:58.040: INFO: Waiting up to 5m0s for pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804" in namespace "emptydir-3193" to be "Succeeded or Failed"
Aug 29 20:33:58.045: INFO: Pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804": Phase="Pending", Reason="", readiness=false. Elapsed: 4.531378ms
Aug 29 20:34:00.050: INFO: Pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009908652s
Aug 29 20:34:02.051: INFO: Pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010521678s
STEP: Saw pod success 08/29/23 20:34:02.051
Aug 29 20:34:02.051: INFO: Pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804" satisfied condition "Succeeded or Failed"
Aug 29 20:34:02.055: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804 container test-container: <nil>
STEP: delete the pod 08/29/23 20:34:02.064
Aug 29 20:34:02.079: INFO: Waiting for pod pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804 to disappear
Aug 29 20:34:02.083: INFO: Pod pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:34:02.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3193" for this suite. 08/29/23 20:34:02.089
------------------------------
• [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:33:58.003
    Aug 29 20:33:58.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:33:58.004
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:33:58.023
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:33:58.026
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/29/23 20:33:58.03
    Aug 29 20:33:58.040: INFO: Waiting up to 5m0s for pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804" in namespace "emptydir-3193" to be "Succeeded or Failed"
    Aug 29 20:33:58.045: INFO: Pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804": Phase="Pending", Reason="", readiness=false. Elapsed: 4.531378ms
    Aug 29 20:34:00.050: INFO: Pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009908652s
    Aug 29 20:34:02.051: INFO: Pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010521678s
    STEP: Saw pod success 08/29/23 20:34:02.051
    Aug 29 20:34:02.051: INFO: Pod "pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804" satisfied condition "Succeeded or Failed"
    Aug 29 20:34:02.055: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804 container test-container: <nil>
    STEP: delete the pod 08/29/23 20:34:02.064
    Aug 29 20:34:02.079: INFO: Waiting for pod pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804 to disappear
    Aug 29 20:34:02.083: INFO: Pod pod-dd12ba3c-2df4-49cf-8a6d-52dee732d804 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:34:02.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3193" for this suite. 08/29/23 20:34:02.089
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:34:02.098
Aug 29 20:34:02.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename prestop 08/29/23 20:34:02.099
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:02.118
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:02.122
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-1999 08/29/23 20:34:02.126
STEP: Waiting for pods to come up. 08/29/23 20:34:02.14
Aug 29 20:34:02.141: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1999" to be "running"
Aug 29 20:34:02.146: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.160689ms
Aug 29 20:34:04.149: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008820252s
Aug 29 20:34:06.150: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.009868699s
Aug 29 20:34:06.151: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-1999 08/29/23 20:34:06.154
Aug 29 20:34:06.162: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1999" to be "running"
Aug 29 20:34:06.167: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.401787ms
Aug 29 20:34:08.173: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.010996093s
Aug 29 20:34:08.173: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/29/23 20:34:08.173
Aug 29 20:34:13.191: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/29/23 20:34:13.191
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Aug 29 20:34:13.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-1999" for this suite. 08/29/23 20:34:13.217
------------------------------
• [SLOW TEST] [11.128 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:34:02.098
    Aug 29 20:34:02.098: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename prestop 08/29/23 20:34:02.099
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:02.118
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:02.122
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-1999 08/29/23 20:34:02.126
    STEP: Waiting for pods to come up. 08/29/23 20:34:02.14
    Aug 29 20:34:02.141: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-1999" to be "running"
    Aug 29 20:34:02.146: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 5.160689ms
    Aug 29 20:34:04.149: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008820252s
    Aug 29 20:34:06.150: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.009868699s
    Aug 29 20:34:06.151: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-1999 08/29/23 20:34:06.154
    Aug 29 20:34:06.162: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-1999" to be "running"
    Aug 29 20:34:06.167: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 5.401787ms
    Aug 29 20:34:08.173: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.010996093s
    Aug 29 20:34:08.173: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/29/23 20:34:08.173
    Aug 29 20:34:13.191: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/29/23 20:34:13.191
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:34:13.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-1999" for this suite. 08/29/23 20:34:13.217
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:34:13.226
Aug 29 20:34:13.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename dns 08/29/23 20:34:13.227
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:13.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:13.247
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/29/23 20:34:13.25
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/29/23 20:34:13.25
STEP: creating a pod to probe DNS 08/29/23 20:34:13.25
STEP: submitting the pod to kubernetes 08/29/23 20:34:13.25
Aug 29 20:34:13.264: INFO: Waiting up to 15m0s for pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3" in namespace "dns-9366" to be "running"
Aug 29 20:34:13.268: INFO: Pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663998ms
Aug 29 20:34:15.273: INFO: Pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008920653s
Aug 29 20:34:17.273: INFO: Pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008968405s
Aug 29 20:34:17.273: INFO: Pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3" satisfied condition "running"
STEP: retrieving the pod 08/29/23 20:34:17.273
STEP: looking for the results for each expected name from probers 08/29/23 20:34:17.277
Aug 29 20:34:17.293: INFO: DNS probes using dns-9366/dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3 succeeded

STEP: deleting the pod 08/29/23 20:34:17.293
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Aug 29 20:34:17.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9366" for this suite. 08/29/23 20:34:17.313
------------------------------
• [4.094 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:34:13.226
    Aug 29 20:34:13.226: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename dns 08/29/23 20:34:13.227
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:13.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:13.247
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/29/23 20:34:13.25
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/29/23 20:34:13.25
    STEP: creating a pod to probe DNS 08/29/23 20:34:13.25
    STEP: submitting the pod to kubernetes 08/29/23 20:34:13.25
    Aug 29 20:34:13.264: INFO: Waiting up to 15m0s for pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3" in namespace "dns-9366" to be "running"
    Aug 29 20:34:13.268: INFO: Pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.663998ms
    Aug 29 20:34:15.273: INFO: Pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008920653s
    Aug 29 20:34:17.273: INFO: Pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3": Phase="Running", Reason="", readiness=true. Elapsed: 4.008968405s
    Aug 29 20:34:17.273: INFO: Pod "dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3" satisfied condition "running"
    STEP: retrieving the pod 08/29/23 20:34:17.273
    STEP: looking for the results for each expected name from probers 08/29/23 20:34:17.277
    Aug 29 20:34:17.293: INFO: DNS probes using dns-9366/dns-test-eb44ed21-0963-454f-9f96-3410fdf2f1a3 succeeded

    STEP: deleting the pod 08/29/23 20:34:17.293
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:34:17.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9366" for this suite. 08/29/23 20:34:17.313
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:34:17.321
Aug 29 20:34:17.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:34:17.322
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:17.34
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:17.344
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/29/23 20:34:17.347
Aug 29 20:34:17.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/29/23 20:34:25.213
Aug 29 20:34:25.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:34:27.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:34:36.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9211" for this suite. 08/29/23 20:34:36.072
------------------------------
• [SLOW TEST] [18.761 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:34:17.321
    Aug 29 20:34:17.321: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:34:17.322
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:17.34
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:17.344
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/29/23 20:34:17.347
    Aug 29 20:34:17.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/29/23 20:34:25.213
    Aug 29 20:34:25.214: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:34:27.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:34:36.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9211" for this suite. 08/29/23 20:34:36.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:34:36.085
Aug 29 20:34:36.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:34:36.086
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:36.105
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:36.108
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:34:36.111
Aug 29 20:34:36.122: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4" in namespace "projected-4982" to be "Succeeded or Failed"
Aug 29 20:34:36.127: INFO: Pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.648524ms
Aug 29 20:34:38.132: INFO: Pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349335s
Aug 29 20:34:40.133: INFO: Pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010989884s
STEP: Saw pod success 08/29/23 20:34:40.133
Aug 29 20:34:40.133: INFO: Pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4" satisfied condition "Succeeded or Failed"
Aug 29 20:34:40.138: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4 container client-container: <nil>
STEP: delete the pod 08/29/23 20:34:40.15
Aug 29 20:34:40.168: INFO: Waiting for pod downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4 to disappear
Aug 29 20:34:40.174: INFO: Pod downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 20:34:40.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4982" for this suite. 08/29/23 20:34:40.18
------------------------------
• [4.105 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:34:36.085
    Aug 29 20:34:36.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:34:36.086
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:36.105
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:36.108
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:34:36.111
    Aug 29 20:34:36.122: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4" in namespace "projected-4982" to be "Succeeded or Failed"
    Aug 29 20:34:36.127: INFO: Pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.648524ms
    Aug 29 20:34:38.132: INFO: Pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010349335s
    Aug 29 20:34:40.133: INFO: Pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010989884s
    STEP: Saw pod success 08/29/23 20:34:40.133
    Aug 29 20:34:40.133: INFO: Pod "downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4" satisfied condition "Succeeded or Failed"
    Aug 29 20:34:40.138: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4 container client-container: <nil>
    STEP: delete the pod 08/29/23 20:34:40.15
    Aug 29 20:34:40.168: INFO: Waiting for pod downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4 to disappear
    Aug 29 20:34:40.174: INFO: Pod downwardapi-volume-f3a21b56-c145-49ea-ae08-920d59f5b8b4 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:34:40.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4982" for this suite. 08/29/23 20:34:40.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:34:40.19
Aug 29 20:34:40.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replicaset 08/29/23 20:34:40.192
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:40.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:40.216
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 29 20:34:40.220: INFO: Creating ReplicaSet my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e
Aug 29 20:34:40.236: INFO: Pod name my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e: Found 0 pods out of 1
Aug 29 20:34:45.241: INFO: Pod name my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e: Found 1 pods out of 1
Aug 29 20:34:45.241: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e" is running
Aug 29 20:34:45.241: INFO: Waiting up to 5m0s for pod "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb" in namespace "replicaset-442" to be "running"
Aug 29 20:34:45.248: INFO: Pod "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb": Phase="Running", Reason="", readiness=true. Elapsed: 6.581955ms
Aug 29 20:34:45.248: INFO: Pod "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb" satisfied condition "running"
Aug 29 20:34:45.248: INFO: Pod "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 20:34:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 20:34:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 20:34:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 20:34:40 +0000 UTC Reason: Message:}])
Aug 29 20:34:45.248: INFO: Trying to dial the pod
Aug 29 20:34:50.264: INFO: Controller my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e: Got expected result from replica 1 [my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb]: "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:34:50.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-442" for this suite. 08/29/23 20:34:50.269
------------------------------
• [SLOW TEST] [10.087 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:34:40.19
    Aug 29 20:34:40.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replicaset 08/29/23 20:34:40.192
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:40.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:40.216
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 29 20:34:40.220: INFO: Creating ReplicaSet my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e
    Aug 29 20:34:40.236: INFO: Pod name my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e: Found 0 pods out of 1
    Aug 29 20:34:45.241: INFO: Pod name my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e: Found 1 pods out of 1
    Aug 29 20:34:45.241: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e" is running
    Aug 29 20:34:45.241: INFO: Waiting up to 5m0s for pod "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb" in namespace "replicaset-442" to be "running"
    Aug 29 20:34:45.248: INFO: Pod "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb": Phase="Running", Reason="", readiness=true. Elapsed: 6.581955ms
    Aug 29 20:34:45.248: INFO: Pod "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb" satisfied condition "running"
    Aug 29 20:34:45.248: INFO: Pod "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 20:34:40 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 20:34:41 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 20:34:41 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-29 20:34:40 +0000 UTC Reason: Message:}])
    Aug 29 20:34:45.248: INFO: Trying to dial the pod
    Aug 29 20:34:50.264: INFO: Controller my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e: Got expected result from replica 1 [my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb]: "my-hostname-basic-608a67a7-8073-49eb-ace9-e0b9ee8e869e-9nrjb", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:34:50.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-442" for this suite. 08/29/23 20:34:50.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:34:50.279
Aug 29 20:34:50.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 20:34:50.28
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:50.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:50.303
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 08/29/23 20:34:50.306
STEP: Counting existing ResourceQuota 08/29/23 20:34:55.31
STEP: Creating a ResourceQuota 08/29/23 20:35:00.314
STEP: Ensuring resource quota status is calculated 08/29/23 20:35:00.323
STEP: Creating a Secret 08/29/23 20:35:02.328
STEP: Ensuring resource quota status captures secret creation 08/29/23 20:35:02.343
STEP: Deleting a secret 08/29/23 20:35:04.352
STEP: Ensuring resource quota status released usage 08/29/23 20:35:04.361
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:06.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-948" for this suite. 08/29/23 20:35:06.371
------------------------------
• [SLOW TEST] [16.100 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:34:50.279
    Aug 29 20:34:50.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 20:34:50.28
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:34:50.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:34:50.303
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 08/29/23 20:34:50.306
    STEP: Counting existing ResourceQuota 08/29/23 20:34:55.31
    STEP: Creating a ResourceQuota 08/29/23 20:35:00.314
    STEP: Ensuring resource quota status is calculated 08/29/23 20:35:00.323
    STEP: Creating a Secret 08/29/23 20:35:02.328
    STEP: Ensuring resource quota status captures secret creation 08/29/23 20:35:02.343
    STEP: Deleting a secret 08/29/23 20:35:04.352
    STEP: Ensuring resource quota status released usage 08/29/23 20:35:04.361
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:06.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-948" for this suite. 08/29/23 20:35:06.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:06.38
Aug 29 20:35:06.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename namespaces 08/29/23 20:35:06.381
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:06.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:06.404
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-mbk79" 08/29/23 20:35:06.407
Aug 29 20:35:06.422: INFO: Namespace "e2e-ns-mbk79-9844" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-mbk79-9844" 08/29/23 20:35:06.422
Aug 29 20:35:06.432: INFO: Namespace "e2e-ns-mbk79-9844" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-mbk79-9844" 08/29/23 20:35:06.432
Aug 29 20:35:06.442: INFO: Namespace "e2e-ns-mbk79-9844" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:06.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-46" for this suite. 08/29/23 20:35:06.447
STEP: Destroying namespace "e2e-ns-mbk79-9844" for this suite. 08/29/23 20:35:06.452
------------------------------
• [0.080 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:06.38
    Aug 29 20:35:06.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename namespaces 08/29/23 20:35:06.381
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:06.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:06.404
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-mbk79" 08/29/23 20:35:06.407
    Aug 29 20:35:06.422: INFO: Namespace "e2e-ns-mbk79-9844" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-mbk79-9844" 08/29/23 20:35:06.422
    Aug 29 20:35:06.432: INFO: Namespace "e2e-ns-mbk79-9844" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-mbk79-9844" 08/29/23 20:35:06.432
    Aug 29 20:35:06.442: INFO: Namespace "e2e-ns-mbk79-9844" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:06.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-46" for this suite. 08/29/23 20:35:06.447
    STEP: Destroying namespace "e2e-ns-mbk79-9844" for this suite. 08/29/23 20:35:06.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:06.46
Aug 29 20:35:06.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:35:06.462
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:06.486
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:06.489
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:35:06.513
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:35:06.98
STEP: Deploying the webhook pod 08/29/23 20:35:06.992
STEP: Wait for the deployment to be ready 08/29/23 20:35:07.012
Aug 29 20:35:07.022: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 20:35:09.033
STEP: Verifying the service has paired with the endpoint 08/29/23 20:35:09.049
Aug 29 20:35:10.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Aug 29 20:35:10.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/29/23 20:35:10.568
STEP: Creating a custom resource that should be denied by the webhook 08/29/23 20:35:10.589
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/29/23 20:35:12.632
STEP: Updating the custom resource with disallowed data should be denied 08/29/23 20:35:12.643
STEP: Deleting the custom resource should be denied 08/29/23 20:35:12.653
STEP: Remove the offending key and value from the custom resource data 08/29/23 20:35:12.66
STEP: Deleting the updated custom resource should be successful 08/29/23 20:35:12.67
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:13.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5622" for this suite. 08/29/23 20:35:13.265
STEP: Destroying namespace "webhook-5622-markers" for this suite. 08/29/23 20:35:13.274
------------------------------
• [SLOW TEST] [6.821 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:06.46
    Aug 29 20:35:06.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:35:06.462
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:06.486
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:06.489
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:35:06.513
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:35:06.98
    STEP: Deploying the webhook pod 08/29/23 20:35:06.992
    STEP: Wait for the deployment to be ready 08/29/23 20:35:07.012
    Aug 29 20:35:07.022: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 20:35:09.033
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:35:09.049
    Aug 29 20:35:10.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Aug 29 20:35:10.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/29/23 20:35:10.568
    STEP: Creating a custom resource that should be denied by the webhook 08/29/23 20:35:10.589
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/29/23 20:35:12.632
    STEP: Updating the custom resource with disallowed data should be denied 08/29/23 20:35:12.643
    STEP: Deleting the custom resource should be denied 08/29/23 20:35:12.653
    STEP: Remove the offending key and value from the custom resource data 08/29/23 20:35:12.66
    STEP: Deleting the updated custom resource should be successful 08/29/23 20:35:12.67
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:13.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5622" for this suite. 08/29/23 20:35:13.265
    STEP: Destroying namespace "webhook-5622-markers" for this suite. 08/29/23 20:35:13.274
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:13.282
Aug 29 20:35:13.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:35:13.285
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:13.305
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:13.307
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:13.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6526" for this suite. 08/29/23 20:35:13.318
------------------------------
• [0.042 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:13.282
    Aug 29 20:35:13.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:35:13.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:13.305
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:13.307
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:13.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6526" for this suite. 08/29/23 20:35:13.318
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:13.332
Aug 29 20:35:13.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:35:13.333
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:13.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:13.355
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 08/29/23 20:35:13.358
Aug 29 20:35:13.371: INFO: Waiting up to 5m0s for pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff" in namespace "emptydir-4973" to be "Succeeded or Failed"
Aug 29 20:35:13.375: INFO: Pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856589ms
Aug 29 20:35:15.379: INFO: Pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008110414s
Aug 29 20:35:17.380: INFO: Pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009396094s
STEP: Saw pod success 08/29/23 20:35:17.38
Aug 29 20:35:17.380: INFO: Pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff" satisfied condition "Succeeded or Failed"
Aug 29 20:35:17.384: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff container test-container: <nil>
STEP: delete the pod 08/29/23 20:35:17.392
Aug 29 20:35:17.409: INFO: Waiting for pod pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff to disappear
Aug 29 20:35:17.413: INFO: Pod pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:17.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4973" for this suite. 08/29/23 20:35:17.419
------------------------------
• [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:13.332
    Aug 29 20:35:13.332: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:35:13.333
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:13.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:13.355
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/29/23 20:35:13.358
    Aug 29 20:35:13.371: INFO: Waiting up to 5m0s for pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff" in namespace "emptydir-4973" to be "Succeeded or Failed"
    Aug 29 20:35:13.375: INFO: Pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.856589ms
    Aug 29 20:35:15.379: INFO: Pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008110414s
    Aug 29 20:35:17.380: INFO: Pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009396094s
    STEP: Saw pod success 08/29/23 20:35:17.38
    Aug 29 20:35:17.380: INFO: Pod "pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff" satisfied condition "Succeeded or Failed"
    Aug 29 20:35:17.384: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff container test-container: <nil>
    STEP: delete the pod 08/29/23 20:35:17.392
    Aug 29 20:35:17.409: INFO: Waiting for pod pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff to disappear
    Aug 29 20:35:17.413: INFO: Pod pod-e29eb701-d81e-4c40-80f2-c08839d6d3ff no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:17.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4973" for this suite. 08/29/23 20:35:17.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:17.427
Aug 29 20:35:17.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename security-context-test 08/29/23 20:35:17.428
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:17.446
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:17.451
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Aug 29 20:35:17.461: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25" in namespace "security-context-test-4426" to be "Succeeded or Failed"
Aug 29 20:35:17.464: INFO: Pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073535ms
Aug 29 20:35:19.469: INFO: Pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00770809s
Aug 29 20:35:21.470: INFO: Pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008419358s
Aug 29 20:35:21.470: INFO: Pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25" satisfied condition "Succeeded or Failed"
Aug 29 20:35:21.478: INFO: Got logs for pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:21.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4426" for this suite. 08/29/23 20:35:21.484
------------------------------
• [4.065 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:17.427
    Aug 29 20:35:17.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename security-context-test 08/29/23 20:35:17.428
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:17.446
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:17.451
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Aug 29 20:35:17.461: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25" in namespace "security-context-test-4426" to be "Succeeded or Failed"
    Aug 29 20:35:17.464: INFO: Pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25": Phase="Pending", Reason="", readiness=false. Elapsed: 3.073535ms
    Aug 29 20:35:19.469: INFO: Pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00770809s
    Aug 29 20:35:21.470: INFO: Pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008419358s
    Aug 29 20:35:21.470: INFO: Pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25" satisfied condition "Succeeded or Failed"
    Aug 29 20:35:21.478: INFO: Got logs for pod "busybox-privileged-false-29452581-00be-4a2f-80a8-76cd1d20ed25": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:21.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4426" for this suite. 08/29/23 20:35:21.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:21.494
Aug 29 20:35:21.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:35:21.495
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:21.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:21.522
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:35:21.526
Aug 29 20:35:21.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15" in namespace "projected-4600" to be "Succeeded or Failed"
Aug 29 20:35:21.540: INFO: Pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881186ms
Aug 29 20:35:23.545: INFO: Pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009231484s
Aug 29 20:35:25.547: INFO: Pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011500455s
STEP: Saw pod success 08/29/23 20:35:25.547
Aug 29 20:35:25.548: INFO: Pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15" satisfied condition "Succeeded or Failed"
Aug 29 20:35:25.550: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15 container client-container: <nil>
STEP: delete the pod 08/29/23 20:35:25.558
Aug 29 20:35:25.574: INFO: Waiting for pod downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15 to disappear
Aug 29 20:35:25.578: INFO: Pod downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4600" for this suite. 08/29/23 20:35:25.583
------------------------------
• [4.096 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:21.494
    Aug 29 20:35:21.494: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:35:21.495
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:21.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:21.522
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:35:21.526
    Aug 29 20:35:21.536: INFO: Waiting up to 5m0s for pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15" in namespace "projected-4600" to be "Succeeded or Failed"
    Aug 29 20:35:21.540: INFO: Pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15": Phase="Pending", Reason="", readiness=false. Elapsed: 3.881186ms
    Aug 29 20:35:23.545: INFO: Pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009231484s
    Aug 29 20:35:25.547: INFO: Pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011500455s
    STEP: Saw pod success 08/29/23 20:35:25.547
    Aug 29 20:35:25.548: INFO: Pod "downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15" satisfied condition "Succeeded or Failed"
    Aug 29 20:35:25.550: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15 container client-container: <nil>
    STEP: delete the pod 08/29/23 20:35:25.558
    Aug 29 20:35:25.574: INFO: Waiting for pod downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15 to disappear
    Aug 29 20:35:25.578: INFO: Pod downwardapi-volume-654d70c2-271a-48e6-97b7-1e4d4e9e3a15 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:25.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4600" for this suite. 08/29/23 20:35:25.583
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:25.591
Aug 29 20:35:25.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename statefulset 08/29/23 20:35:25.592
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:25.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:25.609
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-616 08/29/23 20:35:25.613
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 08/29/23 20:35:25.618
STEP: Creating pod with conflicting port in namespace statefulset-616 08/29/23 20:35:25.624
STEP: Waiting until pod test-pod will start running in namespace statefulset-616 08/29/23 20:35:25.636
Aug 29 20:35:25.636: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-616" to be "running"
Aug 29 20:35:25.640: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526559ms
Aug 29 20:35:27.645: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009566881s
Aug 29 20:35:27.645: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-616 08/29/23 20:35:27.645
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-616 08/29/23 20:35:27.652
Aug 29 20:35:27.667: INFO: Observed stateful pod in namespace: statefulset-616, name: ss-0, uid: beba47b4-b28f-43c7-b5b4-f4668c5fcd68, status phase: Pending. Waiting for statefulset controller to delete.
Aug 29 20:35:27.706: INFO: Observed stateful pod in namespace: statefulset-616, name: ss-0, uid: beba47b4-b28f-43c7-b5b4-f4668c5fcd68, status phase: Failed. Waiting for statefulset controller to delete.
Aug 29 20:35:27.735: INFO: Observed stateful pod in namespace: statefulset-616, name: ss-0, uid: beba47b4-b28f-43c7-b5b4-f4668c5fcd68, status phase: Failed. Waiting for statefulset controller to delete.
Aug 29 20:35:27.738: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-616
STEP: Removing pod with conflicting port in namespace statefulset-616 08/29/23 20:35:27.738
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-616 and will be in running state 08/29/23 20:35:27.755
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 29 20:35:31.777: INFO: Deleting all statefulset in ns statefulset-616
Aug 29 20:35:31.782: INFO: Scaling statefulset ss to 0
Aug 29 20:35:41.814: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 20:35:41.818: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:41.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-616" for this suite. 08/29/23 20:35:41.851
------------------------------
• [SLOW TEST] [16.275 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:25.591
    Aug 29 20:35:25.591: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename statefulset 08/29/23 20:35:25.592
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:25.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:25.609
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-616 08/29/23 20:35:25.613
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 08/29/23 20:35:25.618
    STEP: Creating pod with conflicting port in namespace statefulset-616 08/29/23 20:35:25.624
    STEP: Waiting until pod test-pod will start running in namespace statefulset-616 08/29/23 20:35:25.636
    Aug 29 20:35:25.636: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-616" to be "running"
    Aug 29 20:35:25.640: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.526559ms
    Aug 29 20:35:27.645: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009566881s
    Aug 29 20:35:27.645: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-616 08/29/23 20:35:27.645
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-616 08/29/23 20:35:27.652
    Aug 29 20:35:27.667: INFO: Observed stateful pod in namespace: statefulset-616, name: ss-0, uid: beba47b4-b28f-43c7-b5b4-f4668c5fcd68, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 29 20:35:27.706: INFO: Observed stateful pod in namespace: statefulset-616, name: ss-0, uid: beba47b4-b28f-43c7-b5b4-f4668c5fcd68, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 29 20:35:27.735: INFO: Observed stateful pod in namespace: statefulset-616, name: ss-0, uid: beba47b4-b28f-43c7-b5b4-f4668c5fcd68, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 29 20:35:27.738: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-616
    STEP: Removing pod with conflicting port in namespace statefulset-616 08/29/23 20:35:27.738
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-616 and will be in running state 08/29/23 20:35:27.755
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 29 20:35:31.777: INFO: Deleting all statefulset in ns statefulset-616
    Aug 29 20:35:31.782: INFO: Scaling statefulset ss to 0
    Aug 29 20:35:41.814: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 20:35:41.818: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:41.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-616" for this suite. 08/29/23 20:35:41.851
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:41.866
Aug 29 20:35:41.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename disruption 08/29/23 20:35:41.868
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:41.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:41.886
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 08/29/23 20:35:41.894
STEP: Waiting for all pods to be running 08/29/23 20:35:41.935
Aug 29 20:35:41.940: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:43.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-8750" for this suite. 08/29/23 20:35:43.953
------------------------------
• [2.096 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:41.866
    Aug 29 20:35:41.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename disruption 08/29/23 20:35:41.868
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:41.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:41.886
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 08/29/23 20:35:41.894
    STEP: Waiting for all pods to be running 08/29/23 20:35:41.935
    Aug 29 20:35:41.940: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:43.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-8750" for this suite. 08/29/23 20:35:43.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:43.964
Aug 29 20:35:43.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-lifecycle-hook 08/29/23 20:35:43.965
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:43.984
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:43.986
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 08/29/23 20:35:43.994
Aug 29 20:35:44.004: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6522" to be "running and ready"
Aug 29 20:35:44.008: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796394ms
Aug 29 20:35:44.008: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:35:46.012: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008716603s
Aug 29 20:35:46.013: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 29 20:35:46.013: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 08/29/23 20:35:46.016
Aug 29 20:35:46.025: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6522" to be "running and ready"
Aug 29 20:35:46.029: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.956117ms
Aug 29 20:35:46.029: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:35:48.033: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008742277s
Aug 29 20:35:48.033: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 29 20:35:48.033: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/29/23 20:35:48.037
Aug 29 20:35:48.048: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 29 20:35:48.051: INFO: Pod pod-with-prestop-http-hook still exists
Aug 29 20:35:50.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 29 20:35:50.060: INFO: Pod pod-with-prestop-http-hook still exists
Aug 29 20:35:52.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 29 20:35:52.057: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/29/23 20:35:52.057
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:52.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-6522" for this suite. 08/29/23 20:35:52.07
------------------------------
• [SLOW TEST] [8.115 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:43.964
    Aug 29 20:35:43.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/29/23 20:35:43.965
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:43.984
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:43.986
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 08/29/23 20:35:43.994
    Aug 29 20:35:44.004: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6522" to be "running and ready"
    Aug 29 20:35:44.008: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.796394ms
    Aug 29 20:35:44.008: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:35:46.012: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008716603s
    Aug 29 20:35:46.013: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 29 20:35:46.013: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 08/29/23 20:35:46.016
    Aug 29 20:35:46.025: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-6522" to be "running and ready"
    Aug 29 20:35:46.029: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.956117ms
    Aug 29 20:35:46.029: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:35:48.033: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.008742277s
    Aug 29 20:35:48.033: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 29 20:35:48.033: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/29/23 20:35:48.037
    Aug 29 20:35:48.048: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 29 20:35:48.051: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 29 20:35:50.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 29 20:35:50.060: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 29 20:35:52.052: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 29 20:35:52.057: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/29/23 20:35:52.057
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:52.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-6522" for this suite. 08/29/23 20:35:52.07
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:52.079
Aug 29 20:35:52.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/29/23 20:35:52.081
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:52.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:52.104
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/29/23 20:35:52.107
STEP: Creating hostNetwork=false pod 08/29/23 20:35:52.107
Aug 29 20:35:52.118: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-478" to be "running and ready"
Aug 29 20:35:52.121: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592978ms
Aug 29 20:35:52.121: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:35:54.127: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009268554s
Aug 29 20:35:54.127: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 29 20:35:54.127: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/29/23 20:35:54.131
Aug 29 20:35:54.139: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-478" to be "running and ready"
Aug 29 20:35:54.144: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208527ms
Aug 29 20:35:54.144: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:35:56.148: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008439589s
Aug 29 20:35:56.148: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 29 20:35:56.148: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/29/23 20:35:56.151
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/29/23 20:35:56.151
Aug 29 20:35:56.151: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.151: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.151: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 29 20:35:56.223: INFO: Exec stderr: ""
Aug 29 20:35:56.223: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.224: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.224: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 29 20:35:56.316: INFO: Exec stderr: ""
Aug 29 20:35:56.316: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.316: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.317: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 29 20:35:56.400: INFO: Exec stderr: ""
Aug 29 20:35:56.400: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.400: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.400: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 29 20:35:56.475: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/29/23 20:35:56.475
Aug 29 20:35:56.475: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.476: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.476: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 29 20:35:56.548: INFO: Exec stderr: ""
Aug 29 20:35:56.548: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.549: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.549: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 29 20:35:56.628: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/29/23 20:35:56.628
Aug 29 20:35:56.628: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.628: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.629: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 29 20:35:56.706: INFO: Exec stderr: ""
Aug 29 20:35:56.706: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.706: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.707: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 29 20:35:56.782: INFO: Exec stderr: ""
Aug 29 20:35:56.782: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.783: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.783: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 29 20:35:56.857: INFO: Exec stderr: ""
Aug 29 20:35:56.857: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:35:56.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:35:56.858: INFO: ExecWithOptions: Clientset creation
Aug 29 20:35:56.858: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 29 20:35:56.931: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Aug 29 20:35:56.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-478" for this suite. 08/29/23 20:35:56.937
------------------------------
• [4.866 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:52.079
    Aug 29 20:35:52.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/29/23 20:35:52.081
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:52.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:52.104
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/29/23 20:35:52.107
    STEP: Creating hostNetwork=false pod 08/29/23 20:35:52.107
    Aug 29 20:35:52.118: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-478" to be "running and ready"
    Aug 29 20:35:52.121: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.592978ms
    Aug 29 20:35:52.121: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:35:54.127: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009268554s
    Aug 29 20:35:54.127: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 29 20:35:54.127: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/29/23 20:35:54.131
    Aug 29 20:35:54.139: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-478" to be "running and ready"
    Aug 29 20:35:54.144: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.208527ms
    Aug 29 20:35:54.144: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:35:56.148: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008439589s
    Aug 29 20:35:56.148: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 29 20:35:56.148: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/29/23 20:35:56.151
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/29/23 20:35:56.151
    Aug 29 20:35:56.151: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.151: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.151: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 29 20:35:56.223: INFO: Exec stderr: ""
    Aug 29 20:35:56.223: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.224: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.224: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 29 20:35:56.316: INFO: Exec stderr: ""
    Aug 29 20:35:56.316: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.316: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.316: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.317: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 29 20:35:56.400: INFO: Exec stderr: ""
    Aug 29 20:35:56.400: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.400: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.400: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 29 20:35:56.475: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/29/23 20:35:56.475
    Aug 29 20:35:56.475: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.476: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.476: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.476: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 29 20:35:56.548: INFO: Exec stderr: ""
    Aug 29 20:35:56.548: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.548: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.549: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.549: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 29 20:35:56.628: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/29/23 20:35:56.628
    Aug 29 20:35:56.628: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.628: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.629: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 29 20:35:56.706: INFO: Exec stderr: ""
    Aug 29 20:35:56.706: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.706: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.707: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 29 20:35:56.782: INFO: Exec stderr: ""
    Aug 29 20:35:56.782: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.782: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.783: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.783: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 29 20:35:56.857: INFO: Exec stderr: ""
    Aug 29 20:35:56.857: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-478 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:35:56.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:35:56.858: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:35:56.858: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-478/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 29 20:35:56.931: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:35:56.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-478" for this suite. 08/29/23 20:35:56.937
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:35:56.947
Aug 29 20:35:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename statefulset 08/29/23 20:35:56.948
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:56.965
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:56.97
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-1471 08/29/23 20:35:56.973
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 08/29/23 20:35:56.979
Aug 29 20:35:56.994: INFO: Found 0 stateful pods, waiting for 3
Aug 29 20:36:06.999: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 20:36:06.999: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 20:36:06.999: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 29 20:36:07.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-1471 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 20:36:07.185: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 20:36:07.185: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 20:36:07.185: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/29/23 20:36:17.206
Aug 29 20:36:17.227: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/29/23 20:36:17.228
STEP: Updating Pods in reverse ordinal order 08/29/23 20:36:27.25
Aug 29 20:36:27.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-1471 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 20:36:27.431: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 20:36:27.431: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 20:36:27.431: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 08/29/23 20:36:37.456
Aug 29 20:36:37.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-1471 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 29 20:36:37.634: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 29 20:36:37.634: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 29 20:36:37.634: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 29 20:36:47.673: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/29/23 20:36:57.694
Aug 29 20:36:57.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-1471 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 29 20:36:57.857: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 29 20:36:57.857: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 29 20:36:57.857: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Aug 29 20:37:07.886: INFO: Deleting all statefulset in ns statefulset-1471
Aug 29 20:37:07.890: INFO: Scaling statefulset ss2 to 0
Aug 29 20:37:17.915: INFO: Waiting for statefulset status.replicas updated to 0
Aug 29 20:37:17.918: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:37:17.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-1471" for this suite. 08/29/23 20:37:17.942
------------------------------
• [SLOW TEST] [81.004 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:35:56.947
    Aug 29 20:35:56.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename statefulset 08/29/23 20:35:56.948
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:35:56.965
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:35:56.97
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-1471 08/29/23 20:35:56.973
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 08/29/23 20:35:56.979
    Aug 29 20:35:56.994: INFO: Found 0 stateful pods, waiting for 3
    Aug 29 20:36:06.999: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 20:36:06.999: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 20:36:06.999: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 29 20:36:07.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-1471 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 20:36:07.185: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 20:36:07.185: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 20:36:07.185: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 08/29/23 20:36:17.206
    Aug 29 20:36:17.227: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/29/23 20:36:17.228
    STEP: Updating Pods in reverse ordinal order 08/29/23 20:36:27.25
    Aug 29 20:36:27.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-1471 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 20:36:27.431: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 29 20:36:27.431: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 20:36:27.431: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 08/29/23 20:36:37.456
    Aug 29 20:36:37.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-1471 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 29 20:36:37.634: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 29 20:36:37.634: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 29 20:36:37.634: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 29 20:36:47.673: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/29/23 20:36:57.694
    Aug 29 20:36:57.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=statefulset-1471 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 29 20:36:57.857: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 29 20:36:57.857: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 29 20:36:57.857: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Aug 29 20:37:07.886: INFO: Deleting all statefulset in ns statefulset-1471
    Aug 29 20:37:07.890: INFO: Scaling statefulset ss2 to 0
    Aug 29 20:37:17.915: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 29 20:37:17.918: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:37:17.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-1471" for this suite. 08/29/23 20:37:17.942
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:37:17.951
Aug 29 20:37:17.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:37:17.952
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:37:17.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:37:17.975
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/29/23 20:37:17.978
Aug 29 20:37:17.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:37:20.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:37:28.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5433" for this suite. 08/29/23 20:37:28.975
------------------------------
• [SLOW TEST] [11.035 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:37:17.951
    Aug 29 20:37:17.952: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename crd-publish-openapi 08/29/23 20:37:17.952
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:37:17.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:37:17.975
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/29/23 20:37:17.978
    Aug 29 20:37:17.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:37:20.195: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:37:28.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5433" for this suite. 08/29/23 20:37:28.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:37:28.987
Aug 29 20:37:28.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-probe 08/29/23 20:37:28.988
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:37:29.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:37:29.009
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-b1f8710d-3776-4204-8e6a-34d2868c567a in namespace container-probe-2875 08/29/23 20:37:29.012
Aug 29 20:37:29.024: INFO: Waiting up to 5m0s for pod "busybox-b1f8710d-3776-4204-8e6a-34d2868c567a" in namespace "container-probe-2875" to be "not pending"
Aug 29 20:37:29.027: INFO: Pod "busybox-b1f8710d-3776-4204-8e6a-34d2868c567a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.315457ms
Aug 29 20:37:31.033: INFO: Pod "busybox-b1f8710d-3776-4204-8e6a-34d2868c567a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009592275s
Aug 29 20:37:31.033: INFO: Pod "busybox-b1f8710d-3776-4204-8e6a-34d2868c567a" satisfied condition "not pending"
Aug 29 20:37:31.034: INFO: Started pod busybox-b1f8710d-3776-4204-8e6a-34d2868c567a in namespace container-probe-2875
STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 20:37:31.034
Aug 29 20:37:31.037: INFO: Initial restart count of pod busybox-b1f8710d-3776-4204-8e6a-34d2868c567a is 0
Aug 29 20:38:21.178: INFO: Restart count of pod container-probe-2875/busybox-b1f8710d-3776-4204-8e6a-34d2868c567a is now 1 (50.140427194s elapsed)
STEP: deleting the pod 08/29/23 20:38:21.178
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:21.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2875" for this suite. 08/29/23 20:38:21.21
------------------------------
• [SLOW TEST] [52.230 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:37:28.987
    Aug 29 20:37:28.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-probe 08/29/23 20:37:28.988
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:37:29.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:37:29.009
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-b1f8710d-3776-4204-8e6a-34d2868c567a in namespace container-probe-2875 08/29/23 20:37:29.012
    Aug 29 20:37:29.024: INFO: Waiting up to 5m0s for pod "busybox-b1f8710d-3776-4204-8e6a-34d2868c567a" in namespace "container-probe-2875" to be "not pending"
    Aug 29 20:37:29.027: INFO: Pod "busybox-b1f8710d-3776-4204-8e6a-34d2868c567a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.315457ms
    Aug 29 20:37:31.033: INFO: Pod "busybox-b1f8710d-3776-4204-8e6a-34d2868c567a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009592275s
    Aug 29 20:37:31.033: INFO: Pod "busybox-b1f8710d-3776-4204-8e6a-34d2868c567a" satisfied condition "not pending"
    Aug 29 20:37:31.034: INFO: Started pod busybox-b1f8710d-3776-4204-8e6a-34d2868c567a in namespace container-probe-2875
    STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 20:37:31.034
    Aug 29 20:37:31.037: INFO: Initial restart count of pod busybox-b1f8710d-3776-4204-8e6a-34d2868c567a is 0
    Aug 29 20:38:21.178: INFO: Restart count of pod container-probe-2875/busybox-b1f8710d-3776-4204-8e6a-34d2868c567a is now 1 (50.140427194s elapsed)
    STEP: deleting the pod 08/29/23 20:38:21.178
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:21.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2875" for this suite. 08/29/23 20:38:21.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:21.221
Aug 29 20:38:21.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:38:21.222
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:21.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:21.247
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-8439/configmap-test-d0a4264c-a266-48f3-990f-5de882fc24a8 08/29/23 20:38:21.25
STEP: Creating a pod to test consume configMaps 08/29/23 20:38:21.255
Aug 29 20:38:21.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57" in namespace "configmap-8439" to be "Succeeded or Failed"
Aug 29 20:38:21.266: INFO: Pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.298993ms
Aug 29 20:38:23.272: INFO: Pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009052509s
Aug 29 20:38:25.271: INFO: Pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008323388s
STEP: Saw pod success 08/29/23 20:38:25.271
Aug 29 20:38:25.272: INFO: Pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57" satisfied condition "Succeeded or Failed"
Aug 29 20:38:25.275: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57 container env-test: <nil>
STEP: delete the pod 08/29/23 20:38:25.297
Aug 29 20:38:25.311: INFO: Waiting for pod pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57 to disappear
Aug 29 20:38:25.314: INFO: Pod pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:25.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8439" for this suite. 08/29/23 20:38:25.32
------------------------------
• [4.105 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:21.221
    Aug 29 20:38:21.221: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:38:21.222
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:21.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:21.247
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-8439/configmap-test-d0a4264c-a266-48f3-990f-5de882fc24a8 08/29/23 20:38:21.25
    STEP: Creating a pod to test consume configMaps 08/29/23 20:38:21.255
    Aug 29 20:38:21.263: INFO: Waiting up to 5m0s for pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57" in namespace "configmap-8439" to be "Succeeded or Failed"
    Aug 29 20:38:21.266: INFO: Pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57": Phase="Pending", Reason="", readiness=false. Elapsed: 3.298993ms
    Aug 29 20:38:23.272: INFO: Pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009052509s
    Aug 29 20:38:25.271: INFO: Pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008323388s
    STEP: Saw pod success 08/29/23 20:38:25.271
    Aug 29 20:38:25.272: INFO: Pod "pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57" satisfied condition "Succeeded or Failed"
    Aug 29 20:38:25.275: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57 container env-test: <nil>
    STEP: delete the pod 08/29/23 20:38:25.297
    Aug 29 20:38:25.311: INFO: Waiting for pod pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57 to disappear
    Aug 29 20:38:25.314: INFO: Pod pod-configmaps-213439ad-daee-4482-8215-68c8d82b2e57 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:25.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8439" for this suite. 08/29/23 20:38:25.32
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:25.326
Aug 29 20:38:25.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename configmap 08/29/23 20:38:25.327
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:25.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:25.352
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-df98dbb2-9625-4227-bae5-615419e66c4e 08/29/23 20:38:25.355
STEP: Creating a pod to test consume configMaps 08/29/23 20:38:25.36
Aug 29 20:38:25.368: INFO: Waiting up to 5m0s for pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f" in namespace "configmap-5104" to be "Succeeded or Failed"
Aug 29 20:38:25.373: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371032ms
Aug 29 20:38:27.377: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009150206s
Aug 29 20:38:29.381: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f": Phase="Running", Reason="", readiness=false. Elapsed: 4.012867948s
Aug 29 20:38:31.378: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009351534s
STEP: Saw pod success 08/29/23 20:38:31.378
Aug 29 20:38:31.378: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f" satisfied condition "Succeeded or Failed"
Aug 29 20:38:31.382: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:38:31.389
Aug 29 20:38:31.402: INFO: Waiting for pod pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f to disappear
Aug 29 20:38:31.406: INFO: Pod pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:31.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5104" for this suite. 08/29/23 20:38:31.411
------------------------------
• [SLOW TEST] [6.091 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:25.326
    Aug 29 20:38:25.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename configmap 08/29/23 20:38:25.327
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:25.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:25.352
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-df98dbb2-9625-4227-bae5-615419e66c4e 08/29/23 20:38:25.355
    STEP: Creating a pod to test consume configMaps 08/29/23 20:38:25.36
    Aug 29 20:38:25.368: INFO: Waiting up to 5m0s for pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f" in namespace "configmap-5104" to be "Succeeded or Failed"
    Aug 29 20:38:25.373: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.371032ms
    Aug 29 20:38:27.377: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009150206s
    Aug 29 20:38:29.381: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f": Phase="Running", Reason="", readiness=false. Elapsed: 4.012867948s
    Aug 29 20:38:31.378: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009351534s
    STEP: Saw pod success 08/29/23 20:38:31.378
    Aug 29 20:38:31.378: INFO: Pod "pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f" satisfied condition "Succeeded or Failed"
    Aug 29 20:38:31.382: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:38:31.389
    Aug 29 20:38:31.402: INFO: Waiting for pod pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f to disappear
    Aug 29 20:38:31.406: INFO: Pod pod-configmaps-de9ac3ef-344f-404e-8a4c-4fcabda32f0f no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:31.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5104" for this suite. 08/29/23 20:38:31.411
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:31.42
Aug 29 20:38:31.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename server-version 08/29/23 20:38:31.421
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:31.443
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:31.447
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/29/23 20:38:31.449
STEP: Confirm major version 08/29/23 20:38:31.451
Aug 29 20:38:31.451: INFO: Major version: 1
STEP: Confirm minor version 08/29/23 20:38:31.451
Aug 29 20:38:31.451: INFO: cleanMinorVersion: 26
Aug 29 20:38:31.451: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:31.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-9768" for this suite. 08/29/23 20:38:31.456
------------------------------
• [0.044 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:31.42
    Aug 29 20:38:31.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename server-version 08/29/23 20:38:31.421
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:31.443
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:31.447
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/29/23 20:38:31.449
    STEP: Confirm major version 08/29/23 20:38:31.451
    Aug 29 20:38:31.451: INFO: Major version: 1
    STEP: Confirm minor version 08/29/23 20:38:31.451
    Aug 29 20:38:31.451: INFO: cleanMinorVersion: 26
    Aug 29 20:38:31.451: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:31.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-9768" for this suite. 08/29/23 20:38:31.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:31.466
Aug 29 20:38:31.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:38:31.467
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:31.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:31.495
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Aug 29 20:38:31.502: INFO: Got root ca configmap in namespace "svcaccounts-1357"
Aug 29 20:38:31.509: INFO: Deleted root ca configmap in namespace "svcaccounts-1357"
STEP: waiting for a new root ca configmap created 08/29/23 20:38:32.009
Aug 29 20:38:32.014: INFO: Recreated root ca configmap in namespace "svcaccounts-1357"
Aug 29 20:38:32.021: INFO: Updated root ca configmap in namespace "svcaccounts-1357"
STEP: waiting for the root ca configmap reconciled 08/29/23 20:38:32.522
Aug 29 20:38:32.527: INFO: Reconciled root ca configmap in namespace "svcaccounts-1357"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:32.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1357" for this suite. 08/29/23 20:38:32.531
------------------------------
• [1.074 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:31.466
    Aug 29 20:38:31.466: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:38:31.467
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:31.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:31.495
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Aug 29 20:38:31.502: INFO: Got root ca configmap in namespace "svcaccounts-1357"
    Aug 29 20:38:31.509: INFO: Deleted root ca configmap in namespace "svcaccounts-1357"
    STEP: waiting for a new root ca configmap created 08/29/23 20:38:32.009
    Aug 29 20:38:32.014: INFO: Recreated root ca configmap in namespace "svcaccounts-1357"
    Aug 29 20:38:32.021: INFO: Updated root ca configmap in namespace "svcaccounts-1357"
    STEP: waiting for the root ca configmap reconciled 08/29/23 20:38:32.522
    Aug 29 20:38:32.527: INFO: Reconciled root ca configmap in namespace "svcaccounts-1357"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:32.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1357" for this suite. 08/29/23 20:38:32.531
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:32.54
Aug 29 20:38:32.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename ephemeral-containers-test 08/29/23 20:38:32.541
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:32.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:32.567
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/29/23 20:38:32.569
Aug 29 20:38:32.580: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5211" to be "running and ready"
Aug 29 20:38:32.583: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.009196ms
Aug 29 20:38:32.583: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:38:34.593: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012755913s
Aug 29 20:38:34.593: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 29 20:38:34.593: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/29/23 20:38:34.597
Aug 29 20:38:34.610: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5211" to be "container debugger running"
Aug 29 20:38:34.614: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.64709ms
Aug 29 20:38:36.618: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008175882s
Aug 29 20:38:38.619: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009155196s
Aug 29 20:38:38.619: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/29/23 20:38:38.619
Aug 29 20:38:38.619: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5211 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 29 20:38:38.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
Aug 29 20:38:38.620: INFO: ExecWithOptions: Clientset creation
Aug 29 20:38:38.620: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/ephemeral-containers-test-5211/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 29 20:38:38.710: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:38.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-5211" for this suite. 08/29/23 20:38:38.724
------------------------------
• [SLOW TEST] [6.192 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:32.54
    Aug 29 20:38:32.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/29/23 20:38:32.541
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:32.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:32.567
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/29/23 20:38:32.569
    Aug 29 20:38:32.580: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5211" to be "running and ready"
    Aug 29 20:38:32.583: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.009196ms
    Aug 29 20:38:32.583: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:38:34.593: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.012755913s
    Aug 29 20:38:34.593: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 29 20:38:34.593: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/29/23 20:38:34.597
    Aug 29 20:38:34.610: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-5211" to be "container debugger running"
    Aug 29 20:38:34.614: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.64709ms
    Aug 29 20:38:36.618: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008175882s
    Aug 29 20:38:38.619: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009155196s
    Aug 29 20:38:38.619: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/29/23 20:38:38.619
    Aug 29 20:38:38.619: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5211 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 29 20:38:38.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    Aug 29 20:38:38.620: INFO: ExecWithOptions: Clientset creation
    Aug 29 20:38:38.620: INFO: ExecWithOptions: execute(POST https://172.19.0.1:443/api/v1/namespaces/ephemeral-containers-test-5211/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 29 20:38:38.710: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:38.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-5211" for this suite. 08/29/23 20:38:38.724
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:38.734
Aug 29 20:38:38.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename disruption 08/29/23 20:38:38.735
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:38.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:38.764
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 08/29/23 20:38:38.781
STEP: Updating PodDisruptionBudget status 08/29/23 20:38:40.789
STEP: Waiting for all pods to be running 08/29/23 20:38:40.8
Aug 29 20:38:40.804: INFO: running pods: 0 < 1
STEP: locating a running pod 08/29/23 20:38:42.811
STEP: Waiting for the pdb to be processed 08/29/23 20:38:42.826
STEP: Patching PodDisruptionBudget status 08/29/23 20:38:42.833
STEP: Waiting for the pdb to be processed 08/29/23 20:38:42.846
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:42.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2782" for this suite. 08/29/23 20:38:42.855
------------------------------
• [4.131 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:38.734
    Aug 29 20:38:38.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename disruption 08/29/23 20:38:38.735
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:38.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:38.764
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 08/29/23 20:38:38.781
    STEP: Updating PodDisruptionBudget status 08/29/23 20:38:40.789
    STEP: Waiting for all pods to be running 08/29/23 20:38:40.8
    Aug 29 20:38:40.804: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/29/23 20:38:42.811
    STEP: Waiting for the pdb to be processed 08/29/23 20:38:42.826
    STEP: Patching PodDisruptionBudget status 08/29/23 20:38:42.833
    STEP: Waiting for the pdb to be processed 08/29/23 20:38:42.846
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:42.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2782" for this suite. 08/29/23 20:38:42.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:42.867
Aug 29 20:38:42.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename runtimeclass 08/29/23 20:38:42.868
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:42.889
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:42.892
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-5085-delete-me 08/29/23 20:38:42.901
STEP: Waiting for the RuntimeClass to disappear 08/29/23 20:38:42.907
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:42.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-5085" for this suite. 08/29/23 20:38:42.927
------------------------------
• [0.068 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:42.867
    Aug 29 20:38:42.867: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename runtimeclass 08/29/23 20:38:42.868
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:42.889
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:42.892
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-5085-delete-me 08/29/23 20:38:42.901
    STEP: Waiting for the RuntimeClass to disappear 08/29/23 20:38:42.907
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:42.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-5085" for this suite. 08/29/23 20:38:42.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:42.936
Aug 29 20:38:42.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replicaset 08/29/23 20:38:42.937
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:42.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:42.961
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/29/23 20:38:42.964
Aug 29 20:38:42.973: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 29 20:38:47.982: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/29/23 20:38:47.982
STEP: getting scale subresource 08/29/23 20:38:47.982
STEP: updating a scale subresource 08/29/23 20:38:47.985
STEP: verifying the replicaset Spec.Replicas was modified 08/29/23 20:38:47.994
STEP: Patch a scale subresource 08/29/23 20:38:47.998
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:48.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-2785" for this suite. 08/29/23 20:38:48.02
------------------------------
• [SLOW TEST] [5.091 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:42.936
    Aug 29 20:38:42.936: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replicaset 08/29/23 20:38:42.937
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:42.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:42.961
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/29/23 20:38:42.964
    Aug 29 20:38:42.973: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 29 20:38:47.982: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/29/23 20:38:47.982
    STEP: getting scale subresource 08/29/23 20:38:47.982
    STEP: updating a scale subresource 08/29/23 20:38:47.985
    STEP: verifying the replicaset Spec.Replicas was modified 08/29/23 20:38:47.994
    STEP: Patch a scale subresource 08/29/23 20:38:47.998
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:48.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-2785" for this suite. 08/29/23 20:38:48.02
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:48.027
Aug 29 20:38:48.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:38:48.029
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:48.054
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:48.058
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 08/29/23 20:38:48.067
STEP: watching for the Service to be added 08/29/23 20:38:48.084
Aug 29 20:38:48.086: INFO: Found Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 29 20:38:48.086: INFO: Service test-service-sjzz4 created
STEP: Getting /status 08/29/23 20:38:48.086
Aug 29 20:38:48.092: INFO: Service test-service-sjzz4 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/29/23 20:38:48.092
STEP: watching for the Service to be patched 08/29/23 20:38:48.102
Aug 29 20:38:48.104: INFO: observed Service test-service-sjzz4 in namespace services-5953 with annotations: map[] & LoadBalancer: {[]}
Aug 29 20:38:48.104: INFO: Found Service test-service-sjzz4 in namespace services-5953 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 29 20:38:48.104: INFO: Service test-service-sjzz4 has service status patched
STEP: updating the ServiceStatus 08/29/23 20:38:48.104
Aug 29 20:38:48.127: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/29/23 20:38:48.127
Aug 29 20:38:48.129: INFO: Observed Service test-service-sjzz4 in namespace services-5953 with annotations: map[] & Conditions: {[]}
Aug 29 20:38:48.129: INFO: Observed event: &Service{ObjectMeta:{test-service-sjzz4  services-5953  beec7a39-bde0-45d4-b6b6-55786a3cbd06 41800 0 2023-08-29 20:38:48 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-29 20:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-29 20:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.19.229.19,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.19.229.19],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 29 20:38:48.129: INFO: Found Service test-service-sjzz4 in namespace services-5953 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 29 20:38:48.129: INFO: Service test-service-sjzz4 has service status updated
STEP: patching the service 08/29/23 20:38:48.129
STEP: watching for the Service to be patched 08/29/23 20:38:48.153
Aug 29 20:38:48.155: INFO: observed Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service-static:true]
Aug 29 20:38:48.155: INFO: observed Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service-static:true]
Aug 29 20:38:48.155: INFO: observed Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service-static:true]
Aug 29 20:38:48.155: INFO: Found Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service:patched test-service-static:true]
Aug 29 20:38:48.155: INFO: Service test-service-sjzz4 patched
STEP: deleting the service 08/29/23 20:38:48.155
STEP: watching for the Service to be deleted 08/29/23 20:38:48.172
Aug 29 20:38:48.174: INFO: Observed event: ADDED
Aug 29 20:38:48.174: INFO: Observed event: MODIFIED
Aug 29 20:38:48.174: INFO: Observed event: MODIFIED
Aug 29 20:38:48.174: INFO: Observed event: MODIFIED
Aug 29 20:38:48.174: INFO: Found Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 29 20:38:48.174: INFO: Service test-service-sjzz4 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:48.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-5953" for this suite. 08/29/23 20:38:48.179
------------------------------
• [0.160 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:48.027
    Aug 29 20:38:48.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:38:48.029
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:48.054
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:48.058
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 08/29/23 20:38:48.067
    STEP: watching for the Service to be added 08/29/23 20:38:48.084
    Aug 29 20:38:48.086: INFO: Found Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 29 20:38:48.086: INFO: Service test-service-sjzz4 created
    STEP: Getting /status 08/29/23 20:38:48.086
    Aug 29 20:38:48.092: INFO: Service test-service-sjzz4 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/29/23 20:38:48.092
    STEP: watching for the Service to be patched 08/29/23 20:38:48.102
    Aug 29 20:38:48.104: INFO: observed Service test-service-sjzz4 in namespace services-5953 with annotations: map[] & LoadBalancer: {[]}
    Aug 29 20:38:48.104: INFO: Found Service test-service-sjzz4 in namespace services-5953 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 29 20:38:48.104: INFO: Service test-service-sjzz4 has service status patched
    STEP: updating the ServiceStatus 08/29/23 20:38:48.104
    Aug 29 20:38:48.127: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/29/23 20:38:48.127
    Aug 29 20:38:48.129: INFO: Observed Service test-service-sjzz4 in namespace services-5953 with annotations: map[] & Conditions: {[]}
    Aug 29 20:38:48.129: INFO: Observed event: &Service{ObjectMeta:{test-service-sjzz4  services-5953  beec7a39-bde0-45d4-b6b6-55786a3cbd06 41800 0 2023-08-29 20:38:48 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-29 20:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-29 20:38:48 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.19.229.19,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.19.229.19],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 29 20:38:48.129: INFO: Found Service test-service-sjzz4 in namespace services-5953 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 29 20:38:48.129: INFO: Service test-service-sjzz4 has service status updated
    STEP: patching the service 08/29/23 20:38:48.129
    STEP: watching for the Service to be patched 08/29/23 20:38:48.153
    Aug 29 20:38:48.155: INFO: observed Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service-static:true]
    Aug 29 20:38:48.155: INFO: observed Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service-static:true]
    Aug 29 20:38:48.155: INFO: observed Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service-static:true]
    Aug 29 20:38:48.155: INFO: Found Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service:patched test-service-static:true]
    Aug 29 20:38:48.155: INFO: Service test-service-sjzz4 patched
    STEP: deleting the service 08/29/23 20:38:48.155
    STEP: watching for the Service to be deleted 08/29/23 20:38:48.172
    Aug 29 20:38:48.174: INFO: Observed event: ADDED
    Aug 29 20:38:48.174: INFO: Observed event: MODIFIED
    Aug 29 20:38:48.174: INFO: Observed event: MODIFIED
    Aug 29 20:38:48.174: INFO: Observed event: MODIFIED
    Aug 29 20:38:48.174: INFO: Found Service test-service-sjzz4 in namespace services-5953 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 29 20:38:48.174: INFO: Service test-service-sjzz4 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:48.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-5953" for this suite. 08/29/23 20:38:48.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:48.189
Aug 29 20:38:48.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename tables 08/29/23 20:38:48.19
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:48.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:48.218
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:48.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-3240" for this suite. 08/29/23 20:38:48.232
------------------------------
• [0.056 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:48.189
    Aug 29 20:38:48.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename tables 08/29/23 20:38:48.19
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:48.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:48.218
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:48.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-3240" for this suite. 08/29/23 20:38:48.232
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:48.245
Aug 29 20:38:48.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:38:48.246
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:48.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:48.274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Aug 29 20:38:48.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 create -f -'
Aug 29 20:38:49.288: INFO: stderr: ""
Aug 29 20:38:49.288: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 29 20:38:49.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 create -f -'
Aug 29 20:38:49.672: INFO: stderr: ""
Aug 29 20:38:49.672: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/29/23 20:38:49.672
Aug 29 20:38:50.677: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 20:38:50.677: INFO: Found 1 / 1
Aug 29 20:38:50.677: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 29 20:38:50.681: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 29 20:38:50.681: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 29 20:38:50.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe pod agnhost-primary-rwqdx'
Aug 29 20:38:50.783: INFO: stderr: ""
Aug 29 20:38:50.783: INFO: stdout: "Name:             agnhost-primary-rwqdx\nNamespace:        kubectl-2202\nPriority:         0\nService Account:  default\nNode:             loki-15bd39-worker-1/10.45.35.206\nStart Time:       Tue, 29 Aug 2023 20:38:49 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 6e298b71360484751612a882f54cda88f58b8f9f6cc800a0d143bf4e4ed90164\n                  cni.projectcalico.org/podIP: 172.20.30.184/32\n                  cni.projectcalico.org/podIPs: 172.20.30.184/32\nStatus:           Running\nIP:               172.20.30.184\nIPs:\n  IP:           172.20.30.184\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://ba183c377bf59a98b3058149693393fd4b9986c8035eec08cc7e0d6014cac302\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 29 Aug 2023 20:38:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bncw8 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bncw8:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2202/agnhost-primary-rwqdx to loki-15bd39-worker-1\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
Aug 29 20:38:50.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe rc agnhost-primary'
Aug 29 20:38:50.887: INFO: stderr: ""
Aug 29 20:38:50.887: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2202\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-rwqdx\n"
Aug 29 20:38:50.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe service agnhost-primary'
Aug 29 20:38:50.996: INFO: stderr: ""
Aug 29 20:38:50.996: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2202\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.19.250.121\nIPs:               172.19.250.121\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.20.30.184:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 29 20:38:51.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe node loki-15bd39-master-0'
Aug 29 20:38:51.142: INFO: stderr: ""
Aug 29 20:38:51.143: INFO: stdout: "Name:               loki-15bd39-master-0\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=loki-15bd39-master-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=master\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/master=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.nutanix.com\":\"loki-15bd39-master-0\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.45.35.202/27\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 29 Aug 2023 19:11:19 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  loki-15bd39-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 29 Aug 2023 20:38:45 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 29 Aug 2023 19:18:12 +0000   Tue, 29 Aug 2023 19:18:12 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 29 Aug 2023 20:36:37 +0000   Tue, 29 Aug 2023 19:11:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 29 Aug 2023 20:36:37 +0000   Tue, 29 Aug 2023 19:11:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 29 Aug 2023 20:36:37 +0000   Tue, 29 Aug 2023 19:11:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 29 Aug 2023 20:36:37 +0000   Tue, 29 Aug 2023 19:11:19 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.45.35.202\n  Hostname:    loki-15bd39-master-0\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      123723328Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3843768Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    4\n  ephemeral-storage:      123723328Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3434168Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 931846f6db7e448d9f30b00be2a4f7c3\n  System UUID:                931846F6-DB7E-448D-9F30-B00BE2A4F7C3\n  Boot ID:                    f2da001a-d4b1-4558-b3a4-0f7e2e6c859b\n  Kernel Version:             3.10.0-1160.88.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.16\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      172.20.0.0/24\nPodCIDRs:                     172.20.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-f4bng                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         81m\n  kube-system                 kube-apiserver-loki-15bd39-master-0                        300m (7%)     0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                 kube-proxy-ds-7pn7t                                        100m (2%)     100m (2%)   150Mi (4%)       150Mi (4%)     81m\n  ntnx-system                 alertmanager-main-2                                        200m (5%)     300m (7%)   150Mi (4%)       250Mi (7%)     18m\n  ntnx-system                 fluent-bit-x2ntj                                           100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     80m\n  ntnx-system                 node-exporter-lsprp                                        112m (2%)     600m (15%)  200Mi (5%)       220Mi (6%)     77m\n  ntnx-system                 nutanix-csi-node-mqc5m                                     200m (5%)     200m (5%)   400Mi (11%)      400Mi (11%)    75m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests      Limits\n  --------               --------      ------\n  cpu                    1262m (31%)   1300m (32%)\n  memory                 1000Mi (29%)  1120Mi (33%)\n  ephemeral-storage      0 (0%)        0 (0%)\n  hugepages-1Gi          0 (0%)        0 (0%)\n  hugepages-2Mi          0 (0%)        0 (0%)\n  scheduling.k8s.io/foo  0             0\nEvents:                  <none>\n"
Aug 29 20:38:51.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe namespace kubectl-2202'
Aug 29 20:38:51.243: INFO: stderr: ""
Aug 29 20:38:51.243: INFO: stdout: "Name:         kubectl-2202\nLabels:       e2e-framework=kubectl\n              e2e-run=359ac39d-857c-4768-9030-64ce090d54e9\n              kubernetes.io/metadata.name=kubectl-2202\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:38:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2202" for this suite. 08/29/23 20:38:51.249
------------------------------
• [3.012 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:48.245
    Aug 29 20:38:48.245: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:38:48.246
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:48.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:48.274
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Aug 29 20:38:48.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 create -f -'
    Aug 29 20:38:49.288: INFO: stderr: ""
    Aug 29 20:38:49.288: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 29 20:38:49.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 create -f -'
    Aug 29 20:38:49.672: INFO: stderr: ""
    Aug 29 20:38:49.672: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/29/23 20:38:49.672
    Aug 29 20:38:50.677: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 20:38:50.677: INFO: Found 1 / 1
    Aug 29 20:38:50.677: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 29 20:38:50.681: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 29 20:38:50.681: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 29 20:38:50.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe pod agnhost-primary-rwqdx'
    Aug 29 20:38:50.783: INFO: stderr: ""
    Aug 29 20:38:50.783: INFO: stdout: "Name:             agnhost-primary-rwqdx\nNamespace:        kubectl-2202\nPriority:         0\nService Account:  default\nNode:             loki-15bd39-worker-1/10.45.35.206\nStart Time:       Tue, 29 Aug 2023 20:38:49 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 6e298b71360484751612a882f54cda88f58b8f9f6cc800a0d143bf4e4ed90164\n                  cni.projectcalico.org/podIP: 172.20.30.184/32\n                  cni.projectcalico.org/podIPs: 172.20.30.184/32\nStatus:           Running\nIP:               172.20.30.184\nIPs:\n  IP:           172.20.30.184\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://ba183c377bf59a98b3058149693393fd4b9986c8035eec08cc7e0d6014cac302\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 29 Aug 2023 20:38:50 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bncw8 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-bncw8:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2202/agnhost-primary-rwqdx to loki-15bd39-worker-1\n  Normal  Pulled     0s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
    Aug 29 20:38:50.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe rc agnhost-primary'
    Aug 29 20:38:50.887: INFO: stderr: ""
    Aug 29 20:38:50.887: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2202\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-rwqdx\n"
    Aug 29 20:38:50.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe service agnhost-primary'
    Aug 29 20:38:50.996: INFO: stderr: ""
    Aug 29 20:38:50.996: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2202\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.19.250.121\nIPs:               172.19.250.121\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.20.30.184:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 29 20:38:51.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe node loki-15bd39-master-0'
    Aug 29 20:38:51.142: INFO: stderr: ""
    Aug 29 20:38:51.143: INFO: stdout: "Name:               loki-15bd39-master-0\nRoles:              control-plane,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=loki-15bd39-master-0\n                    kubernetes.io/os=linux\n                    kubernetes.io/role=master\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/master=\nAnnotations:        csi.volume.kubernetes.io/nodeid: {\"csi.nutanix.com\":\"loki-15bd39-master-0\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.45.35.202/27\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 29 Aug 2023 19:11:19 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  loki-15bd39-master-0\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 29 Aug 2023 20:38:45 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 29 Aug 2023 19:18:12 +0000   Tue, 29 Aug 2023 19:18:12 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 29 Aug 2023 20:36:37 +0000   Tue, 29 Aug 2023 19:11:19 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 29 Aug 2023 20:36:37 +0000   Tue, 29 Aug 2023 19:11:19 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 29 Aug 2023 20:36:37 +0000   Tue, 29 Aug 2023 19:11:19 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 29 Aug 2023 20:36:37 +0000   Tue, 29 Aug 2023 19:11:19 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.45.35.202\n  Hostname:    loki-15bd39-master-0\nCapacity:\n  cpu:                    4\n  ephemeral-storage:      123723328Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3843768Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    4\n  ephemeral-storage:      123723328Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 3434168Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 931846f6db7e448d9f30b00be2a4f7c3\n  System UUID:                931846F6-DB7E-448D-9F30-B00BE2A4F7C3\n  Boot ID:                    f2da001a-d4b1-4558-b3a4-0f7e2e6c859b\n  Kernel Version:             3.10.0-1160.88.1.el7.x86_64\n  OS Image:                   CentOS Linux 7 (Core)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.16\n  Kubelet Version:            v1.26.8\n  Kube-Proxy Version:         v1.26.8\nPodCIDR:                      172.20.0.0/24\nPodCIDRs:                     172.20.0.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-f4bng                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         81m\n  kube-system                 kube-apiserver-loki-15bd39-master-0                        300m (7%)     0 (0%)      0 (0%)           0 (0%)         86m\n  kube-system                 kube-proxy-ds-7pn7t                                        100m (2%)     100m (2%)   150Mi (4%)       150Mi (4%)     81m\n  ntnx-system                 alertmanager-main-2                                        200m (5%)     300m (7%)   150Mi (4%)       250Mi (7%)     18m\n  ntnx-system                 fluent-bit-x2ntj                                           100m (2%)     100m (2%)   100Mi (2%)       100Mi (2%)     80m\n  ntnx-system                 node-exporter-lsprp                                        112m (2%)     600m (15%)  200Mi (5%)       220Mi (6%)     77m\n  ntnx-system                 nutanix-csi-node-mqc5m                                     200m (5%)     200m (5%)   400Mi (11%)      400Mi (11%)    75m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         70m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests      Limits\n  --------               --------      ------\n  cpu                    1262m (31%)   1300m (32%)\n  memory                 1000Mi (29%)  1120Mi (33%)\n  ephemeral-storage      0 (0%)        0 (0%)\n  hugepages-1Gi          0 (0%)        0 (0%)\n  hugepages-2Mi          0 (0%)        0 (0%)\n  scheduling.k8s.io/foo  0             0\nEvents:                  <none>\n"
    Aug 29 20:38:51.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-2202 describe namespace kubectl-2202'
    Aug 29 20:38:51.243: INFO: stderr: ""
    Aug 29 20:38:51.243: INFO: stdout: "Name:         kubectl-2202\nLabels:       e2e-framework=kubectl\n              e2e-run=359ac39d-857c-4768-9030-64ce090d54e9\n              kubernetes.io/metadata.name=kubectl-2202\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:38:51.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2202" for this suite. 08/29/23 20:38:51.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:38:51.258
Aug 29 20:38:51.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename daemonsets 08/29/23 20:38:51.26
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:51.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:51.284
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385
Aug 29 20:38:51.323: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 20:38:51.33
Aug 29 20:38:51.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:38:51.338: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:38:52.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:38:52.349: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:38:53.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 20:38:53.350: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Update daemon pods image. 08/29/23 20:38:53.366
STEP: Check that daemon pods images are updated. 08/29/23 20:38:53.387
Aug 29 20:38:53.390: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:53.391: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:53.391: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:53.391: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:53.391: INFO: Wrong image for pod: daemon-set-nhrn4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:54.402: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:54.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:54.402: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:54.402: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:55.401: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:55.401: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:55.401: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:55.401: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:56.403: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:56.403: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:56.403: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:56.403: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:56.403: INFO: Pod daemon-set-sc8m4 is not available
Aug 29 20:38:57.402: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:57.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:57.402: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:57.402: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:57.402: INFO: Pod daemon-set-sc8m4 is not available
Aug 29 20:38:58.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:58.402: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:58.402: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:59.401: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:59.401: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:59.401: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:38:59.401: INFO: Pod daemon-set-zlh68 is not available
Aug 29 20:39:00.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:00.402: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:01.401: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:01.401: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:01.401: INFO: Pod daemon-set-t4lw2 is not available
Aug 29 20:39:02.401: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:02.401: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:02.401: INFO: Pod daemon-set-t4lw2 is not available
Aug 29 20:39:03.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:04.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:05.403: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Aug 29 20:39:05.403: INFO: Pod daemon-set-kcdvj is not available
Aug 29 20:39:07.401: INFO: Pod daemon-set-xqlwj is not available
STEP: Check that daemon pods are still running on every node of the cluster. 08/29/23 20:39:07.406
Aug 29 20:39:07.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:39:07.414: INFO: Node loki-15bd39-worker-1 is running 0 daemon pod, expected 1
Aug 29 20:39:08.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:39:08.425: INFO: Node loki-15bd39-worker-1 is running 0 daemon pod, expected 1
Aug 29 20:39:09.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 20:39:09.425: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/29/23 20:39:09.442
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-925, will wait for the garbage collector to delete the pods 08/29/23 20:39:09.443
Aug 29 20:39:09.504: INFO: Deleting DaemonSet.extensions daemon-set took: 7.65363ms
Aug 29 20:39:09.605: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.129743ms
Aug 29 20:39:11.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:39:11.713: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 20:39:11.720: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42221"},"items":null}

Aug 29 20:39:11.728: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42221"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:39:11.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-925" for this suite. 08/29/23 20:39:11.769
------------------------------
• [SLOW TEST] [20.521 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:38:51.258
    Aug 29 20:38:51.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename daemonsets 08/29/23 20:38:51.26
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:38:51.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:38:51.284
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:385
    Aug 29 20:38:51.323: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 20:38:51.33
    Aug 29 20:38:51.338: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:38:51.338: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:38:52.349: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:38:52.349: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:38:53.350: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 20:38:53.350: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Update daemon pods image. 08/29/23 20:38:53.366
    STEP: Check that daemon pods images are updated. 08/29/23 20:38:53.387
    Aug 29 20:38:53.390: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:53.391: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:53.391: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:53.391: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:53.391: INFO: Wrong image for pod: daemon-set-nhrn4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:54.402: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:54.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:54.402: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:54.402: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:55.401: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:55.401: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:55.401: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:55.401: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:56.403: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:56.403: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:56.403: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:56.403: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:56.403: INFO: Pod daemon-set-sc8m4 is not available
    Aug 29 20:38:57.402: INFO: Wrong image for pod: daemon-set-4vfl9. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:57.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:57.402: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:57.402: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:57.402: INFO: Pod daemon-set-sc8m4 is not available
    Aug 29 20:38:58.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:58.402: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:58.402: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:59.401: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:59.401: INFO: Wrong image for pod: daemon-set-g8dfh. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:59.401: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:38:59.401: INFO: Pod daemon-set-zlh68 is not available
    Aug 29 20:39:00.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:00.402: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:01.401: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:01.401: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:01.401: INFO: Pod daemon-set-t4lw2 is not available
    Aug 29 20:39:02.401: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:02.401: INFO: Wrong image for pod: daemon-set-m9cd4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:02.401: INFO: Pod daemon-set-t4lw2 is not available
    Aug 29 20:39:03.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:04.402: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:05.403: INFO: Wrong image for pod: daemon-set-9m5nn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Aug 29 20:39:05.403: INFO: Pod daemon-set-kcdvj is not available
    Aug 29 20:39:07.401: INFO: Pod daemon-set-xqlwj is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 08/29/23 20:39:07.406
    Aug 29 20:39:07.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:39:07.414: INFO: Node loki-15bd39-worker-1 is running 0 daemon pod, expected 1
    Aug 29 20:39:08.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:39:08.425: INFO: Node loki-15bd39-worker-1 is running 0 daemon pod, expected 1
    Aug 29 20:39:09.425: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 20:39:09.425: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/29/23 20:39:09.442
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-925, will wait for the garbage collector to delete the pods 08/29/23 20:39:09.443
    Aug 29 20:39:09.504: INFO: Deleting DaemonSet.extensions daemon-set took: 7.65363ms
    Aug 29 20:39:09.605: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.129743ms
    Aug 29 20:39:11.713: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:39:11.713: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 29 20:39:11.720: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"42221"},"items":null}

    Aug 29 20:39:11.728: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"42221"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:39:11.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-925" for this suite. 08/29/23 20:39:11.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:39:11.78
Aug 29 20:39:11.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:39:11.781
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:39:11.809
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:39:11.813
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 08/29/23 20:39:11.816
Aug 29 20:39:11.816: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-9974 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/29/23 20:39:11.89
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:39:11.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9974" for this suite. 08/29/23 20:39:11.91
------------------------------
• [0.139 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:39:11.78
    Aug 29 20:39:11.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:39:11.781
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:39:11.809
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:39:11.813
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 08/29/23 20:39:11.816
    Aug 29 20:39:11.816: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-9974 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/29/23 20:39:11.89
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:39:11.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9974" for this suite. 08/29/23 20:39:11.91
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:39:11.919
Aug 29 20:39:11.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:39:11.921
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:39:11.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:39:11.95
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 08/29/23 20:39:11.953
Aug 29 20:39:11.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 create -f -'
Aug 29 20:39:12.304: INFO: stderr: ""
Aug 29 20:39:12.304: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/29/23 20:39:12.304
Aug 29 20:39:12.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 20:39:12.420: INFO: stderr: ""
Aug 29 20:39:12.420: INFO: stdout: "update-demo-nautilus-c9hcc update-demo-nautilus-rqhvt "
Aug 29 20:39:12.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 20:39:12.518: INFO: stderr: ""
Aug 29 20:39:12.518: INFO: stdout: ""
Aug 29 20:39:12.518: INFO: update-demo-nautilus-c9hcc is created but not running
Aug 29 20:39:17.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 20:39:17.640: INFO: stderr: ""
Aug 29 20:39:17.640: INFO: stdout: "update-demo-nautilus-c9hcc update-demo-nautilus-rqhvt "
Aug 29 20:39:17.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 20:39:17.747: INFO: stderr: ""
Aug 29 20:39:17.747: INFO: stdout: "true"
Aug 29 20:39:17.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 20:39:17.844: INFO: stderr: ""
Aug 29 20:39:17.845: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 20:39:17.845: INFO: validating pod update-demo-nautilus-c9hcc
Aug 29 20:39:17.851: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 20:39:17.851: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 20:39:17.851: INFO: update-demo-nautilus-c9hcc is verified up and running
Aug 29 20:39:17.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-rqhvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 20:39:17.953: INFO: stderr: ""
Aug 29 20:39:17.953: INFO: stdout: ""
Aug 29 20:39:17.953: INFO: update-demo-nautilus-rqhvt is created but not running
Aug 29 20:39:22.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 29 20:39:23.054: INFO: stderr: ""
Aug 29 20:39:23.054: INFO: stdout: "update-demo-nautilus-c9hcc update-demo-nautilus-rqhvt "
Aug 29 20:39:23.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 20:39:23.145: INFO: stderr: ""
Aug 29 20:39:23.145: INFO: stdout: "true"
Aug 29 20:39:23.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 20:39:23.243: INFO: stderr: ""
Aug 29 20:39:23.243: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 20:39:23.243: INFO: validating pod update-demo-nautilus-c9hcc
Aug 29 20:39:23.247: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 20:39:23.247: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 20:39:23.247: INFO: update-demo-nautilus-c9hcc is verified up and running
Aug 29 20:39:23.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-rqhvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 29 20:39:23.338: INFO: stderr: ""
Aug 29 20:39:23.339: INFO: stdout: "true"
Aug 29 20:39:23.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-rqhvt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 29 20:39:23.440: INFO: stderr: ""
Aug 29 20:39:23.440: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Aug 29 20:39:23.440: INFO: validating pod update-demo-nautilus-rqhvt
Aug 29 20:39:23.447: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 29 20:39:23.447: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 29 20:39:23.447: INFO: update-demo-nautilus-rqhvt is verified up and running
STEP: using delete to clean up resources 08/29/23 20:39:23.447
Aug 29 20:39:23.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 delete --grace-period=0 --force -f -'
Aug 29 20:39:23.554: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 29 20:39:23.554: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 29 20:39:23.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get rc,svc -l name=update-demo --no-headers'
Aug 29 20:39:23.667: INFO: stderr: "No resources found in kubectl-5125 namespace.\n"
Aug 29 20:39:23.668: INFO: stdout: ""
Aug 29 20:39:23.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 29 20:39:23.774: INFO: stderr: ""
Aug 29 20:39:23.774: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:39:23.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5125" for this suite. 08/29/23 20:39:23.78
------------------------------
• [SLOW TEST] [11.874 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:39:11.919
    Aug 29 20:39:11.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:39:11.921
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:39:11.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:39:11.95
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 08/29/23 20:39:11.953
    Aug 29 20:39:11.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 create -f -'
    Aug 29 20:39:12.304: INFO: stderr: ""
    Aug 29 20:39:12.304: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/29/23 20:39:12.304
    Aug 29 20:39:12.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 20:39:12.420: INFO: stderr: ""
    Aug 29 20:39:12.420: INFO: stdout: "update-demo-nautilus-c9hcc update-demo-nautilus-rqhvt "
    Aug 29 20:39:12.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 20:39:12.518: INFO: stderr: ""
    Aug 29 20:39:12.518: INFO: stdout: ""
    Aug 29 20:39:12.518: INFO: update-demo-nautilus-c9hcc is created but not running
    Aug 29 20:39:17.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 20:39:17.640: INFO: stderr: ""
    Aug 29 20:39:17.640: INFO: stdout: "update-demo-nautilus-c9hcc update-demo-nautilus-rqhvt "
    Aug 29 20:39:17.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 20:39:17.747: INFO: stderr: ""
    Aug 29 20:39:17.747: INFO: stdout: "true"
    Aug 29 20:39:17.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 20:39:17.844: INFO: stderr: ""
    Aug 29 20:39:17.845: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 20:39:17.845: INFO: validating pod update-demo-nautilus-c9hcc
    Aug 29 20:39:17.851: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 20:39:17.851: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 20:39:17.851: INFO: update-demo-nautilus-c9hcc is verified up and running
    Aug 29 20:39:17.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-rqhvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 20:39:17.953: INFO: stderr: ""
    Aug 29 20:39:17.953: INFO: stdout: ""
    Aug 29 20:39:17.953: INFO: update-demo-nautilus-rqhvt is created but not running
    Aug 29 20:39:22.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 29 20:39:23.054: INFO: stderr: ""
    Aug 29 20:39:23.054: INFO: stdout: "update-demo-nautilus-c9hcc update-demo-nautilus-rqhvt "
    Aug 29 20:39:23.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 20:39:23.145: INFO: stderr: ""
    Aug 29 20:39:23.145: INFO: stdout: "true"
    Aug 29 20:39:23.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-c9hcc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 20:39:23.243: INFO: stderr: ""
    Aug 29 20:39:23.243: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 20:39:23.243: INFO: validating pod update-demo-nautilus-c9hcc
    Aug 29 20:39:23.247: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 20:39:23.247: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 20:39:23.247: INFO: update-demo-nautilus-c9hcc is verified up and running
    Aug 29 20:39:23.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-rqhvt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 29 20:39:23.338: INFO: stderr: ""
    Aug 29 20:39:23.339: INFO: stdout: "true"
    Aug 29 20:39:23.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods update-demo-nautilus-rqhvt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 29 20:39:23.440: INFO: stderr: ""
    Aug 29 20:39:23.440: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Aug 29 20:39:23.440: INFO: validating pod update-demo-nautilus-rqhvt
    Aug 29 20:39:23.447: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 29 20:39:23.447: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 29 20:39:23.447: INFO: update-demo-nautilus-rqhvt is verified up and running
    STEP: using delete to clean up resources 08/29/23 20:39:23.447
    Aug 29 20:39:23.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 delete --grace-period=0 --force -f -'
    Aug 29 20:39:23.554: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 29 20:39:23.554: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 29 20:39:23.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get rc,svc -l name=update-demo --no-headers'
    Aug 29 20:39:23.667: INFO: stderr: "No resources found in kubectl-5125 namespace.\n"
    Aug 29 20:39:23.668: INFO: stdout: ""
    Aug 29 20:39:23.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5125 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 29 20:39:23.774: INFO: stderr: ""
    Aug 29 20:39:23.774: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:39:23.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5125" for this suite. 08/29/23 20:39:23.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:39:23.794
Aug 29 20:39:23.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-probe 08/29/23 20:39:23.795
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:39:23.886
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:39:23.89
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70 in namespace container-probe-4893 08/29/23 20:39:23.894
Aug 29 20:39:23.913: INFO: Waiting up to 5m0s for pod "liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70" in namespace "container-probe-4893" to be "not pending"
Aug 29 20:39:23.917: INFO: Pod "liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.623784ms
Aug 29 20:39:26.103: INFO: Pod "liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70": Phase="Running", Reason="", readiness=true. Elapsed: 2.190364692s
Aug 29 20:39:26.103: INFO: Pod "liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70" satisfied condition "not pending"
Aug 29 20:39:26.103: INFO: Started pod liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70 in namespace container-probe-4893
STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 20:39:26.103
Aug 29 20:39:26.111: INFO: Initial restart count of pod liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70 is 0
STEP: deleting the pod 08/29/23 20:43:26.786
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 29 20:43:26.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4893" for this suite. 08/29/23 20:43:26.812
------------------------------
• [SLOW TEST] [243.026 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:39:23.794
    Aug 29 20:39:23.794: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-probe 08/29/23 20:39:23.795
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:39:23.886
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:39:23.89
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70 in namespace container-probe-4893 08/29/23 20:39:23.894
    Aug 29 20:39:23.913: INFO: Waiting up to 5m0s for pod "liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70" in namespace "container-probe-4893" to be "not pending"
    Aug 29 20:39:23.917: INFO: Pod "liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70": Phase="Pending", Reason="", readiness=false. Elapsed: 4.623784ms
    Aug 29 20:39:26.103: INFO: Pod "liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70": Phase="Running", Reason="", readiness=true. Elapsed: 2.190364692s
    Aug 29 20:39:26.103: INFO: Pod "liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70" satisfied condition "not pending"
    Aug 29 20:39:26.103: INFO: Started pod liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70 in namespace container-probe-4893
    STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 20:39:26.103
    Aug 29 20:39:26.111: INFO: Initial restart count of pod liveness-d3715a5e-03d9-4cb1-b6be-56f9a923fb70 is 0
    STEP: deleting the pod 08/29/23 20:43:26.786
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:43:26.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4893" for this suite. 08/29/23 20:43:26.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:43:26.821
Aug 29 20:43:26.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-probe 08/29/23 20:43:26.822
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:43:26.843
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:43:26.847
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 in namespace container-probe-9802 08/29/23 20:43:26.851
Aug 29 20:43:26.862: INFO: Waiting up to 5m0s for pod "liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9" in namespace "container-probe-9802" to be "not pending"
Aug 29 20:43:26.866: INFO: Pod "liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026403ms
Aug 29 20:43:28.872: INFO: Pod "liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009476391s
Aug 29 20:43:28.872: INFO: Pod "liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9" satisfied condition "not pending"
Aug 29 20:43:28.872: INFO: Started pod liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 in namespace container-probe-9802
STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 20:43:28.872
Aug 29 20:43:28.876: INFO: Initial restart count of pod liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is 0
Aug 29 20:43:48.934: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 1 (20.057723275s elapsed)
Aug 29 20:44:08.986: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 2 (40.110094505s elapsed)
Aug 29 20:44:29.037: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 3 (1m0.160446777s elapsed)
Aug 29 20:44:49.091: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 4 (1m20.214960121s elapsed)
Aug 29 20:45:55.276: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 5 (2m26.399219883s elapsed)
STEP: deleting the pod 08/29/23 20:45:55.276
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 29 20:45:55.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-9802" for this suite. 08/29/23 20:45:55.296
------------------------------
• [SLOW TEST] [148.498 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:43:26.821
    Aug 29 20:43:26.821: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-probe 08/29/23 20:43:26.822
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:43:26.843
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:43:26.847
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 in namespace container-probe-9802 08/29/23 20:43:26.851
    Aug 29 20:43:26.862: INFO: Waiting up to 5m0s for pod "liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9" in namespace "container-probe-9802" to be "not pending"
    Aug 29 20:43:26.866: INFO: Pod "liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026403ms
    Aug 29 20:43:28.872: INFO: Pod "liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9": Phase="Running", Reason="", readiness=true. Elapsed: 2.009476391s
    Aug 29 20:43:28.872: INFO: Pod "liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9" satisfied condition "not pending"
    Aug 29 20:43:28.872: INFO: Started pod liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 in namespace container-probe-9802
    STEP: checking the pod's current state and verifying that restartCount is present 08/29/23 20:43:28.872
    Aug 29 20:43:28.876: INFO: Initial restart count of pod liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is 0
    Aug 29 20:43:48.934: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 1 (20.057723275s elapsed)
    Aug 29 20:44:08.986: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 2 (40.110094505s elapsed)
    Aug 29 20:44:29.037: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 3 (1m0.160446777s elapsed)
    Aug 29 20:44:49.091: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 4 (1m20.214960121s elapsed)
    Aug 29 20:45:55.276: INFO: Restart count of pod container-probe-9802/liveness-9b2a8850-830c-46e7-96c1-655b619ef4b9 is now 5 (2m26.399219883s elapsed)
    STEP: deleting the pod 08/29/23 20:45:55.276
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:45:55.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-9802" for this suite. 08/29/23 20:45:55.296
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:45:55.32
Aug 29 20:45:55.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename webhook 08/29/23 20:45:55.321
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:45:55.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:45:55.345
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 08/29/23 20:45:55.365
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:45:56.255
STEP: Deploying the webhook pod 08/29/23 20:45:56.269
STEP: Wait for the deployment to be ready 08/29/23 20:45:56.286
Aug 29 20:45:56.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/29/23 20:45:58.307
STEP: Verifying the service has paired with the endpoint 08/29/23 20:45:58.32
Aug 29 20:45:59.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 08/29/23 20:45:59.326
STEP: create a pod that should be denied by the webhook 08/29/23 20:45:59.346
STEP: create a pod that causes the webhook to hang 08/29/23 20:45:59.364
STEP: create a configmap that should be denied by the webhook 08/29/23 20:46:09.372
STEP: create a configmap that should be admitted by the webhook 08/29/23 20:46:09.382
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/29/23 20:46:09.396
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/29/23 20:46:09.406
STEP: create a namespace that bypass the webhook 08/29/23 20:46:09.413
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/29/23 20:46:09.422
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:46:09.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1832" for this suite. 08/29/23 20:46:09.513
STEP: Destroying namespace "webhook-1832-markers" for this suite. 08/29/23 20:46:09.522
------------------------------
• [SLOW TEST] [14.211 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:45:55.32
    Aug 29 20:45:55.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename webhook 08/29/23 20:45:55.321
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:45:55.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:45:55.345
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 08/29/23 20:45:55.365
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/29/23 20:45:56.255
    STEP: Deploying the webhook pod 08/29/23 20:45:56.269
    STEP: Wait for the deployment to be ready 08/29/23 20:45:56.286
    Aug 29 20:45:56.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/29/23 20:45:58.307
    STEP: Verifying the service has paired with the endpoint 08/29/23 20:45:58.32
    Aug 29 20:45:59.321: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 08/29/23 20:45:59.326
    STEP: create a pod that should be denied by the webhook 08/29/23 20:45:59.346
    STEP: create a pod that causes the webhook to hang 08/29/23 20:45:59.364
    STEP: create a configmap that should be denied by the webhook 08/29/23 20:46:09.372
    STEP: create a configmap that should be admitted by the webhook 08/29/23 20:46:09.382
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/29/23 20:46:09.396
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/29/23 20:46:09.406
    STEP: create a namespace that bypass the webhook 08/29/23 20:46:09.413
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/29/23 20:46:09.422
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:46:09.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1832" for this suite. 08/29/23 20:46:09.513
    STEP: Destroying namespace "webhook-1832-markers" for this suite. 08/29/23 20:46:09.522
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:46:09.531
Aug 29 20:46:09.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 20:46:09.533
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:09.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:46:09.559
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:46:09.562
Aug 29 20:46:09.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc" in namespace "downward-api-9101" to be "Succeeded or Failed"
Aug 29 20:46:09.576: INFO: Pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811257ms
Aug 29 20:46:11.581: INFO: Pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008647891s
Aug 29 20:46:13.582: INFO: Pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009646178s
STEP: Saw pod success 08/29/23 20:46:13.582
Aug 29 20:46:13.582: INFO: Pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc" satisfied condition "Succeeded or Failed"
Aug 29 20:46:13.586: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc container client-container: <nil>
STEP: delete the pod 08/29/23 20:46:13.606
Aug 29 20:46:13.623: INFO: Waiting for pod downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc to disappear
Aug 29 20:46:13.626: INFO: Pod downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 20:46:13.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9101" for this suite. 08/29/23 20:46:13.631
------------------------------
• [4.107 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:46:09.531
    Aug 29 20:46:09.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 20:46:09.533
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:09.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:46:09.559
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:46:09.562
    Aug 29 20:46:09.572: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc" in namespace "downward-api-9101" to be "Succeeded or Failed"
    Aug 29 20:46:09.576: INFO: Pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.811257ms
    Aug 29 20:46:11.581: INFO: Pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008647891s
    Aug 29 20:46:13.582: INFO: Pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009646178s
    STEP: Saw pod success 08/29/23 20:46:13.582
    Aug 29 20:46:13.582: INFO: Pod "downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc" satisfied condition "Succeeded or Failed"
    Aug 29 20:46:13.586: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc container client-container: <nil>
    STEP: delete the pod 08/29/23 20:46:13.606
    Aug 29 20:46:13.623: INFO: Waiting for pod downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc to disappear
    Aug 29 20:46:13.626: INFO: Pod downwardapi-volume-a6f9414c-3fb1-48c7-ad90-feb6a82a1fdc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:46:13.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9101" for this suite. 08/29/23 20:46:13.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:46:13.64
Aug 29 20:46:13.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename namespaces 08/29/23 20:46:13.641
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:13.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:46:13.666
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 08/29/23 20:46:13.668
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:13.685
STEP: Creating a pod in the namespace 08/29/23 20:46:13.689
STEP: Waiting for the pod to have running status 08/29/23 20:46:13.699
Aug 29 20:46:13.700: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4423" to be "running"
Aug 29 20:46:13.703: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.802745ms
Aug 29 20:46:15.708: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00826832s
Aug 29 20:46:15.708: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/29/23 20:46:15.708
STEP: Waiting for the namespace to be removed. 08/29/23 20:46:15.716
STEP: Recreating the namespace 08/29/23 20:46:26.721
STEP: Verifying there are no pods in the namespace 08/29/23 20:46:26.743
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:46:26.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5716" for this suite. 08/29/23 20:46:26.752
STEP: Destroying namespace "nsdeletetest-4423" for this suite. 08/29/23 20:46:26.763
Aug 29 20:46:26.766: INFO: Namespace nsdeletetest-4423 was already deleted
STEP: Destroying namespace "nsdeletetest-5090" for this suite. 08/29/23 20:46:26.766
------------------------------
• [SLOW TEST] [13.133 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:46:13.64
    Aug 29 20:46:13.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename namespaces 08/29/23 20:46:13.641
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:13.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:46:13.666
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 08/29/23 20:46:13.668
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:13.685
    STEP: Creating a pod in the namespace 08/29/23 20:46:13.689
    STEP: Waiting for the pod to have running status 08/29/23 20:46:13.699
    Aug 29 20:46:13.700: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-4423" to be "running"
    Aug 29 20:46:13.703: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.802745ms
    Aug 29 20:46:15.708: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00826832s
    Aug 29 20:46:15.708: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/29/23 20:46:15.708
    STEP: Waiting for the namespace to be removed. 08/29/23 20:46:15.716
    STEP: Recreating the namespace 08/29/23 20:46:26.721
    STEP: Verifying there are no pods in the namespace 08/29/23 20:46:26.743
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:46:26.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5716" for this suite. 08/29/23 20:46:26.752
    STEP: Destroying namespace "nsdeletetest-4423" for this suite. 08/29/23 20:46:26.763
    Aug 29 20:46:26.766: INFO: Namespace nsdeletetest-4423 was already deleted
    STEP: Destroying namespace "nsdeletetest-5090" for this suite. 08/29/23 20:46:26.766
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:46:26.774
Aug 29 20:46:26.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:46:26.775
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:26.795
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:46:26.798
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 08/29/23 20:46:26.802
Aug 29 20:46:26.814: INFO: created test-pod-1
Aug 29 20:46:26.823: INFO: created test-pod-2
Aug 29 20:46:26.830: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/29/23 20:46:26.83
Aug 29 20:46:26.830: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5026' to be running and ready
Aug 29 20:46:26.842: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 20:46:26.842: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 20:46:26.842: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 20:46:26.842: INFO: 0 / 3 pods in namespace 'pods-5026' are running and ready (0 seconds elapsed)
Aug 29 20:46:26.842: INFO: expected 0 pod replicas in namespace 'pods-5026', 0 are Running and Ready.
Aug 29 20:46:26.842: INFO: POD         NODE                  PHASE    GRACE  CONDITIONS
Aug 29 20:46:26.842: INFO: test-pod-1  loki-15bd39-worker-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
Aug 29 20:46:26.842: INFO: test-pod-2  loki-15bd39-worker-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
Aug 29 20:46:26.842: INFO: test-pod-3  loki-15bd39-worker-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
Aug 29 20:46:26.842: INFO: 
Aug 29 20:46:28.855: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 20:46:28.855: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 20:46:28.855: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 29 20:46:28.855: INFO: 0 / 3 pods in namespace 'pods-5026' are running and ready (2 seconds elapsed)
Aug 29 20:46:28.855: INFO: expected 0 pod replicas in namespace 'pods-5026', 0 are Running and Ready.
Aug 29 20:46:28.855: INFO: POD         NODE                  PHASE    GRACE  CONDITIONS
Aug 29 20:46:28.855: INFO: test-pod-1  loki-15bd39-worker-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
Aug 29 20:46:28.855: INFO: test-pod-2  loki-15bd39-worker-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
Aug 29 20:46:28.855: INFO: test-pod-3  loki-15bd39-worker-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
Aug 29 20:46:28.855: INFO: 
Aug 29 20:46:30.855: INFO: 3 / 3 pods in namespace 'pods-5026' are running and ready (4 seconds elapsed)
Aug 29 20:46:30.855: INFO: expected 0 pod replicas in namespace 'pods-5026', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/29/23 20:46:30.882
Aug 29 20:46:30.886: INFO: Pod quantity 3 is different from expected quantity 0
Aug 29 20:46:31.891: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:46:32.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5026" for this suite. 08/29/23 20:46:32.897
------------------------------
• [SLOW TEST] [6.131 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:46:26.774
    Aug 29 20:46:26.774: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:46:26.775
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:26.795
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:46:26.798
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 08/29/23 20:46:26.802
    Aug 29 20:46:26.814: INFO: created test-pod-1
    Aug 29 20:46:26.823: INFO: created test-pod-2
    Aug 29 20:46:26.830: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/29/23 20:46:26.83
    Aug 29 20:46:26.830: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-5026' to be running and ready
    Aug 29 20:46:26.842: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 29 20:46:26.842: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 29 20:46:26.842: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 29 20:46:26.842: INFO: 0 / 3 pods in namespace 'pods-5026' are running and ready (0 seconds elapsed)
    Aug 29 20:46:26.842: INFO: expected 0 pod replicas in namespace 'pods-5026', 0 are Running and Ready.
    Aug 29 20:46:26.842: INFO: POD         NODE                  PHASE    GRACE  CONDITIONS
    Aug 29 20:46:26.842: INFO: test-pod-1  loki-15bd39-worker-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
    Aug 29 20:46:26.842: INFO: test-pod-2  loki-15bd39-worker-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
    Aug 29 20:46:26.842: INFO: test-pod-3  loki-15bd39-worker-1  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
    Aug 29 20:46:26.842: INFO: 
    Aug 29 20:46:28.855: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 29 20:46:28.855: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 29 20:46:28.855: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 29 20:46:28.855: INFO: 0 / 3 pods in namespace 'pods-5026' are running and ready (2 seconds elapsed)
    Aug 29 20:46:28.855: INFO: expected 0 pod replicas in namespace 'pods-5026', 0 are Running and Ready.
    Aug 29 20:46:28.855: INFO: POD         NODE                  PHASE    GRACE  CONDITIONS
    Aug 29 20:46:28.855: INFO: test-pod-1  loki-15bd39-worker-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
    Aug 29 20:46:28.855: INFO: test-pod-2  loki-15bd39-worker-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
    Aug 29 20:46:28.855: INFO: test-pod-3  loki-15bd39-worker-1  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-29 20:46:26 +0000 UTC  }]
    Aug 29 20:46:28.855: INFO: 
    Aug 29 20:46:30.855: INFO: 3 / 3 pods in namespace 'pods-5026' are running and ready (4 seconds elapsed)
    Aug 29 20:46:30.855: INFO: expected 0 pod replicas in namespace 'pods-5026', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/29/23 20:46:30.882
    Aug 29 20:46:30.886: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 29 20:46:31.891: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:46:32.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5026" for this suite. 08/29/23 20:46:32.897
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:46:32.905
Aug 29 20:46:32.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename cronjob 08/29/23 20:46:32.906
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:32.927
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:46:32.929
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/29/23 20:46:32.932
STEP: Ensuring no jobs are scheduled 08/29/23 20:46:32.939
STEP: Ensuring no job exists by listing jobs explicitly 08/29/23 20:51:32.948
STEP: Removing cronjob 08/29/23 20:51:32.952
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Aug 29 20:51:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4240" for this suite. 08/29/23 20:51:32.964
------------------------------
• [SLOW TEST] [300.065 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:46:32.905
    Aug 29 20:46:32.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename cronjob 08/29/23 20:46:32.906
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:46:32.927
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:46:32.929
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/29/23 20:46:32.932
    STEP: Ensuring no jobs are scheduled 08/29/23 20:46:32.939
    STEP: Ensuring no job exists by listing jobs explicitly 08/29/23 20:51:32.948
    STEP: Removing cronjob 08/29/23 20:51:32.952
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:51:32.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4240" for this suite. 08/29/23 20:51:32.964
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:51:32.971
Aug 29 20:51:32.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-preemption 08/29/23 20:51:32.972
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:51:32.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:51:32.994
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 29 20:51:33.034: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 20:52:33.086: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:52:33.09
Aug 29 20:52:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-preemption-path 08/29/23 20:52:33.091
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:33.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:33.116
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:576
STEP: Finding an available node 08/29/23 20:52:33.119
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/29/23 20:52:33.119
Aug 29 20:52:33.131: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5581" to be "running"
Aug 29 20:52:33.135: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.740513ms
Aug 29 20:52:35.139: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008311315s
Aug 29 20:52:35.139: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/29/23 20:52:35.143
Aug 29 20:52:35.160: INFO: found a healthy node: loki-15bd39-worker-1
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:624
Aug 29 20:52:41.253: INFO: pods created so far: [1 1 1]
Aug 29 20:52:41.253: INFO: length of pods created so far: 3
Aug 29 20:52:43.268: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Aug 29 20:52:50.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:549
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:52:50.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-5581" for this suite. 08/29/23 20:52:50.422
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-8293" for this suite. 08/29/23 20:52:50.431
------------------------------
• [SLOW TEST] [77.468 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:537
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:624

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:51:32.971
    Aug 29 20:51:32.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-preemption 08/29/23 20:51:32.972
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:51:32.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:51:32.994
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 29 20:51:33.034: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 29 20:52:33.086: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:52:33.09
    Aug 29 20:52:33.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-preemption-path 08/29/23 20:52:33.091
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:33.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:33.116
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:576
    STEP: Finding an available node 08/29/23 20:52:33.119
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/29/23 20:52:33.119
    Aug 29 20:52:33.131: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-5581" to be "running"
    Aug 29 20:52:33.135: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.740513ms
    Aug 29 20:52:35.139: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008311315s
    Aug 29 20:52:35.139: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/29/23 20:52:35.143
    Aug 29 20:52:35.160: INFO: found a healthy node: loki-15bd39-worker-1
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:624
    Aug 29 20:52:41.253: INFO: pods created so far: [1 1 1]
    Aug 29 20:52:41.253: INFO: length of pods created so far: 3
    Aug 29 20:52:43.268: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:52:50.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:549
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:52:50.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-5581" for this suite. 08/29/23 20:52:50.422
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-8293" for this suite. 08/29/23 20:52:50.431
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:52:50.439
Aug 29 20:52:50.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubectl 08/29/23 20:52:50.44
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:50.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:50.466
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 08/29/23 20:52:50.469
Aug 29 20:52:50.470: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5591 proxy --unix-socket=/tmp/kubectl-proxy-unix890610812/test'
STEP: retrieving proxy /api/ output 08/29/23 20:52:50.54
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Aug 29 20:52:50.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-5591" for this suite. 08/29/23 20:52:50.547
------------------------------
• [0.116 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:52:50.439
    Aug 29 20:52:50.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubectl 08/29/23 20:52:50.44
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:50.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:50.466
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 08/29/23 20:52:50.469
    Aug 29 20:52:50.470: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=kubectl-5591 proxy --unix-socket=/tmp/kubectl-proxy-unix890610812/test'
    STEP: retrieving proxy /api/ output 08/29/23 20:52:50.54
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:52:50.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-5591" for this suite. 08/29/23 20:52:50.547
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:52:50.556
Aug 29 20:52:50.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:52:50.557
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:50.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:50.58
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-3da2ac97-a205-4db5-b384-1386f20b9d3b 08/29/23 20:52:50.583
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:52:50.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2148" for this suite. 08/29/23 20:52:50.59
------------------------------
• [0.041 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:52:50.556
    Aug 29 20:52:50.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:52:50.557
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:50.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:50.58
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-3da2ac97-a205-4db5-b384-1386f20b9d3b 08/29/23 20:52:50.583
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:52:50.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2148" for this suite. 08/29/23 20:52:50.59
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:52:50.597
Aug 29 20:52:50.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:52:50.598
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:50.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:50.622
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-f019b0aa-6eb4-412e-b07b-b2c426353cf7 08/29/23 20:52:50.625
STEP: Creating a pod to test consume secrets 08/29/23 20:52:50.631
Aug 29 20:52:50.640: INFO: Waiting up to 5m0s for pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd" in namespace "secrets-2416" to be "Succeeded or Failed"
Aug 29 20:52:50.643: INFO: Pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.85786ms
Aug 29 20:52:52.649: INFO: Pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008668057s
Aug 29 20:52:54.649: INFO: Pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009245258s
STEP: Saw pod success 08/29/23 20:52:54.649
Aug 29 20:52:54.649: INFO: Pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd" satisfied condition "Succeeded or Failed"
Aug 29 20:52:54.652: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd container secret-volume-test: <nil>
STEP: delete the pod 08/29/23 20:52:54.674
Aug 29 20:52:54.691: INFO: Waiting for pod pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd to disappear
Aug 29 20:52:54.694: INFO: Pod pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:52:54.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2416" for this suite. 08/29/23 20:52:54.699
------------------------------
• [4.109 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:52:50.597
    Aug 29 20:52:50.597: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:52:50.598
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:50.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:50.622
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-f019b0aa-6eb4-412e-b07b-b2c426353cf7 08/29/23 20:52:50.625
    STEP: Creating a pod to test consume secrets 08/29/23 20:52:50.631
    Aug 29 20:52:50.640: INFO: Waiting up to 5m0s for pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd" in namespace "secrets-2416" to be "Succeeded or Failed"
    Aug 29 20:52:50.643: INFO: Pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.85786ms
    Aug 29 20:52:52.649: INFO: Pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008668057s
    Aug 29 20:52:54.649: INFO: Pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009245258s
    STEP: Saw pod success 08/29/23 20:52:54.649
    Aug 29 20:52:54.649: INFO: Pod "pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd" satisfied condition "Succeeded or Failed"
    Aug 29 20:52:54.652: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd container secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:52:54.674
    Aug 29 20:52:54.691: INFO: Waiting for pod pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd to disappear
    Aug 29 20:52:54.694: INFO: Pod pod-secrets-7a8f75da-9625-4b95-8e78-6af0aa1779dd no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:52:54.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2416" for this suite. 08/29/23 20:52:54.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:52:54.708
Aug 29 20:52:54.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename replication-controller 08/29/23 20:52:54.709
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:54.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:54.735
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 08/29/23 20:52:54.738
Aug 29 20:52:54.752: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7083" to be "running and ready"
Aug 29 20:52:54.756: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.823501ms
Aug 29 20:52:54.756: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:52:56.762: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010476253s
Aug 29 20:52:56.762: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 29 20:52:56.762: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/29/23 20:52:56.765
STEP: Then the orphan pod is adopted 08/29/23 20:52:56.771
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Aug 29 20:52:57.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7083" for this suite. 08/29/23 20:52:57.785
------------------------------
• [3.085 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:52:54.708
    Aug 29 20:52:54.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename replication-controller 08/29/23 20:52:54.709
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:54.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:54.735
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/29/23 20:52:54.738
    Aug 29 20:52:54.752: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-7083" to be "running and ready"
    Aug 29 20:52:54.756: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 4.823501ms
    Aug 29 20:52:54.756: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:52:56.762: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.010476253s
    Aug 29 20:52:56.762: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 29 20:52:56.762: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/29/23 20:52:56.765
    STEP: Then the orphan pod is adopted 08/29/23 20:52:56.771
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:52:57.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7083" for this suite. 08/29/23 20:52:57.785
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:52:57.793
Aug 29 20:52:57.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename containers 08/29/23 20:52:57.794
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:57.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:57.817
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 08/29/23 20:52:57.82
Aug 29 20:52:57.829: INFO: Waiting up to 5m0s for pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570" in namespace "containers-8169" to be "Succeeded or Failed"
Aug 29 20:52:57.832: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478322ms
Aug 29 20:52:59.838: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570": Phase="Running", Reason="", readiness=true. Elapsed: 2.00923343s
Aug 29 20:53:01.838: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570": Phase="Running", Reason="", readiness=false. Elapsed: 4.009031007s
Aug 29 20:53:03.837: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008487359s
STEP: Saw pod success 08/29/23 20:53:03.837
Aug 29 20:53:03.838: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570" satisfied condition "Succeeded or Failed"
Aug 29 20:53:03.841: INFO: Trying to get logs from node loki-15bd39-worker-1 pod client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570 container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:53:03.85
Aug 29 20:53:03.868: INFO: Waiting for pod client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570 to disappear
Aug 29 20:53:03.871: INFO: Pod client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 29 20:53:03.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8169" for this suite. 08/29/23 20:53:03.876
------------------------------
• [SLOW TEST] [6.090 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:52:57.793
    Aug 29 20:52:57.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename containers 08/29/23 20:52:57.794
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:52:57.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:52:57.817
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 08/29/23 20:52:57.82
    Aug 29 20:52:57.829: INFO: Waiting up to 5m0s for pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570" in namespace "containers-8169" to be "Succeeded or Failed"
    Aug 29 20:52:57.832: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478322ms
    Aug 29 20:52:59.838: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570": Phase="Running", Reason="", readiness=true. Elapsed: 2.00923343s
    Aug 29 20:53:01.838: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570": Phase="Running", Reason="", readiness=false. Elapsed: 4.009031007s
    Aug 29 20:53:03.837: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008487359s
    STEP: Saw pod success 08/29/23 20:53:03.837
    Aug 29 20:53:03.838: INFO: Pod "client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570" satisfied condition "Succeeded or Failed"
    Aug 29 20:53:03.841: INFO: Trying to get logs from node loki-15bd39-worker-1 pod client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570 container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:53:03.85
    Aug 29 20:53:03.868: INFO: Waiting for pod client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570 to disappear
    Aug 29 20:53:03.871: INFO: Pod client-containers-c7a896e5-e46e-435b-9836-d0fd3ec98570 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:53:03.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8169" for this suite. 08/29/23 20:53:03.876
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:53:03.885
Aug 29 20:53:03.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:53:03.886
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:53:03.905
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:53:03.909
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-1858 08/29/23 20:53:03.913
STEP: creating service affinity-clusterip-transition in namespace services-1858 08/29/23 20:53:03.913
STEP: creating replication controller affinity-clusterip-transition in namespace services-1858 08/29/23 20:53:03.928
I0829 20:53:03.936353      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1858, replica count: 3
I0829 20:53:06.988066      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 20:53:06.998: INFO: Creating new exec pod
Aug 29 20:53:07.010: INFO: Waiting up to 5m0s for pod "execpod-affinitygxjff" in namespace "services-1858" to be "running"
Aug 29 20:53:07.014: INFO: Pod "execpod-affinitygxjff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349079ms
Aug 29 20:53:09.018: INFO: Pod "execpod-affinitygxjff": Phase="Running", Reason="", readiness=true. Elapsed: 2.008155284s
Aug 29 20:53:09.019: INFO: Pod "execpod-affinitygxjff" satisfied condition "running"
Aug 29 20:53:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Aug 29 20:53:12.348: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 29 20:53:12.348: INFO: stdout: ""
Aug 29 20:53:12.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c nc -v -z -w 2 172.19.206.173 80'
Aug 29 20:53:12.546: INFO: stderr: "+ nc -v -z -w 2 172.19.206.173 80\nConnection to 172.19.206.173 80 port [tcp/http] succeeded!\n"
Aug 29 20:53:12.546: INFO: stdout: ""
Aug 29 20:53:12.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.206.173:80/ ; done'
Aug 29 20:53:12.813: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n"
Aug 29 20:53:12.813: INFO: stdout: "\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk"
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:42.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.206.173:80/ ; done'
Aug 29 20:53:43.074: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n"
Aug 29 20:53:43.074: INFO: stdout: "\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-cqfn9"
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.206.173:80/ ; done'
Aug 29 20:53:43.354: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n"
Aug 29 20:53:43.355: INFO: stdout: "\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-z7fmk"
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-z7fmk
Aug 29 20:54:13.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.206.173:80/ ; done'
Aug 29 20:54:13.624: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n"
Aug 29 20:54:13.624: INFO: stdout: "\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9"
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
Aug 29 20:54:13.624: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1858, will wait for the garbage collector to delete the pods 08/29/23 20:54:13.64
Aug 29 20:54:13.703: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.594077ms
Aug 29 20:54:13.803: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.342223ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:54:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1858" for this suite. 08/29/23 20:54:16.245
------------------------------
• [SLOW TEST] [72.369 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:53:03.885
    Aug 29 20:53:03.885: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:53:03.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:53:03.905
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:53:03.909
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-1858 08/29/23 20:53:03.913
    STEP: creating service affinity-clusterip-transition in namespace services-1858 08/29/23 20:53:03.913
    STEP: creating replication controller affinity-clusterip-transition in namespace services-1858 08/29/23 20:53:03.928
    I0829 20:53:03.936353      19 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-1858, replica count: 3
    I0829 20:53:06.988066      19 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 20:53:06.998: INFO: Creating new exec pod
    Aug 29 20:53:07.010: INFO: Waiting up to 5m0s for pod "execpod-affinitygxjff" in namespace "services-1858" to be "running"
    Aug 29 20:53:07.014: INFO: Pod "execpod-affinitygxjff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349079ms
    Aug 29 20:53:09.018: INFO: Pod "execpod-affinitygxjff": Phase="Running", Reason="", readiness=true. Elapsed: 2.008155284s
    Aug 29 20:53:09.019: INFO: Pod "execpod-affinitygxjff" satisfied condition "running"
    Aug 29 20:53:10.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Aug 29 20:53:12.348: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 29 20:53:12.348: INFO: stdout: ""
    Aug 29 20:53:12.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c nc -v -z -w 2 172.19.206.173 80'
    Aug 29 20:53:12.546: INFO: stderr: "+ nc -v -z -w 2 172.19.206.173 80\nConnection to 172.19.206.173 80 port [tcp/http] succeeded!\n"
    Aug 29 20:53:12.546: INFO: stdout: ""
    Aug 29 20:53:12.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.206.173:80/ ; done'
    Aug 29 20:53:12.813: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n"
    Aug 29 20:53:12.813: INFO: stdout: "\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk"
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:12.813: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:42.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.206.173:80/ ; done'
    Aug 29 20:53:43.074: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n"
    Aug 29 20:53:43.074: INFO: stdout: "\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-cqfn9"
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:43.074: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.206.173:80/ ; done'
    Aug 29 20:53:43.354: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n"
    Aug 29 20:53:43.355: INFO: stdout: "\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-kgf8s\naffinity-clusterip-transition-z7fmk\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-z7fmk"
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-kgf8s
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:53:43.355: INFO: Received response from host: affinity-clusterip-transition-z7fmk
    Aug 29 20:54:13.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1858 exec execpod-affinitygxjff -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.206.173:80/ ; done'
    Aug 29 20:54:13.624: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.206.173:80/\n"
    Aug 29 20:54:13.624: INFO: stdout: "\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9\naffinity-clusterip-transition-cqfn9"
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Received response from host: affinity-clusterip-transition-cqfn9
    Aug 29 20:54:13.624: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-1858, will wait for the garbage collector to delete the pods 08/29/23 20:54:13.64
    Aug 29 20:54:13.703: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.594077ms
    Aug 29 20:54:13.803: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.342223ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:54:16.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1858" for this suite. 08/29/23 20:54:16.245
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:54:16.254
Aug 29 20:54:16.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename daemonsets 08/29/23 20:54:16.255
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:16.276
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:16.283
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:157
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177
STEP: Creating simple DaemonSet "daemon-set" 08/29/23 20:54:16.314
STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 20:54:16.32
Aug 29 20:54:16.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:54:16.330: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:54:17.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:54:17.343: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:54:18.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:54:18.340: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
Aug 29 20:54:19.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 20:54:19.342: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/29/23 20:54:19.345
Aug 29 20:54:19.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:54:19.369: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 20:54:20.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:54:20.380: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 20:54:21.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:54:21.389: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 20:54:22.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
Aug 29 20:54:22.380: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
Aug 29 20:54:23.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
Aug 29 20:54:23.379: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:122
STEP: Deleting DaemonSet "daemon-set" 08/29/23 20:54:23.382
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7419, will wait for the garbage collector to delete the pods 08/29/23 20:54:23.383
Aug 29 20:54:23.444: INFO: Deleting DaemonSet.extensions daemon-set took: 7.628187ms
Aug 29 20:54:23.545: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.30316ms
Aug 29 20:54:26.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 29 20:54:26.149: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 29 20:54:26.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45813"},"items":null}

Aug 29 20:54:26.160: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45813"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:54:26.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7419" for this suite. 08/29/23 20:54:26.185
------------------------------
• [SLOW TEST] [9.939 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:54:16.254
    Aug 29 20:54:16.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename daemonsets 08/29/23 20:54:16.255
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:16.276
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:16.283
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:157
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:177
    STEP: Creating simple DaemonSet "daemon-set" 08/29/23 20:54:16.314
    STEP: Check that daemon pods launch on every node of the cluster. 08/29/23 20:54:16.32
    Aug 29 20:54:16.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:54:16.330: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:54:17.343: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:54:17.343: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:54:18.340: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:54:18.340: INFO: Node loki-15bd39-master-0 is running 0 daemon pod, expected 1
    Aug 29 20:54:19.342: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 20:54:19.342: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/29/23 20:54:19.345
    Aug 29 20:54:19.369: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:54:19.369: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 20:54:20.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:54:20.380: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 20:54:21.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:54:21.389: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 20:54:22.380: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 4
    Aug 29 20:54:22.380: INFO: Node loki-15bd39-master-1 is running 0 daemon pod, expected 1
    Aug 29 20:54:23.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 5
    Aug 29 20:54:23.379: INFO: Number of running nodes: 5, number of available pods: 5 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:122
    STEP: Deleting DaemonSet "daemon-set" 08/29/23 20:54:23.382
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7419, will wait for the garbage collector to delete the pods 08/29/23 20:54:23.383
    Aug 29 20:54:23.444: INFO: Deleting DaemonSet.extensions daemon-set took: 7.628187ms
    Aug 29 20:54:23.545: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.30316ms
    Aug 29 20:54:26.149: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 29 20:54:26.149: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 29 20:54:26.156: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"45813"},"items":null}

    Aug 29 20:54:26.160: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"45813"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:54:26.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7419" for this suite. 08/29/23 20:54:26.185
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:54:26.194
Aug 29 20:54:26.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:54:26.195
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:26.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:26.216
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-1126 08/29/23 20:54:26.219
STEP: creating service affinity-clusterip in namespace services-1126 08/29/23 20:54:26.219
STEP: creating replication controller affinity-clusterip in namespace services-1126 08/29/23 20:54:26.232
I0829 20:54:26.253983      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1126, replica count: 3
I0829 20:54:29.305285      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 29 20:54:29.312: INFO: Creating new exec pod
Aug 29 20:54:29.321: INFO: Waiting up to 5m0s for pod "execpod-affinity6jxtp" in namespace "services-1126" to be "running"
Aug 29 20:54:29.324: INFO: Pod "execpod-affinity6jxtp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.836072ms
Aug 29 20:54:31.328: INFO: Pod "execpod-affinity6jxtp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007170585s
Aug 29 20:54:31.328: INFO: Pod "execpod-affinity6jxtp" satisfied condition "running"
Aug 29 20:54:32.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1126 exec execpod-affinity6jxtp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Aug 29 20:54:32.512: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 29 20:54:32.512: INFO: stdout: ""
Aug 29 20:54:32.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1126 exec execpod-affinity6jxtp -- /bin/sh -x -c nc -v -z -w 2 172.19.179.131 80'
Aug 29 20:54:32.689: INFO: stderr: "+ nc -v -z -w 2 172.19.179.131 80\nConnection to 172.19.179.131 80 port [tcp/http] succeeded!\n"
Aug 29 20:54:32.689: INFO: stdout: ""
Aug 29 20:54:32.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1126 exec execpod-affinity6jxtp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.179.131:80/ ; done'
Aug 29 20:54:32.949: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n"
Aug 29 20:54:32.949: INFO: stdout: "\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86"
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
Aug 29 20:54:32.949: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1126, will wait for the garbage collector to delete the pods 08/29/23 20:54:32.963
Aug 29 20:54:33.026: INFO: Deleting ReplicationController affinity-clusterip took: 8.085024ms
Aug 29 20:54:33.126: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.442772ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:54:35.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1126" for this suite. 08/29/23 20:54:35.357
------------------------------
• [SLOW TEST] [9.170 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:54:26.194
    Aug 29 20:54:26.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:54:26.195
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:26.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:26.216
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-1126 08/29/23 20:54:26.219
    STEP: creating service affinity-clusterip in namespace services-1126 08/29/23 20:54:26.219
    STEP: creating replication controller affinity-clusterip in namespace services-1126 08/29/23 20:54:26.232
    I0829 20:54:26.253983      19 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1126, replica count: 3
    I0829 20:54:29.305285      19 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 29 20:54:29.312: INFO: Creating new exec pod
    Aug 29 20:54:29.321: INFO: Waiting up to 5m0s for pod "execpod-affinity6jxtp" in namespace "services-1126" to be "running"
    Aug 29 20:54:29.324: INFO: Pod "execpod-affinity6jxtp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.836072ms
    Aug 29 20:54:31.328: INFO: Pod "execpod-affinity6jxtp": Phase="Running", Reason="", readiness=true. Elapsed: 2.007170585s
    Aug 29 20:54:31.328: INFO: Pod "execpod-affinity6jxtp" satisfied condition "running"
    Aug 29 20:54:32.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1126 exec execpod-affinity6jxtp -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Aug 29 20:54:32.512: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 29 20:54:32.512: INFO: stdout: ""
    Aug 29 20:54:32.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1126 exec execpod-affinity6jxtp -- /bin/sh -x -c nc -v -z -w 2 172.19.179.131 80'
    Aug 29 20:54:32.689: INFO: stderr: "+ nc -v -z -w 2 172.19.179.131 80\nConnection to 172.19.179.131 80 port [tcp/http] succeeded!\n"
    Aug 29 20:54:32.689: INFO: stdout: ""
    Aug 29 20:54:32.689: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-1126 exec execpod-affinity6jxtp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.19.179.131:80/ ; done'
    Aug 29 20:54:32.949: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.19.179.131:80/\n"
    Aug 29 20:54:32.949: INFO: stdout: "\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86\naffinity-clusterip-88j86"
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Received response from host: affinity-clusterip-88j86
    Aug 29 20:54:32.949: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1126, will wait for the garbage collector to delete the pods 08/29/23 20:54:32.963
    Aug 29 20:54:33.026: INFO: Deleting ReplicationController affinity-clusterip took: 8.085024ms
    Aug 29 20:54:33.126: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.442772ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:54:35.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1126" for this suite. 08/29/23 20:54:35.357
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:54:35.365
Aug 29 20:54:35.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename watch 08/29/23 20:54:35.366
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:35.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:35.386
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/29/23 20:54:35.39
STEP: creating a watch on configmaps with label B 08/29/23 20:54:35.392
STEP: creating a watch on configmaps with label A or B 08/29/23 20:54:35.393
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/29/23 20:54:35.394
Aug 29 20:54:35.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46003 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:54:35.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46003 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/29/23 20:54:35.401
Aug 29 20:54:35.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46004 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:54:35.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46004 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/29/23 20:54:35.41
Aug 29 20:54:35.417: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46005 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:54:35.417: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46005 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/29/23 20:54:35.417
Aug 29 20:54:35.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46006 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:54:35.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46006 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/29/23 20:54:35.424
Aug 29 20:54:35.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9344  6e5612f9-ddc8-4a05-81ca-25a342abe30d 46007 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:54:35.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9344  6e5612f9-ddc8-4a05-81ca-25a342abe30d 46007 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/29/23 20:54:45.432
Aug 29 20:54:45.440: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9344  6e5612f9-ddc8-4a05-81ca-25a342abe30d 46066 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 29 20:54:45.440: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9344  6e5612f9-ddc8-4a05-81ca-25a342abe30d 46066 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 29 20:54:55.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-9344" for this suite. 08/29/23 20:54:55.449
------------------------------
• [SLOW TEST] [20.091 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:54:35.365
    Aug 29 20:54:35.365: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename watch 08/29/23 20:54:35.366
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:35.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:35.386
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/29/23 20:54:35.39
    STEP: creating a watch on configmaps with label B 08/29/23 20:54:35.392
    STEP: creating a watch on configmaps with label A or B 08/29/23 20:54:35.393
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/29/23 20:54:35.394
    Aug 29 20:54:35.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46003 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:54:35.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46003 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/29/23 20:54:35.401
    Aug 29 20:54:35.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46004 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:54:35.409: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46004 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/29/23 20:54:35.41
    Aug 29 20:54:35.417: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46005 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:54:35.417: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46005 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/29/23 20:54:35.417
    Aug 29 20:54:35.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46006 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:54:35.424: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9344  64b58844-33ac-496c-b353-e9878dec9dac 46006 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/29/23 20:54:35.424
    Aug 29 20:54:35.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9344  6e5612f9-ddc8-4a05-81ca-25a342abe30d 46007 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:54:35.431: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9344  6e5612f9-ddc8-4a05-81ca-25a342abe30d 46007 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/29/23 20:54:45.432
    Aug 29 20:54:45.440: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9344  6e5612f9-ddc8-4a05-81ca-25a342abe30d 46066 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 29 20:54:45.440: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9344  6e5612f9-ddc8-4a05-81ca-25a342abe30d 46066 0 2023-08-29 20:54:35 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-29 20:54:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:54:55.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-9344" for this suite. 08/29/23 20:54:55.449
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:54:55.456
Aug 29 20:54:55.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:54:55.457
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:55.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:55.483
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-a978ef27-52b0-4078-b36d-1bce14fbf700 08/29/23 20:54:55.485
STEP: Creating a pod to test consume secrets 08/29/23 20:54:55.491
Aug 29 20:54:55.501: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c" in namespace "projected-9402" to be "Succeeded or Failed"
Aug 29 20:54:55.504: INFO: Pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.540709ms
Aug 29 20:54:57.509: INFO: Pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008129043s
Aug 29 20:54:59.508: INFO: Pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00767461s
STEP: Saw pod success 08/29/23 20:54:59.508
Aug 29 20:54:59.509: INFO: Pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c" satisfied condition "Succeeded or Failed"
Aug 29 20:54:59.513: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c container secret-volume-test: <nil>
STEP: delete the pod 08/29/23 20:54:59.531
Aug 29 20:54:59.545: INFO: Waiting for pod pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c to disappear
Aug 29 20:54:59.548: INFO: Pod pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 29 20:54:59.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-9402" for this suite. 08/29/23 20:54:59.552
------------------------------
• [4.103 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:54:55.456
    Aug 29 20:54:55.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:54:55.457
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:55.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:55.483
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-a978ef27-52b0-4078-b36d-1bce14fbf700 08/29/23 20:54:55.485
    STEP: Creating a pod to test consume secrets 08/29/23 20:54:55.491
    Aug 29 20:54:55.501: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c" in namespace "projected-9402" to be "Succeeded or Failed"
    Aug 29 20:54:55.504: INFO: Pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.540709ms
    Aug 29 20:54:57.509: INFO: Pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008129043s
    Aug 29 20:54:59.508: INFO: Pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00767461s
    STEP: Saw pod success 08/29/23 20:54:59.508
    Aug 29 20:54:59.509: INFO: Pod "pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c" satisfied condition "Succeeded or Failed"
    Aug 29 20:54:59.513: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c container secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:54:59.531
    Aug 29 20:54:59.545: INFO: Waiting for pod pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c to disappear
    Aug 29 20:54:59.548: INFO: Pod pod-projected-secrets-8e070a33-d3c9-4e48-bd66-debcc3ee866c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:54:59.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-9402" for this suite. 08/29/23 20:54:59.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:54:59.561
Aug 29 20:54:59.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:54:59.562
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:59.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:59.586
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/29/23 20:54:59.589
Aug 29 20:54:59.599: INFO: Waiting up to 5m0s for pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e" in namespace "emptydir-6549" to be "Succeeded or Failed"
Aug 29 20:54:59.603: INFO: Pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525994ms
Aug 29 20:55:01.607: INFO: Pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008008114s
Aug 29 20:55:03.608: INFO: Pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00830301s
STEP: Saw pod success 08/29/23 20:55:03.608
Aug 29 20:55:03.608: INFO: Pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e" satisfied condition "Succeeded or Failed"
Aug 29 20:55:03.611: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-f38033d8-06ca-4059-b730-89bb4fffef4e container test-container: <nil>
STEP: delete the pod 08/29/23 20:55:03.62
Aug 29 20:55:03.633: INFO: Waiting for pod pod-f38033d8-06ca-4059-b730-89bb4fffef4e to disappear
Aug 29 20:55:03.637: INFO: Pod pod-f38033d8-06ca-4059-b730-89bb4fffef4e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6549" for this suite. 08/29/23 20:55:03.642
------------------------------
• [4.092 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:54:59.561
    Aug 29 20:54:59.561: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:54:59.562
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:54:59.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:54:59.586
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/29/23 20:54:59.589
    Aug 29 20:54:59.599: INFO: Waiting up to 5m0s for pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e" in namespace "emptydir-6549" to be "Succeeded or Failed"
    Aug 29 20:54:59.603: INFO: Pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.525994ms
    Aug 29 20:55:01.607: INFO: Pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008008114s
    Aug 29 20:55:03.608: INFO: Pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00830301s
    STEP: Saw pod success 08/29/23 20:55:03.608
    Aug 29 20:55:03.608: INFO: Pod "pod-f38033d8-06ca-4059-b730-89bb4fffef4e" satisfied condition "Succeeded or Failed"
    Aug 29 20:55:03.611: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-f38033d8-06ca-4059-b730-89bb4fffef4e container test-container: <nil>
    STEP: delete the pod 08/29/23 20:55:03.62
    Aug 29 20:55:03.633: INFO: Waiting for pod pod-f38033d8-06ca-4059-b730-89bb4fffef4e to disappear
    Aug 29 20:55:03.637: INFO: Pod pod-f38033d8-06ca-4059-b730-89bb4fffef4e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:03.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6549" for this suite. 08/29/23 20:55:03.642
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:03.653
Aug 29 20:55:03.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:55:03.655
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:03.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:03.679
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-b12b0149-bc62-496d-8afa-0feee043a63d 08/29/23 20:55:03.682
STEP: Creating a pod to test consume configMaps 08/29/23 20:55:03.687
Aug 29 20:55:03.699: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e" in namespace "projected-4487" to be "Succeeded or Failed"
Aug 29 20:55:03.702: INFO: Pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258261ms
Aug 29 20:55:05.708: INFO: Pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00946351s
Aug 29 20:55:07.706: INFO: Pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007385801s
STEP: Saw pod success 08/29/23 20:55:07.706
Aug 29 20:55:07.706: INFO: Pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e" satisfied condition "Succeeded or Failed"
Aug 29 20:55:07.710: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:55:07.719
Aug 29 20:55:07.733: INFO: Waiting for pod pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e to disappear
Aug 29 20:55:07.739: INFO: Pod pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:07.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4487" for this suite. 08/29/23 20:55:07.745
------------------------------
• [4.101 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:03.653
    Aug 29 20:55:03.654: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:55:03.655
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:03.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:03.679
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-b12b0149-bc62-496d-8afa-0feee043a63d 08/29/23 20:55:03.682
    STEP: Creating a pod to test consume configMaps 08/29/23 20:55:03.687
    Aug 29 20:55:03.699: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e" in namespace "projected-4487" to be "Succeeded or Failed"
    Aug 29 20:55:03.702: INFO: Pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.258261ms
    Aug 29 20:55:05.708: INFO: Pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00946351s
    Aug 29 20:55:07.706: INFO: Pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007385801s
    STEP: Saw pod success 08/29/23 20:55:07.706
    Aug 29 20:55:07.706: INFO: Pod "pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e" satisfied condition "Succeeded or Failed"
    Aug 29 20:55:07.710: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:55:07.719
    Aug 29 20:55:07.733: INFO: Waiting for pod pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e to disappear
    Aug 29 20:55:07.739: INFO: Pod pod-projected-configmaps-cd8de0ec-635f-449b-ac7d-d117ddc4366e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:07.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4487" for this suite. 08/29/23 20:55:07.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:07.756
Aug 29 20:55:07.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename ingressclass 08/29/23 20:55:07.756
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:07.774
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:07.777
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/29/23 20:55:07.779
STEP: getting /apis/networking.k8s.io 08/29/23 20:55:07.782
STEP: getting /apis/networking.k8s.iov1 08/29/23 20:55:07.783
STEP: creating 08/29/23 20:55:07.784
STEP: getting 08/29/23 20:55:07.801
STEP: listing 08/29/23 20:55:07.804
STEP: watching 08/29/23 20:55:07.807
Aug 29 20:55:07.807: INFO: starting watch
STEP: patching 08/29/23 20:55:07.809
STEP: updating 08/29/23 20:55:07.815
Aug 29 20:55:07.819: INFO: waiting for watch events with expected annotations
Aug 29 20:55:07.819: INFO: saw patched and updated annotations
STEP: deleting 08/29/23 20:55:07.819
STEP: deleting a collection 08/29/23 20:55:07.837
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:07.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-9441" for this suite. 08/29/23 20:55:07.861
------------------------------
• [0.118 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:07.756
    Aug 29 20:55:07.756: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename ingressclass 08/29/23 20:55:07.756
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:07.774
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:07.777
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/29/23 20:55:07.779
    STEP: getting /apis/networking.k8s.io 08/29/23 20:55:07.782
    STEP: getting /apis/networking.k8s.iov1 08/29/23 20:55:07.783
    STEP: creating 08/29/23 20:55:07.784
    STEP: getting 08/29/23 20:55:07.801
    STEP: listing 08/29/23 20:55:07.804
    STEP: watching 08/29/23 20:55:07.807
    Aug 29 20:55:07.807: INFO: starting watch
    STEP: patching 08/29/23 20:55:07.809
    STEP: updating 08/29/23 20:55:07.815
    Aug 29 20:55:07.819: INFO: waiting for watch events with expected annotations
    Aug 29 20:55:07.819: INFO: saw patched and updated annotations
    STEP: deleting 08/29/23 20:55:07.819
    STEP: deleting a collection 08/29/23 20:55:07.837
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:07.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-9441" for this suite. 08/29/23 20:55:07.861
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:07.874
Aug 29 20:55:07.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 20:55:07.876
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:07.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:07.898
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:55:07.901
Aug 29 20:55:07.911: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0" in namespace "downward-api-8697" to be "Succeeded or Failed"
Aug 29 20:55:07.922: INFO: Pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.284819ms
Aug 29 20:55:09.926: INFO: Pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015464878s
Aug 29 20:55:11.926: INFO: Pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015275251s
STEP: Saw pod success 08/29/23 20:55:11.926
Aug 29 20:55:11.926: INFO: Pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0" satisfied condition "Succeeded or Failed"
Aug 29 20:55:11.930: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0 container client-container: <nil>
STEP: delete the pod 08/29/23 20:55:11.938
Aug 29 20:55:11.952: INFO: Waiting for pod downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0 to disappear
Aug 29 20:55:11.955: INFO: Pod downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:11.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8697" for this suite. 08/29/23 20:55:11.96
------------------------------
• [4.095 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:07.874
    Aug 29 20:55:07.874: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 20:55:07.876
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:07.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:07.898
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:55:07.901
    Aug 29 20:55:07.911: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0" in namespace "downward-api-8697" to be "Succeeded or Failed"
    Aug 29 20:55:07.922: INFO: Pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.284819ms
    Aug 29 20:55:09.926: INFO: Pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015464878s
    Aug 29 20:55:11.926: INFO: Pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015275251s
    STEP: Saw pod success 08/29/23 20:55:11.926
    Aug 29 20:55:11.926: INFO: Pod "downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0" satisfied condition "Succeeded or Failed"
    Aug 29 20:55:11.930: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0 container client-container: <nil>
    STEP: delete the pod 08/29/23 20:55:11.938
    Aug 29 20:55:11.952: INFO: Waiting for pod downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0 to disappear
    Aug 29 20:55:11.955: INFO: Pod downwardapi-volume-cb542a6a-dfa4-42c1-8d1a-b4f3cb1e9ca0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:11.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8697" for this suite. 08/29/23 20:55:11.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:11.971
Aug 29 20:55:11.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename downward-api 08/29/23 20:55:11.972
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:11.991
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:11.995
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:55:11.998
Aug 29 20:55:12.009: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d" in namespace "downward-api-5632" to be "Succeeded or Failed"
Aug 29 20:55:12.013: INFO: Pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368029ms
Aug 29 20:55:14.019: INFO: Pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009988878s
Aug 29 20:55:16.019: INFO: Pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009930754s
STEP: Saw pod success 08/29/23 20:55:16.019
Aug 29 20:55:16.019: INFO: Pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d" satisfied condition "Succeeded or Failed"
Aug 29 20:55:16.022: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d container client-container: <nil>
STEP: delete the pod 08/29/23 20:55:16.03
Aug 29 20:55:16.046: INFO: Waiting for pod downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d to disappear
Aug 29 20:55:16.050: INFO: Pod downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:16.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5632" for this suite. 08/29/23 20:55:16.055
------------------------------
• [4.091 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:11.971
    Aug 29 20:55:11.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename downward-api 08/29/23 20:55:11.972
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:11.991
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:11.995
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:55:11.998
    Aug 29 20:55:12.009: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d" in namespace "downward-api-5632" to be "Succeeded or Failed"
    Aug 29 20:55:12.013: INFO: Pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.368029ms
    Aug 29 20:55:14.019: INFO: Pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009988878s
    Aug 29 20:55:16.019: INFO: Pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009930754s
    STEP: Saw pod success 08/29/23 20:55:16.019
    Aug 29 20:55:16.019: INFO: Pod "downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d" satisfied condition "Succeeded or Failed"
    Aug 29 20:55:16.022: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d container client-container: <nil>
    STEP: delete the pod 08/29/23 20:55:16.03
    Aug 29 20:55:16.046: INFO: Waiting for pod downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d to disappear
    Aug 29 20:55:16.050: INFO: Pod downwardapi-volume-5bba1447-5f6b-4cc7-a526-ae769256196d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:16.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5632" for this suite. 08/29/23 20:55:16.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:16.062
Aug 29 20:55:16.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename events 08/29/23 20:55:16.064
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:16.081
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:16.084
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/29/23 20:55:16.087
STEP: listing events in all namespaces 08/29/23 20:55:16.099
STEP: listing events in test namespace 08/29/23 20:55:16.104
STEP: listing events with field selection filtering on source 08/29/23 20:55:16.108
STEP: listing events with field selection filtering on reportingController 08/29/23 20:55:16.112
STEP: getting the test event 08/29/23 20:55:16.115
STEP: patching the test event 08/29/23 20:55:16.118
STEP: getting the test event 08/29/23 20:55:16.132
STEP: updating the test event 08/29/23 20:55:16.135
STEP: getting the test event 08/29/23 20:55:16.144
STEP: deleting the test event 08/29/23 20:55:16.147
STEP: listing events in all namespaces 08/29/23 20:55:16.155
STEP: listing events in test namespace 08/29/23 20:55:16.16
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:16.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-5557" for this suite. 08/29/23 20:55:16.168
------------------------------
• [0.113 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:16.062
    Aug 29 20:55:16.063: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename events 08/29/23 20:55:16.064
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:16.081
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:16.084
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/29/23 20:55:16.087
    STEP: listing events in all namespaces 08/29/23 20:55:16.099
    STEP: listing events in test namespace 08/29/23 20:55:16.104
    STEP: listing events with field selection filtering on source 08/29/23 20:55:16.108
    STEP: listing events with field selection filtering on reportingController 08/29/23 20:55:16.112
    STEP: getting the test event 08/29/23 20:55:16.115
    STEP: patching the test event 08/29/23 20:55:16.118
    STEP: getting the test event 08/29/23 20:55:16.132
    STEP: updating the test event 08/29/23 20:55:16.135
    STEP: getting the test event 08/29/23 20:55:16.144
    STEP: deleting the test event 08/29/23 20:55:16.147
    STEP: listing events in all namespaces 08/29/23 20:55:16.155
    STEP: listing events in test namespace 08/29/23 20:55:16.16
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:16.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-5557" for this suite. 08/29/23 20:55:16.168
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:16.175
Aug 29 20:55:16.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename var-expansion 08/29/23 20:55:16.176
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:16.197
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:16.2
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 08/29/23 20:55:16.203
Aug 29 20:55:16.212: INFO: Waiting up to 5m0s for pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa" in namespace "var-expansion-1280" to be "Succeeded or Failed"
Aug 29 20:55:16.215: INFO: Pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213151ms
Aug 29 20:55:18.220: INFO: Pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007572097s
Aug 29 20:55:20.222: INFO: Pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009748218s
STEP: Saw pod success 08/29/23 20:55:20.222
Aug 29 20:55:20.222: INFO: Pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa" satisfied condition "Succeeded or Failed"
Aug 29 20:55:20.225: INFO: Trying to get logs from node loki-15bd39-worker-1 pod var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa container dapi-container: <nil>
STEP: delete the pod 08/29/23 20:55:20.233
Aug 29 20:55:20.249: INFO: Waiting for pod var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa to disappear
Aug 29 20:55:20.253: INFO: Pod var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:20.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-1280" for this suite. 08/29/23 20:55:20.257
------------------------------
• [4.089 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:16.175
    Aug 29 20:55:16.175: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename var-expansion 08/29/23 20:55:16.176
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:16.197
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:16.2
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 08/29/23 20:55:16.203
    Aug 29 20:55:16.212: INFO: Waiting up to 5m0s for pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa" in namespace "var-expansion-1280" to be "Succeeded or Failed"
    Aug 29 20:55:16.215: INFO: Pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa": Phase="Pending", Reason="", readiness=false. Elapsed: 3.213151ms
    Aug 29 20:55:18.220: INFO: Pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007572097s
    Aug 29 20:55:20.222: INFO: Pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009748218s
    STEP: Saw pod success 08/29/23 20:55:20.222
    Aug 29 20:55:20.222: INFO: Pod "var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa" satisfied condition "Succeeded or Failed"
    Aug 29 20:55:20.225: INFO: Trying to get logs from node loki-15bd39-worker-1 pod var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa container dapi-container: <nil>
    STEP: delete the pod 08/29/23 20:55:20.233
    Aug 29 20:55:20.249: INFO: Waiting for pod var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa to disappear
    Aug 29 20:55:20.253: INFO: Pod var-expansion-2acb2dec-645c-4701-bb48-6bc656c3e6aa no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:20.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-1280" for this suite. 08/29/23 20:55:20.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:20.265
Aug 29 20:55:20.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:55:20.266
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:20.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:20.293
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 08/29/23 20:55:20.296
STEP: setting up watch 08/29/23 20:55:20.296
STEP: submitting the pod to kubernetes 08/29/23 20:55:20.402
STEP: verifying the pod is in kubernetes 08/29/23 20:55:20.415
STEP: verifying pod creation was observed 08/29/23 20:55:20.418
Aug 29 20:55:20.418: INFO: Waiting up to 5m0s for pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a" in namespace "pods-5772" to be "running"
Aug 29 20:55:20.424: INFO: Pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868776ms
Aug 29 20:55:22.428: INFO: Pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01001847s
Aug 29 20:55:24.428: INFO: Pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a": Phase="Running", Reason="", readiness=true. Elapsed: 4.010377987s
Aug 29 20:55:24.428: INFO: Pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a" satisfied condition "running"
STEP: deleting the pod gracefully 08/29/23 20:55:24.432
STEP: verifying pod deletion was observed 08/29/23 20:55:24.442
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:25.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5772" for this suite. 08/29/23 20:55:25.489
------------------------------
• [SLOW TEST] [5.231 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:20.265
    Aug 29 20:55:20.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:55:20.266
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:20.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:20.293
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 08/29/23 20:55:20.296
    STEP: setting up watch 08/29/23 20:55:20.296
    STEP: submitting the pod to kubernetes 08/29/23 20:55:20.402
    STEP: verifying the pod is in kubernetes 08/29/23 20:55:20.415
    STEP: verifying pod creation was observed 08/29/23 20:55:20.418
    Aug 29 20:55:20.418: INFO: Waiting up to 5m0s for pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a" in namespace "pods-5772" to be "running"
    Aug 29 20:55:20.424: INFO: Pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.868776ms
    Aug 29 20:55:22.428: INFO: Pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01001847s
    Aug 29 20:55:24.428: INFO: Pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a": Phase="Running", Reason="", readiness=true. Elapsed: 4.010377987s
    Aug 29 20:55:24.428: INFO: Pod "pod-submit-remove-c447411d-3767-433c-9f71-f37670d0058a" satisfied condition "running"
    STEP: deleting the pod gracefully 08/29/23 20:55:24.432
    STEP: verifying pod deletion was observed 08/29/23 20:55:24.442
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:25.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5772" for this suite. 08/29/23 20:55:25.489
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:25.498
Aug 29 20:55:25.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:55:25.499
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:25.516
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:25.519
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  08/29/23 20:55:25.522
Aug 29 20:55:25.533: INFO: Waiting up to 5m0s for pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef" in namespace "svcaccounts-8739" to be "Succeeded or Failed"
Aug 29 20:55:25.537: INFO: Pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.682865ms
Aug 29 20:55:27.542: INFO: Pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008504104s
Aug 29 20:55:29.541: INFO: Pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007947891s
STEP: Saw pod success 08/29/23 20:55:29.541
Aug 29 20:55:29.541: INFO: Pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef" satisfied condition "Succeeded or Failed"
Aug 29 20:55:29.545: INFO: Trying to get logs from node loki-15bd39-worker-1 pod test-pod-574c20ae-3e7b-407d-8788-281c398866ef container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:55:29.553
Aug 29 20:55:29.568: INFO: Waiting for pod test-pod-574c20ae-3e7b-407d-8788-281c398866ef to disappear
Aug 29 20:55:29.571: INFO: Pod test-pod-574c20ae-3e7b-407d-8788-281c398866ef no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:29.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8739" for this suite. 08/29/23 20:55:29.576
------------------------------
• [4.086 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:25.498
    Aug 29 20:55:25.498: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename svcaccounts 08/29/23 20:55:25.499
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:25.516
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:25.519
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  08/29/23 20:55:25.522
    Aug 29 20:55:25.533: INFO: Waiting up to 5m0s for pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef" in namespace "svcaccounts-8739" to be "Succeeded or Failed"
    Aug 29 20:55:25.537: INFO: Pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef": Phase="Pending", Reason="", readiness=false. Elapsed: 3.682865ms
    Aug 29 20:55:27.542: INFO: Pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008504104s
    Aug 29 20:55:29.541: INFO: Pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007947891s
    STEP: Saw pod success 08/29/23 20:55:29.541
    Aug 29 20:55:29.541: INFO: Pod "test-pod-574c20ae-3e7b-407d-8788-281c398866ef" satisfied condition "Succeeded or Failed"
    Aug 29 20:55:29.545: INFO: Trying to get logs from node loki-15bd39-worker-1 pod test-pod-574c20ae-3e7b-407d-8788-281c398866ef container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:55:29.553
    Aug 29 20:55:29.568: INFO: Waiting for pod test-pod-574c20ae-3e7b-407d-8788-281c398866ef to disappear
    Aug 29 20:55:29.571: INFO: Pod test-pod-574c20ae-3e7b-407d-8788-281c398866ef no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:29.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8739" for this suite. 08/29/23 20:55:29.576
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:29.584
Aug 29 20:55:29.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename services 08/29/23 20:55:29.585
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:29.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:29.61
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-8980 08/29/23 20:55:29.612
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[] 08/29/23 20:55:29.625
Aug 29 20:55:29.628: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Aug 29 20:55:30.638: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-8980 08/29/23 20:55:30.638
Aug 29 20:55:30.646: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8980" to be "running and ready"
Aug 29 20:55:30.649: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12598ms
Aug 29 20:55:30.649: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:55:32.654: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007454716s
Aug 29 20:55:32.654: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 29 20:55:32.654: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod1:[80]] 08/29/23 20:55:32.657
Aug 29 20:55:32.668: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/29/23 20:55:32.668
Aug 29 20:55:32.668: INFO: Creating new exec pod
Aug 29 20:55:32.673: INFO: Waiting up to 5m0s for pod "execpodglz2v" in namespace "services-8980" to be "running"
Aug 29 20:55:32.677: INFO: Pod "execpodglz2v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.499465ms
Aug 29 20:55:34.682: INFO: Pod "execpodglz2v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008248224s
Aug 29 20:55:34.682: INFO: Pod "execpodglz2v" satisfied condition "running"
Aug 29 20:55:35.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 29 20:55:35.843: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 20:55:35.843: INFO: stdout: ""
Aug 29 20:55:35.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 172.19.242.92 80'
Aug 29 20:55:36.011: INFO: stderr: "+ nc -v -z -w 2 172.19.242.92 80\nConnection to 172.19.242.92 80 port [tcp/http] succeeded!\n"
Aug 29 20:55:36.011: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-8980 08/29/23 20:55:36.011
Aug 29 20:55:36.020: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8980" to be "running and ready"
Aug 29 20:55:36.023: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506471ms
Aug 29 20:55:36.023: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:55:38.031: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010935584s
Aug 29 20:55:38.031: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 29 20:55:38.031: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod1:[80] pod2:[80]] 08/29/23 20:55:38.035
Aug 29 20:55:38.050: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/29/23 20:55:38.05
Aug 29 20:55:39.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 29 20:55:39.215: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 20:55:39.215: INFO: stdout: ""
Aug 29 20:55:39.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 172.19.242.92 80'
Aug 29 20:55:39.407: INFO: stderr: "+ nc -v -z -w 2 172.19.242.92 80\nConnection to 172.19.242.92 80 port [tcp/http] succeeded!\n"
Aug 29 20:55:39.407: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-8980 08/29/23 20:55:39.407
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod2:[80]] 08/29/23 20:55:39.422
Aug 29 20:55:40.450: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/29/23 20:55:40.45
Aug 29 20:55:41.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Aug 29 20:55:41.636: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 29 20:55:41.636: INFO: stdout: ""
Aug 29 20:55:41.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 172.19.242.92 80'
Aug 29 20:55:41.851: INFO: stderr: "+ nc -v -z -w 2 172.19.242.92 80\nConnection to 172.19.242.92 80 port [tcp/http] succeeded!\n"
Aug 29 20:55:41.851: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-8980 08/29/23 20:55:41.851
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[] 08/29/23 20:55:41.871
Aug 29 20:55:42.898: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:42.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-8980" for this suite. 08/29/23 20:55:42.925
------------------------------
• [SLOW TEST] [13.348 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:29.584
    Aug 29 20:55:29.584: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename services 08/29/23 20:55:29.585
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:29.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:29.61
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-8980 08/29/23 20:55:29.612
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[] 08/29/23 20:55:29.625
    Aug 29 20:55:29.628: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Aug 29 20:55:30.638: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-8980 08/29/23 20:55:30.638
    Aug 29 20:55:30.646: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-8980" to be "running and ready"
    Aug 29 20:55:30.649: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12598ms
    Aug 29 20:55:30.649: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:55:32.654: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007454716s
    Aug 29 20:55:32.654: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 29 20:55:32.654: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod1:[80]] 08/29/23 20:55:32.657
    Aug 29 20:55:32.668: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/29/23 20:55:32.668
    Aug 29 20:55:32.668: INFO: Creating new exec pod
    Aug 29 20:55:32.673: INFO: Waiting up to 5m0s for pod "execpodglz2v" in namespace "services-8980" to be "running"
    Aug 29 20:55:32.677: INFO: Pod "execpodglz2v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.499465ms
    Aug 29 20:55:34.682: INFO: Pod "execpodglz2v": Phase="Running", Reason="", readiness=true. Elapsed: 2.008248224s
    Aug 29 20:55:34.682: INFO: Pod "execpodglz2v" satisfied condition "running"
    Aug 29 20:55:35.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 29 20:55:35.843: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 29 20:55:35.843: INFO: stdout: ""
    Aug 29 20:55:35.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 172.19.242.92 80'
    Aug 29 20:55:36.011: INFO: stderr: "+ nc -v -z -w 2 172.19.242.92 80\nConnection to 172.19.242.92 80 port [tcp/http] succeeded!\n"
    Aug 29 20:55:36.011: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-8980 08/29/23 20:55:36.011
    Aug 29 20:55:36.020: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-8980" to be "running and ready"
    Aug 29 20:55:36.023: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.506471ms
    Aug 29 20:55:36.023: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:55:38.031: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.010935584s
    Aug 29 20:55:38.031: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 29 20:55:38.031: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod1:[80] pod2:[80]] 08/29/23 20:55:38.035
    Aug 29 20:55:38.050: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/29/23 20:55:38.05
    Aug 29 20:55:39.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 29 20:55:39.215: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 29 20:55:39.215: INFO: stdout: ""
    Aug 29 20:55:39.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 172.19.242.92 80'
    Aug 29 20:55:39.407: INFO: stderr: "+ nc -v -z -w 2 172.19.242.92 80\nConnection to 172.19.242.92 80 port [tcp/http] succeeded!\n"
    Aug 29 20:55:39.407: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-8980 08/29/23 20:55:39.407
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[pod2:[80]] 08/29/23 20:55:39.422
    Aug 29 20:55:40.450: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/29/23 20:55:40.45
    Aug 29 20:55:41.451: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Aug 29 20:55:41.636: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 29 20:55:41.636: INFO: stdout: ""
    Aug 29 20:55:41.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3913487528 --namespace=services-8980 exec execpodglz2v -- /bin/sh -x -c nc -v -z -w 2 172.19.242.92 80'
    Aug 29 20:55:41.851: INFO: stderr: "+ nc -v -z -w 2 172.19.242.92 80\nConnection to 172.19.242.92 80 port [tcp/http] succeeded!\n"
    Aug 29 20:55:41.851: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-8980 08/29/23 20:55:41.851
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8980 to expose endpoints map[] 08/29/23 20:55:41.871
    Aug 29 20:55:42.898: INFO: successfully validated that service endpoint-test2 in namespace services-8980 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:42.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-8980" for this suite. 08/29/23 20:55:42.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:42.932
Aug 29 20:55:42.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:55:42.934
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:42.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:42.958
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-02476a8d-6b85-4ecb-8f0e-e73e3c396bd1 08/29/23 20:55:43.007
STEP: Creating secret with name s-test-opt-upd-1f21ca70-e57c-4c3e-92c1-f326186e9844 08/29/23 20:55:43.012
STEP: Creating the pod 08/29/23 20:55:43.02
Aug 29 20:55:43.034: INFO: Waiting up to 5m0s for pod "pod-secrets-cffa0714-64df-48a5-be65-d89706809005" in namespace "secrets-1648" to be "running and ready"
Aug 29 20:55:43.040: INFO: Pod "pod-secrets-cffa0714-64df-48a5-be65-d89706809005": Phase="Pending", Reason="", readiness=false. Elapsed: 6.235707ms
Aug 29 20:55:43.040: INFO: The phase of Pod pod-secrets-cffa0714-64df-48a5-be65-d89706809005 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:55:45.045: INFO: Pod "pod-secrets-cffa0714-64df-48a5-be65-d89706809005": Phase="Running", Reason="", readiness=true. Elapsed: 2.011064717s
Aug 29 20:55:45.045: INFO: The phase of Pod pod-secrets-cffa0714-64df-48a5-be65-d89706809005 is Running (Ready = true)
Aug 29 20:55:45.045: INFO: Pod "pod-secrets-cffa0714-64df-48a5-be65-d89706809005" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-02476a8d-6b85-4ecb-8f0e-e73e3c396bd1 08/29/23 20:55:45.07
STEP: Updating secret s-test-opt-upd-1f21ca70-e57c-4c3e-92c1-f326186e9844 08/29/23 20:55:45.077
STEP: Creating secret with name s-test-opt-create-11671a66-e548-406f-8083-9b1220041634 08/29/23 20:55:45.082
STEP: waiting to observe update in volume 08/29/23 20:55:45.087
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:47.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1648" for this suite. 08/29/23 20:55:47.125
------------------------------
• [4.201 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:42.932
    Aug 29 20:55:42.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:55:42.934
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:42.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:42.958
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-02476a8d-6b85-4ecb-8f0e-e73e3c396bd1 08/29/23 20:55:43.007
    STEP: Creating secret with name s-test-opt-upd-1f21ca70-e57c-4c3e-92c1-f326186e9844 08/29/23 20:55:43.012
    STEP: Creating the pod 08/29/23 20:55:43.02
    Aug 29 20:55:43.034: INFO: Waiting up to 5m0s for pod "pod-secrets-cffa0714-64df-48a5-be65-d89706809005" in namespace "secrets-1648" to be "running and ready"
    Aug 29 20:55:43.040: INFO: Pod "pod-secrets-cffa0714-64df-48a5-be65-d89706809005": Phase="Pending", Reason="", readiness=false. Elapsed: 6.235707ms
    Aug 29 20:55:43.040: INFO: The phase of Pod pod-secrets-cffa0714-64df-48a5-be65-d89706809005 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:55:45.045: INFO: Pod "pod-secrets-cffa0714-64df-48a5-be65-d89706809005": Phase="Running", Reason="", readiness=true. Elapsed: 2.011064717s
    Aug 29 20:55:45.045: INFO: The phase of Pod pod-secrets-cffa0714-64df-48a5-be65-d89706809005 is Running (Ready = true)
    Aug 29 20:55:45.045: INFO: Pod "pod-secrets-cffa0714-64df-48a5-be65-d89706809005" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-02476a8d-6b85-4ecb-8f0e-e73e3c396bd1 08/29/23 20:55:45.07
    STEP: Updating secret s-test-opt-upd-1f21ca70-e57c-4c3e-92c1-f326186e9844 08/29/23 20:55:45.077
    STEP: Creating secret with name s-test-opt-create-11671a66-e548-406f-8083-9b1220041634 08/29/23 20:55:45.082
    STEP: waiting to observe update in volume 08/29/23 20:55:45.087
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:47.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1648" for this suite. 08/29/23 20:55:47.125
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:47.134
Aug 29 20:55:47.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:55:47.135
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:47.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:47.157
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 29 20:55:47.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:55:53.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-5653" for this suite. 08/29/23 20:55:53.709
------------------------------
• [SLOW TEST] [6.586 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:47.134
    Aug 29 20:55:47.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename custom-resource-definition 08/29/23 20:55:47.135
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:47.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:47.157
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 29 20:55:47.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:55:53.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-5653" for this suite. 08/29/23 20:55:53.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:55:53.72
Aug 29 20:55:53.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename deployment 08/29/23 20:55:53.721
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:53.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:53.748
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 29 20:55:53.762: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 29 20:55:58.766: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/29/23 20:55:58.767
Aug 29 20:55:58.767: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 29 20:56:00.772: INFO: Creating deployment "test-rollover-deployment"
Aug 29 20:56:00.780: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 29 20:56:02.791: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 29 20:56:02.798: INFO: Ensure that both replica sets have 1 created replica
Aug 29 20:56:02.807: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 29 20:56:02.819: INFO: Updating deployment test-rollover-deployment
Aug 29 20:56:02.819: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 29 20:56:04.829: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 29 20:56:04.835: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 29 20:56:04.843: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 20:56:04.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:56:06.853: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 20:56:06.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:56:08.853: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 20:56:08.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:56:10.854: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 20:56:10.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:56:12.852: INFO: all replica sets need to contain the pod-template-hash label
Aug 29 20:56:12.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 29 20:56:14.850: INFO: 
Aug 29 20:56:14.850: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 29 20:56:14.860: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-13  937b0a83-fc65-4b69-bf27-4c08d0494aea 46884 2 2023-08-29 20:56:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0069f95f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-29 20:56:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-29 20:56:14 +0000 UTC,LastTransitionTime:2023-08-29 20:56:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 29 20:56:14.866: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-13  7e7419e9-5471-4f4d-804c-7e829bacb953 46874 2 2023-08-29 20:56:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 937b0a83-fc65-4b69-bf27-4c08d0494aea 0xc00bc2b837 0xc00bc2b838}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937b0a83-fc65-4b69-bf27-4c08d0494aea\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bc2b8e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 29 20:56:14.866: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 29 20:56:14.866: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-13  615d6569-059f-4787-8873-4e039b3d0bc8 46883 2 2023-08-29 20:55:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 937b0a83-fc65-4b69-bf27-4c08d0494aea 0xc00bc2b707 0xc00bc2b708}] [] [{e2e.test Update apps/v1 2023-08-29 20:55:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937b0a83-fc65-4b69-bf27-4c08d0494aea\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00bc2b7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 20:56:14.866: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-13  d3b4d8ed-d7e8-4261-9efa-22fbe1b69f18 46820 2 2023-08-29 20:56:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 937b0a83-fc65-4b69-bf27-4c08d0494aea 0xc00bc2b957 0xc00bc2b958}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937b0a83-fc65-4b69-bf27-4c08d0494aea\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bc2ba08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 29 20:56:14.870: INFO: Pod "test-rollover-deployment-6c6df9974f-9r57n" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-9r57n test-rollover-deployment-6c6df9974f- deployment-13  e6b97053-bae3-4e33-b010-fc4c91af23f8 46841 0 2023-08-29 20:56:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:23bf83cf45ef4b082f802c8cd1e63736e3b70f112cc5c92fa525e5366f077e63 cni.projectcalico.org/podIP:172.20.30.157/32 cni.projectcalico.org/podIPs:172.20.30.157/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 7e7419e9-5471-4f4d-804c-7e829bacb953 0xc00bc2bf97 0xc00bc2bf98}] [] [{kube-controller-manager Update v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e7419e9-5471-4f4d-804c-7e829bacb953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 20:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 20:56:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lszk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lszk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.157,StartTime:2023-08-29 20:56:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:56:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f217aaef3533c948c6342d6ca2943f5f8e91071ebf7abe28476f718feeaea45e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Aug 29 20:56:14.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-13" for this suite. 08/29/23 20:56:14.875
------------------------------
• [SLOW TEST] [21.161 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:55:53.72
    Aug 29 20:55:53.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename deployment 08/29/23 20:55:53.721
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:55:53.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:55:53.748
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 29 20:55:53.762: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 29 20:55:58.766: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/29/23 20:55:58.767
    Aug 29 20:55:58.767: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 29 20:56:00.772: INFO: Creating deployment "test-rollover-deployment"
    Aug 29 20:56:00.780: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 29 20:56:02.791: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 29 20:56:02.798: INFO: Ensure that both replica sets have 1 created replica
    Aug 29 20:56:02.807: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 29 20:56:02.819: INFO: Updating deployment test-rollover-deployment
    Aug 29 20:56:02.819: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 29 20:56:04.829: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 29 20:56:04.835: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 29 20:56:04.843: INFO: all replica sets need to contain the pod-template-hash label
    Aug 29 20:56:04.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:56:06.853: INFO: all replica sets need to contain the pod-template-hash label
    Aug 29 20:56:06.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:56:08.853: INFO: all replica sets need to contain the pod-template-hash label
    Aug 29 20:56:08.853: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:56:10.854: INFO: all replica sets need to contain the pod-template-hash label
    Aug 29 20:56:10.854: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:56:12.852: INFO: all replica sets need to contain the pod-template-hash label
    Aug 29 20:56:12.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 29, 20, 56, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 29, 20, 56, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 29 20:56:14.850: INFO: 
    Aug 29 20:56:14.850: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 29 20:56:14.860: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-13  937b0a83-fc65-4b69-bf27-4c08d0494aea 46884 2 2023-08-29 20:56:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0069f95f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-29 20:56:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-08-29 20:56:14 +0000 UTC,LastTransitionTime:2023-08-29 20:56:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 29 20:56:14.866: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-13  7e7419e9-5471-4f4d-804c-7e829bacb953 46874 2 2023-08-29 20:56:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 937b0a83-fc65-4b69-bf27-4c08d0494aea 0xc00bc2b837 0xc00bc2b838}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937b0a83-fc65-4b69-bf27-4c08d0494aea\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bc2b8e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 20:56:14.866: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 29 20:56:14.866: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-13  615d6569-059f-4787-8873-4e039b3d0bc8 46883 2 2023-08-29 20:55:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 937b0a83-fc65-4b69-bf27-4c08d0494aea 0xc00bc2b707 0xc00bc2b708}] [] [{e2e.test Update apps/v1 2023-08-29 20:55:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937b0a83-fc65-4b69-bf27-4c08d0494aea\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00bc2b7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 20:56:14.866: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-13  d3b4d8ed-d7e8-4261-9efa-22fbe1b69f18 46820 2 2023-08-29 20:56:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 937b0a83-fc65-4b69-bf27-4c08d0494aea 0xc00bc2b957 0xc00bc2b958}] [] [{kube-controller-manager Update apps/v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"937b0a83-fc65-4b69-bf27-4c08d0494aea\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00bc2ba08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 29 20:56:14.870: INFO: Pod "test-rollover-deployment-6c6df9974f-9r57n" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-9r57n test-rollover-deployment-6c6df9974f- deployment-13  e6b97053-bae3-4e33-b010-fc4c91af23f8 46841 0 2023-08-29 20:56:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[cni.projectcalico.org/containerID:23bf83cf45ef4b082f802c8cd1e63736e3b70f112cc5c92fa525e5366f077e63 cni.projectcalico.org/podIP:172.20.30.157/32 cni.projectcalico.org/podIPs:172.20.30.157/32] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f 7e7419e9-5471-4f4d-804c-7e829bacb953 0xc00bc2bf97 0xc00bc2bf98}] [] [{kube-controller-manager Update v1 2023-08-29 20:56:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e7419e9-5471-4f4d-804c-7e829bacb953\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-08-29 20:56:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-08-29 20:56:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.20.30.157\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5lszk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5lszk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:loki-15bd39-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-29 20:56:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.45.35.206,PodIP:172.20.30.157,StartTime:2023-08-29 20:56:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-29 20:56:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://f217aaef3533c948c6342d6ca2943f5f8e91071ebf7abe28476f718feeaea45e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.20.30.157,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:56:14.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-13" for this suite. 08/29/23 20:56:14.875
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:56:14.882
Aug 29 20:56:14.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:56:14.883
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:14.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:14.907
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 08/29/23 20:56:14.91
Aug 29 20:56:14.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f" in namespace "projected-8747" to be "Succeeded or Failed"
Aug 29 20:56:14.929: INFO: Pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.951752ms
Aug 29 20:56:16.934: INFO: Pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012235412s
Aug 29 20:56:18.936: INFO: Pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013957461s
STEP: Saw pod success 08/29/23 20:56:18.936
Aug 29 20:56:18.936: INFO: Pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f" satisfied condition "Succeeded or Failed"
Aug 29 20:56:18.940: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f container client-container: <nil>
STEP: delete the pod 08/29/23 20:56:18.948
Aug 29 20:56:18.964: INFO: Waiting for pod downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f to disappear
Aug 29 20:56:18.967: INFO: Pod downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Aug 29 20:56:18.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8747" for this suite. 08/29/23 20:56:18.972
------------------------------
• [4.097 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:56:14.882
    Aug 29 20:56:14.882: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:56:14.883
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:14.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:14.907
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 08/29/23 20:56:14.91
    Aug 29 20:56:14.922: INFO: Waiting up to 5m0s for pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f" in namespace "projected-8747" to be "Succeeded or Failed"
    Aug 29 20:56:14.929: INFO: Pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.951752ms
    Aug 29 20:56:16.934: INFO: Pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012235412s
    Aug 29 20:56:18.936: INFO: Pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013957461s
    STEP: Saw pod success 08/29/23 20:56:18.936
    Aug 29 20:56:18.936: INFO: Pod "downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f" satisfied condition "Succeeded or Failed"
    Aug 29 20:56:18.940: INFO: Trying to get logs from node loki-15bd39-worker-1 pod downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f container client-container: <nil>
    STEP: delete the pod 08/29/23 20:56:18.948
    Aug 29 20:56:18.964: INFO: Waiting for pod downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f to disappear
    Aug 29 20:56:18.967: INFO: Pod downwardapi-volume-11822d3a-9f3d-41a4-8bd5-a2ddc2b0630f no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:56:18.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8747" for this suite. 08/29/23 20:56:18.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:56:18.98
Aug 29 20:56:18.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename kubelet-test 08/29/23 20:56:18.981
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:19.003
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/29/23 20:56:19.018
Aug 29 20:56:19.018: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc" in namespace "kubelet-test-6924" to be "completed"
Aug 29 20:56:19.022: INFO: Pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.716423ms
Aug 29 20:56:21.028: INFO: Pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00954533s
Aug 29 20:56:23.026: INFO: Pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0083072s
Aug 29 20:56:23.026: INFO: Pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Aug 29 20:56:23.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6924" for this suite. 08/29/23 20:56:23.048
------------------------------
• [4.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:56:18.98
    Aug 29 20:56:18.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename kubelet-test 08/29/23 20:56:18.981
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:19.003
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/29/23 20:56:19.018
    Aug 29 20:56:19.018: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc" in namespace "kubelet-test-6924" to be "completed"
    Aug 29 20:56:19.022: INFO: Pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.716423ms
    Aug 29 20:56:21.028: INFO: Pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00954533s
    Aug 29 20:56:23.026: INFO: Pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0083072s
    Aug 29 20:56:23.026: INFO: Pod "agnhost-host-aliases04d958bb-d9e3-4adb-b767-7cd2e7cc9acc" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:56:23.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6924" for this suite. 08/29/23 20:56:23.048
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:56:23.056
Aug 29 20:56:23.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir 08/29/23 20:56:23.057
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:23.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:23.077
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 08/29/23 20:56:23.08
Aug 29 20:56:23.091: INFO: Waiting up to 5m0s for pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230" in namespace "emptydir-7264" to be "Succeeded or Failed"
Aug 29 20:56:23.094: INFO: Pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.914839ms
Aug 29 20:56:25.099: INFO: Pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008115789s
Aug 29 20:56:27.099: INFO: Pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008121109s
STEP: Saw pod success 08/29/23 20:56:27.099
Aug 29 20:56:27.099: INFO: Pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230" satisfied condition "Succeeded or Failed"
Aug 29 20:56:27.102: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-202de79e-a6e4-41e9-8993-a11a4d8d4230 container test-container: <nil>
STEP: delete the pod 08/29/23 20:56:27.109
Aug 29 20:56:27.127: INFO: Waiting for pod pod-202de79e-a6e4-41e9-8993-a11a4d8d4230 to disappear
Aug 29 20:56:27.130: INFO: Pod pod-202de79e-a6e4-41e9-8993-a11a4d8d4230 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:56:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7264" for this suite. 08/29/23 20:56:27.135
------------------------------
• [4.085 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:56:23.056
    Aug 29 20:56:23.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir 08/29/23 20:56:23.057
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:23.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:23.077
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/29/23 20:56:23.08
    Aug 29 20:56:23.091: INFO: Waiting up to 5m0s for pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230" in namespace "emptydir-7264" to be "Succeeded or Failed"
    Aug 29 20:56:23.094: INFO: Pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.914839ms
    Aug 29 20:56:25.099: INFO: Pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008115789s
    Aug 29 20:56:27.099: INFO: Pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008121109s
    STEP: Saw pod success 08/29/23 20:56:27.099
    Aug 29 20:56:27.099: INFO: Pod "pod-202de79e-a6e4-41e9-8993-a11a4d8d4230" satisfied condition "Succeeded or Failed"
    Aug 29 20:56:27.102: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-202de79e-a6e4-41e9-8993-a11a4d8d4230 container test-container: <nil>
    STEP: delete the pod 08/29/23 20:56:27.109
    Aug 29 20:56:27.127: INFO: Waiting for pod pod-202de79e-a6e4-41e9-8993-a11a4d8d4230 to disappear
    Aug 29 20:56:27.130: INFO: Pod pod-202de79e-a6e4-41e9-8993-a11a4d8d4230 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:56:27.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7264" for this suite. 08/29/23 20:56:27.135
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:56:27.141
Aug 29 20:56:27.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename containers 08/29/23 20:56:27.142
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:27.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:27.164
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 08/29/23 20:56:27.167
Aug 29 20:56:27.177: INFO: Waiting up to 5m0s for pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90" in namespace "containers-9272" to be "Succeeded or Failed"
Aug 29 20:56:27.180: INFO: Pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333731ms
Aug 29 20:56:29.186: INFO: Pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008894809s
Aug 29 20:56:31.186: INFO: Pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009008006s
STEP: Saw pod success 08/29/23 20:56:31.186
Aug 29 20:56:31.186: INFO: Pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90" satisfied condition "Succeeded or Failed"
Aug 29 20:56:31.190: INFO: Trying to get logs from node loki-15bd39-worker-1 pod client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90 container agnhost-container: <nil>
STEP: delete the pod 08/29/23 20:56:31.198
Aug 29 20:56:31.211: INFO: Waiting for pod client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90 to disappear
Aug 29 20:56:31.214: INFO: Pod client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Aug 29 20:56:31.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9272" for this suite. 08/29/23 20:56:31.218
------------------------------
• [4.087 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:56:27.141
    Aug 29 20:56:27.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename containers 08/29/23 20:56:27.142
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:27.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:27.164
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 08/29/23 20:56:27.167
    Aug 29 20:56:27.177: INFO: Waiting up to 5m0s for pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90" in namespace "containers-9272" to be "Succeeded or Failed"
    Aug 29 20:56:27.180: INFO: Pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90": Phase="Pending", Reason="", readiness=false. Elapsed: 3.333731ms
    Aug 29 20:56:29.186: INFO: Pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008894809s
    Aug 29 20:56:31.186: INFO: Pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009008006s
    STEP: Saw pod success 08/29/23 20:56:31.186
    Aug 29 20:56:31.186: INFO: Pod "client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90" satisfied condition "Succeeded or Failed"
    Aug 29 20:56:31.190: INFO: Trying to get logs from node loki-15bd39-worker-1 pod client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90 container agnhost-container: <nil>
    STEP: delete the pod 08/29/23 20:56:31.198
    Aug 29 20:56:31.211: INFO: Waiting for pod client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90 to disappear
    Aug 29 20:56:31.214: INFO: Pod client-containers-ee7cd58c-38b4-43b4-88cc-f80146401e90 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:56:31.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9272" for this suite. 08/29/23 20:56:31.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:56:31.229
Aug 29 20:56:31.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename gc 08/29/23 20:56:31.23
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:31.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:31.251
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/29/23 20:56:31.254
STEP: Wait for the Deployment to create new ReplicaSet 08/29/23 20:56:31.262
STEP: delete the deployment 08/29/23 20:56:31.772
STEP: wait for all rs to be garbage collected 08/29/23 20:56:31.785
STEP: expected 0 pods, got 2 pods 08/29/23 20:56:31.807
STEP: Gathering metrics 08/29/23 20:56:32.319
W0829 20:56:32.330228      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Aug 29 20:56:32.330: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Aug 29 20:56:32.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-506" for this suite. 08/29/23 20:56:32.335
------------------------------
• [1.113 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:56:31.229
    Aug 29 20:56:31.229: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename gc 08/29/23 20:56:31.23
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:31.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:31.251
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/29/23 20:56:31.254
    STEP: Wait for the Deployment to create new ReplicaSet 08/29/23 20:56:31.262
    STEP: delete the deployment 08/29/23 20:56:31.772
    STEP: wait for all rs to be garbage collected 08/29/23 20:56:31.785
    STEP: expected 0 pods, got 2 pods 08/29/23 20:56:31.807
    STEP: Gathering metrics 08/29/23 20:56:32.319
    W0829 20:56:32.330228      19 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Aug 29 20:56:32.330: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:56:32.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-506" for this suite. 08/29/23 20:56:32.335
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:56:32.343
Aug 29 20:56:32.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-preemption 08/29/23 20:56:32.344
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:32.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:32.37
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:97
Aug 29 20:56:32.387: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 29 20:57:32.446: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130
STEP: Create pods that use 4/5 of node resources. 08/29/23 20:57:32.45
Aug 29 20:57:32.480: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 29 20:57:32.490: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 29 20:57:32.521: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 29 20:57:32.530: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Aug 29 20:57:32.563: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Aug 29 20:57:32.571: INFO: Created pod: pod2-1-sched-preemption-medium-priority
Aug 29 20:57:32.595: INFO: Created pod: pod3-0-sched-preemption-medium-priority
Aug 29 20:57:32.604: INFO: Created pod: pod3-1-sched-preemption-medium-priority
Aug 29 20:57:32.636: INFO: Created pod: pod4-0-sched-preemption-medium-priority
Aug 29 20:57:32.649: INFO: Created pod: pod4-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/29/23 20:57:32.649
Aug 29 20:57:32.649: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:32.653: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.878918ms
Aug 29 20:57:34.658: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009308494s
Aug 29 20:57:34.659: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 29 20:57:34.659: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:34.662: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.924257ms
Aug 29 20:57:34.663: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:57:34.663: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:34.666: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.928418ms
Aug 29 20:57:34.667: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:57:34.667: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:34.670: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.893208ms
Aug 29 20:57:34.670: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:57:34.670: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:34.674: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343224ms
Aug 29 20:57:36.679: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008303338s
Aug 29 20:57:36.679: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:57:36.679: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:36.683: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.904687ms
Aug 29 20:57:36.683: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:57:36.683: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:36.686: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.394872ms
Aug 29 20:57:36.686: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:57:36.686: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:36.690: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.393286ms
Aug 29 20:57:36.690: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:57:36.690: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:36.693: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.906811ms
Aug 29 20:57:36.693: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 29 20:57:36.693: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:36.696: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.372949ms
Aug 29 20:57:36.696: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/29/23 20:57:36.696
Aug 29 20:57:36.703: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7337" to be "running"
Aug 29 20:57:36.706: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985464ms
Aug 29 20:57:38.712: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00840753s
Aug 29 20:57:40.711: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008182287s
Aug 29 20:57:40.711: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 20:57:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7337" for this suite. 08/29/23 20:57:40.809
------------------------------
• [SLOW TEST] [68.473 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:56:32.343
    Aug 29 20:56:32.343: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-preemption 08/29/23 20:56:32.344
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:56:32.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:56:32.37
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:97
    Aug 29 20:56:32.387: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 29 20:57:32.446: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:130
    STEP: Create pods that use 4/5 of node resources. 08/29/23 20:57:32.45
    Aug 29 20:57:32.480: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 29 20:57:32.490: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 29 20:57:32.521: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 29 20:57:32.530: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Aug 29 20:57:32.563: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Aug 29 20:57:32.571: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    Aug 29 20:57:32.595: INFO: Created pod: pod3-0-sched-preemption-medium-priority
    Aug 29 20:57:32.604: INFO: Created pod: pod3-1-sched-preemption-medium-priority
    Aug 29 20:57:32.636: INFO: Created pod: pod4-0-sched-preemption-medium-priority
    Aug 29 20:57:32.649: INFO: Created pod: pod4-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/29/23 20:57:32.649
    Aug 29 20:57:32.649: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:32.653: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.878918ms
    Aug 29 20:57:34.658: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.009308494s
    Aug 29 20:57:34.659: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 29 20:57:34.659: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:34.662: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.924257ms
    Aug 29 20:57:34.663: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:57:34.663: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:34.666: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.928418ms
    Aug 29 20:57:34.667: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:57:34.667: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:34.670: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.893208ms
    Aug 29 20:57:34.670: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:57:34.670: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:34.674: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 3.343224ms
    Aug 29 20:57:36.679: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.008303338s
    Aug 29 20:57:36.679: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:57:36.679: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:36.683: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.904687ms
    Aug 29 20:57:36.683: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:57:36.683: INFO: Waiting up to 5m0s for pod "pod3-0-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:36.686: INFO: Pod "pod3-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.394872ms
    Aug 29 20:57:36.686: INFO: Pod "pod3-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:57:36.686: INFO: Waiting up to 5m0s for pod "pod3-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:36.690: INFO: Pod "pod3-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.393286ms
    Aug 29 20:57:36.690: INFO: Pod "pod3-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:57:36.690: INFO: Waiting up to 5m0s for pod "pod4-0-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:36.693: INFO: Pod "pod4-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.906811ms
    Aug 29 20:57:36.693: INFO: Pod "pod4-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 29 20:57:36.693: INFO: Waiting up to 5m0s for pod "pod4-1-sched-preemption-medium-priority" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:36.696: INFO: Pod "pod4-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.372949ms
    Aug 29 20:57:36.696: INFO: Pod "pod4-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/29/23 20:57:36.696
    Aug 29 20:57:36.703: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-7337" to be "running"
    Aug 29 20:57:36.706: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.985464ms
    Aug 29 20:57:38.712: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00840753s
    Aug 29 20:57:40.711: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.008182287s
    Aug 29 20:57:40.711: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:57:40.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7337" for this suite. 08/29/23 20:57:40.809
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:57:40.817
Aug 29 20:57:40.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:57:40.818
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:57:40.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:57:40.843
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 08/29/23 20:57:40.845
Aug 29 20:57:40.855: INFO: Waiting up to 5m0s for pod "pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a" in namespace "pods-8831" to be "running and ready"
Aug 29 20:57:40.858: INFO: Pod "pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.756404ms
Aug 29 20:57:40.858: INFO: The phase of Pod pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:57:42.864: INFO: Pod "pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009262258s
Aug 29 20:57:42.864: INFO: The phase of Pod pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a is Running (Ready = true)
Aug 29 20:57:42.864: INFO: Pod "pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a" satisfied condition "running and ready"
Aug 29 20:57:42.871: INFO: Pod pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a has hostIP: 10.45.35.206
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:57:42.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8831" for this suite. 08/29/23 20:57:42.876
------------------------------
• [2.067 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:57:40.817
    Aug 29 20:57:40.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:57:40.818
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:57:40.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:57:40.843
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 08/29/23 20:57:40.845
    Aug 29 20:57:40.855: INFO: Waiting up to 5m0s for pod "pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a" in namespace "pods-8831" to be "running and ready"
    Aug 29 20:57:40.858: INFO: Pod "pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.756404ms
    Aug 29 20:57:40.858: INFO: The phase of Pod pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:57:42.864: INFO: Pod "pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a": Phase="Running", Reason="", readiness=true. Elapsed: 2.009262258s
    Aug 29 20:57:42.864: INFO: The phase of Pod pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a is Running (Ready = true)
    Aug 29 20:57:42.864: INFO: Pod "pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a" satisfied condition "running and ready"
    Aug 29 20:57:42.871: INFO: Pod pod-hostip-4694f4fc-f9e0-4624-9ef9-bed666cc8f4a has hostIP: 10.45.35.206
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:57:42.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8831" for this suite. 08/29/23 20:57:42.876
  << End Captured GinkgoWriter Output
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:57:42.884
Aug 29 20:57:42.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename events 08/29/23 20:57:42.885
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:57:42.906
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:57:42.909
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/29/23 20:57:42.912
STEP: get a list of Events with a label in the current namespace 08/29/23 20:57:42.935
STEP: delete a list of events 08/29/23 20:57:42.938
Aug 29 20:57:42.938: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/29/23 20:57:42.965
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Aug 29 20:57:42.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-3883" for this suite. 08/29/23 20:57:42.974
------------------------------
• [0.098 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:57:42.884
    Aug 29 20:57:42.884: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename events 08/29/23 20:57:42.885
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:57:42.906
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:57:42.909
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/29/23 20:57:42.912
    STEP: get a list of Events with a label in the current namespace 08/29/23 20:57:42.935
    STEP: delete a list of events 08/29/23 20:57:42.938
    Aug 29 20:57:42.938: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/29/23 20:57:42.965
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:57:42.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-3883" for this suite. 08/29/23 20:57:42.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:57:42.983
Aug 29 20:57:42.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename resourcequota 08/29/23 20:57:42.984
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:57:43.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:57:43.011
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 08/29/23 20:57:43.014
STEP: Ensuring ResourceQuota status is calculated 08/29/23 20:57:43.02
STEP: Creating a ResourceQuota with not terminating scope 08/29/23 20:57:45.026
STEP: Ensuring ResourceQuota status is calculated 08/29/23 20:57:45.035
STEP: Creating a long running pod 08/29/23 20:57:47.039
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/29/23 20:57:47.067
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/29/23 20:57:49.073
STEP: Deleting the pod 08/29/23 20:57:51.079
STEP: Ensuring resource quota status released the pod usage 08/29/23 20:57:51.092
STEP: Creating a terminating pod 08/29/23 20:57:53.097
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/29/23 20:57:53.116
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/29/23 20:57:55.12
STEP: Deleting the pod 08/29/23 20:57:57.125
STEP: Ensuring resource quota status released the pod usage 08/29/23 20:57:57.147
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Aug 29 20:57:59.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-193" for this suite. 08/29/23 20:57:59.157
------------------------------
• [SLOW TEST] [16.181 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:57:42.983
    Aug 29 20:57:42.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename resourcequota 08/29/23 20:57:42.984
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:57:43.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:57:43.011
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 08/29/23 20:57:43.014
    STEP: Ensuring ResourceQuota status is calculated 08/29/23 20:57:43.02
    STEP: Creating a ResourceQuota with not terminating scope 08/29/23 20:57:45.026
    STEP: Ensuring ResourceQuota status is calculated 08/29/23 20:57:45.035
    STEP: Creating a long running pod 08/29/23 20:57:47.039
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/29/23 20:57:47.067
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/29/23 20:57:49.073
    STEP: Deleting the pod 08/29/23 20:57:51.079
    STEP: Ensuring resource quota status released the pod usage 08/29/23 20:57:51.092
    STEP: Creating a terminating pod 08/29/23 20:57:53.097
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/29/23 20:57:53.116
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/29/23 20:57:55.12
    STEP: Deleting the pod 08/29/23 20:57:57.125
    STEP: Ensuring resource quota status released the pod usage 08/29/23 20:57:57.147
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:57:59.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-193" for this suite. 08/29/23 20:57:59.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:57:59.165
Aug 29 20:57:59.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename watch 08/29/23 20:57:59.166
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:57:59.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:57:59.191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/29/23 20:57:59.194
STEP: starting a background goroutine to produce watch events 08/29/23 20:57:59.197
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/29/23 20:57:59.197
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Aug 29 20:58:01.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-6164" for this suite. 08/29/23 20:58:02.023
------------------------------
• [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:57:59.165
    Aug 29 20:57:59.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename watch 08/29/23 20:57:59.166
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:57:59.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:57:59.191
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/29/23 20:57:59.194
    STEP: starting a background goroutine to produce watch events 08/29/23 20:57:59.197
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/29/23 20:57:59.197
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:58:01.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-6164" for this suite. 08/29/23 20:58:02.023
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:58:02.076
Aug 29 20:58:02.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename secrets 08/29/23 20:58:02.077
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:58:02.097
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:58:02.101
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-b4fee5fb-52ea-4dfd-b913-9d73b5efa952 08/29/23 20:58:02.104
STEP: Creating a pod to test consume secrets 08/29/23 20:58:02.109
Aug 29 20:58:02.119: INFO: Waiting up to 5m0s for pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7" in namespace "secrets-3628" to be "Succeeded or Failed"
Aug 29 20:58:02.122: INFO: Pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.090515ms
Aug 29 20:58:04.128: INFO: Pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009086745s
Aug 29 20:58:06.128: INFO: Pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00944115s
STEP: Saw pod success 08/29/23 20:58:06.128
Aug 29 20:58:06.128: INFO: Pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7" satisfied condition "Succeeded or Failed"
Aug 29 20:58:06.136: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7 container secret-volume-test: <nil>
STEP: delete the pod 08/29/23 20:58:06.156
Aug 29 20:58:06.183: INFO: Waiting for pod pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7 to disappear
Aug 29 20:58:06.187: INFO: Pod pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Aug 29 20:58:06.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3628" for this suite. 08/29/23 20:58:06.192
------------------------------
• [4.130 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:58:02.076
    Aug 29 20:58:02.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename secrets 08/29/23 20:58:02.077
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:58:02.097
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:58:02.101
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-b4fee5fb-52ea-4dfd-b913-9d73b5efa952 08/29/23 20:58:02.104
    STEP: Creating a pod to test consume secrets 08/29/23 20:58:02.109
    Aug 29 20:58:02.119: INFO: Waiting up to 5m0s for pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7" in namespace "secrets-3628" to be "Succeeded or Failed"
    Aug 29 20:58:02.122: INFO: Pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.090515ms
    Aug 29 20:58:04.128: INFO: Pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009086745s
    Aug 29 20:58:06.128: INFO: Pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00944115s
    STEP: Saw pod success 08/29/23 20:58:06.128
    Aug 29 20:58:06.128: INFO: Pod "pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7" satisfied condition "Succeeded or Failed"
    Aug 29 20:58:06.136: INFO: Trying to get logs from node loki-15bd39-worker-1 pod pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7 container secret-volume-test: <nil>
    STEP: delete the pod 08/29/23 20:58:06.156
    Aug 29 20:58:06.183: INFO: Waiting for pod pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7 to disappear
    Aug 29 20:58:06.187: INFO: Pod pod-secrets-0693cda1-7b42-470c-aa17-4fa7072a44d7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:58:06.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3628" for this suite. 08/29/23 20:58:06.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:58:06.207
Aug 29 20:58:06.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename emptydir-wrapper 08/29/23 20:58:06.208
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:58:06.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:58:06.231
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/29/23 20:58:06.234
STEP: Creating RC which spawns configmap-volume pods 08/29/23 20:58:06.513
Aug 29 20:58:06.572: INFO: Pod name wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0: Found 2 pods out of 5
Aug 29 20:58:11.581: INFO: Pod name wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/29/23 20:58:11.581
Aug 29 20:58:11.581: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:11.584: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.510681ms
Aug 29 20:58:13.590: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009604164s
Aug 29 20:58:15.590: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009177185s
Aug 29 20:58:17.590: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008982818s
Aug 29 20:58:19.589: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008392591s
Aug 29 20:58:21.591: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Running", Reason="", readiness=true. Elapsed: 10.01029288s
Aug 29 20:58:21.591: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv" satisfied condition "running"
Aug 29 20:58:21.591: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-dd47d" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:21.596: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-dd47d": Phase="Running", Reason="", readiness=true. Elapsed: 4.617481ms
Aug 29 20:58:21.596: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-dd47d" satisfied condition "running"
Aug 29 20:58:21.596: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-frgdc" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:21.600: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-frgdc": Phase="Running", Reason="", readiness=true. Elapsed: 3.840752ms
Aug 29 20:58:21.600: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-frgdc" satisfied condition "running"
Aug 29 20:58:21.600: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-kcnh6" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:21.603: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-kcnh6": Phase="Running", Reason="", readiness=true. Elapsed: 3.452669ms
Aug 29 20:58:21.603: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-kcnh6" satisfied condition "running"
Aug 29 20:58:21.603: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-z47nt" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:21.615: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-z47nt": Phase="Running", Reason="", readiness=true. Elapsed: 11.983548ms
Aug 29 20:58:21.615: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-z47nt" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0 in namespace emptydir-wrapper-7234, will wait for the garbage collector to delete the pods 08/29/23 20:58:21.615
Aug 29 20:58:21.682: INFO: Deleting ReplicationController wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0 took: 10.675136ms
Aug 29 20:58:21.782: INFO: Terminating ReplicationController wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0 pods took: 100.247041ms
STEP: Creating RC which spawns configmap-volume pods 08/29/23 20:58:24.989
Aug 29 20:58:25.006: INFO: Pod name wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39: Found 0 pods out of 5
Aug 29 20:58:30.014: INFO: Pod name wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/29/23 20:58:30.014
Aug 29 20:58:30.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:30.020: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.267904ms
Aug 29 20:58:32.027: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012272735s
Aug 29 20:58:34.029: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014839531s
Aug 29 20:58:36.027: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01300915s
Aug 29 20:58:38.031: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016413855s
Aug 29 20:58:40.026: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Running", Reason="", readiness=true. Elapsed: 10.011255918s
Aug 29 20:58:40.026: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d" satisfied condition "running"
Aug 29 20:58:40.026: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-8wz57" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:40.030: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-8wz57": Phase="Running", Reason="", readiness=true. Elapsed: 4.358276ms
Aug 29 20:58:40.030: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-8wz57" satisfied condition "running"
Aug 29 20:58:40.030: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-kh26q" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:40.034: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-kh26q": Phase="Running", Reason="", readiness=true. Elapsed: 3.973431ms
Aug 29 20:58:40.034: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-kh26q" satisfied condition "running"
Aug 29 20:58:40.034: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-rn2bv" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:40.037: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-rn2bv": Phase="Running", Reason="", readiness=true. Elapsed: 3.167244ms
Aug 29 20:58:40.037: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-rn2bv" satisfied condition "running"
Aug 29 20:58:40.037: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-v7rqh" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:40.041: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-v7rqh": Phase="Running", Reason="", readiness=true. Elapsed: 3.711646ms
Aug 29 20:58:40.041: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-v7rqh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39 in namespace emptydir-wrapper-7234, will wait for the garbage collector to delete the pods 08/29/23 20:58:40.041
Aug 29 20:58:40.105: INFO: Deleting ReplicationController wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39 took: 8.541248ms
Aug 29 20:58:40.205: INFO: Terminating ReplicationController wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39 pods took: 100.137745ms
STEP: Creating RC which spawns configmap-volume pods 08/29/23 20:58:44.111
Aug 29 20:58:44.130: INFO: Pod name wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f: Found 0 pods out of 5
Aug 29 20:58:49.140: INFO: Pod name wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/29/23 20:58:49.14
Aug 29 20:58:49.140: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:49.145: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.337511ms
Aug 29 20:58:51.151: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010976139s
Aug 29 20:58:53.150: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00950437s
Aug 29 20:58:55.151: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01030689s
Aug 29 20:58:57.152: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011851735s
Aug 29 20:58:59.150: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Running", Reason="", readiness=true. Elapsed: 10.009951961s
Aug 29 20:58:59.150: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz" satisfied condition "running"
Aug 29 20:58:59.151: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-76kbz" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:59.154: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-76kbz": Phase="Running", Reason="", readiness=true. Elapsed: 3.475568ms
Aug 29 20:58:59.154: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-76kbz" satisfied condition "running"
Aug 29 20:58:59.154: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkk9j" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:59.158: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkk9j": Phase="Running", Reason="", readiness=true. Elapsed: 3.510826ms
Aug 29 20:58:59.158: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkk9j" satisfied condition "running"
Aug 29 20:58:59.158: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkrt9" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:59.162: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkrt9": Phase="Running", Reason="", readiness=true. Elapsed: 3.919325ms
Aug 29 20:58:59.162: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkrt9" satisfied condition "running"
Aug 29 20:58:59.162: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-q8ndj" in namespace "emptydir-wrapper-7234" to be "running"
Aug 29 20:58:59.166: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-q8ndj": Phase="Running", Reason="", readiness=true. Elapsed: 4.030652ms
Aug 29 20:58:59.166: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-q8ndj" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f in namespace emptydir-wrapper-7234, will wait for the garbage collector to delete the pods 08/29/23 20:58:59.166
Aug 29 20:58:59.233: INFO: Deleting ReplicationController wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f took: 11.555558ms
Aug 29 20:58:59.333: INFO: Terminating ReplicationController wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f pods took: 100.361562ms
STEP: Cleaning up the configMaps 08/29/23 20:59:03.034
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Aug 29 20:59:03.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-7234" for this suite. 08/29/23 20:59:03.37
------------------------------
• [SLOW TEST] [57.175 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:58:06.207
    Aug 29 20:58:06.207: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename emptydir-wrapper 08/29/23 20:58:06.208
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:58:06.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:58:06.231
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/29/23 20:58:06.234
    STEP: Creating RC which spawns configmap-volume pods 08/29/23 20:58:06.513
    Aug 29 20:58:06.572: INFO: Pod name wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0: Found 2 pods out of 5
    Aug 29 20:58:11.581: INFO: Pod name wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/29/23 20:58:11.581
    Aug 29 20:58:11.581: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:11.584: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.510681ms
    Aug 29 20:58:13.590: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009604164s
    Aug 29 20:58:15.590: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009177185s
    Aug 29 20:58:17.590: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008982818s
    Aug 29 20:58:19.589: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008392591s
    Aug 29 20:58:21.591: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv": Phase="Running", Reason="", readiness=true. Elapsed: 10.01029288s
    Aug 29 20:58:21.591: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-6mcvv" satisfied condition "running"
    Aug 29 20:58:21.591: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-dd47d" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:21.596: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-dd47d": Phase="Running", Reason="", readiness=true. Elapsed: 4.617481ms
    Aug 29 20:58:21.596: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-dd47d" satisfied condition "running"
    Aug 29 20:58:21.596: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-frgdc" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:21.600: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-frgdc": Phase="Running", Reason="", readiness=true. Elapsed: 3.840752ms
    Aug 29 20:58:21.600: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-frgdc" satisfied condition "running"
    Aug 29 20:58:21.600: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-kcnh6" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:21.603: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-kcnh6": Phase="Running", Reason="", readiness=true. Elapsed: 3.452669ms
    Aug 29 20:58:21.603: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-kcnh6" satisfied condition "running"
    Aug 29 20:58:21.603: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-z47nt" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:21.615: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-z47nt": Phase="Running", Reason="", readiness=true. Elapsed: 11.983548ms
    Aug 29 20:58:21.615: INFO: Pod "wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0-z47nt" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0 in namespace emptydir-wrapper-7234, will wait for the garbage collector to delete the pods 08/29/23 20:58:21.615
    Aug 29 20:58:21.682: INFO: Deleting ReplicationController wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0 took: 10.675136ms
    Aug 29 20:58:21.782: INFO: Terminating ReplicationController wrapped-volume-race-0923fc38-16c0-441c-89dc-ee7fcdce8bf0 pods took: 100.247041ms
    STEP: Creating RC which spawns configmap-volume pods 08/29/23 20:58:24.989
    Aug 29 20:58:25.006: INFO: Pod name wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39: Found 0 pods out of 5
    Aug 29 20:58:30.014: INFO: Pod name wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/29/23 20:58:30.014
    Aug 29 20:58:30.014: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:30.020: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.267904ms
    Aug 29 20:58:32.027: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012272735s
    Aug 29 20:58:34.029: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.014839531s
    Aug 29 20:58:36.027: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01300915s
    Aug 29 20:58:38.031: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.016413855s
    Aug 29 20:58:40.026: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d": Phase="Running", Reason="", readiness=true. Elapsed: 10.011255918s
    Aug 29 20:58:40.026: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-74h5d" satisfied condition "running"
    Aug 29 20:58:40.026: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-8wz57" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:40.030: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-8wz57": Phase="Running", Reason="", readiness=true. Elapsed: 4.358276ms
    Aug 29 20:58:40.030: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-8wz57" satisfied condition "running"
    Aug 29 20:58:40.030: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-kh26q" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:40.034: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-kh26q": Phase="Running", Reason="", readiness=true. Elapsed: 3.973431ms
    Aug 29 20:58:40.034: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-kh26q" satisfied condition "running"
    Aug 29 20:58:40.034: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-rn2bv" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:40.037: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-rn2bv": Phase="Running", Reason="", readiness=true. Elapsed: 3.167244ms
    Aug 29 20:58:40.037: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-rn2bv" satisfied condition "running"
    Aug 29 20:58:40.037: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-v7rqh" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:40.041: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-v7rqh": Phase="Running", Reason="", readiness=true. Elapsed: 3.711646ms
    Aug 29 20:58:40.041: INFO: Pod "wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39-v7rqh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39 in namespace emptydir-wrapper-7234, will wait for the garbage collector to delete the pods 08/29/23 20:58:40.041
    Aug 29 20:58:40.105: INFO: Deleting ReplicationController wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39 took: 8.541248ms
    Aug 29 20:58:40.205: INFO: Terminating ReplicationController wrapped-volume-race-da879ad8-4e12-4adb-a700-a4d1c4127a39 pods took: 100.137745ms
    STEP: Creating RC which spawns configmap-volume pods 08/29/23 20:58:44.111
    Aug 29 20:58:44.130: INFO: Pod name wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f: Found 0 pods out of 5
    Aug 29 20:58:49.140: INFO: Pod name wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/29/23 20:58:49.14
    Aug 29 20:58:49.140: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:49.145: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.337511ms
    Aug 29 20:58:51.151: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010976139s
    Aug 29 20:58:53.150: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 4.00950437s
    Aug 29 20:58:55.151: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 6.01030689s
    Aug 29 20:58:57.152: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Pending", Reason="", readiness=false. Elapsed: 8.011851735s
    Aug 29 20:58:59.150: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz": Phase="Running", Reason="", readiness=true. Elapsed: 10.009951961s
    Aug 29 20:58:59.150: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-4h8kz" satisfied condition "running"
    Aug 29 20:58:59.151: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-76kbz" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:59.154: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-76kbz": Phase="Running", Reason="", readiness=true. Elapsed: 3.475568ms
    Aug 29 20:58:59.154: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-76kbz" satisfied condition "running"
    Aug 29 20:58:59.154: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkk9j" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:59.158: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkk9j": Phase="Running", Reason="", readiness=true. Elapsed: 3.510826ms
    Aug 29 20:58:59.158: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkk9j" satisfied condition "running"
    Aug 29 20:58:59.158: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkrt9" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:59.162: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkrt9": Phase="Running", Reason="", readiness=true. Elapsed: 3.919325ms
    Aug 29 20:58:59.162: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-kkrt9" satisfied condition "running"
    Aug 29 20:58:59.162: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-q8ndj" in namespace "emptydir-wrapper-7234" to be "running"
    Aug 29 20:58:59.166: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-q8ndj": Phase="Running", Reason="", readiness=true. Elapsed: 4.030652ms
    Aug 29 20:58:59.166: INFO: Pod "wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f-q8ndj" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f in namespace emptydir-wrapper-7234, will wait for the garbage collector to delete the pods 08/29/23 20:58:59.166
    Aug 29 20:58:59.233: INFO: Deleting ReplicationController wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f took: 11.555558ms
    Aug 29 20:58:59.333: INFO: Terminating ReplicationController wrapped-volume-race-837f3af1-cddc-404c-9cc1-ea1ecea90c2f pods took: 100.361562ms
    STEP: Cleaning up the configMaps 08/29/23 20:59:03.034
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:59:03.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-7234" for this suite. 08/29/23 20:59:03.37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:59:03.383
Aug 29 20:59:03.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename pods 08/29/23 20:59:03.384
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:59:03.405
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:59:03.409
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 08/29/23 20:59:03.412
STEP: submitting the pod to kubernetes 08/29/23 20:59:03.412
Aug 29 20:59:03.424: INFO: Waiting up to 5m0s for pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be" in namespace "pods-4509" to be "running and ready"
Aug 29 20:59:03.427: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921177ms
Aug 29 20:59:03.427: INFO: The phase of Pod pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:59:05.433: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be": Phase="Running", Reason="", readiness=true. Elapsed: 2.008647091s
Aug 29 20:59:05.433: INFO: The phase of Pod pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be is Running (Ready = true)
Aug 29 20:59:05.433: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/29/23 20:59:05.437
STEP: updating the pod 08/29/23 20:59:05.441
Aug 29 20:59:05.957: INFO: Successfully updated pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be"
Aug 29 20:59:05.957: INFO: Waiting up to 5m0s for pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be" in namespace "pods-4509" to be "running"
Aug 29 20:59:05.960: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be": Phase="Running", Reason="", readiness=true. Elapsed: 3.043092ms
Aug 29 20:59:05.960: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/29/23 20:59:05.96
Aug 29 20:59:05.963: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Aug 29 20:59:05.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-4509" for this suite. 08/29/23 20:59:05.968
------------------------------
• [2.592 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:59:03.383
    Aug 29 20:59:03.384: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename pods 08/29/23 20:59:03.384
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:59:03.405
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:59:03.409
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 08/29/23 20:59:03.412
    STEP: submitting the pod to kubernetes 08/29/23 20:59:03.412
    Aug 29 20:59:03.424: INFO: Waiting up to 5m0s for pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be" in namespace "pods-4509" to be "running and ready"
    Aug 29 20:59:03.427: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921177ms
    Aug 29 20:59:03.427: INFO: The phase of Pod pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:59:05.433: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be": Phase="Running", Reason="", readiness=true. Elapsed: 2.008647091s
    Aug 29 20:59:05.433: INFO: The phase of Pod pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be is Running (Ready = true)
    Aug 29 20:59:05.433: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/29/23 20:59:05.437
    STEP: updating the pod 08/29/23 20:59:05.441
    Aug 29 20:59:05.957: INFO: Successfully updated pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be"
    Aug 29 20:59:05.957: INFO: Waiting up to 5m0s for pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be" in namespace "pods-4509" to be "running"
    Aug 29 20:59:05.960: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be": Phase="Running", Reason="", readiness=true. Elapsed: 3.043092ms
    Aug 29 20:59:05.960: INFO: Pod "pod-update-1534fb71-6c83-4428-88ff-7c3cb52e00be" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/29/23 20:59:05.96
    Aug 29 20:59:05.963: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Aug 29 20:59:05.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-4509" for this suite. 08/29/23 20:59:05.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 20:59:05.976
Aug 29 20:59:05.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename projected 08/29/23 20:59:05.977
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:59:05.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:59:05.999
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-52417069-ac97-4da5-8fea-2b42ecfa998d 08/29/23 20:59:06.007
STEP: Creating secret with name s-test-opt-upd-e5010b72-9bee-4bf3-9208-98be77b0b61e 08/29/23 20:59:06.012
STEP: Creating the pod 08/29/23 20:59:06.021
Aug 29 20:59:06.035: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6" in namespace "projected-7295" to be "running and ready"
Aug 29 20:59:06.038: INFO: Pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.342668ms
Aug 29 20:59:06.038: INFO: The phase of Pod pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:59:08.043: INFO: Pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008746152s
Aug 29 20:59:08.043: INFO: The phase of Pod pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 20:59:10.043: INFO: Pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6": Phase="Running", Reason="", readiness=true. Elapsed: 4.008539451s
Aug 29 20:59:10.043: INFO: The phase of Pod pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6 is Running (Ready = true)
Aug 29 20:59:10.043: INFO: Pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-52417069-ac97-4da5-8fea-2b42ecfa998d 08/29/23 20:59:10.069
STEP: Updating secret s-test-opt-upd-e5010b72-9bee-4bf3-9208-98be77b0b61e 08/29/23 20:59:10.076
STEP: Creating secret with name s-test-opt-create-69305dd8-40cd-4da0-b1aa-41ccd8ca3f93 08/29/23 20:59:10.081
STEP: waiting to observe update in volume 08/29/23 20:59:10.086
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Aug 29 21:00:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7295" for this suite. 08/29/23 21:00:42.655
------------------------------
• [SLOW TEST] [96.687 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 20:59:05.976
    Aug 29 20:59:05.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename projected 08/29/23 20:59:05.977
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 20:59:05.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 20:59:05.999
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-52417069-ac97-4da5-8fea-2b42ecfa998d 08/29/23 20:59:06.007
    STEP: Creating secret with name s-test-opt-upd-e5010b72-9bee-4bf3-9208-98be77b0b61e 08/29/23 20:59:06.012
    STEP: Creating the pod 08/29/23 20:59:06.021
    Aug 29 20:59:06.035: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6" in namespace "projected-7295" to be "running and ready"
    Aug 29 20:59:06.038: INFO: Pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.342668ms
    Aug 29 20:59:06.038: INFO: The phase of Pod pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:59:08.043: INFO: Pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008746152s
    Aug 29 20:59:08.043: INFO: The phase of Pod pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 20:59:10.043: INFO: Pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6": Phase="Running", Reason="", readiness=true. Elapsed: 4.008539451s
    Aug 29 20:59:10.043: INFO: The phase of Pod pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6 is Running (Ready = true)
    Aug 29 20:59:10.043: INFO: Pod "pod-projected-secrets-40124784-2998-4017-88e0-e096e9f473c6" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-52417069-ac97-4da5-8fea-2b42ecfa998d 08/29/23 20:59:10.069
    STEP: Updating secret s-test-opt-upd-e5010b72-9bee-4bf3-9208-98be77b0b61e 08/29/23 20:59:10.076
    STEP: Creating secret with name s-test-opt-create-69305dd8-40cd-4da0-b1aa-41ccd8ca3f93 08/29/23 20:59:10.081
    STEP: waiting to observe update in volume 08/29/23 20:59:10.086
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Aug 29 21:00:42.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7295" for this suite. 08/29/23 21:00:42.655
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 21:00:42.676
Aug 29 21:00:42.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename container-probe 08/29/23 21:00:42.677
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 21:00:42.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 21:00:42.7
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Aug 29 21:00:42.712: INFO: Waiting up to 5m0s for pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85" in namespace "container-probe-7788" to be "running and ready"
Aug 29 21:00:42.715: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446848ms
Aug 29 21:00:42.715: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Pending, waiting for it to be Running (with Ready = true)
Aug 29 21:00:44.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 2.009630584s
Aug 29 21:00:44.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:00:46.722: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 4.010891368s
Aug 29 21:00:46.723: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:00:48.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 6.008625707s
Aug 29 21:00:48.720: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:00:50.722: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 8.009978147s
Aug 29 21:00:50.722: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:00:52.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 10.009347899s
Aug 29 21:00:52.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:00:54.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 12.008072009s
Aug 29 21:00:54.720: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:00:56.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 14.009448567s
Aug 29 21:00:56.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:00:58.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 16.009158413s
Aug 29 21:00:58.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:01:00.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 18.008963025s
Aug 29 21:01:00.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:01:02.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 20.008081947s
Aug 29 21:01:02.720: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
Aug 29 21:01:04.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=true. Elapsed: 22.008372277s
Aug 29 21:01:04.720: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = true)
Aug 29 21:01:04.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85" satisfied condition "running and ready"
Aug 29 21:01:04.724: INFO: Container started at 2023-08-29 21:00:43 +0000 UTC, pod became ready at 2023-08-29 21:01:03 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Aug 29 21:01:04.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-7788" for this suite. 08/29/23 21:01:04.731
------------------------------
• [SLOW TEST] [22.063 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 21:00:42.676
    Aug 29 21:00:42.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename container-probe 08/29/23 21:00:42.677
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 21:00:42.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 21:00:42.7
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Aug 29 21:00:42.712: INFO: Waiting up to 5m0s for pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85" in namespace "container-probe-7788" to be "running and ready"
    Aug 29 21:00:42.715: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Pending", Reason="", readiness=false. Elapsed: 3.446848ms
    Aug 29 21:00:42.715: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Pending, waiting for it to be Running (with Ready = true)
    Aug 29 21:00:44.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 2.009630584s
    Aug 29 21:00:44.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:00:46.722: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 4.010891368s
    Aug 29 21:00:46.723: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:00:48.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 6.008625707s
    Aug 29 21:00:48.720: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:00:50.722: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 8.009978147s
    Aug 29 21:00:50.722: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:00:52.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 10.009347899s
    Aug 29 21:00:52.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:00:54.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 12.008072009s
    Aug 29 21:00:54.720: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:00:56.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 14.009448567s
    Aug 29 21:00:56.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:00:58.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 16.009158413s
    Aug 29 21:00:58.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:01:00.721: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 18.008963025s
    Aug 29 21:01:00.721: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:01:02.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=false. Elapsed: 20.008081947s
    Aug 29 21:01:02.720: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = false)
    Aug 29 21:01:04.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85": Phase="Running", Reason="", readiness=true. Elapsed: 22.008372277s
    Aug 29 21:01:04.720: INFO: The phase of Pod test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 is Running (Ready = true)
    Aug 29 21:01:04.720: INFO: Pod "test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85" satisfied condition "running and ready"
    Aug 29 21:01:04.724: INFO: Container started at 2023-08-29 21:00:43 +0000 UTC, pod became ready at 2023-08-29 21:01:03 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Aug 29 21:01:04.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-7788" for this suite. 08/29/23 21:01:04.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 08/29/23 21:01:04.741
Aug 29 21:01:04.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
STEP: Building a namespace api object, basename sched-pred 08/29/23 21:01:04.742
STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 21:01:04.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 21:01:04.768
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Aug 29 21:01:04.772: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 29 21:01:04.785: INFO: Waiting for terminating namespaces to be deleted...
Aug 29 21:01:04.789: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-master-0 before test
Aug 29 21:01:04.801: INFO: calico-node-f4bng from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.801: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 21:01:04.801: INFO: kube-apiserver-loki-15bd39-master-0 from kube-system started at 2023-08-29 19:11:07 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.801: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 29 21:01:04.801: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 29 21:01:04.801: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 29 21:01:04.801: INFO: kube-proxy-ds-7pn7t from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.801: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 21:01:04.801: INFO: alertmanager-main-2 from ntnx-system started at 2023-08-29 20:20:42 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.801: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 21:01:04.801: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 21:01:04.801: INFO: fluent-bit-x2ntj from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.801: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 21:01:04.801: INFO: node-exporter-lsprp from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.801: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 21:01:04.801: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 21:01:04.801: INFO: nutanix-csi-node-mqc5m from ntnx-system started at 2023-08-29 19:23:42 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.801: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 21:01:04.801: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 21:01:04.801: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 21:01:04.801: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.801: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 21:01:04.801: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 21:01:04.801: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-master-1 before test
Aug 29 21:01:04.811: INFO: calico-node-snrmp from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.811: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 21:01:04.811: INFO: calico-typha-787bcdb57c-c7xxf from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.812: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 21:01:04.812: INFO: kube-apiserver-loki-15bd39-master-1 from kube-system started at 2023-08-29 19:12:53 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.812: INFO: 	Container kube-apiserver ready: true, restart count 0
Aug 29 21:01:04.812: INFO: 	Container kube-controller-manager ready: true, restart count 0
Aug 29 21:01:04.812: INFO: 	Container kube-scheduler ready: true, restart count 0
Aug 29 21:01:04.812: INFO: kube-proxy-ds-c665w from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.812: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 21:01:04.812: INFO: fluent-bit-nmdm2 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.812: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 21:01:04.812: INFO: node-exporter-q5l7x from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 21:01:04.812: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 21:01:04.812: INFO: nutanix-csi-node-r85kq from ntnx-system started at 2023-08-29 19:23:49 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.812: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 21:01:04.812: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 21:01:04.812: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 21:01:04.812: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.812: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 21:01:04.812: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 21:01:04.812: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-0 before test
Aug 29 21:01:04.826: INFO: calico-node-mms8p from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 21:01:04.826: INFO: calico-typha-787bcdb57c-ppmlg from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 21:01:04.826: INFO: coredns-5d88b659b9-6fjb6 from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container coredns ready: true, restart count 0
Aug 29 21:01:04.826: INFO: kube-proxy-ds-nmtgg from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 21:01:04.826: INFO: alertmanager-main-0 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 21:01:04.826: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 21:01:04.826: INFO: blackbox-exporter-7954b6f4db-5vl2v from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container module-configmap-reloader ready: true, restart count 0
Aug 29 21:01:04.826: INFO: csi-snapshot-controller-6d467d46bf-fbxgt from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container snapshot-controller ready: true, restart count 0
Aug 29 21:01:04.826: INFO: csi-snapshot-webhook-69668f7b57-g5r2n from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container snapshot-validation ready: true, restart count 0
Aug 29 21:01:04.826: INFO: fluent-bit-lbvlt from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 21:01:04.826: INFO: kubernetes-events-printer-74464fd469-rrxtq from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
Aug 29 21:01:04.826: INFO: node-exporter-wwfmq from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 21:01:04.826: INFO: nutanix-csi-node-hql5k from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 21:01:04.826: INFO: prometheus-adapter-6b6d856c7-htsdt from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 29 21:01:04.826: INFO: prometheus-k8s-1 from ntnx-system started at 2023-08-29 19:21:14 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container prometheus ready: true, restart count 0
Aug 29 21:01:04.826: INFO: prometheus-operator-557c85cd6b-b2tpm from ntnx-system started at 2023-08-29 20:20:40 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container prometheus-operator ready: true, restart count 0
Aug 29 21:01:04.826: INFO: sonobuoy-e2e-job-161e88352cfa47dc from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container e2e ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 21:01:04.826: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 21:01:04.826: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-1 before test
Aug 29 21:01:04.837: INFO: test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 from container-probe-7788 started at 2023-08-29 21:00:42 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.837: INFO: 	Container test-webserver ready: true, restart count 0
Aug 29 21:01:04.837: INFO: calico-node-sb7l5 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.837: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 21:01:04.837: INFO: kube-proxy-ds-jt2tk from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.837: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 21:01:04.837: INFO: fluent-bit-zmvl7 from ntnx-system started at 2023-08-29 20:21:06 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.837: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 21:01:04.837: INFO: node-exporter-8gt7t from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 21:01:04.837: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 21:01:04.837: INFO: nutanix-csi-node-mxmhr from ntnx-system started at 2023-08-29 20:21:06 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.837: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 21:01:04.837: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 21:01:04.837: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 21:01:04.837: INFO: prometheus-k8s-0 from ntnx-system started at 2023-08-29 20:21:14 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.837: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 21:01:04.837: INFO: 	Container prometheus ready: true, restart count 0
Aug 29 21:01:04.837: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.837: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 21:01:04.837: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 29 21:01:04.837: INFO: 
Logging pods the apiserver thinks is on node loki-15bd39-worker-2 before test
Aug 29 21:01:04.848: INFO: calico-kube-controllers-d9df5649-k5gf4 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 29 21:01:04.848: INFO: calico-node-9pl5x from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container calico-node ready: true, restart count 0
Aug 29 21:01:04.848: INFO: calico-typha-787bcdb57c-6bcfm from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container calico-typha ready: true, restart count 0
Aug 29 21:01:04.848: INFO: coredns-5d88b659b9-fqwjz from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container coredns ready: true, restart count 0
Aug 29 21:01:04.848: INFO: kube-proxy-ds-l9vp5 from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 29 21:01:04.848: INFO: alertmanager-main-1 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container alertmanager ready: true, restart count 1
Aug 29 21:01:04.848: INFO: 	Container config-reloader ready: true, restart count 0
Aug 29 21:01:04.848: INFO: fluent-bit-6wl9b from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container fluent-bit ready: true, restart count 0
Aug 29 21:01:04.848: INFO: kube-state-metrics-d459f9d68-z5xtj from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container kube-state-metrics ready: true, restart count 0
Aug 29 21:01:04.848: INFO: node-exporter-xhwmw from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container node-exporter ready: true, restart count 0
Aug 29 21:01:04.848: INFO: nutanix-csi-controller-657cc74bd5-7j77l from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (5 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container csi-provisioner ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container csi-resizer ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container csi-snapshotter ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container nutanix-csi-plugin ready: true, restart count 0
Aug 29 21:01:04.848: INFO: nutanix-csi-node-hd5rc from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container driver-registrar ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container liveness-probe ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container nutanix-csi-node ready: true, restart count 0
Aug 29 21:01:04.848: INFO: prometheus-adapter-6b6d856c7-qg8lv from ntnx-system started at 2023-08-29 20:20:40 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container prometheus-adapter ready: true, restart count 0
Aug 29 21:01:04.848: INFO: sonobuoy from sonobuoy started at 2023-08-29 19:28:22 +0000 UTC (1 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 29 21:01:04.848: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
Aug 29 21:01:04.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 29 21:01:04.848: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/29/23 21:01:04.848
Aug 29 21:01:04.859: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1329" to be "running"
Aug 29 21:01:04.862: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.299981ms
Aug 29 21:01:06.869: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009622059s
Aug 29 21:01:06.869: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/29/23 21:01:06.873
STEP: Trying to apply a random label on the found node. 08/29/23 21:01:06.893
STEP: verifying the node has the label kubernetes.io/e2e-bfb10d70-c339-463e-a7bc-ddda8569f381 95 08/29/23 21:01:06.908
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/29/23 21:01:06.913
Aug 29 21:01:06.919: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1329" to be "not pending"
Aug 29 21:01:06.922: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.575261ms
Aug 29 21:01:08.927: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007740045s
Aug 29 21:01:08.927: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.45.35.206 on the node which pod4 resides and expect not scheduled 08/29/23 21:01:08.927
Aug 29 21:01:08.935: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1329" to be "not pending"
Aug 29 21:01:08.938: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.147807ms
Aug 29 21:01:10.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007793731s
Aug 29 21:01:12.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009289322s
Aug 29 21:01:14.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008471586s
Aug 29 21:01:16.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009299903s
Aug 29 21:01:18.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00853579s
Aug 29 21:01:20.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009397771s
Aug 29 21:01:22.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008388026s
Aug 29 21:01:24.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010492971s
Aug 29 21:01:26.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008545979s
Aug 29 21:01:28.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009192412s
Aug 29 21:01:30.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01029339s
Aug 29 21:01:32.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010617999s
Aug 29 21:01:34.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008738594s
Aug 29 21:01:36.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011666538s
Aug 29 21:01:38.949: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.014339037s
Aug 29 21:01:40.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008402726s
Aug 29 21:01:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008149811s
Aug 29 21:01:44.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008402796s
Aug 29 21:01:46.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008694444s
Aug 29 21:01:48.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007798822s
Aug 29 21:01:50.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009162073s
Aug 29 21:01:52.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008852294s
Aug 29 21:01:54.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007521217s
Aug 29 21:01:56.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007795542s
Aug 29 21:01:58.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008346466s
Aug 29 21:02:00.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009049974s
Aug 29 21:02:02.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009420801s
Aug 29 21:02:04.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008802544s
Aug 29 21:02:06.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007790478s
Aug 29 21:02:08.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008464704s
Aug 29 21:02:10.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009016058s
Aug 29 21:02:12.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009470774s
Aug 29 21:02:14.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007120375s
Aug 29 21:02:16.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007897519s
Aug 29 21:02:18.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007892979s
Aug 29 21:02:20.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007694095s
Aug 29 21:02:22.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007555689s
Aug 29 21:02:24.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008772054s
Aug 29 21:02:26.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008494289s
Aug 29 21:02:28.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008238425s
Aug 29 21:02:30.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010028682s
Aug 29 21:02:32.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011687707s
Aug 29 21:02:34.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008538432s
Aug 29 21:02:36.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008159744s
Aug 29 21:02:38.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007790804s
Aug 29 21:02:40.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008549144s
Aug 29 21:02:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008540309s
Aug 29 21:02:44.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009011294s
Aug 29 21:02:46.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009369241s
Aug 29 21:02:48.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007831823s
Aug 29 21:02:50.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008456581s
Aug 29 21:02:52.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007806521s
Aug 29 21:02:54.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.0086804s
Aug 29 21:02:56.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007382764s
Aug 29 21:02:58.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008148867s
Aug 29 21:03:00.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009364645s
Aug 29 21:03:02.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01174148s
Aug 29 21:03:04.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007561037s
Aug 29 21:03:06.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009862931s
Aug 29 21:03:08.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007784157s
Aug 29 21:03:10.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008734032s
Aug 29 21:03:12.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009471781s
Aug 29 21:03:14.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008038509s
Aug 29 21:03:16.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009175803s
Aug 29 21:03:18.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007793482s
Aug 29 21:03:20.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008302022s
Aug 29 21:03:22.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009757537s
Aug 29 21:03:24.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.008767071s
Aug 29 21:03:26.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008052831s
Aug 29 21:03:28.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.009786567s
Aug 29 21:03:30.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008099039s
Aug 29 21:03:32.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009631046s
Aug 29 21:03:34.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007893867s
Aug 29 21:03:36.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009528662s
Aug 29 21:03:38.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008398195s
Aug 29 21:03:40.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008986634s
Aug 29 21:03:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008041153s
Aug 29 21:03:44.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008306033s
Aug 29 21:03:46.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009865602s
Aug 29 21:03:48.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009390396s
Aug 29 21:03:50.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009458185s
Aug 29 21:03:52.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009594434s
Aug 29 21:03:54.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009062953s
Aug 29 21:03:56.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008609734s
Aug 29 21:03:58.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009106952s
Aug 29 21:04:00.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008405727s
Aug 29 21:04:02.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009191406s
Aug 29 21:04:04.951: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.016316123s
Aug 29 21:04:06.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009834242s
Aug 29 21:04:08.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008570425s
Aug 29 21:04:10.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.007603653s
Aug 29 21:04:12.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00772815s
Aug 29 21:04:14.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007487235s
Aug 29 21:04:16.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009134259s
Aug 29 21:04:18.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007620385s
Aug 29 21:04:20.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008557525s
Aug 29 21:04:22.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008229245s
Aug 29 21:04:24.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009995917s
Aug 29 21:04:26.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009649223s
Aug 29 21:04:28.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008558015s
Aug 29 21:04:30.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007655949s
Aug 29 21:04:32.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007803862s
Aug 29 21:04:34.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.010747527s
Aug 29 21:04:36.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.01032614s
Aug 29 21:04:38.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008524622s
Aug 29 21:04:40.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009467457s
Aug 29 21:04:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008557373s
Aug 29 21:04:44.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009320402s
Aug 29 21:04:46.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009083365s
Aug 29 21:04:48.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008931244s
Aug 29 21:04:50.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008348987s
Aug 29 21:04:52.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008361215s
Aug 29 21:04:54.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008420468s
Aug 29 21:04:56.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010372542s
Aug 29 21:04:58.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.009132461s
Aug 29 21:05:00.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009530192s
Aug 29 21:05:02.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007839206s
Aug 29 21:05:04.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.011443188s
Aug 29 21:05:06.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009715044s
Aug 29 21:05:08.951: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.016489399s
Aug 29 21:05:10.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008901639s
Aug 29 21:05:12.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008721335s
Aug 29 21:05:14.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007807659s
Aug 29 21:05:16.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.008219375s
Aug 29 21:05:18.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008270057s
Aug 29 21:05:20.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.007282102s
Aug 29 21:05:22.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009530537s
Aug 29 21:05:24.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007872708s
Aug 29 21:05:26.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008600927s
Aug 29 21:05:28.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00825554s
Aug 29 21:05:30.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008187528s
Aug 29 21:05:32.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008541551s
Aug 29 21:05:34.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009044913s
Aug 29 21:05:36.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008138202s
Aug 29 21:05:38.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.009451824s
Aug 29 21:05:40.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008337232s
Aug 29 21:05:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008195781s
Aug 29 21:05:44.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009593084s
Aug 29 21:05:46.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009135195s
Aug 29 21:05:48.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007478768s
Aug 29 21:05:50.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008560746s
Aug 29 21:05:52.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007661026s
Aug 29 21:05:54.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008238591s
Aug 29 21:05:56.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008336699s
Aug 29 21:05:58.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00849654s
Aug 29 21:06:00.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009324906s
Aug 29 21:06:02.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008289728s
Aug 29 21:06:04.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008772606s
Aug 29 21:06:06.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007543067s
Aug 29 21:06:08.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009128537s
Aug 29 21:06:08.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012346196s
STEP: removing the label kubernetes.io/e2e-bfb10d70-c339-463e-a7bc-ddda8569f381 off the node loki-15bd39-worker-1 08/29/23 21:06:08.947
STEP: verifying the node doesn't have the label kubernetes.io/e2e-bfb10d70-c339-463e-a7bc-ddda8569f381 08/29/23 21:06:08.964
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Aug 29 21:06:08.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-1329" for this suite. 08/29/23 21:06:08.974
------------------------------
• [SLOW TEST] [304.241 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 08/29/23 21:01:04.741
    Aug 29 21:01:04.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3913487528
    STEP: Building a namespace api object, basename sched-pred 08/29/23 21:01:04.742
    STEP: Waiting for a default service account to be provisioned in namespace 08/29/23 21:01:04.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/29/23 21:01:04.768
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Aug 29 21:01:04.772: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 29 21:01:04.785: INFO: Waiting for terminating namespaces to be deleted...
    Aug 29 21:01:04.789: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-master-0 before test
    Aug 29 21:01:04.801: INFO: calico-node-f4bng from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.801: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: kube-apiserver-loki-15bd39-master-0 from kube-system started at 2023-08-29 19:11:07 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.801: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: kube-proxy-ds-7pn7t from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.801: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: alertmanager-main-2 from ntnx-system started at 2023-08-29 20:20:42 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.801: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 21:01:04.801: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: fluent-bit-x2ntj from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.801: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: node-exporter-lsprp from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.801: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: nutanix-csi-node-mqc5m from ntnx-system started at 2023-08-29 19:23:42 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.801: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-whbwm from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.801: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 21:01:04.801: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-master-1 before test
    Aug 29 21:01:04.811: INFO: calico-node-snrmp from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.811: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 21:01:04.811: INFO: calico-typha-787bcdb57c-c7xxf from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.812: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: kube-apiserver-loki-15bd39-master-1 from kube-system started at 2023-08-29 19:12:53 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.812: INFO: 	Container kube-apiserver ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: 	Container kube-controller-manager ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: 	Container kube-scheduler ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: kube-proxy-ds-c665w from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.812: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: fluent-bit-nmdm2 from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.812: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: node-exporter-q5l7x from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.812: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: nutanix-csi-node-r85kq from ntnx-system started at 2023-08-29 19:23:49 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.812: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x4m9s from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.812: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 21:01:04.812: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-0 before test
    Aug 29 21:01:04.826: INFO: calico-node-mms8p from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: calico-typha-787bcdb57c-ppmlg from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: coredns-5d88b659b9-6fjb6 from kube-system started at 2023-08-29 19:18:33 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container coredns ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: kube-proxy-ds-nmtgg from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: alertmanager-main-0 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 21:01:04.826: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: blackbox-exporter-7954b6f4db-5vl2v from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container blackbox-exporter ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container module-configmap-reloader ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: csi-snapshot-controller-6d467d46bf-fbxgt from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container snapshot-controller ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: csi-snapshot-webhook-69668f7b57-g5r2n from ntnx-system started at 2023-08-29 19:18:40 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container snapshot-validation ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: fluent-bit-lbvlt from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: kubernetes-events-printer-74464fd469-rrxtq from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container kubernetes-events-printer ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: node-exporter-wwfmq from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: nutanix-csi-node-hql5k from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: prometheus-adapter-6b6d856c7-htsdt from ntnx-system started at 2023-08-29 19:21:04 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: prometheus-k8s-1 from ntnx-system started at 2023-08-29 19:21:14 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container prometheus ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: prometheus-operator-557c85cd6b-b2tpm from ntnx-system started at 2023-08-29 20:20:40 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container prometheus-operator ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: sonobuoy-e2e-job-161e88352cfa47dc from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container e2e ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-x8gzr from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.826: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 21:01:04.826: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-1 before test
    Aug 29 21:01:04.837: INFO: test-webserver-2abab8d3-5401-475b-846d-a620f0b76a85 from container-probe-7788 started at 2023-08-29 21:00:42 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.837: INFO: 	Container test-webserver ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: calico-node-sb7l5 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.837: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: kube-proxy-ds-jt2tk from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.837: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: fluent-bit-zmvl7 from ntnx-system started at 2023-08-29 20:21:06 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.837: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: node-exporter-8gt7t from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.837: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: nutanix-csi-node-mxmhr from ntnx-system started at 2023-08-29 20:21:06 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.837: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: prometheus-k8s-0 from ntnx-system started at 2023-08-29 20:21:14 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.837: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: 	Container prometheus ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-zxhgv from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.837: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 29 21:01:04.837: INFO: 
    Logging pods the apiserver thinks is on node loki-15bd39-worker-2 before test
    Aug 29 21:01:04.848: INFO: calico-kube-controllers-d9df5649-k5gf4 from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: calico-node-9pl5x from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container calico-node ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: calico-typha-787bcdb57c-6bcfm from kube-system started at 2023-08-29 19:17:51 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container calico-typha ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: coredns-5d88b659b9-fqwjz from kube-system started at 2023-08-29 19:46:43 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container coredns ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: kube-proxy-ds-l9vp5 from kube-system started at 2023-08-29 19:17:42 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: alertmanager-main-1 from ntnx-system started at 2023-08-29 19:21:07 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container alertmanager ready: true, restart count 1
    Aug 29 21:01:04.848: INFO: 	Container config-reloader ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: fluent-bit-6wl9b from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container fluent-bit ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: kube-state-metrics-d459f9d68-z5xtj from ntnx-system started at 2023-08-29 19:46:43 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: node-exporter-xhwmw from ntnx-system started at 2023-08-29 19:21:03 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container node-exporter ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: nutanix-csi-controller-657cc74bd5-7j77l from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (5 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container csi-provisioner ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container csi-resizer ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container csi-snapshotter ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container nutanix-csi-plugin ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: nutanix-csi-node-hd5rc from ntnx-system started at 2023-08-29 19:18:41 +0000 UTC (3 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container driver-registrar ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container liveness-probe ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container nutanix-csi-node ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: prometheus-adapter-6b6d856c7-qg8lv from ntnx-system started at 2023-08-29 20:20:40 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: sonobuoy from sonobuoy started at 2023-08-29 19:28:22 +0000 UTC (1 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: sonobuoy-systemd-logs-daemon-set-0e4989b9e6ac420c-9kc6g from sonobuoy started at 2023-08-29 19:28:26 +0000 UTC (2 container statuses recorded)
    Aug 29 21:01:04.848: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 29 21:01:04.848: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/29/23 21:01:04.848
    Aug 29 21:01:04.859: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-1329" to be "running"
    Aug 29 21:01:04.862: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.299981ms
    Aug 29 21:01:06.869: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.009622059s
    Aug 29 21:01:06.869: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/29/23 21:01:06.873
    STEP: Trying to apply a random label on the found node. 08/29/23 21:01:06.893
    STEP: verifying the node has the label kubernetes.io/e2e-bfb10d70-c339-463e-a7bc-ddda8569f381 95 08/29/23 21:01:06.908
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/29/23 21:01:06.913
    Aug 29 21:01:06.919: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-1329" to be "not pending"
    Aug 29 21:01:06.922: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.575261ms
    Aug 29 21:01:08.927: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.007740045s
    Aug 29 21:01:08.927: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.45.35.206 on the node which pod4 resides and expect not scheduled 08/29/23 21:01:08.927
    Aug 29 21:01:08.935: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-1329" to be "not pending"
    Aug 29 21:01:08.938: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.147807ms
    Aug 29 21:01:10.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007793731s
    Aug 29 21:01:12.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009289322s
    Aug 29 21:01:14.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008471586s
    Aug 29 21:01:16.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009299903s
    Aug 29 21:01:18.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00853579s
    Aug 29 21:01:20.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009397771s
    Aug 29 21:01:22.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008388026s
    Aug 29 21:01:24.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010492971s
    Aug 29 21:01:26.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008545979s
    Aug 29 21:01:28.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.009192412s
    Aug 29 21:01:30.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.01029339s
    Aug 29 21:01:32.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010617999s
    Aug 29 21:01:34.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008738594s
    Aug 29 21:01:36.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.011666538s
    Aug 29 21:01:38.949: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.014339037s
    Aug 29 21:01:40.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008402726s
    Aug 29 21:01:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.008149811s
    Aug 29 21:01:44.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.008402796s
    Aug 29 21:01:46.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.008694444s
    Aug 29 21:01:48.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007798822s
    Aug 29 21:01:50.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009162073s
    Aug 29 21:01:52.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008852294s
    Aug 29 21:01:54.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007521217s
    Aug 29 21:01:56.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007795542s
    Aug 29 21:01:58.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.008346466s
    Aug 29 21:02:00.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.009049974s
    Aug 29 21:02:02.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009420801s
    Aug 29 21:02:04.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.008802544s
    Aug 29 21:02:06.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007790478s
    Aug 29 21:02:08.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008464704s
    Aug 29 21:02:10.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009016058s
    Aug 29 21:02:12.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.009470774s
    Aug 29 21:02:14.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007120375s
    Aug 29 21:02:16.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007897519s
    Aug 29 21:02:18.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007892979s
    Aug 29 21:02:20.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007694095s
    Aug 29 21:02:22.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.007555689s
    Aug 29 21:02:24.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008772054s
    Aug 29 21:02:26.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.008494289s
    Aug 29 21:02:28.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.008238425s
    Aug 29 21:02:30.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010028682s
    Aug 29 21:02:32.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.011687707s
    Aug 29 21:02:34.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.008538432s
    Aug 29 21:02:36.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.008159744s
    Aug 29 21:02:38.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.007790804s
    Aug 29 21:02:40.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.008549144s
    Aug 29 21:02:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.008540309s
    Aug 29 21:02:44.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009011294s
    Aug 29 21:02:46.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.009369241s
    Aug 29 21:02:48.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007831823s
    Aug 29 21:02:50.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.008456581s
    Aug 29 21:02:52.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007806521s
    Aug 29 21:02:54.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.0086804s
    Aug 29 21:02:56.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.007382764s
    Aug 29 21:02:58.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008148867s
    Aug 29 21:03:00.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009364645s
    Aug 29 21:03:02.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.01174148s
    Aug 29 21:03:04.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007561037s
    Aug 29 21:03:06.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009862931s
    Aug 29 21:03:08.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.007784157s
    Aug 29 21:03:10.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008734032s
    Aug 29 21:03:12.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.009471781s
    Aug 29 21:03:14.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.008038509s
    Aug 29 21:03:16.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.009175803s
    Aug 29 21:03:18.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.007793482s
    Aug 29 21:03:20.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.008302022s
    Aug 29 21:03:22.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.009757537s
    Aug 29 21:03:24.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.008767071s
    Aug 29 21:03:26.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.008052831s
    Aug 29 21:03:28.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.009786567s
    Aug 29 21:03:30.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.008099039s
    Aug 29 21:03:32.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009631046s
    Aug 29 21:03:34.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.007893867s
    Aug 29 21:03:36.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.009528662s
    Aug 29 21:03:38.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.008398195s
    Aug 29 21:03:40.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.008986634s
    Aug 29 21:03:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.008041153s
    Aug 29 21:03:44.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.008306033s
    Aug 29 21:03:46.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.009865602s
    Aug 29 21:03:48.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009390396s
    Aug 29 21:03:50.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.009458185s
    Aug 29 21:03:52.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.009594434s
    Aug 29 21:03:54.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.009062953s
    Aug 29 21:03:56.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.008609734s
    Aug 29 21:03:58.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009106952s
    Aug 29 21:04:00.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008405727s
    Aug 29 21:04:02.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009191406s
    Aug 29 21:04:04.951: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.016316123s
    Aug 29 21:04:06.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.009834242s
    Aug 29 21:04:08.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008570425s
    Aug 29 21:04:10.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.007603653s
    Aug 29 21:04:12.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.00772815s
    Aug 29 21:04:14.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007487235s
    Aug 29 21:04:16.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.009134259s
    Aug 29 21:04:18.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.007620385s
    Aug 29 21:04:20.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.008557525s
    Aug 29 21:04:22.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.008229245s
    Aug 29 21:04:24.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009995917s
    Aug 29 21:04:26.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.009649223s
    Aug 29 21:04:28.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.008558015s
    Aug 29 21:04:30.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.007655949s
    Aug 29 21:04:32.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007803862s
    Aug 29 21:04:34.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.010747527s
    Aug 29 21:04:36.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.01032614s
    Aug 29 21:04:38.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.008524622s
    Aug 29 21:04:40.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.009467457s
    Aug 29 21:04:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.008557373s
    Aug 29 21:04:44.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.009320402s
    Aug 29 21:04:46.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009083365s
    Aug 29 21:04:48.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008931244s
    Aug 29 21:04:50.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008348987s
    Aug 29 21:04:52.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.008361215s
    Aug 29 21:04:54.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.008420468s
    Aug 29 21:04:56.945: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010372542s
    Aug 29 21:04:58.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.009132461s
    Aug 29 21:05:00.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009530192s
    Aug 29 21:05:02.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.007839206s
    Aug 29 21:05:04.946: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.011443188s
    Aug 29 21:05:06.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.009715044s
    Aug 29 21:05:08.951: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.016489399s
    Aug 29 21:05:10.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008901639s
    Aug 29 21:05:12.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.008721335s
    Aug 29 21:05:14.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007807659s
    Aug 29 21:05:16.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.008219375s
    Aug 29 21:05:18.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008270057s
    Aug 29 21:05:20.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.007282102s
    Aug 29 21:05:22.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.009530537s
    Aug 29 21:05:24.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007872708s
    Aug 29 21:05:26.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.008600927s
    Aug 29 21:05:28.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00825554s
    Aug 29 21:05:30.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008187528s
    Aug 29 21:05:32.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.008541551s
    Aug 29 21:05:34.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.009044913s
    Aug 29 21:05:36.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.008138202s
    Aug 29 21:05:38.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.009451824s
    Aug 29 21:05:40.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.008337232s
    Aug 29 21:05:42.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.008195781s
    Aug 29 21:05:44.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.009593084s
    Aug 29 21:05:46.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.009135195s
    Aug 29 21:05:48.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.007478768s
    Aug 29 21:05:50.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008560746s
    Aug 29 21:05:52.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.007661026s
    Aug 29 21:05:54.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.008238591s
    Aug 29 21:05:56.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.008336699s
    Aug 29 21:05:58.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.00849654s
    Aug 29 21:06:00.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009324906s
    Aug 29 21:06:02.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.008289728s
    Aug 29 21:06:04.943: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008772606s
    Aug 29 21:06:06.942: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.007543067s
    Aug 29 21:06:08.944: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.009128537s
    Aug 29 21:06:08.947: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012346196s
    STEP: removing the label kubernetes.io/e2e-bfb10d70-c339-463e-a7bc-ddda8569f381 off the node loki-15bd39-worker-1 08/29/23 21:06:08.947
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-bfb10d70-c339-463e-a7bc-ddda8569f381 08/29/23 21:06:08.964
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Aug 29 21:06:08.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-1329" for this suite. 08/29/23 21:06:08.974
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Aug 29 21:06:08.985: INFO: Running AfterSuite actions on node 1
Aug 29 21:06:08.985: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Aug 29 21:06:08.985: INFO: Running AfterSuite actions on node 1
    Aug 29 21:06:08.985: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.116 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5845.371 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h37m25.759323903s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

