I0105 19:01:46.284913      23 e2e.go:126] Starting e2e run "8bfd2e17-1f7d-4837-af63-bc3dee1c8575" on Ginkgo node 1
Jan  5 19:01:46.326: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1672945306 - will randomize all specs

Will run 368 of 7069 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan  5 19:01:46.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:01:46.678: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jan  5 19:01:46.693: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jan  5 19:01:46.739: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jan  5 19:01:46.740: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
Jan  5 19:01:46.740: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jan  5 19:01:46.756: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'fluentbit-gke' (0 seconds elapsed)
Jan  5 19:01:46.756: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'fluentbit-gke-256pd' (0 seconds elapsed)
Jan  5 19:01:46.756: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'fluentbit-gke-max' (0 seconds elapsed)
Jan  5 19:01:46.757: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent' (0 seconds elapsed)
Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent-scaling-10' (0 seconds elapsed)
Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent-scaling-20' (0 seconds elapsed)
Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent-windows' (0 seconds elapsed)
Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'metadata-proxy-v0.1' (0 seconds elapsed)
Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nccl-fastsocket-installer' (0 seconds elapsed)
Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin-large' (0 seconds elapsed)
Jan  5 19:01:46.758: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin-medium' (0 seconds elapsed)
Jan  5 19:01:46.758: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin-small' (0 seconds elapsed)
Jan  5 19:01:46.758: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'pdcsi-node' (0 seconds elapsed)
Jan  5 19:01:46.758: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'pdcsi-node-windows' (0 seconds elapsed)
Jan  5 19:01:46.758: INFO: e2e test version: v1.26.0
Jan  5 19:01:46.760: INFO: kube-apiserver version: v1.26.0-gke.1500
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:77
Jan  5 19:01:46.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:01:46.765: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.089 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan  5 19:01:46.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:01:46.678: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jan  5 19:01:46.693: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jan  5 19:01:46.739: INFO: 22 / 22 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jan  5 19:01:46.740: INFO: expected 10 pod replicas in namespace 'kube-system', 10 are Running and Ready.
    Jan  5 19:01:46.740: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jan  5 19:01:46.756: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'fluentbit-gke' (0 seconds elapsed)
    Jan  5 19:01:46.756: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'fluentbit-gke-256pd' (0 seconds elapsed)
    Jan  5 19:01:46.756: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'fluentbit-gke-max' (0 seconds elapsed)
    Jan  5 19:01:46.757: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent' (0 seconds elapsed)
    Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent-scaling-10' (0 seconds elapsed)
    Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent-scaling-20' (0 seconds elapsed)
    Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'gke-metrics-agent-windows' (0 seconds elapsed)
    Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'metadata-proxy-v0.1' (0 seconds elapsed)
    Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nccl-fastsocket-installer' (0 seconds elapsed)
    Jan  5 19:01:46.757: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin-large' (0 seconds elapsed)
    Jan  5 19:01:46.758: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin-medium' (0 seconds elapsed)
    Jan  5 19:01:46.758: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'nvidia-gpu-device-plugin-small' (0 seconds elapsed)
    Jan  5 19:01:46.758: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'pdcsi-node' (0 seconds elapsed)
    Jan  5 19:01:46.758: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'pdcsi-node-windows' (0 seconds elapsed)
    Jan  5 19:01:46.758: INFO: e2e test version: v1.26.0
    Jan  5 19:01:46.760: INFO: kube-apiserver version: v1.26.0-gke.1500
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:77
    Jan  5 19:01:46.760: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:01:46.765: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:01:46.805
Jan  5 19:01:46.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 19:01:46.806
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:01:46.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:01:46.826
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45
STEP: Creating configMap configmap-6692/configmap-test-0be96175-de72-4c37-8298-31ddbba06342 01/05/23 19:01:46.83
STEP: Creating a pod to test consume configMaps 01/05/23 19:01:46.833
Jan  5 19:01:46.844: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702" in namespace "configmap-6692" to be "Succeeded or Failed"
Jan  5 19:01:46.850: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702": Phase="Pending", Reason="", readiness=false. Elapsed: 5.945795ms
Jan  5 19:01:48.854: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702": Phase="Running", Reason="", readiness=true. Elapsed: 2.009438171s
Jan  5 19:01:50.854: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702": Phase="Running", Reason="", readiness=false. Elapsed: 4.010084546s
Jan  5 19:01:52.853: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009094871s
STEP: Saw pod success 01/05/23 19:01:52.853
Jan  5 19:01:52.854: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702" satisfied condition "Succeeded or Failed"
Jan  5 19:01:52.856: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702 container env-test: <nil>
STEP: delete the pod 01/05/23 19:01:52.871
Jan  5 19:01:52.885: INFO: Waiting for pod pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702 to disappear
Jan  5 19:01:52.888: INFO: Pod pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:01:52.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6692" for this suite. 01/05/23 19:01:52.893
------------------------------
• [SLOW TEST] [6.097 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:01:46.805
    Jan  5 19:01:46.805: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 19:01:46.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:01:46.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:01:46.826
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:45
    STEP: Creating configMap configmap-6692/configmap-test-0be96175-de72-4c37-8298-31ddbba06342 01/05/23 19:01:46.83
    STEP: Creating a pod to test consume configMaps 01/05/23 19:01:46.833
    Jan  5 19:01:46.844: INFO: Waiting up to 5m0s for pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702" in namespace "configmap-6692" to be "Succeeded or Failed"
    Jan  5 19:01:46.850: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702": Phase="Pending", Reason="", readiness=false. Elapsed: 5.945795ms
    Jan  5 19:01:48.854: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702": Phase="Running", Reason="", readiness=true. Elapsed: 2.009438171s
    Jan  5 19:01:50.854: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702": Phase="Running", Reason="", readiness=false. Elapsed: 4.010084546s
    Jan  5 19:01:52.853: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009094871s
    STEP: Saw pod success 01/05/23 19:01:52.853
    Jan  5 19:01:52.854: INFO: Pod "pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702" satisfied condition "Succeeded or Failed"
    Jan  5 19:01:52.856: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702 container env-test: <nil>
    STEP: delete the pod 01/05/23 19:01:52.871
    Jan  5 19:01:52.885: INFO: Waiting for pod pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702 to disappear
    Jan  5 19:01:52.888: INFO: Pod pod-configmaps-4a62e2ad-56bb-4931-917d-bf7abbc21702 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:01:52.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6692" for this suite. 01/05/23 19:01:52.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:01:52.903
Jan  5 19:01:52.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:01:52.905
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:01:52.916
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:01:52.919
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75
STEP: Counting existing ResourceQuota 01/05/23 19:01:52.923
STEP: Creating a ResourceQuota 01/05/23 19:01:57.926
STEP: Ensuring resource quota status is calculated 01/05/23 19:01:57.932
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:01:59.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4990" for this suite. 01/05/23 19:01:59.94
------------------------------
• [SLOW TEST] [7.042 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:01:52.903
    Jan  5 19:01:52.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:01:52.905
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:01:52.916
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:01:52.919
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:75
    STEP: Counting existing ResourceQuota 01/05/23 19:01:52.923
    STEP: Creating a ResourceQuota 01/05/23 19:01:57.926
    STEP: Ensuring resource quota status is calculated 01/05/23 19:01:57.932
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:01:59.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4990" for this suite. 01/05/23 19:01:59.94
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:01:59.945
Jan  5 19:01:59.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:01:59.947
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:01:59.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:01:59.974
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618
Jan  5 19:01:59.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: creating the pod 01/05/23 19:01:59.978
STEP: submitting the pod to kubernetes 01/05/23 19:01:59.979
Jan  5 19:01:59.987: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d" in namespace "pods-6511" to be "running and ready"
Jan  5 19:01:59.990: INFO: Pod "pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907746ms
Jan  5 19:01:59.990: INFO: The phase of Pod pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:02:01.993: INFO: Pod "pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006240823s
Jan  5 19:02:01.993: INFO: The phase of Pod pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d is Running (Ready = true)
Jan  5 19:02:01.994: INFO: Pod "pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:02.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-6511" for this suite. 01/05/23 19:02:02.01
------------------------------
• [2.070 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:618

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:01:59.945
    Jan  5 19:01:59.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:01:59.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:01:59.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:01:59.974
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:618
    Jan  5 19:01:59.978: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: creating the pod 01/05/23 19:01:59.978
    STEP: submitting the pod to kubernetes 01/05/23 19:01:59.979
    Jan  5 19:01:59.987: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d" in namespace "pods-6511" to be "running and ready"
    Jan  5 19:01:59.990: INFO: Pod "pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.907746ms
    Jan  5 19:01:59.990: INFO: The phase of Pod pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:02:01.993: INFO: Pod "pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d": Phase="Running", Reason="", readiness=true. Elapsed: 2.006240823s
    Jan  5 19:02:01.993: INFO: The phase of Pod pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d is Running (Ready = true)
    Jan  5 19:02:01.994: INFO: Pod "pod-logs-websocket-7efd490c-7309-4c72-a12b-20bddf60dc9d" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:02.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-6511" for this suite. 01/05/23 19:02:02.01
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:02.016
Jan  5 19:02:02.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:02:02.017
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:02.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:02.035
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 19:02:02.038
Jan  5 19:02:02.048: INFO: Waiting up to 5m0s for pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6" in namespace "emptydir-7867" to be "Succeeded or Failed"
Jan  5 19:02:02.051: INFO: Pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.960929ms
Jan  5 19:02:04.055: INFO: Pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007507459s
Jan  5 19:02:06.056: INFO: Pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007799701s
STEP: Saw pod success 01/05/23 19:02:06.056
Jan  5 19:02:06.056: INFO: Pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6" satisfied condition "Succeeded or Failed"
Jan  5 19:02:06.059: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6 container test-container: <nil>
STEP: delete the pod 01/05/23 19:02:06.076
Jan  5 19:02:06.090: INFO: Waiting for pod pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6 to disappear
Jan  5 19:02:06.093: INFO: Pod pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:06.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-7867" for this suite. 01/05/23 19:02:06.097
------------------------------
• [4.088 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:02.016
    Jan  5 19:02:02.016: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:02:02.017
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:02.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:02.035
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:137
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 19:02:02.038
    Jan  5 19:02:02.048: INFO: Waiting up to 5m0s for pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6" in namespace "emptydir-7867" to be "Succeeded or Failed"
    Jan  5 19:02:02.051: INFO: Pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.960929ms
    Jan  5 19:02:04.055: INFO: Pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007507459s
    Jan  5 19:02:06.056: INFO: Pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007799701s
    STEP: Saw pod success 01/05/23 19:02:06.056
    Jan  5 19:02:06.056: INFO: Pod "pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6" satisfied condition "Succeeded or Failed"
    Jan  5 19:02:06.059: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6 container test-container: <nil>
    STEP: delete the pod 01/05/23 19:02:06.076
    Jan  5 19:02:06.090: INFO: Waiting for pod pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6 to disappear
    Jan  5 19:02:06.093: INFO: Pod pod-3fcc2bbf-0b7d-44ed-89bc-73dd32b1d4a6 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:06.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-7867" for this suite. 01/05/23 19:02:06.097
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:06.107
Jan  5 19:02:06.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:02:06.109
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:06.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:06.123
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:02:06.126
Jan  5 19:02:06.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e" in namespace "downward-api-6213" to be "Succeeded or Failed"
Jan  5 19:02:06.141: INFO: Pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.518581ms
Jan  5 19:02:08.145: INFO: Pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00842752s
Jan  5 19:02:10.145: INFO: Pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007957291s
STEP: Saw pod success 01/05/23 19:02:10.145
Jan  5 19:02:10.145: INFO: Pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e" satisfied condition "Succeeded or Failed"
Jan  5 19:02:10.148: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e container client-container: <nil>
STEP: delete the pod 01/05/23 19:02:10.154
Jan  5 19:02:10.168: INFO: Waiting for pod downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e to disappear
Jan  5 19:02:10.172: INFO: Pod downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:10.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6213" for this suite. 01/05/23 19:02:10.177
------------------------------
• [4.076 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:06.107
    Jan  5 19:02:06.107: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:02:06.109
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:06.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:06.123
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:207
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:02:06.126
    Jan  5 19:02:06.137: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e" in namespace "downward-api-6213" to be "Succeeded or Failed"
    Jan  5 19:02:06.141: INFO: Pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.518581ms
    Jan  5 19:02:08.145: INFO: Pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00842752s
    Jan  5 19:02:10.145: INFO: Pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007957291s
    STEP: Saw pod success 01/05/23 19:02:10.145
    Jan  5 19:02:10.145: INFO: Pod "downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e" satisfied condition "Succeeded or Failed"
    Jan  5 19:02:10.148: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e container client-container: <nil>
    STEP: delete the pod 01/05/23 19:02:10.154
    Jan  5 19:02:10.168: INFO: Waiting for pod downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e to disappear
    Jan  5 19:02:10.172: INFO: Pod downwardapi-volume-bc2ac865-8238-4510-ba91-ac67def8807e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:10.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6213" for this suite. 01/05/23 19:02:10.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:10.189
Jan  5 19:02:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:02:10.19
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:10.206
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:10.211
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95
STEP: creating secret secrets-7385/secret-test-2d1de651-e0dc-418e-b840-0f10010e4d4f 01/05/23 19:02:10.214
STEP: Creating a pod to test consume secrets 01/05/23 19:02:10.218
Jan  5 19:02:10.228: INFO: Waiting up to 5m0s for pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3" in namespace "secrets-7385" to be "Succeeded or Failed"
Jan  5 19:02:10.233: INFO: Pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73783ms
Jan  5 19:02:12.237: INFO: Pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008709867s
Jan  5 19:02:14.236: INFO: Pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008345096s
STEP: Saw pod success 01/05/23 19:02:14.236
Jan  5 19:02:14.237: INFO: Pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3" satisfied condition "Succeeded or Failed"
Jan  5 19:02:14.239: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3 container env-test: <nil>
STEP: delete the pod 01/05/23 19:02:14.246
Jan  5 19:02:14.257: INFO: Waiting for pod pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3 to disappear
Jan  5 19:02:14.259: INFO: Pod pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:14.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7385" for this suite. 01/05/23 19:02:14.264
------------------------------
• [4.082 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:95

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:10.189
    Jan  5 19:02:10.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:02:10.19
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:10.206
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:10.211
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:95
    STEP: creating secret secrets-7385/secret-test-2d1de651-e0dc-418e-b840-0f10010e4d4f 01/05/23 19:02:10.214
    STEP: Creating a pod to test consume secrets 01/05/23 19:02:10.218
    Jan  5 19:02:10.228: INFO: Waiting up to 5m0s for pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3" in namespace "secrets-7385" to be "Succeeded or Failed"
    Jan  5 19:02:10.233: INFO: Pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.73783ms
    Jan  5 19:02:12.237: INFO: Pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008709867s
    Jan  5 19:02:14.236: INFO: Pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008345096s
    STEP: Saw pod success 01/05/23 19:02:14.236
    Jan  5 19:02:14.237: INFO: Pod "pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3" satisfied condition "Succeeded or Failed"
    Jan  5 19:02:14.239: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3 container env-test: <nil>
    STEP: delete the pod 01/05/23 19:02:14.246
    Jan  5 19:02:14.257: INFO: Waiting for pod pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3 to disappear
    Jan  5 19:02:14.259: INFO: Pod pod-configmaps-d32df1fa-ca25-4d16-a9ca-28e1f2912fc3 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:14.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7385" for this suite. 01/05/23 19:02:14.264
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:14.278
Jan  5 19:02:14.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:02:14.28
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:14.293
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:14.296
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:339
STEP: creating a replication controller 01/05/23 19:02:14.3
Jan  5 19:02:14.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 create -f -'
Jan  5 19:02:15.665: INFO: stderr: ""
Jan  5 19:02:15.665: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 19:02:15.665
Jan  5 19:02:15.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 19:02:15.753: INFO: stderr: ""
Jan  5 19:02:15.754: INFO: stdout: "update-demo-nautilus-dtp7z update-demo-nautilus-px8ss "
Jan  5 19:02:15.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-dtp7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:02:15.838: INFO: stderr: ""
Jan  5 19:02:15.838: INFO: stdout: ""
Jan  5 19:02:15.838: INFO: update-demo-nautilus-dtp7z is created but not running
Jan  5 19:02:20.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 19:02:20.933: INFO: stderr: ""
Jan  5 19:02:20.933: INFO: stdout: "update-demo-nautilus-dtp7z update-demo-nautilus-px8ss "
Jan  5 19:02:20.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-dtp7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:02:21.015: INFO: stderr: ""
Jan  5 19:02:21.015: INFO: stdout: "true"
Jan  5 19:02:21.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-dtp7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 19:02:21.102: INFO: stderr: ""
Jan  5 19:02:21.102: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  5 19:02:21.102: INFO: validating pod update-demo-nautilus-dtp7z
Jan  5 19:02:21.114: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 19:02:21.114: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 19:02:21.114: INFO: update-demo-nautilus-dtp7z is verified up and running
Jan  5 19:02:21.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-px8ss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:02:21.191: INFO: stderr: ""
Jan  5 19:02:21.191: INFO: stdout: "true"
Jan  5 19:02:21.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-px8ss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 19:02:21.266: INFO: stderr: ""
Jan  5 19:02:21.266: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  5 19:02:21.266: INFO: validating pod update-demo-nautilus-px8ss
Jan  5 19:02:21.274: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 19:02:21.275: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 19:02:21.275: INFO: update-demo-nautilus-px8ss is verified up and running
STEP: using delete to clean up resources 01/05/23 19:02:21.275
Jan  5 19:02:21.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 delete --grace-period=0 --force -f -'
Jan  5 19:02:21.355: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 19:02:21.355: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  5 19:02:21.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get rc,svc -l name=update-demo --no-headers'
Jan  5 19:02:21.523: INFO: stderr: "No resources found in kubectl-7228 namespace.\n"
Jan  5 19:02:21.523: INFO: stdout: ""
Jan  5 19:02:21.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 19:02:21.655: INFO: stderr: ""
Jan  5 19:02:21.655: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:21.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-7228" for this suite. 01/05/23 19:02:21.66
------------------------------
• [SLOW TEST] [7.388 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:339

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:14.278
    Jan  5 19:02:14.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:02:14.28
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:14.293
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:14.296
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:339
    STEP: creating a replication controller 01/05/23 19:02:14.3
    Jan  5 19:02:14.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 create -f -'
    Jan  5 19:02:15.665: INFO: stderr: ""
    Jan  5 19:02:15.665: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 19:02:15.665
    Jan  5 19:02:15.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 19:02:15.753: INFO: stderr: ""
    Jan  5 19:02:15.754: INFO: stdout: "update-demo-nautilus-dtp7z update-demo-nautilus-px8ss "
    Jan  5 19:02:15.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-dtp7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:02:15.838: INFO: stderr: ""
    Jan  5 19:02:15.838: INFO: stdout: ""
    Jan  5 19:02:15.838: INFO: update-demo-nautilus-dtp7z is created but not running
    Jan  5 19:02:20.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 19:02:20.933: INFO: stderr: ""
    Jan  5 19:02:20.933: INFO: stdout: "update-demo-nautilus-dtp7z update-demo-nautilus-px8ss "
    Jan  5 19:02:20.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-dtp7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:02:21.015: INFO: stderr: ""
    Jan  5 19:02:21.015: INFO: stdout: "true"
    Jan  5 19:02:21.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-dtp7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 19:02:21.102: INFO: stderr: ""
    Jan  5 19:02:21.102: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  5 19:02:21.102: INFO: validating pod update-demo-nautilus-dtp7z
    Jan  5 19:02:21.114: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 19:02:21.114: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 19:02:21.114: INFO: update-demo-nautilus-dtp7z is verified up and running
    Jan  5 19:02:21.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-px8ss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:02:21.191: INFO: stderr: ""
    Jan  5 19:02:21.191: INFO: stdout: "true"
    Jan  5 19:02:21.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods update-demo-nautilus-px8ss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 19:02:21.266: INFO: stderr: ""
    Jan  5 19:02:21.266: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  5 19:02:21.266: INFO: validating pod update-demo-nautilus-px8ss
    Jan  5 19:02:21.274: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 19:02:21.275: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 19:02:21.275: INFO: update-demo-nautilus-px8ss is verified up and running
    STEP: using delete to clean up resources 01/05/23 19:02:21.275
    Jan  5 19:02:21.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 delete --grace-period=0 --force -f -'
    Jan  5 19:02:21.355: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 19:02:21.355: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  5 19:02:21.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get rc,svc -l name=update-demo --no-headers'
    Jan  5 19:02:21.523: INFO: stderr: "No resources found in kubectl-7228 namespace.\n"
    Jan  5 19:02:21.523: INFO: stdout: ""
    Jan  5 19:02:21.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-7228 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 19:02:21.655: INFO: stderr: ""
    Jan  5 19:02:21.655: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:21.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-7228" for this suite. 01/05/23 19:02:21.66
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:21.667
Jan  5 19:02:21.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename deployment 01/05/23 19:02:21.67
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:21.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:21.686
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jan  5 19:02:21.693: INFO: Creating deployment "webserver-deployment"
Jan  5 19:02:21.699: INFO: Waiting for observed generation 1
Jan  5 19:02:23.707: INFO: Waiting for all required pods to come up
Jan  5 19:02:23.712: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 01/05/23 19:02:23.714
Jan  5 19:02:23.714: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5tgdf" in namespace "deployment-2263" to be "running"
Jan  5 19:02:23.715: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6f4vc" in namespace "deployment-2263" to be "running"
Jan  5 19:02:23.715: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bgbqk" in namespace "deployment-2263" to be "running"
Jan  5 19:02:23.719: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k7nkd" in namespace "deployment-2263" to be "running"
Jan  5 19:02:23.719: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-kfg6r" in namespace "deployment-2263" to be "running"
Jan  5 19:02:23.719: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-q8j2q" in namespace "deployment-2263" to be "running"
Jan  5 19:02:23.719: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-w27qq" in namespace "deployment-2263" to be "running"
Jan  5 19:02:23.720: INFO: Pod "webserver-deployment-7f5969cbc7-5tgdf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.85471ms
Jan  5 19:02:23.721: INFO: Pod "webserver-deployment-7f5969cbc7-6f4vc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.565301ms
Jan  5 19:02:23.727: INFO: Pod "webserver-deployment-7f5969cbc7-bgbqk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.151527ms
Jan  5 19:02:23.727: INFO: Pod "webserver-deployment-7f5969cbc7-k7nkd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.794953ms
Jan  5 19:02:23.727: INFO: Pod "webserver-deployment-7f5969cbc7-q8j2q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.858252ms
Jan  5 19:02:23.733: INFO: Pod "webserver-deployment-7f5969cbc7-kfg6r": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391861ms
Jan  5 19:02:23.734: INFO: Pod "webserver-deployment-7f5969cbc7-w27qq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.786014ms
Jan  5 19:02:25.731: INFO: Pod "webserver-deployment-7f5969cbc7-6f4vc": Phase="Running", Reason="", readiness=true. Elapsed: 2.015688108s
Jan  5 19:02:25.731: INFO: Pod "webserver-deployment-7f5969cbc7-6f4vc" satisfied condition "running"
Jan  5 19:02:25.731: INFO: Pod "webserver-deployment-7f5969cbc7-q8j2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007100397s
Jan  5 19:02:25.731: INFO: Pod "webserver-deployment-7f5969cbc7-q8j2q" satisfied condition "running"
Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-bgbqk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010963153s
Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-bgbqk" satisfied condition "running"
Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-k7nkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010439425s
Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-k7nkd" satisfied condition "running"
Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-5tgdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017237407s
Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-5tgdf" satisfied condition "running"
Jan  5 19:02:25.737: INFO: Pod "webserver-deployment-7f5969cbc7-kfg6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.013113059s
Jan  5 19:02:25.737: INFO: Pod "webserver-deployment-7f5969cbc7-kfg6r" satisfied condition "running"
Jan  5 19:02:25.738: INFO: Pod "webserver-deployment-7f5969cbc7-w27qq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011576503s
Jan  5 19:02:25.738: INFO: Pod "webserver-deployment-7f5969cbc7-w27qq" satisfied condition "running"
Jan  5 19:02:25.738: INFO: Waiting for deployment "webserver-deployment" to complete
Jan  5 19:02:25.742: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jan  5 19:02:25.752: INFO: Updating deployment webserver-deployment
Jan  5 19:02:25.752: INFO: Waiting for observed generation 2
Jan  5 19:02:27.759: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jan  5 19:02:27.763: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jan  5 19:02:27.765: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  5 19:02:27.774: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jan  5 19:02:27.774: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jan  5 19:02:27.777: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jan  5 19:02:27.782: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jan  5 19:02:27.783: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jan  5 19:02:27.793: INFO: Updating deployment webserver-deployment
Jan  5 19:02:27.794: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jan  5 19:02:27.802: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jan  5 19:02:27.808: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 19:02:27.827: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-2263  012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373 49570 3 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c23318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-05 19:02:25 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 19:02:27 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jan  5 19:02:27.847: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2263  52348989-6972-44ad-b4d2-579fbd7a0ccf 49568 3 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373 0xc004c23817 0xc004c23818}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c238b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:02:27.854: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jan  5 19:02:27.855: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2263  6f0eced5-1d09-4634-91e2-8c2d36a806bc 49567 3 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373 0xc004c23727 0xc004c23728}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c237b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:02:27.885: INFO: Pod "webserver-deployment-7f5969cbc7-5g5rb" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5g5rb webserver-deployment-7f5969cbc7- deployment-2263  8c60303e-e1ac-4efb-baf5-3c7579d40b3a 49463 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004cca967 0xc004cca968}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z49q7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z49q7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.102,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://05b7fbd553bda0652e3f3daeae2f8a6d6dfb2a010456ec69e1de56d8bde165e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.897: INFO: Pod "webserver-deployment-7f5969cbc7-6f4vc" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6f4vc webserver-deployment-7f5969cbc7- deployment-2263  9ebc8d65-c62e-4a0f-b3e0-9607d9b84ce5 49480 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccab40 0xc004ccab41}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.0.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b25xq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b25xq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.37,PodIP:10.16.0.64,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f42b4be0b9dcc61c9960be34ca80c5783a238ed39a67d9676e8f8a9d986a0975,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.0.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.901: INFO: Pod "webserver-deployment-7f5969cbc7-bgbqk" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bgbqk webserver-deployment-7f5969cbc7- deployment-2263  62583b86-edd8-491c-83d5-2873e9f370ec 49478 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccad10 0xc004ccad11}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.0.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b22nn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b22nn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.37,PodIP:10.16.0.62,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18fa24adead8c28e7a50265d9e13d9defa0f7229f57d91b869564f2b80d2310c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.0.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.902: INFO: Pod "webserver-deployment-7f5969cbc7-k7nkd" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k7nkd webserver-deployment-7f5969cbc7- deployment-2263  cb656f75-09cf-4a73-b73d-c192842904ec 49471 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccaee0 0xc004ccaee1}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kgj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kgj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.103,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b4fb834de75f0d5eab39f0e6d32b96037c2d5c9f2859495f3ef8b7f271d813ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.904: INFO: Pod "webserver-deployment-7f5969cbc7-kfg6r" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kfg6r webserver-deployment-7f5969cbc7- deployment-2263  5828ef30-f9b4-492f-b43b-67aef3e6f37d 49483 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb0b0 0xc004ccb0b1}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.0.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9pzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9pzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.37,PodIP:10.16.0.63,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://731acc18b198b25b3cf6cffb5f620b2e16e7a88b778d64949446d78e4886b472,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.0.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.914: INFO: Pod "webserver-deployment-7f5969cbc7-kpchj" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kpchj webserver-deployment-7f5969cbc7- deployment-2263  a8889b9b-3ac0-4a33-8fc4-7ebf6a48d8c8 49578 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb280 0xc004ccb281}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwlcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwlcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.914: INFO: Pod "webserver-deployment-7f5969cbc7-n8p98" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n8p98 webserver-deployment-7f5969cbc7- deployment-2263  1a846382-f325-4e11-974f-ae535af2db3e 49577 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb3b7 0xc004ccb3b8}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lrrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lrrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.914: INFO: Pod "webserver-deployment-7f5969cbc7-q8j2q" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q8j2q webserver-deployment-7f5969cbc7- deployment-2263  a6d1bbb2-da7d-4ca3-9084-f61c90786517 49493 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb510 0xc004ccb511}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9sfzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9sfzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.175,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://88154d41f63e9e7ebf5e9cea9b24e39a84b18d15a5c1ade8cfba4a267cf4a316,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.915: INFO: Pod "webserver-deployment-7f5969cbc7-xskjx" is not available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xskjx webserver-deployment-7f5969cbc7- deployment-2263  f60962df-d9ce-46d6-8aba-9d7d136af11a 49579 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb6e0 0xc004ccb6e1}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pksc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pksc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:,StartTime:2023-01-05 19:02:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.920: INFO: Pod "webserver-deployment-7f5969cbc7-xzbgq" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xzbgq webserver-deployment-7f5969cbc7- deployment-2263  2a63f6cb-4354-4dc9-8e00-1f7cd541f9c7 49466 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb897 0xc004ccb898}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbkvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbkvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.174,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5cf15ffccfeb301353f6e0e3440a9a601e1723f1f88575ff22749c5d436b37e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.936: INFO: Pod "webserver-deployment-7f5969cbc7-zh74m" is available:
&Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zh74m webserver-deployment-7f5969cbc7- deployment-2263  31f8b681-74d8-414f-a313-a55aafd652d2 49461 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccba70 0xc004ccba71}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvkrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvkrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.104,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3f29cc3a40e012d6f741f467d32dc681a2d50cdcdf695dffd54527ef4d78c851,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.941: INFO: Pod "webserver-deployment-d9f79cb5-26jzv" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-26jzv webserver-deployment-d9f79cb5- deployment-2263  9f05d7ab-bf1a-42e5-b451-2811074c53f6 49576 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004ccbc2f 0xc004ccbc40}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5rfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5rfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.944: INFO: Pod "webserver-deployment-d9f79cb5-4qhpp" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4qhpp webserver-deployment-d9f79cb5- deployment-2263  110d45d3-e0c2-46ed-b278-5ff16b0bf9b5 49564 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004ccbd8f 0xc004ccbda0}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5jrmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5jrmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.105,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.945: INFO: Pod "webserver-deployment-d9f79cb5-57nqz" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-57nqz webserver-deployment-d9f79cb5- deployment-2263  1c346551-d8a7-4168-9073-79b5028d51a7 49565 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004ccbf8f 0xc004ccbfa0}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mf7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mf7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.178,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.946: INFO: Pod "webserver-deployment-d9f79cb5-7tw89" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7tw89 webserver-deployment-d9f79cb5- deployment-2263  1842861d-641a-440e-a6b2-c80a40d1abe9 49544 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce18f 0xc004dce1a0}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xp8fp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xp8fp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.947: INFO: Pod "webserver-deployment-d9f79cb5-d62nj" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d62nj webserver-deployment-d9f79cb5- deployment-2263  941ed978-21d7-4d4c-8b2f-2a420b1fb172 49532 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce35f 0xc004dce370}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57krk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57krk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.37,PodIP:,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.947: INFO: Pod "webserver-deployment-d9f79cb5-f9m68" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f9m68 webserver-deployment-d9f79cb5- deployment-2263  22387f66-b1b2-411c-9a63-2a2ec55519fd 49543 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce52f 0xc004dce540}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq5db,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq5db,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.948: INFO: Pod "webserver-deployment-d9f79cb5-xmfbt" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xmfbt webserver-deployment-d9f79cb5- deployment-2263  34c8af3e-2ffb-46c9-af74-657e99cd5199 49580 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce6ff 0xc004dce710}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grd5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grd5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:02:27.948: INFO: Pod "webserver-deployment-d9f79cb5-xv95x" is not available:
&Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xv95x webserver-deployment-d9f79cb5- deployment-2263  af9dc618-5f61-431c-9ab5-ade7a0c053e6 49581 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce857 0xc004dce858}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w952q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w952q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:27.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2263" for this suite. 01/05/23 19:02:27.977
------------------------------
• [SLOW TEST] [6.330 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:21.667
    Jan  5 19:02:21.667: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename deployment 01/05/23 19:02:21.67
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:21.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:21.686
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jan  5 19:02:21.693: INFO: Creating deployment "webserver-deployment"
    Jan  5 19:02:21.699: INFO: Waiting for observed generation 1
    Jan  5 19:02:23.707: INFO: Waiting for all required pods to come up
    Jan  5 19:02:23.712: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 01/05/23 19:02:23.714
    Jan  5 19:02:23.714: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-5tgdf" in namespace "deployment-2263" to be "running"
    Jan  5 19:02:23.715: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-6f4vc" in namespace "deployment-2263" to be "running"
    Jan  5 19:02:23.715: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-bgbqk" in namespace "deployment-2263" to be "running"
    Jan  5 19:02:23.719: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-k7nkd" in namespace "deployment-2263" to be "running"
    Jan  5 19:02:23.719: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-kfg6r" in namespace "deployment-2263" to be "running"
    Jan  5 19:02:23.719: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-q8j2q" in namespace "deployment-2263" to be "running"
    Jan  5 19:02:23.719: INFO: Waiting up to 5m0s for pod "webserver-deployment-7f5969cbc7-w27qq" in namespace "deployment-2263" to be "running"
    Jan  5 19:02:23.720: INFO: Pod "webserver-deployment-7f5969cbc7-5tgdf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.85471ms
    Jan  5 19:02:23.721: INFO: Pod "webserver-deployment-7f5969cbc7-6f4vc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.565301ms
    Jan  5 19:02:23.727: INFO: Pod "webserver-deployment-7f5969cbc7-bgbqk": Phase="Pending", Reason="", readiness=false. Elapsed: 6.151527ms
    Jan  5 19:02:23.727: INFO: Pod "webserver-deployment-7f5969cbc7-k7nkd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.794953ms
    Jan  5 19:02:23.727: INFO: Pod "webserver-deployment-7f5969cbc7-q8j2q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.858252ms
    Jan  5 19:02:23.733: INFO: Pod "webserver-deployment-7f5969cbc7-kfg6r": Phase="Pending", Reason="", readiness=false. Elapsed: 9.391861ms
    Jan  5 19:02:23.734: INFO: Pod "webserver-deployment-7f5969cbc7-w27qq": Phase="Pending", Reason="", readiness=false. Elapsed: 7.786014ms
    Jan  5 19:02:25.731: INFO: Pod "webserver-deployment-7f5969cbc7-6f4vc": Phase="Running", Reason="", readiness=true. Elapsed: 2.015688108s
    Jan  5 19:02:25.731: INFO: Pod "webserver-deployment-7f5969cbc7-6f4vc" satisfied condition "running"
    Jan  5 19:02:25.731: INFO: Pod "webserver-deployment-7f5969cbc7-q8j2q": Phase="Running", Reason="", readiness=true. Elapsed: 2.007100397s
    Jan  5 19:02:25.731: INFO: Pod "webserver-deployment-7f5969cbc7-q8j2q" satisfied condition "running"
    Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-bgbqk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010963153s
    Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-bgbqk" satisfied condition "running"
    Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-k7nkd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010439425s
    Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-k7nkd" satisfied condition "running"
    Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-5tgdf": Phase="Running", Reason="", readiness=true. Elapsed: 2.017237407s
    Jan  5 19:02:25.732: INFO: Pod "webserver-deployment-7f5969cbc7-5tgdf" satisfied condition "running"
    Jan  5 19:02:25.737: INFO: Pod "webserver-deployment-7f5969cbc7-kfg6r": Phase="Running", Reason="", readiness=true. Elapsed: 2.013113059s
    Jan  5 19:02:25.737: INFO: Pod "webserver-deployment-7f5969cbc7-kfg6r" satisfied condition "running"
    Jan  5 19:02:25.738: INFO: Pod "webserver-deployment-7f5969cbc7-w27qq": Phase="Running", Reason="", readiness=true. Elapsed: 2.011576503s
    Jan  5 19:02:25.738: INFO: Pod "webserver-deployment-7f5969cbc7-w27qq" satisfied condition "running"
    Jan  5 19:02:25.738: INFO: Waiting for deployment "webserver-deployment" to complete
    Jan  5 19:02:25.742: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jan  5 19:02:25.752: INFO: Updating deployment webserver-deployment
    Jan  5 19:02:25.752: INFO: Waiting for observed generation 2
    Jan  5 19:02:27.759: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jan  5 19:02:27.763: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jan  5 19:02:27.765: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 19:02:27.774: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jan  5 19:02:27.774: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jan  5 19:02:27.777: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 19:02:27.782: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jan  5 19:02:27.783: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jan  5 19:02:27.793: INFO: Updating deployment webserver-deployment
    Jan  5 19:02:27.794: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jan  5 19:02:27.802: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jan  5 19:02:27.808: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 19:02:27.827: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-2263  012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373 49570 3 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c23318 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-d9f79cb5" is progressing.,LastUpdateTime:2023-01-05 19:02:25 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 19:02:27 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jan  5 19:02:27.847: INFO: New ReplicaSet "webserver-deployment-d9f79cb5" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-d9f79cb5  deployment-2263  52348989-6972-44ad-b4d2-579fbd7a0ccf 49568 3 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373 0xc004c23817 0xc004c23818}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: d9f79cb5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c238b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:02:27.854: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jan  5 19:02:27.855: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-7f5969cbc7  deployment-2263  6f0eced5-1d09-4634-91e2-8c2d36a806bc 49567 3 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373 0xc004c23727 0xc004c23728}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"012ecf4b-bc7c-48ae-b5c6-ad0ee2b49373\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c237b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:02:27.885: INFO: Pod "webserver-deployment-7f5969cbc7-5g5rb" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-5g5rb webserver-deployment-7f5969cbc7- deployment-2263  8c60303e-e1ac-4efb-baf5-3c7579d40b3a 49463 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004cca967 0xc004cca968}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.102\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z49q7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z49q7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.102,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://05b7fbd553bda0652e3f3daeae2f8a6d6dfb2a010456ec69e1de56d8bde165e0,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.102,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.897: INFO: Pod "webserver-deployment-7f5969cbc7-6f4vc" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-6f4vc webserver-deployment-7f5969cbc7- deployment-2263  9ebc8d65-c62e-4a0f-b3e0-9607d9b84ce5 49480 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccab40 0xc004ccab41}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.0.64\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b25xq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b25xq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.37,PodIP:10.16.0.64,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f42b4be0b9dcc61c9960be34ca80c5783a238ed39a67d9676e8f8a9d986a0975,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.0.64,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.901: INFO: Pod "webserver-deployment-7f5969cbc7-bgbqk" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-bgbqk webserver-deployment-7f5969cbc7- deployment-2263  62583b86-edd8-491c-83d5-2873e9f370ec 49478 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccad10 0xc004ccad11}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.0.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b22nn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b22nn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.37,PodIP:10.16.0.62,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://18fa24adead8c28e7a50265d9e13d9defa0f7229f57d91b869564f2b80d2310c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.0.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.902: INFO: Pod "webserver-deployment-7f5969cbc7-k7nkd" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-k7nkd webserver-deployment-7f5969cbc7- deployment-2263  cb656f75-09cf-4a73-b73d-c192842904ec 49471 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccaee0 0xc004ccaee1}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.103\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2kgj5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2kgj5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.103,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b4fb834de75f0d5eab39f0e6d32b96037c2d5c9f2859495f3ef8b7f271d813ed,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.103,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.904: INFO: Pod "webserver-deployment-7f5969cbc7-kfg6r" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kfg6r webserver-deployment-7f5969cbc7- deployment-2263  5828ef30-f9b4-492f-b43b-67aef3e6f37d 49483 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb0b0 0xc004ccb0b1}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.0.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h9pzl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h9pzl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.37,PodIP:10.16.0.63,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://731acc18b198b25b3cf6cffb5f620b2e16e7a88b778d64949446d78e4886b472,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.0.63,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.914: INFO: Pod "webserver-deployment-7f5969cbc7-kpchj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-kpchj webserver-deployment-7f5969cbc7- deployment-2263  a8889b9b-3ac0-4a33-8fc4-7ebf6a48d8c8 49578 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb280 0xc004ccb281}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwlcr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwlcr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.914: INFO: Pod "webserver-deployment-7f5969cbc7-n8p98" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-n8p98 webserver-deployment-7f5969cbc7- deployment-2263  1a846382-f325-4e11-974f-ae535af2db3e 49577 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb3b7 0xc004ccb3b8}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lrrv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lrrv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.914: INFO: Pod "webserver-deployment-7f5969cbc7-q8j2q" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-q8j2q webserver-deployment-7f5969cbc7- deployment-2263  a6d1bbb2-da7d-4ca3-9084-f61c90786517 49493 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb510 0xc004ccb511}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9sfzb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9sfzb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.175,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://88154d41f63e9e7ebf5e9cea9b24e39a84b18d15a5c1ade8cfba4a267cf4a316,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.175,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.915: INFO: Pod "webserver-deployment-7f5969cbc7-xskjx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xskjx webserver-deployment-7f5969cbc7- deployment-2263  f60962df-d9ce-46d6-8aba-9d7d136af11a 49579 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb6e0 0xc004ccb6e1}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pksc4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pksc4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:,StartTime:2023-01-05 19:02:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.920: INFO: Pod "webserver-deployment-7f5969cbc7-xzbgq" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-xzbgq webserver-deployment-7f5969cbc7- deployment-2263  2a63f6cb-4354-4dc9-8e00-1f7cd541f9c7 49466 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccb897 0xc004ccb898}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rbkvc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rbkvc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.174,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5cf15ffccfeb301353f6e0e3440a9a601e1723f1f88575ff22749c5d436b37e7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.174,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.936: INFO: Pod "webserver-deployment-7f5969cbc7-zh74m" is available:
    &Pod{ObjectMeta:{webserver-deployment-7f5969cbc7-zh74m webserver-deployment-7f5969cbc7- deployment-2263  31f8b681-74d8-414f-a313-a55aafd652d2 49461 0 2023-01-05 19:02:21 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet webserver-deployment-7f5969cbc7 6f0eced5-1d09-4634-91e2-8c2d36a806bc 0xc004ccba70 0xc004ccba71}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:21 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f0eced5-1d09-4634-91e2-8c2d36a806bc\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvkrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvkrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:21 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.104,StartTime:2023-01-05 19:02:21 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3f29cc3a40e012d6f741f467d32dc681a2d50cdcdf695dffd54527ef4d78c851,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.941: INFO: Pod "webserver-deployment-d9f79cb5-26jzv" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-26jzv webserver-deployment-d9f79cb5- deployment-2263  9f05d7ab-bf1a-42e5-b451-2811074c53f6 49576 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004ccbc2f 0xc004ccbc40}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n5rfj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n5rfj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.944: INFO: Pod "webserver-deployment-d9f79cb5-4qhpp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-4qhpp webserver-deployment-d9f79cb5- deployment-2263  110d45d3-e0c2-46ed-b278-5ff16b0bf9b5 49564 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004ccbd8f 0xc004ccbda0}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5jrmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5jrmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.105,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.105,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.945: INFO: Pod "webserver-deployment-d9f79cb5-57nqz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-57nqz webserver-deployment-d9f79cb5- deployment-2263  1c346551-d8a7-4168-9073-79b5028d51a7 49565 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004ccbf8f 0xc004ccbfa0}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mf7m,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mf7m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.178,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.946: INFO: Pod "webserver-deployment-d9f79cb5-7tw89" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-7tw89 webserver-deployment-d9f79cb5- deployment-2263  1842861d-641a-440e-a6b2-c80a40d1abe9 49544 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce18f 0xc004dce1a0}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xp8fp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xp8fp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.947: INFO: Pod "webserver-deployment-d9f79cb5-d62nj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-d62nj webserver-deployment-d9f79cb5- deployment-2263  941ed978-21d7-4d4c-8b2f-2a420b1fb172 49532 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce35f 0xc004dce370}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-57krk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-57krk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-dbpc,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.37,PodIP:,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.947: INFO: Pod "webserver-deployment-d9f79cb5-f9m68" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-f9m68 webserver-deployment-d9f79cb5- deployment-2263  22387f66-b1b2-411c-9a63-2a2ec55519fd 49543 0 2023-01-05 19:02:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce52f 0xc004dce540}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sq5db,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sq5db,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:,StartTime:2023-01-05 19:02:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.948: INFO: Pod "webserver-deployment-d9f79cb5-xmfbt" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xmfbt webserver-deployment-d9f79cb5- deployment-2263  34c8af3e-2ffb-46c9-af74-657e99cd5199 49580 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce6ff 0xc004dce710}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grd5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grd5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:02:27.948: INFO: Pod "webserver-deployment-d9f79cb5-xv95x" is not available:
    &Pod{ObjectMeta:{webserver-deployment-d9f79cb5-xv95x webserver-deployment-d9f79cb5- deployment-2263  af9dc618-5f61-431c-9ab5-ade7a0c053e6 49581 0 2023-01-05 19:02:27 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:d9f79cb5] map[] [{apps/v1 ReplicaSet webserver-deployment-d9f79cb5 52348989-6972-44ad-b4d2-579fbd7a0ccf 0xc004dce857 0xc004dce858}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:27 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"52348989-6972-44ad-b4d2-579fbd7a0ccf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w952q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w952q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:27.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2263" for this suite. 01/05/23 19:02:27.977
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:28.013
Jan  5 19:02:28.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename init-container 01/05/23 19:02:28.016
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:28.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:28.08
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458
STEP: creating the pod 01/05/23 19:02:28.091
Jan  5 19:02:28.091: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:37.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-1005" for this suite. 01/05/23 19:02:37.166
------------------------------
• [SLOW TEST] [9.160 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:458

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:28.013
    Jan  5 19:02:28.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename init-container 01/05/23 19:02:28.016
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:28.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:28.08
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:458
    STEP: creating the pod 01/05/23 19:02:28.091
    Jan  5 19:02:28.091: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:37.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-1005" for this suite. 01/05/23 19:02:37.166
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:37.173
Jan  5 19:02:37.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename runtimeclass 01/05/23 19:02:37.176
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:37.191
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:37.197
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-2079-delete-me 01/05/23 19:02:37.209
STEP: Waiting for the RuntimeClass to disappear 01/05/23 19:02:37.215
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:37.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2079" for this suite. 01/05/23 19:02:37.234
------------------------------
• [0.066 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:37.173
    Jan  5 19:02:37.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 19:02:37.176
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:37.191
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:37.197
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-2079-delete-me 01/05/23 19:02:37.209
    STEP: Waiting for the RuntimeClass to disappear 01/05/23 19:02:37.215
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:37.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2079" for this suite. 01/05/23 19:02:37.234
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:37.243
Jan  5 19:02:37.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:02:37.244
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:37.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:37.269
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109
STEP: Creating configMap with name projected-configmap-test-volume-map-c3358d77-3445-4eae-90c3-842447cc2860 01/05/23 19:02:37.276
STEP: Creating a pod to test consume configMaps 01/05/23 19:02:37.285
Jan  5 19:02:37.294: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52" in namespace "projected-4624" to be "Succeeded or Failed"
Jan  5 19:02:37.301: INFO: Pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52": Phase="Pending", Reason="", readiness=false. Elapsed: 6.92285ms
Jan  5 19:02:39.315: INFO: Pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021400987s
Jan  5 19:02:41.305: INFO: Pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011553075s
STEP: Saw pod success 01/05/23 19:02:41.305
Jan  5 19:02:41.306: INFO: Pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52" satisfied condition "Succeeded or Failed"
Jan  5 19:02:41.309: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:02:41.316
Jan  5 19:02:41.328: INFO: Waiting for pod pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52 to disappear
Jan  5 19:02:41.331: INFO: Pod pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:41.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4624" for this suite. 01/05/23 19:02:41.335
------------------------------
• [4.097 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:37.243
    Jan  5 19:02:37.243: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:02:37.244
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:37.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:37.269
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:109
    STEP: Creating configMap with name projected-configmap-test-volume-map-c3358d77-3445-4eae-90c3-842447cc2860 01/05/23 19:02:37.276
    STEP: Creating a pod to test consume configMaps 01/05/23 19:02:37.285
    Jan  5 19:02:37.294: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52" in namespace "projected-4624" to be "Succeeded or Failed"
    Jan  5 19:02:37.301: INFO: Pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52": Phase="Pending", Reason="", readiness=false. Elapsed: 6.92285ms
    Jan  5 19:02:39.315: INFO: Pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021400987s
    Jan  5 19:02:41.305: INFO: Pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011553075s
    STEP: Saw pod success 01/05/23 19:02:41.305
    Jan  5 19:02:41.306: INFO: Pod "pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52" satisfied condition "Succeeded or Failed"
    Jan  5 19:02:41.309: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:02:41.316
    Jan  5 19:02:41.328: INFO: Waiting for pod pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52 to disappear
    Jan  5 19:02:41.331: INFO: Pod pod-projected-configmaps-4fd9438b-d752-4922-8345-b31d6129bb52 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:41.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4624" for this suite. 01/05/23 19:02:41.335
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:41.346
Jan  5 19:02:41.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename deployment 01/05/23 19:02:41.347
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:41.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:41.362
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 01/05/23 19:02:41.368
Jan  5 19:02:41.369: INFO: Creating simple deployment test-deployment-qcpbn
Jan  5 19:02:41.378: INFO: new replicaset for deployment "test-deployment-qcpbn" is yet to be created
STEP: Getting /status 01/05/23 19:02:43.387
Jan  5 19:02:43.392: INFO: Deployment test-deployment-qcpbn has Conditions: [{Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}]
STEP: updating Deployment Status 01/05/23 19:02:43.392
Jan  5 19:02:43.400: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 2, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 2, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 2, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 2, 41, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-qcpbn-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 01/05/23 19:02:43.401
Jan  5 19:02:43.402: INFO: Observed &Deployment event: ADDED
Jan  5 19:02:43.402: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qcpbn-54bc444df"}
Jan  5 19:02:43.403: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.403: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qcpbn-54bc444df"}
Jan  5 19:02:43.403: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 19:02:43.404: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.404: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 19:02:43.404: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qcpbn-54bc444df" is progressing.}
Jan  5 19:02:43.404: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.404: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 19:02:43.405: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}
Jan  5 19:02:43.405: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.405: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 19:02:43.405: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}
Jan  5 19:02:43.405: INFO: Found Deployment test-deployment-qcpbn in namespace deployment-2366 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 19:02:43.405: INFO: Deployment test-deployment-qcpbn has an updated status
STEP: patching the Statefulset Status 01/05/23 19:02:43.406
Jan  5 19:02:43.406: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 19:02:43.416: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 01/05/23 19:02:43.416
Jan  5 19:02:43.418: INFO: Observed &Deployment event: ADDED
Jan  5 19:02:43.418: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qcpbn-54bc444df"}
Jan  5 19:02:43.419: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.419: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qcpbn-54bc444df"}
Jan  5 19:02:43.420: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 19:02:43.420: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.420: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jan  5 19:02:43.420: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qcpbn-54bc444df" is progressing.}
Jan  5 19:02:43.420: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}
Jan  5 19:02:43.421: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}
Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 19:02:43.422: INFO: Observed &Deployment event: MODIFIED
Jan  5 19:02:43.422: INFO: Found deployment test-deployment-qcpbn in namespace deployment-2366 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jan  5 19:02:43.422: INFO: Deployment test-deployment-qcpbn has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 19:02:43.425: INFO: Deployment "test-deployment-qcpbn":
&Deployment{ObjectMeta:{test-deployment-qcpbn  deployment-2366  6d700604-8024-416f-829c-5b7b26ef6f65 49902 1 2023-01-05 19:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-05 19:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-05 19:02:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-05 19:02:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00346f728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-qcpbn-54bc444df",LastUpdateTime:2023-01-05 19:02:43 +0000 UTC,LastTransitionTime:2023-01-05 19:02:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 19:02:43.428: INFO: New ReplicaSet "test-deployment-qcpbn-54bc444df" of Deployment "test-deployment-qcpbn":
&ReplicaSet{ObjectMeta:{test-deployment-qcpbn-54bc444df  deployment-2366  d8a49abe-7807-488b-a551-0a0e9a8705a1 49893 1 2023-01-05 19:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-qcpbn 6d700604-8024-416f-829c-5b7b26ef6f65 0xc0036ac780 0xc0036ac781}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d700604-8024-416f-829c-5b7b26ef6f65\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:02:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036ac828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:02:43.432: INFO: Pod "test-deployment-qcpbn-54bc444df-49qsp" is available:
&Pod{ObjectMeta:{test-deployment-qcpbn-54bc444df-49qsp test-deployment-qcpbn-54bc444df- deployment-2366  cb8ed670-95b5-40b5-876f-f0de79840677 49892 0 2023-01-05 19:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-qcpbn-54bc444df d8a49abe-7807-488b-a551-0a0e9a8705a1 0xc00346fb00 0xc00346fb01}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8a49abe-7807-488b-a551-0a0e9a8705a1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7jg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7jg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.181,StartTime:2023-01-05 19:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e5b75205f9d5fae6859cbace49e543667b4c4aa84a6f6174970b531662886167,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  5 19:02:43.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-2366" for this suite. 01/05/23 19:02:43.438
------------------------------
• [2.097 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:41.346
    Jan  5 19:02:41.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename deployment 01/05/23 19:02:41.347
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:41.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:41.362
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 01/05/23 19:02:41.368
    Jan  5 19:02:41.369: INFO: Creating simple deployment test-deployment-qcpbn
    Jan  5 19:02:41.378: INFO: new replicaset for deployment "test-deployment-qcpbn" is yet to be created
    STEP: Getting /status 01/05/23 19:02:43.387
    Jan  5 19:02:43.392: INFO: Deployment test-deployment-qcpbn has Conditions: [{Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}]
    STEP: updating Deployment Status 01/05/23 19:02:43.392
    Jan  5 19:02:43.400: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 2, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 2, 42, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 2, 42, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 2, 41, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-qcpbn-54bc444df\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 01/05/23 19:02:43.401
    Jan  5 19:02:43.402: INFO: Observed &Deployment event: ADDED
    Jan  5 19:02:43.402: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qcpbn-54bc444df"}
    Jan  5 19:02:43.403: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.403: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qcpbn-54bc444df"}
    Jan  5 19:02:43.403: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 19:02:43.404: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.404: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 19:02:43.404: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qcpbn-54bc444df" is progressing.}
    Jan  5 19:02:43.404: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.404: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 19:02:43.405: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}
    Jan  5 19:02:43.405: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.405: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 19:02:43.405: INFO: Observed Deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}
    Jan  5 19:02:43.405: INFO: Found Deployment test-deployment-qcpbn in namespace deployment-2366 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 19:02:43.405: INFO: Deployment test-deployment-qcpbn has an updated status
    STEP: patching the Statefulset Status 01/05/23 19:02:43.406
    Jan  5 19:02:43.406: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 19:02:43.416: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 01/05/23 19:02:43.416
    Jan  5 19:02:43.418: INFO: Observed &Deployment event: ADDED
    Jan  5 19:02:43.418: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qcpbn-54bc444df"}
    Jan  5 19:02:43.419: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.419: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-qcpbn-54bc444df"}
    Jan  5 19:02:43.420: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 19:02:43.420: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.420: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jan  5 19:02:43.420: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:41 +0000 UTC 2023-01-05 19:02:41 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-qcpbn-54bc444df" is progressing.}
    Jan  5 19:02:43.420: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}
    Jan  5 19:02:43.421: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-01-05 19:02:42 +0000 UTC 2023-01-05 19:02:41 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-qcpbn-54bc444df" has successfully progressed.}
    Jan  5 19:02:43.421: INFO: Observed deployment test-deployment-qcpbn in namespace deployment-2366 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 19:02:43.422: INFO: Observed &Deployment event: MODIFIED
    Jan  5 19:02:43.422: INFO: Found deployment test-deployment-qcpbn in namespace deployment-2366 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jan  5 19:02:43.422: INFO: Deployment test-deployment-qcpbn has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 19:02:43.425: INFO: Deployment "test-deployment-qcpbn":
    &Deployment{ObjectMeta:{test-deployment-qcpbn  deployment-2366  6d700604-8024-416f-829c-5b7b26ef6f65 49902 1 2023-01-05 19:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-01-05 19:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-01-05 19:02:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-01-05 19:02:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00346f728 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-qcpbn-54bc444df",LastUpdateTime:2023-01-05 19:02:43 +0000 UTC,LastTransitionTime:2023-01-05 19:02:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 19:02:43.428: INFO: New ReplicaSet "test-deployment-qcpbn-54bc444df" of Deployment "test-deployment-qcpbn":
    &ReplicaSet{ObjectMeta:{test-deployment-qcpbn-54bc444df  deployment-2366  d8a49abe-7807-488b-a551-0a0e9a8705a1 49893 1 2023-01-05 19:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-qcpbn 6d700604-8024-416f-829c-5b7b26ef6f65 0xc0036ac780 0xc0036ac781}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6d700604-8024-416f-829c-5b7b26ef6f65\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:02:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 54bc444df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0036ac828 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:02:43.432: INFO: Pod "test-deployment-qcpbn-54bc444df-49qsp" is available:
    &Pod{ObjectMeta:{test-deployment-qcpbn-54bc444df-49qsp test-deployment-qcpbn-54bc444df- deployment-2366  cb8ed670-95b5-40b5-876f-f0de79840677 49892 0 2023-01-05 19:02:41 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:54bc444df] map[] [{apps/v1 ReplicaSet test-deployment-qcpbn-54bc444df d8a49abe-7807-488b-a551-0a0e9a8705a1 0xc00346fb00 0xc00346fb01}] [] [{kube-controller-manager Update v1 2023-01-05 19:02:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8a49abe-7807-488b-a551-0a0e9a8705a1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:02:42 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.181\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7jg5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7jg5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:42 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:02:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.181,StartTime:2023-01-05 19:02:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:02:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e5b75205f9d5fae6859cbace49e543667b4c4aa84a6f6174970b531662886167,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.181,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:02:43.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-2366" for this suite. 01/05/23 19:02:43.438
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:02:43.446
Jan  5 19:02:43.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename subpath 01/05/23 19:02:43.447
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:43.459
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:43.462
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 19:02:43.465
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-p67m 01/05/23 19:02:43.475
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 19:02:43.475
Jan  5 19:02:43.486: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p67m" in namespace "subpath-7923" to be "Succeeded or Failed"
Jan  5 19:02:43.491: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741734ms
Jan  5 19:02:45.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 2.008195732s
Jan  5 19:02:47.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 4.008222109s
Jan  5 19:02:49.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 6.007824244s
Jan  5 19:02:51.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 8.008517828s
Jan  5 19:02:53.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 10.008432961s
Jan  5 19:02:55.495: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 12.009039677s
Jan  5 19:02:57.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 14.008010924s
Jan  5 19:02:59.495: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 16.00930789s
Jan  5 19:03:01.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 18.008456525s
Jan  5 19:03:03.496: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 20.009695737s
Jan  5 19:03:05.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=false. Elapsed: 22.008197452s
Jan  5 19:03:07.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00807477s
STEP: Saw pod success 01/05/23 19:03:07.494
Jan  5 19:03:07.494: INFO: Pod "pod-subpath-test-projected-p67m" satisfied condition "Succeeded or Failed"
Jan  5 19:03:07.497: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-subpath-test-projected-p67m container test-container-subpath-projected-p67m: <nil>
STEP: delete the pod 01/05/23 19:03:07.504
Jan  5 19:03:07.515: INFO: Waiting for pod pod-subpath-test-projected-p67m to disappear
Jan  5 19:03:07.519: INFO: Pod pod-subpath-test-projected-p67m no longer exists
STEP: Deleting pod pod-subpath-test-projected-p67m 01/05/23 19:03:07.519
Jan  5 19:03:07.519: INFO: Deleting pod "pod-subpath-test-projected-p67m" in namespace "subpath-7923"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  5 19:03:07.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-7923" for this suite. 01/05/23 19:03:07.526
------------------------------
• [SLOW TEST] [24.086 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:02:43.446
    Jan  5 19:02:43.446: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename subpath 01/05/23 19:02:43.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:02:43.459
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:02:43.462
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 19:02:43.465
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-p67m 01/05/23 19:02:43.475
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 19:02:43.475
    Jan  5 19:02:43.486: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-p67m" in namespace "subpath-7923" to be "Succeeded or Failed"
    Jan  5 19:02:43.491: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741734ms
    Jan  5 19:02:45.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 2.008195732s
    Jan  5 19:02:47.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 4.008222109s
    Jan  5 19:02:49.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 6.007824244s
    Jan  5 19:02:51.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 8.008517828s
    Jan  5 19:02:53.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 10.008432961s
    Jan  5 19:02:55.495: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 12.009039677s
    Jan  5 19:02:57.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 14.008010924s
    Jan  5 19:02:59.495: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 16.00930789s
    Jan  5 19:03:01.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 18.008456525s
    Jan  5 19:03:03.496: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=true. Elapsed: 20.009695737s
    Jan  5 19:03:05.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Running", Reason="", readiness=false. Elapsed: 22.008197452s
    Jan  5 19:03:07.494: INFO: Pod "pod-subpath-test-projected-p67m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.00807477s
    STEP: Saw pod success 01/05/23 19:03:07.494
    Jan  5 19:03:07.494: INFO: Pod "pod-subpath-test-projected-p67m" satisfied condition "Succeeded or Failed"
    Jan  5 19:03:07.497: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-subpath-test-projected-p67m container test-container-subpath-projected-p67m: <nil>
    STEP: delete the pod 01/05/23 19:03:07.504
    Jan  5 19:03:07.515: INFO: Waiting for pod pod-subpath-test-projected-p67m to disappear
    Jan  5 19:03:07.519: INFO: Pod pod-subpath-test-projected-p67m no longer exists
    STEP: Deleting pod pod-subpath-test-projected-p67m 01/05/23 19:03:07.519
    Jan  5 19:03:07.519: INFO: Deleting pod "pod-subpath-test-projected-p67m" in namespace "subpath-7923"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:03:07.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-7923" for this suite. 01/05/23 19:03:07.526
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:03:07.533
Jan  5 19:03:07.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:03:07.538
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:07.551
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:07.554
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448
STEP: Counting existing ResourceQuota 01/05/23 19:03:07.558
STEP: Creating a ResourceQuota 01/05/23 19:03:12.56
STEP: Ensuring resource quota status is calculated 01/05/23 19:03:12.566
STEP: Creating a ReplicaSet 01/05/23 19:03:14.57
STEP: Ensuring resource quota status captures replicaset creation 01/05/23 19:03:14.583
STEP: Deleting a ReplicaSet 01/05/23 19:03:16.587
STEP: Ensuring resource quota status released usage 01/05/23 19:03:16.593
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:03:18.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2326" for this suite. 01/05/23 19:03:18.601
------------------------------
• [SLOW TEST] [11.073 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:448

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:03:07.533
    Jan  5 19:03:07.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:03:07.538
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:07.551
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:07.554
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:448
    STEP: Counting existing ResourceQuota 01/05/23 19:03:07.558
    STEP: Creating a ResourceQuota 01/05/23 19:03:12.56
    STEP: Ensuring resource quota status is calculated 01/05/23 19:03:12.566
    STEP: Creating a ReplicaSet 01/05/23 19:03:14.57
    STEP: Ensuring resource quota status captures replicaset creation 01/05/23 19:03:14.583
    STEP: Deleting a ReplicaSet 01/05/23 19:03:16.587
    STEP: Ensuring resource quota status released usage 01/05/23 19:03:16.593
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:03:18.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2326" for this suite. 01/05/23 19:03:18.601
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:03:18.61
Jan  5 19:03:18.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:03:18.611
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:18.627
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:18.63
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:03:18.633
Jan  5 19:03:18.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267" in namespace "downward-api-693" to be "Succeeded or Failed"
Jan  5 19:03:18.647: INFO: Pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267": Phase="Pending", Reason="", readiness=false. Elapsed: 2.57185ms
Jan  5 19:03:20.652: INFO: Pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006933676s
Jan  5 19:03:22.651: INFO: Pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006056804s
STEP: Saw pod success 01/05/23 19:03:22.651
Jan  5 19:03:22.651: INFO: Pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267" satisfied condition "Succeeded or Failed"
Jan  5 19:03:22.653: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267 container client-container: <nil>
STEP: delete the pod 01/05/23 19:03:22.659
Jan  5 19:03:22.673: INFO: Waiting for pod downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267 to disappear
Jan  5 19:03:22.676: INFO: Pod downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:03:22.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-693" for this suite. 01/05/23 19:03:22.681
------------------------------
• [4.076 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:03:18.61
    Jan  5 19:03:18.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:03:18.611
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:18.627
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:18.63
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:68
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:03:18.633
    Jan  5 19:03:18.645: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267" in namespace "downward-api-693" to be "Succeeded or Failed"
    Jan  5 19:03:18.647: INFO: Pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267": Phase="Pending", Reason="", readiness=false. Elapsed: 2.57185ms
    Jan  5 19:03:20.652: INFO: Pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006933676s
    Jan  5 19:03:22.651: INFO: Pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006056804s
    STEP: Saw pod success 01/05/23 19:03:22.651
    Jan  5 19:03:22.651: INFO: Pod "downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267" satisfied condition "Succeeded or Failed"
    Jan  5 19:03:22.653: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:03:22.659
    Jan  5 19:03:22.673: INFO: Waiting for pod downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267 to disappear
    Jan  5 19:03:22.676: INFO: Pod downwardapi-volume-3ef4dee7-fed4-4e26-9768-51ad1beeb267 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:03:22.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-693" for this suite. 01/05/23 19:03:22.681
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:03:22.689
Jan  5 19:03:22.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename dns 01/05/23 19:03:22.691
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:22.703
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:22.706
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 01/05/23 19:03:22.709
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
 01/05/23 19:03:22.721
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
 01/05/23 19:03:22.721
STEP: creating a pod to probe DNS 01/05/23 19:03:22.722
STEP: submitting the pod to kubernetes 01/05/23 19:03:22.722
Jan  5 19:03:22.732: INFO: Waiting up to 15m0s for pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619" in namespace "dns-5185" to be "running"
Jan  5 19:03:22.737: INFO: Pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619": Phase="Pending", Reason="", readiness=false. Elapsed: 4.955429ms
Jan  5 19:03:24.742: INFO: Pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009280856s
Jan  5 19:03:26.741: INFO: Pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619": Phase="Running", Reason="", readiness=true. Elapsed: 4.008840213s
Jan  5 19:03:26.741: INFO: Pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:03:26.741
STEP: looking for the results for each expected name from probers 01/05/23 19:03:26.744
Jan  5 19:03:26.758: INFO: DNS probes using dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619 succeeded

STEP: deleting the pod 01/05/23 19:03:26.759
STEP: changing the externalName to bar.example.com 01/05/23 19:03:26.771
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
 01/05/23 19:03:26.78
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
 01/05/23 19:03:26.78
STEP: creating a second pod to probe DNS 01/05/23 19:03:26.78
STEP: submitting the pod to kubernetes 01/05/23 19:03:26.781
Jan  5 19:03:26.791: INFO: Waiting up to 15m0s for pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3" in namespace "dns-5185" to be "running"
Jan  5 19:03:26.795: INFO: Pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679ms
Jan  5 19:03:28.798: INFO: Pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007359129s
Jan  5 19:03:30.798: INFO: Pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.006910693s
Jan  5 19:03:30.798: INFO: Pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:03:30.798
STEP: looking for the results for each expected name from probers 01/05/23 19:03:30.801
Jan  5 19:03:30.820: INFO: DNS probes using dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3 succeeded

STEP: deleting the pod 01/05/23 19:03:30.82
STEP: changing the service to type=ClusterIP 01/05/23 19:03:30.835
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
 01/05/23 19:03:30.847
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
 01/05/23 19:03:30.848
STEP: creating a third pod to probe DNS 01/05/23 19:03:30.849
STEP: submitting the pod to kubernetes 01/05/23 19:03:30.855
Jan  5 19:03:30.864: INFO: Waiting up to 15m0s for pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba" in namespace "dns-5185" to be "running"
Jan  5 19:03:30.868: INFO: Pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669387ms
Jan  5 19:03:32.872: INFO: Pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007221046s
Jan  5 19:03:34.873: INFO: Pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba": Phase="Running", Reason="", readiness=true. Elapsed: 4.008238043s
Jan  5 19:03:34.873: INFO: Pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:03:34.873
STEP: looking for the results for each expected name from probers 01/05/23 19:03:34.875
Jan  5 19:03:34.886: INFO: DNS probes using dns-test-b376c352-32f8-45a0-bf9f-26972e467aba succeeded

STEP: deleting the pod 01/05/23 19:03:34.887
STEP: deleting the test externalName service 01/05/23 19:03:34.9
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  5 19:03:34.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5185" for this suite. 01/05/23 19:03:34.929
------------------------------
• [SLOW TEST] [12.252 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:03:22.689
    Jan  5 19:03:22.689: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename dns 01/05/23 19:03:22.691
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:22.703
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:22.706
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 01/05/23 19:03:22.709
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
     01/05/23 19:03:22.721
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
     01/05/23 19:03:22.721
    STEP: creating a pod to probe DNS 01/05/23 19:03:22.722
    STEP: submitting the pod to kubernetes 01/05/23 19:03:22.722
    Jan  5 19:03:22.732: INFO: Waiting up to 15m0s for pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619" in namespace "dns-5185" to be "running"
    Jan  5 19:03:22.737: INFO: Pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619": Phase="Pending", Reason="", readiness=false. Elapsed: 4.955429ms
    Jan  5 19:03:24.742: INFO: Pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009280856s
    Jan  5 19:03:26.741: INFO: Pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619": Phase="Running", Reason="", readiness=true. Elapsed: 4.008840213s
    Jan  5 19:03:26.741: INFO: Pod "dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:03:26.741
    STEP: looking for the results for each expected name from probers 01/05/23 19:03:26.744
    Jan  5 19:03:26.758: INFO: DNS probes using dns-test-75e7e33a-1291-4ace-96a1-7c671ed52619 succeeded

    STEP: deleting the pod 01/05/23 19:03:26.759
    STEP: changing the externalName to bar.example.com 01/05/23 19:03:26.771
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
     01/05/23 19:03:26.78
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
     01/05/23 19:03:26.78
    STEP: creating a second pod to probe DNS 01/05/23 19:03:26.78
    STEP: submitting the pod to kubernetes 01/05/23 19:03:26.781
    Jan  5 19:03:26.791: INFO: Waiting up to 15m0s for pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3" in namespace "dns-5185" to be "running"
    Jan  5 19:03:26.795: INFO: Pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.679ms
    Jan  5 19:03:28.798: INFO: Pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007359129s
    Jan  5 19:03:30.798: INFO: Pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3": Phase="Running", Reason="", readiness=true. Elapsed: 4.006910693s
    Jan  5 19:03:30.798: INFO: Pod "dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:03:30.798
    STEP: looking for the results for each expected name from probers 01/05/23 19:03:30.801
    Jan  5 19:03:30.820: INFO: DNS probes using dns-test-600c8eed-a68f-4f39-b2b9-6d57168014b3 succeeded

    STEP: deleting the pod 01/05/23 19:03:30.82
    STEP: changing the service to type=ClusterIP 01/05/23 19:03:30.835
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
     01/05/23 19:03:30.847
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-5185.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-5185.svc.cluster.local; sleep 1; done
     01/05/23 19:03:30.848
    STEP: creating a third pod to probe DNS 01/05/23 19:03:30.849
    STEP: submitting the pod to kubernetes 01/05/23 19:03:30.855
    Jan  5 19:03:30.864: INFO: Waiting up to 15m0s for pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba" in namespace "dns-5185" to be "running"
    Jan  5 19:03:30.868: INFO: Pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba": Phase="Pending", Reason="", readiness=false. Elapsed: 3.669387ms
    Jan  5 19:03:32.872: INFO: Pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007221046s
    Jan  5 19:03:34.873: INFO: Pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba": Phase="Running", Reason="", readiness=true. Elapsed: 4.008238043s
    Jan  5 19:03:34.873: INFO: Pod "dns-test-b376c352-32f8-45a0-bf9f-26972e467aba" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:03:34.873
    STEP: looking for the results for each expected name from probers 01/05/23 19:03:34.875
    Jan  5 19:03:34.886: INFO: DNS probes using dns-test-b376c352-32f8-45a0-bf9f-26972e467aba succeeded

    STEP: deleting the pod 01/05/23 19:03:34.887
    STEP: deleting the test externalName service 01/05/23 19:03:34.9
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:03:34.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5185" for this suite. 01/05/23 19:03:34.929
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:03:34.969
Jan  5 19:03:34.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubelet-test 01/05/23 19:03:34.973
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:35.038
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:35.041
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 01/05/23 19:03:35.052
Jan  5 19:03:35.052: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9" in namespace "kubelet-test-6263" to be "completed"
Jan  5 19:03:35.056: INFO: Pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.321803ms
Jan  5 19:03:37.060: INFO: Pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007227437s
Jan  5 19:03:39.061: INFO: Pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008487822s
Jan  5 19:03:39.061: INFO: Pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:03:39.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-6263" for this suite. 01/05/23 19:03:39.082
------------------------------
• [4.120 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:03:34.969
    Jan  5 19:03:34.971: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 19:03:34.973
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:35.038
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:35.041
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 01/05/23 19:03:35.052
    Jan  5 19:03:35.052: INFO: Waiting up to 3m0s for pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9" in namespace "kubelet-test-6263" to be "completed"
    Jan  5 19:03:35.056: INFO: Pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.321803ms
    Jan  5 19:03:37.060: INFO: Pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007227437s
    Jan  5 19:03:39.061: INFO: Pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008487822s
    Jan  5 19:03:39.061: INFO: Pod "agnhost-host-aliases09ccc5f2-063e-45f0-a348-15fb2952aee9" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:03:39.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-6263" for this suite. 01/05/23 19:03:39.082
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:03:39.089
Jan  5 19:03:39.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:03:39.091
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:39.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:39.11
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 01/05/23 19:03:39.114
STEP: submitting the pod to kubernetes 01/05/23 19:03:39.114
STEP: verifying QOS class is set on the pod 01/05/23 19:03:39.128
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/node/init/init.go:32
Jan  5 19:03:39.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods Extended
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods Extended
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods Extended
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-78" for this suite. 01/05/23 19:03:39.152
------------------------------
• [0.069 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:03:39.089
    Jan  5 19:03:39.089: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:03:39.091
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:39.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:39.11
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 01/05/23 19:03:39.114
    STEP: submitting the pod to kubernetes 01/05/23 19:03:39.114
    STEP: verifying QOS class is set on the pod 01/05/23 19:03:39.128
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:03:39.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods Extended
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods Extended
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods Extended
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-78" for this suite. 01/05/23 19:03:39.152
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:03:39.162
Jan  5 19:03:39.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:03:39.165
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:39.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:39.187
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217
STEP: Creating a pod to test downward api env vars 01/05/23 19:03:39.196
Jan  5 19:03:39.206: INFO: Waiting up to 5m0s for pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0" in namespace "downward-api-2009" to be "Succeeded or Failed"
Jan  5 19:03:39.212: INFO: Pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.349178ms
Jan  5 19:03:41.216: INFO: Pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009594009s
Jan  5 19:03:43.216: INFO: Pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009581412s
STEP: Saw pod success 01/05/23 19:03:43.216
Jan  5 19:03:43.216: INFO: Pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0" satisfied condition "Succeeded or Failed"
Jan  5 19:03:43.218: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0 container dapi-container: <nil>
STEP: delete the pod 01/05/23 19:03:43.227
Jan  5 19:03:43.239: INFO: Waiting for pod downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0 to disappear
Jan  5 19:03:43.242: INFO: Pod downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  5 19:03:43.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-2009" for this suite. 01/05/23 19:03:43.246
------------------------------
• [4.090 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:03:39.162
    Jan  5 19:03:39.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:03:39.165
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:39.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:39.187
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:217
    STEP: Creating a pod to test downward api env vars 01/05/23 19:03:39.196
    Jan  5 19:03:39.206: INFO: Waiting up to 5m0s for pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0" in namespace "downward-api-2009" to be "Succeeded or Failed"
    Jan  5 19:03:39.212: INFO: Pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.349178ms
    Jan  5 19:03:41.216: INFO: Pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009594009s
    Jan  5 19:03:43.216: INFO: Pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009581412s
    STEP: Saw pod success 01/05/23 19:03:43.216
    Jan  5 19:03:43.216: INFO: Pod "downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0" satisfied condition "Succeeded or Failed"
    Jan  5 19:03:43.218: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 19:03:43.227
    Jan  5 19:03:43.239: INFO: Waiting for pod downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0 to disappear
    Jan  5 19:03:43.242: INFO: Pod downward-api-8b57436f-1f9e-4e83-90e6-6bf2fe7c63f0 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:03:43.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-2009" for this suite. 01/05/23 19:03:43.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:03:43.255
Jan  5 19:03:43.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename endpointslicemirroring 01/05/23 19:03:43.257
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:43.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:43.269
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 01/05/23 19:03:43.283
STEP: mirroring an update to a custom Endpoint 01/05/23 19:03:43.299
STEP: mirroring deletion of a custom Endpoint 01/05/23 19:03:43.314
Jan  5 19:03:43.321: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/node/init/init.go:32
Jan  5 19:03:45.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslicemirroring-1207" for this suite. 01/05/23 19:03:45.33
------------------------------
• [2.080 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:03:43.255
    Jan  5 19:03:43.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename endpointslicemirroring 01/05/23 19:03:43.257
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:43.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:43.269
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 01/05/23 19:03:43.283
    STEP: mirroring an update to a custom Endpoint 01/05/23 19:03:43.299
    STEP: mirroring deletion of a custom Endpoint 01/05/23 19:03:43.314
    Jan  5 19:03:43.321: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:03:45.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSliceMirroring
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslicemirroring-1207" for this suite. 01/05/23 19:03:45.33
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:03:45.344
Jan  5 19:03:45.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:03:45.346
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:45.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:45.361
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690
STEP: Creating a ResourceQuota with terminating scope 01/05/23 19:03:45.364
STEP: Ensuring ResourceQuota status is calculated 01/05/23 19:03:45.369
STEP: Creating a ResourceQuota with not terminating scope 01/05/23 19:03:47.372
STEP: Ensuring ResourceQuota status is calculated 01/05/23 19:03:47.377
STEP: Creating a long running pod 01/05/23 19:03:49.383
STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/05/23 19:03:49.405
STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/05/23 19:03:51.409
STEP: Deleting the pod 01/05/23 19:03:53.413
STEP: Ensuring resource quota status released the pod usage 01/05/23 19:03:53.425
STEP: Creating a terminating pod 01/05/23 19:03:55.43
STEP: Ensuring resource quota with terminating scope captures the pod usage 01/05/23 19:03:55.442
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/05/23 19:03:57.446
STEP: Deleting the pod 01/05/23 19:03:59.45
STEP: Ensuring resource quota status released the pod usage 01/05/23 19:03:59.466
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:01.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7603" for this suite. 01/05/23 19:04:01.473
------------------------------
• [SLOW TEST] [16.136 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:690

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:03:45.344
    Jan  5 19:03:45.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:03:45.346
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:03:45.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:03:45.361
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:690
    STEP: Creating a ResourceQuota with terminating scope 01/05/23 19:03:45.364
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 19:03:45.369
    STEP: Creating a ResourceQuota with not terminating scope 01/05/23 19:03:47.372
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 19:03:47.377
    STEP: Creating a long running pod 01/05/23 19:03:49.383
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 01/05/23 19:03:49.405
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 01/05/23 19:03:51.409
    STEP: Deleting the pod 01/05/23 19:03:53.413
    STEP: Ensuring resource quota status released the pod usage 01/05/23 19:03:53.425
    STEP: Creating a terminating pod 01/05/23 19:03:55.43
    STEP: Ensuring resource quota with terminating scope captures the pod usage 01/05/23 19:03:55.442
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 01/05/23 19:03:57.446
    STEP: Deleting the pod 01/05/23 19:03:59.45
    STEP: Ensuring resource quota status released the pod usage 01/05/23 19:03:59.466
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:01.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7603" for this suite. 01/05/23 19:04:01.473
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:01.489
Jan  5 19:04:01.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename limitrange 01/05/23 19:04:01.49
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:01.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:01.505
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239
STEP: Creating LimitRange "e2e-limitrange-n9r5z" in namespace "limitrange-5739" 01/05/23 19:04:01.509
STEP: Creating another limitRange in another namespace 01/05/23 19:04:01.514
Jan  5 19:04:01.526: INFO: Namespace "e2e-limitrange-n9r5z-1882" created
Jan  5 19:04:01.526: INFO: Creating LimitRange "e2e-limitrange-n9r5z" in namespace "e2e-limitrange-n9r5z-1882"
STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-n9r5z" 01/05/23 19:04:01.53
Jan  5 19:04:01.532: INFO: Found 2 limitRanges
STEP: Patching LimitRange "e2e-limitrange-n9r5z" in "limitrange-5739" namespace 01/05/23 19:04:01.532
Jan  5 19:04:01.540: INFO: LimitRange "e2e-limitrange-n9r5z" has been patched
STEP: Delete LimitRange "e2e-limitrange-n9r5z" by Collection with labelSelector: "e2e-limitrange-n9r5z=patched" 01/05/23 19:04:01.54
STEP: Confirm that the limitRange "e2e-limitrange-n9r5z" has been deleted 01/05/23 19:04:01.545
Jan  5 19:04:01.545: INFO: Requesting list of LimitRange to confirm quantity
Jan  5 19:04:01.548: INFO: Found 0 LimitRange with label "e2e-limitrange-n9r5z=patched"
Jan  5 19:04:01.548: INFO: LimitRange "e2e-limitrange-n9r5z" has been deleted.
STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-n9r5z" 01/05/23 19:04:01.548
Jan  5 19:04:01.551: INFO: Found 1 limitRange
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:01.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-5739" for this suite. 01/05/23 19:04:01.554
STEP: Destroying namespace "e2e-limitrange-n9r5z-1882" for this suite. 01/05/23 19:04:01.56
------------------------------
• [0.076 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should list, patch and delete a LimitRange by collection [Conformance]
  test/e2e/scheduling/limit_range.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:01.489
    Jan  5 19:04:01.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename limitrange 01/05/23 19:04:01.49
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:01.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:01.505
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should list, patch and delete a LimitRange by collection [Conformance]
      test/e2e/scheduling/limit_range.go:239
    STEP: Creating LimitRange "e2e-limitrange-n9r5z" in namespace "limitrange-5739" 01/05/23 19:04:01.509
    STEP: Creating another limitRange in another namespace 01/05/23 19:04:01.514
    Jan  5 19:04:01.526: INFO: Namespace "e2e-limitrange-n9r5z-1882" created
    Jan  5 19:04:01.526: INFO: Creating LimitRange "e2e-limitrange-n9r5z" in namespace "e2e-limitrange-n9r5z-1882"
    STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-n9r5z" 01/05/23 19:04:01.53
    Jan  5 19:04:01.532: INFO: Found 2 limitRanges
    STEP: Patching LimitRange "e2e-limitrange-n9r5z" in "limitrange-5739" namespace 01/05/23 19:04:01.532
    Jan  5 19:04:01.540: INFO: LimitRange "e2e-limitrange-n9r5z" has been patched
    STEP: Delete LimitRange "e2e-limitrange-n9r5z" by Collection with labelSelector: "e2e-limitrange-n9r5z=patched" 01/05/23 19:04:01.54
    STEP: Confirm that the limitRange "e2e-limitrange-n9r5z" has been deleted 01/05/23 19:04:01.545
    Jan  5 19:04:01.545: INFO: Requesting list of LimitRange to confirm quantity
    Jan  5 19:04:01.548: INFO: Found 0 LimitRange with label "e2e-limitrange-n9r5z=patched"
    Jan  5 19:04:01.548: INFO: LimitRange "e2e-limitrange-n9r5z" has been deleted.
    STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-n9r5z" 01/05/23 19:04:01.548
    Jan  5 19:04:01.551: INFO: Found 1 limitRange
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:01.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-5739" for this suite. 01/05/23 19:04:01.554
    STEP: Destroying namespace "e2e-limitrange-n9r5z-1882" for this suite. 01/05/23 19:04:01.56
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:01.569
Jan  5 19:04:01.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename namespaces 01/05/23 19:04:01.571
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:01.582
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:01.585
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299
STEP: Read namespace status 01/05/23 19:04:01.588
Jan  5 19:04:01.591: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 01/05/23 19:04:01.591
Jan  5 19:04:01.595: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 01/05/23 19:04:01.596
Jan  5 19:04:01.602: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:01.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-1985" for this suite. 01/05/23 19:04:01.606
------------------------------
• [0.043 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:299

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:01.569
    Jan  5 19:04:01.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename namespaces 01/05/23 19:04:01.571
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:01.582
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:01.585
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:299
    STEP: Read namespace status 01/05/23 19:04:01.588
    Jan  5 19:04:01.591: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 01/05/23 19:04:01.591
    Jan  5 19:04:01.595: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 01/05/23 19:04:01.596
    Jan  5 19:04:01.602: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:01.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-1985" for this suite. 01/05/23 19:04:01.606
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:01.617
Jan  5 19:04:01.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename runtimeclass 01/05/23 19:04:01.618
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:01.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:01.634
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 01/05/23 19:04:01.638
STEP: getting /apis/node.k8s.io 01/05/23 19:04:01.64
STEP: getting /apis/node.k8s.io/v1 01/05/23 19:04:01.642
STEP: creating 01/05/23 19:04:01.643
STEP: watching 01/05/23 19:04:01.655
Jan  5 19:04:01.655: INFO: starting watch
STEP: getting 01/05/23 19:04:01.659
STEP: listing 01/05/23 19:04:01.661
STEP: patching 01/05/23 19:04:01.664
STEP: updating 01/05/23 19:04:01.669
Jan  5 19:04:01.674: INFO: waiting for watch events with expected annotations
STEP: deleting 01/05/23 19:04:01.674
STEP: deleting a collection 01/05/23 19:04:01.684
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:01.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-4493" for this suite. 01/05/23 19:04:01.699
------------------------------
• [0.087 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:01.617
    Jan  5 19:04:01.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 19:04:01.618
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:01.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:01.634
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 01/05/23 19:04:01.638
    STEP: getting /apis/node.k8s.io 01/05/23 19:04:01.64
    STEP: getting /apis/node.k8s.io/v1 01/05/23 19:04:01.642
    STEP: creating 01/05/23 19:04:01.643
    STEP: watching 01/05/23 19:04:01.655
    Jan  5 19:04:01.655: INFO: starting watch
    STEP: getting 01/05/23 19:04:01.659
    STEP: listing 01/05/23 19:04:01.661
    STEP: patching 01/05/23 19:04:01.664
    STEP: updating 01/05/23 19:04:01.669
    Jan  5 19:04:01.674: INFO: waiting for watch events with expected annotations
    STEP: deleting 01/05/23 19:04:01.674
    STEP: deleting a collection 01/05/23 19:04:01.684
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:01.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-4493" for this suite. 01/05/23 19:04:01.699
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:01.705
Jan  5 19:04:01.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename var-expansion 01/05/23 19:04:01.707
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:01.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:01.724
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186
Jan  5 19:04:01.737: INFO: Waiting up to 2m0s for pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d" in namespace "var-expansion-8049" to be "container 0 failed with reason CreateContainerConfigError"
Jan  5 19:04:01.740: INFO: Pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.308589ms
Jan  5 19:04:03.744: INFO: Pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007560215s
Jan  5 19:04:03.745: INFO: Pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  5 19:04:03.745: INFO: Deleting pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d" in namespace "var-expansion-8049"
Jan  5 19:04:03.752: INFO: Wait up to 5m0s for pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:05.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-8049" for this suite. 01/05/23 19:04:05.765
------------------------------
• [4.065 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:01.705
    Jan  5 19:04:01.705: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename var-expansion 01/05/23 19:04:01.707
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:01.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:01.724
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:186
    Jan  5 19:04:01.737: INFO: Waiting up to 2m0s for pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d" in namespace "var-expansion-8049" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  5 19:04:01.740: INFO: Pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.308589ms
    Jan  5 19:04:03.744: INFO: Pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007560215s
    Jan  5 19:04:03.745: INFO: Pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  5 19:04:03.745: INFO: Deleting pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d" in namespace "var-expansion-8049"
    Jan  5 19:04:03.752: INFO: Wait up to 5m0s for pod "var-expansion-c59e236e-415f-4af8-95d3-48ca7900820d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:05.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-8049" for this suite. 01/05/23 19:04:05.765
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:05.773
Jan  5 19:04:05.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:04:05.775
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:05.796
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:05.799
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:962
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/05/23 19:04:05.803
Jan  5 19:04:05.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6009 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  5 19:04:05.889: INFO: stderr: ""
Jan  5 19:04:05.889: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 01/05/23 19:04:05.889
Jan  5 19:04:05.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6009 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
Jan  5 19:04:06.152: INFO: stderr: ""
Jan  5 19:04:06.152: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/05/23 19:04:06.152
Jan  5 19:04:06.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6009 delete pods e2e-test-httpd-pod'
Jan  5 19:04:07.887: INFO: stderr: ""
Jan  5 19:04:07.887: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:07.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6009" for this suite. 01/05/23 19:04:07.895
------------------------------
• [2.129 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:956
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:962

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:05.773
    Jan  5 19:04:05.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:04:05.775
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:05.796
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:05.799
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:962
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/05/23 19:04:05.803
    Jan  5 19:04:05.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6009 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  5 19:04:05.889: INFO: stderr: ""
    Jan  5 19:04:05.889: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 01/05/23 19:04:05.889
    Jan  5 19:04:05.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6009 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
    Jan  5 19:04:06.152: INFO: stderr: ""
    Jan  5 19:04:06.152: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/05/23 19:04:06.152
    Jan  5 19:04:06.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6009 delete pods e2e-test-httpd-pod'
    Jan  5 19:04:07.887: INFO: stderr: ""
    Jan  5 19:04:07.887: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:07.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6009" for this suite. 01/05/23 19:04:07.895
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:07.906
Jan  5 19:04:07.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename containers 01/05/23 19:04:07.908
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:07.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:07.934
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87
STEP: Creating a pod to test override all 01/05/23 19:04:07.937
Jan  5 19:04:07.948: INFO: Waiting up to 5m0s for pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1" in namespace "containers-8748" to be "Succeeded or Failed"
Jan  5 19:04:07.952: INFO: Pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.694032ms
Jan  5 19:04:09.959: INFO: Pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01096512s
Jan  5 19:04:11.954: INFO: Pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006235157s
STEP: Saw pod success 01/05/23 19:04:11.954
Jan  5 19:04:11.955: INFO: Pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1" satisfied condition "Succeeded or Failed"
Jan  5 19:04:11.960: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:04:11.967
Jan  5 19:04:11.980: INFO: Waiting for pod client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1 to disappear
Jan  5 19:04:11.983: INFO: Pod client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:11.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-8748" for this suite. 01/05/23 19:04:11.987
------------------------------
• [4.087 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:07.906
    Jan  5 19:04:07.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename containers 01/05/23 19:04:07.908
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:07.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:07.934
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:87
    STEP: Creating a pod to test override all 01/05/23 19:04:07.937
    Jan  5 19:04:07.948: INFO: Waiting up to 5m0s for pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1" in namespace "containers-8748" to be "Succeeded or Failed"
    Jan  5 19:04:07.952: INFO: Pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.694032ms
    Jan  5 19:04:09.959: INFO: Pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01096512s
    Jan  5 19:04:11.954: INFO: Pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006235157s
    STEP: Saw pod success 01/05/23 19:04:11.954
    Jan  5 19:04:11.955: INFO: Pod "client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1" satisfied condition "Succeeded or Failed"
    Jan  5 19:04:11.960: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:04:11.967
    Jan  5 19:04:11.980: INFO: Waiting for pod client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1 to disappear
    Jan  5 19:04:11.983: INFO: Pod client-containers-a77cb9c8-5db4-4d7e-aaa1-7e16e6fc01d1 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:11.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-8748" for this suite. 01/05/23 19:04:11.987
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:11.998
Jan  5 19:04:11.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename dns 01/05/23 19:04:12
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:12.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:12.018
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1901.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1901.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 01/05/23 19:04:12.021
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1901.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1901.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 01/05/23 19:04:12.021
STEP: creating a pod to probe /etc/hosts 01/05/23 19:04:12.022
STEP: submitting the pod to kubernetes 01/05/23 19:04:12.022
Jan  5 19:04:12.035: INFO: Waiting up to 15m0s for pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6" in namespace "dns-1901" to be "running"
Jan  5 19:04:12.038: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595052ms
Jan  5 19:04:14.042: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007136884s
Jan  5 19:04:16.042: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007724992s
Jan  5 19:04:18.042: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007751281s
Jan  5 19:04:20.044: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Running", Reason="", readiness=true. Elapsed: 8.009334716s
Jan  5 19:04:20.044: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:04:20.044
STEP: looking for the results for each expected name from probers 01/05/23 19:04:20.047
Jan  5 19:04:20.069: INFO: DNS probes using dns-1901/dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6 succeeded

STEP: deleting the pod 01/05/23 19:04:20.069
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:20.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1901" for this suite. 01/05/23 19:04:20.086
------------------------------
• [SLOW TEST] [8.094 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:11.998
    Jan  5 19:04:11.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename dns 01/05/23 19:04:12
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:12.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:12.018
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1901.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1901.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     01/05/23 19:04:12.021
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1901.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1901.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     01/05/23 19:04:12.021
    STEP: creating a pod to probe /etc/hosts 01/05/23 19:04:12.022
    STEP: submitting the pod to kubernetes 01/05/23 19:04:12.022
    Jan  5 19:04:12.035: INFO: Waiting up to 15m0s for pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6" in namespace "dns-1901" to be "running"
    Jan  5 19:04:12.038: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595052ms
    Jan  5 19:04:14.042: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007136884s
    Jan  5 19:04:16.042: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007724992s
    Jan  5 19:04:18.042: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007751281s
    Jan  5 19:04:20.044: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6": Phase="Running", Reason="", readiness=true. Elapsed: 8.009334716s
    Jan  5 19:04:20.044: INFO: Pod "dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:04:20.044
    STEP: looking for the results for each expected name from probers 01/05/23 19:04:20.047
    Jan  5 19:04:20.069: INFO: DNS probes using dns-1901/dns-test-2a2fe7dc-197c-4a2c-9fb0-fbc18680cbd6 succeeded

    STEP: deleting the pod 01/05/23 19:04:20.069
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:20.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1901" for this suite. 01/05/23 19:04:20.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:20.093
Jan  5 19:04:20.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:04:20.094
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:20.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:20.112
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:04:20.114
Jan  5 19:04:20.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7" in namespace "downward-api-8818" to be "Succeeded or Failed"
Jan  5 19:04:20.135: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.118841ms
Jan  5 19:04:22.138: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006066991s
Jan  5 19:04:24.141: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7": Phase="Running", Reason="", readiness=false. Elapsed: 4.008473536s
Jan  5 19:04:26.141: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008678807s
STEP: Saw pod success 01/05/23 19:04:26.141
Jan  5 19:04:26.141: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7" satisfied condition "Succeeded or Failed"
Jan  5 19:04:26.145: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7 container client-container: <nil>
STEP: delete the pod 01/05/23 19:04:26.156
Jan  5 19:04:26.166: INFO: Waiting for pod downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7 to disappear
Jan  5 19:04:26.170: INFO: Pod downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:26.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8818" for this suite. 01/05/23 19:04:26.175
------------------------------
• [SLOW TEST] [6.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:20.093
    Jan  5 19:04:20.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:04:20.094
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:20.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:20.112
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:221
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:04:20.114
    Jan  5 19:04:20.132: INFO: Waiting up to 5m0s for pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7" in namespace "downward-api-8818" to be "Succeeded or Failed"
    Jan  5 19:04:20.135: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.118841ms
    Jan  5 19:04:22.138: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7": Phase="Running", Reason="", readiness=true. Elapsed: 2.006066991s
    Jan  5 19:04:24.141: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7": Phase="Running", Reason="", readiness=false. Elapsed: 4.008473536s
    Jan  5 19:04:26.141: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.008678807s
    STEP: Saw pod success 01/05/23 19:04:26.141
    Jan  5 19:04:26.141: INFO: Pod "downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7" satisfied condition "Succeeded or Failed"
    Jan  5 19:04:26.145: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:04:26.156
    Jan  5 19:04:26.166: INFO: Waiting for pod downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7 to disappear
    Jan  5 19:04:26.170: INFO: Pod downwardapi-volume-033b47c3-3471-47c8-8743-941e62d360f7 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:26.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8818" for this suite. 01/05/23 19:04:26.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:26.187
Jan  5 19:04:26.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replicaset 01/05/23 19:04:26.188
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:26.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:26.21
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 01/05/23 19:04:26.218
STEP: Verify that the required pods have come up. 01/05/23 19:04:26.226
Jan  5 19:04:26.231: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 19:04:31.234: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 19:04:31.234
STEP: Getting /status 01/05/23 19:04:31.234
Jan  5 19:04:31.237: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 01/05/23 19:04:31.237
Jan  5 19:04:31.244: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 01/05/23 19:04:31.244
Jan  5 19:04:31.247: INFO: Observed &ReplicaSet event: ADDED
Jan  5 19:04:31.247: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 19:04:31.247: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 19:04:31.247: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 19:04:31.247: INFO: Found replicaset test-rs in namespace replicaset-3737 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 19:04:31.247: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 01/05/23 19:04:31.247
Jan  5 19:04:31.247: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 19:04:31.254: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 01/05/23 19:04:31.254
Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: ADDED
Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 19:04:31.257: INFO: Observed replicaset test-rs in namespace replicaset-3737 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: MODIFIED
Jan  5 19:04:31.257: INFO: Found replicaset test-rs in namespace replicaset-3737 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jan  5 19:04:31.257: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:31.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-3737" for this suite. 01/05/23 19:04:31.26
------------------------------
• [SLOW TEST] [5.081 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:26.187
    Jan  5 19:04:26.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replicaset 01/05/23 19:04:26.188
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:26.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:26.21
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 01/05/23 19:04:26.218
    STEP: Verify that the required pods have come up. 01/05/23 19:04:26.226
    Jan  5 19:04:26.231: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 19:04:31.234: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 19:04:31.234
    STEP: Getting /status 01/05/23 19:04:31.234
    Jan  5 19:04:31.237: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 01/05/23 19:04:31.237
    Jan  5 19:04:31.244: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 01/05/23 19:04:31.244
    Jan  5 19:04:31.247: INFO: Observed &ReplicaSet event: ADDED
    Jan  5 19:04:31.247: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 19:04:31.247: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 19:04:31.247: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 19:04:31.247: INFO: Found replicaset test-rs in namespace replicaset-3737 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 19:04:31.247: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 01/05/23 19:04:31.247
    Jan  5 19:04:31.247: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 19:04:31.254: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 01/05/23 19:04:31.254
    Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: ADDED
    Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 19:04:31.257: INFO: Observed replicaset test-rs in namespace replicaset-3737 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 19:04:31.257: INFO: Observed &ReplicaSet event: MODIFIED
    Jan  5 19:04:31.257: INFO: Found replicaset test-rs in namespace replicaset-3737 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jan  5 19:04:31.257: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:31.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-3737" for this suite. 01/05/23 19:04:31.26
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:31.272
Jan  5 19:04:31.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename subpath 01/05/23 19:04:31.274
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:31.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:31.297
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 19:04:31.3
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-l89f 01/05/23 19:04:31.311
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 19:04:31.311
Jan  5 19:04:31.321: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-l89f" in namespace "subpath-8971" to be "Succeeded or Failed"
Jan  5 19:04:31.325: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.779165ms
Jan  5 19:04:33.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007225641s
Jan  5 19:04:35.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 4.007186532s
Jan  5 19:04:37.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 6.006911088s
Jan  5 19:04:39.338: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 8.016880602s
Jan  5 19:04:41.329: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 10.007441763s
Jan  5 19:04:43.329: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 12.007879009s
Jan  5 19:04:45.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 14.00718261s
Jan  5 19:04:47.329: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 16.007588809s
Jan  5 19:04:49.331: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 18.00934061s
Jan  5 19:04:51.329: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 20.007408162s
Jan  5 19:04:53.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=false. Elapsed: 22.006913539s
Jan  5 19:04:55.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006908329s
STEP: Saw pod success 01/05/23 19:04:55.328
Jan  5 19:04:55.329: INFO: Pod "pod-subpath-test-configmap-l89f" satisfied condition "Succeeded or Failed"
Jan  5 19:04:55.332: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-subpath-test-configmap-l89f container test-container-subpath-configmap-l89f: <nil>
STEP: delete the pod 01/05/23 19:04:55.34
Jan  5 19:04:55.357: INFO: Waiting for pod pod-subpath-test-configmap-l89f to disappear
Jan  5 19:04:55.362: INFO: Pod pod-subpath-test-configmap-l89f no longer exists
STEP: Deleting pod pod-subpath-test-configmap-l89f 01/05/23 19:04:55.362
Jan  5 19:04:55.362: INFO: Deleting pod "pod-subpath-test-configmap-l89f" in namespace "subpath-8971"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:55.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-8971" for this suite. 01/05/23 19:04:55.368
------------------------------
• [SLOW TEST] [24.100 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:31.272
    Jan  5 19:04:31.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename subpath 01/05/23 19:04:31.274
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:31.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:31.297
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 19:04:31.3
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-l89f 01/05/23 19:04:31.311
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 19:04:31.311
    Jan  5 19:04:31.321: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-l89f" in namespace "subpath-8971" to be "Succeeded or Failed"
    Jan  5 19:04:31.325: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.779165ms
    Jan  5 19:04:33.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007225641s
    Jan  5 19:04:35.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 4.007186532s
    Jan  5 19:04:37.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 6.006911088s
    Jan  5 19:04:39.338: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 8.016880602s
    Jan  5 19:04:41.329: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 10.007441763s
    Jan  5 19:04:43.329: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 12.007879009s
    Jan  5 19:04:45.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 14.00718261s
    Jan  5 19:04:47.329: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 16.007588809s
    Jan  5 19:04:49.331: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 18.00934061s
    Jan  5 19:04:51.329: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=true. Elapsed: 20.007408162s
    Jan  5 19:04:53.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Running", Reason="", readiness=false. Elapsed: 22.006913539s
    Jan  5 19:04:55.328: INFO: Pod "pod-subpath-test-configmap-l89f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006908329s
    STEP: Saw pod success 01/05/23 19:04:55.328
    Jan  5 19:04:55.329: INFO: Pod "pod-subpath-test-configmap-l89f" satisfied condition "Succeeded or Failed"
    Jan  5 19:04:55.332: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-subpath-test-configmap-l89f container test-container-subpath-configmap-l89f: <nil>
    STEP: delete the pod 01/05/23 19:04:55.34
    Jan  5 19:04:55.357: INFO: Waiting for pod pod-subpath-test-configmap-l89f to disappear
    Jan  5 19:04:55.362: INFO: Pod pod-subpath-test-configmap-l89f no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-l89f 01/05/23 19:04:55.362
    Jan  5 19:04:55.362: INFO: Deleting pod "pod-subpath-test-configmap-l89f" in namespace "subpath-8971"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:55.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-8971" for this suite. 01/05/23 19:04:55.368
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:55.374
Jan  5 19:04:55.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:04:55.376
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:55.386
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:55.392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1652
STEP: creating Agnhost RC 01/05/23 19:04:55.395
Jan  5 19:04:55.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9741 create -f -'
Jan  5 19:04:55.643: INFO: stderr: ""
Jan  5 19:04:55.643: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 19:04:55.643
Jan  5 19:04:56.647: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 19:04:56.647: INFO: Found 0 / 1
Jan  5 19:04:57.648: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 19:04:57.648: INFO: Found 1 / 1
Jan  5 19:04:57.648: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 01/05/23 19:04:57.648
Jan  5 19:04:57.650: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 19:04:57.650: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 19:04:57.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9741 patch pod agnhost-primary-nsl6d -p {"metadata":{"annotations":{"x":"y"}}}'
Jan  5 19:04:57.789: INFO: stderr: ""
Jan  5 19:04:57.789: INFO: stdout: "pod/agnhost-primary-nsl6d patched\n"
STEP: checking annotations 01/05/23 19:04:57.789
Jan  5 19:04:57.792: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 19:04:57.792: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:04:57.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9741" for this suite. 01/05/23 19:04:57.796
------------------------------
• [2.429 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1646
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1652

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:55.374
    Jan  5 19:04:55.374: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:04:55.376
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:55.386
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:55.392
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1652
    STEP: creating Agnhost RC 01/05/23 19:04:55.395
    Jan  5 19:04:55.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9741 create -f -'
    Jan  5 19:04:55.643: INFO: stderr: ""
    Jan  5 19:04:55.643: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 19:04:55.643
    Jan  5 19:04:56.647: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 19:04:56.647: INFO: Found 0 / 1
    Jan  5 19:04:57.648: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 19:04:57.648: INFO: Found 1 / 1
    Jan  5 19:04:57.648: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 01/05/23 19:04:57.648
    Jan  5 19:04:57.650: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 19:04:57.650: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 19:04:57.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9741 patch pod agnhost-primary-nsl6d -p {"metadata":{"annotations":{"x":"y"}}}'
    Jan  5 19:04:57.789: INFO: stderr: ""
    Jan  5 19:04:57.789: INFO: stdout: "pod/agnhost-primary-nsl6d patched\n"
    STEP: checking annotations 01/05/23 19:04:57.789
    Jan  5 19:04:57.792: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 19:04:57.792: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:04:57.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9741" for this suite. 01/05/23 19:04:57.796
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:04:57.803
Jan  5 19:04:57.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:04:57.806
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:57.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:57.826
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803
STEP: Creating a ResourceQuota with best effort scope 01/05/23 19:04:57.829
STEP: Ensuring ResourceQuota status is calculated 01/05/23 19:04:57.834
STEP: Creating a ResourceQuota with not best effort scope 01/05/23 19:04:59.837
STEP: Ensuring ResourceQuota status is calculated 01/05/23 19:04:59.842
STEP: Creating a best-effort pod 01/05/23 19:05:01.845
STEP: Ensuring resource quota with best effort scope captures the pod usage 01/05/23 19:05:01.86
STEP: Ensuring resource quota with not best effort ignored the pod usage 01/05/23 19:05:03.864
STEP: Deleting the pod 01/05/23 19:05:05.869
STEP: Ensuring resource quota status released the pod usage 01/05/23 19:05:05.878
STEP: Creating a not best-effort pod 01/05/23 19:05:07.887
STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/05/23 19:05:07.901
STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/05/23 19:05:09.905
STEP: Deleting the pod 01/05/23 19:05:11.909
STEP: Ensuring resource quota status released the pod usage 01/05/23 19:05:11.923
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:13.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-7318" for this suite. 01/05/23 19:05:13.931
------------------------------
• [SLOW TEST] [16.132 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:803

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:04:57.803
    Jan  5 19:04:57.803: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:04:57.806
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:04:57.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:04:57.826
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:803
    STEP: Creating a ResourceQuota with best effort scope 01/05/23 19:04:57.829
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 19:04:57.834
    STEP: Creating a ResourceQuota with not best effort scope 01/05/23 19:04:59.837
    STEP: Ensuring ResourceQuota status is calculated 01/05/23 19:04:59.842
    STEP: Creating a best-effort pod 01/05/23 19:05:01.845
    STEP: Ensuring resource quota with best effort scope captures the pod usage 01/05/23 19:05:01.86
    STEP: Ensuring resource quota with not best effort ignored the pod usage 01/05/23 19:05:03.864
    STEP: Deleting the pod 01/05/23 19:05:05.869
    STEP: Ensuring resource quota status released the pod usage 01/05/23 19:05:05.878
    STEP: Creating a not best-effort pod 01/05/23 19:05:07.887
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 01/05/23 19:05:07.901
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 01/05/23 19:05:09.905
    STEP: Deleting the pod 01/05/23 19:05:11.909
    STEP: Ensuring resource quota status released the pod usage 01/05/23 19:05:11.923
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:13.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-7318" for this suite. 01/05/23 19:05:13.931
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:13.94
Jan  5 19:05:13.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename podtemplate 01/05/23 19:05:13.942
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:13.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:13.955
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 01/05/23 19:05:13.958
Jan  5 19:05:13.963: INFO: created test-podtemplate-1
Jan  5 19:05:13.968: INFO: created test-podtemplate-2
Jan  5 19:05:13.973: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 01/05/23 19:05:13.973
STEP: delete collection of pod templates 01/05/23 19:05:13.975
Jan  5 19:05:13.975: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 01/05/23 19:05:13.986
Jan  5 19:05:13.986: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:13.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-628" for this suite. 01/05/23 19:05:13.992
------------------------------
• [0.056 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:13.94
    Jan  5 19:05:13.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename podtemplate 01/05/23 19:05:13.942
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:13.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:13.955
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 01/05/23 19:05:13.958
    Jan  5 19:05:13.963: INFO: created test-podtemplate-1
    Jan  5 19:05:13.968: INFO: created test-podtemplate-2
    Jan  5 19:05:13.973: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 01/05/23 19:05:13.973
    STEP: delete collection of pod templates 01/05/23 19:05:13.975
    Jan  5 19:05:13.975: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 01/05/23 19:05:13.986
    Jan  5 19:05:13.986: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:13.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-628" for this suite. 01/05/23 19:05:13.992
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:14
Jan  5 19:05:14.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:05:14.001
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:14.016
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:14.019
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:05:14.021
Jan  5 19:05:14.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6" in namespace "projected-1976" to be "Succeeded or Failed"
Jan  5 19:05:14.034: INFO: Pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.265566ms
Jan  5 19:05:16.037: INFO: Pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006187518s
Jan  5 19:05:18.039: INFO: Pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008151847s
STEP: Saw pod success 01/05/23 19:05:18.039
Jan  5 19:05:18.039: INFO: Pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6" satisfied condition "Succeeded or Failed"
Jan  5 19:05:18.042: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6 container client-container: <nil>
STEP: delete the pod 01/05/23 19:05:18.049
Jan  5 19:05:18.062: INFO: Waiting for pod downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6 to disappear
Jan  5 19:05:18.067: INFO: Pod downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:18.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1976" for this suite. 01/05/23 19:05:18.072
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:14
    Jan  5 19:05:14.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:05:14.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:14.016
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:14.019
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:84
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:05:14.021
    Jan  5 19:05:14.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6" in namespace "projected-1976" to be "Succeeded or Failed"
    Jan  5 19:05:14.034: INFO: Pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.265566ms
    Jan  5 19:05:16.037: INFO: Pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006187518s
    Jan  5 19:05:18.039: INFO: Pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008151847s
    STEP: Saw pod success 01/05/23 19:05:18.039
    Jan  5 19:05:18.039: INFO: Pod "downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6" satisfied condition "Succeeded or Failed"
    Jan  5 19:05:18.042: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:05:18.049
    Jan  5 19:05:18.062: INFO: Waiting for pod downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6 to disappear
    Jan  5 19:05:18.067: INFO: Pod downwardapi-volume-d62bfdc8-5268-4430-b2cf-46a563a66bb6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:18.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1976" for this suite. 01/05/23 19:05:18.072
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:18.086
Jan  5 19:05:18.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:05:18.087
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:18.102
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:18.105
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:05:18.108
Jan  5 19:05:18.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3" in namespace "projected-4113" to be "Succeeded or Failed"
Jan  5 19:05:18.123: INFO: Pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885234ms
Jan  5 19:05:20.127: INFO: Pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3": Phase="Running", Reason="", readiness=false. Elapsed: 2.010221287s
Jan  5 19:05:22.126: INFO: Pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00989294s
STEP: Saw pod success 01/05/23 19:05:22.126
Jan  5 19:05:22.127: INFO: Pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3" satisfied condition "Succeeded or Failed"
Jan  5 19:05:22.129: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3 container client-container: <nil>
STEP: delete the pod 01/05/23 19:05:22.136
Jan  5 19:05:22.149: INFO: Waiting for pod downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3 to disappear
Jan  5 19:05:22.153: INFO: Pod downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:22.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4113" for this suite. 01/05/23 19:05:22.157
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:18.086
    Jan  5 19:05:18.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:05:18.087
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:18.102
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:18.105
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:221
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:05:18.108
    Jan  5 19:05:18.116: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3" in namespace "projected-4113" to be "Succeeded or Failed"
    Jan  5 19:05:18.123: INFO: Pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3": Phase="Pending", Reason="", readiness=false. Elapsed: 6.885234ms
    Jan  5 19:05:20.127: INFO: Pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3": Phase="Running", Reason="", readiness=false. Elapsed: 2.010221287s
    Jan  5 19:05:22.126: INFO: Pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00989294s
    STEP: Saw pod success 01/05/23 19:05:22.126
    Jan  5 19:05:22.127: INFO: Pod "downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3" satisfied condition "Succeeded or Failed"
    Jan  5 19:05:22.129: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:05:22.136
    Jan  5 19:05:22.149: INFO: Waiting for pod downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3 to disappear
    Jan  5 19:05:22.153: INFO: Pod downwardapi-volume-08ba0ecb-b88b-441e-a736-58b6fb1533b3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:22.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4113" for this suite. 01/05/23 19:05:22.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:22.168
Jan  5 19:05:22.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:05:22.169
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:22.18
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:22.183
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2003 01/05/23 19:05:22.188
STEP: changing the ExternalName service to type=ClusterIP 01/05/23 19:05:22.203
STEP: creating replication controller externalname-service in namespace services-2003 01/05/23 19:05:22.222
I0105 19:05:22.228213      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2003, replica count: 2
I0105 19:05:25.280809      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 19:05:25.280: INFO: Creating new exec pod
Jan  5 19:05:25.288: INFO: Waiting up to 5m0s for pod "execpod9fmw9" in namespace "services-2003" to be "running"
Jan  5 19:05:25.293: INFO: Pod "execpod9fmw9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112954ms
Jan  5 19:05:27.295: INFO: Pod "execpod9fmw9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006910363s
Jan  5 19:05:27.296: INFO: Pod "execpod9fmw9" satisfied condition "running"
Jan  5 19:05:28.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-2003 exec execpod9fmw9 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan  5 19:05:28.445: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  5 19:05:28.445: INFO: stdout: ""
Jan  5 19:05:28.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-2003 exec execpod9fmw9 -- /bin/sh -x -c nc -v -z -w 2 10.20.11.25 80'
Jan  5 19:05:28.594: INFO: stderr: "+ nc -v -z -w 2 10.20.11.25 80\nConnection to 10.20.11.25 80 port [tcp/http] succeeded!\n"
Jan  5 19:05:28.594: INFO: stdout: ""
Jan  5 19:05:28.594: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:28.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2003" for this suite. 01/05/23 19:05:28.614
------------------------------
• [SLOW TEST] [6.453 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:22.168
    Jan  5 19:05:22.168: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:05:22.169
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:22.18
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:22.183
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1438
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-2003 01/05/23 19:05:22.188
    STEP: changing the ExternalName service to type=ClusterIP 01/05/23 19:05:22.203
    STEP: creating replication controller externalname-service in namespace services-2003 01/05/23 19:05:22.222
    I0105 19:05:22.228213      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-2003, replica count: 2
    I0105 19:05:25.280809      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 19:05:25.280: INFO: Creating new exec pod
    Jan  5 19:05:25.288: INFO: Waiting up to 5m0s for pod "execpod9fmw9" in namespace "services-2003" to be "running"
    Jan  5 19:05:25.293: INFO: Pod "execpod9fmw9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112954ms
    Jan  5 19:05:27.295: INFO: Pod "execpod9fmw9": Phase="Running", Reason="", readiness=true. Elapsed: 2.006910363s
    Jan  5 19:05:27.296: INFO: Pod "execpod9fmw9" satisfied condition "running"
    Jan  5 19:05:28.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-2003 exec execpod9fmw9 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan  5 19:05:28.445: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  5 19:05:28.445: INFO: stdout: ""
    Jan  5 19:05:28.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-2003 exec execpod9fmw9 -- /bin/sh -x -c nc -v -z -w 2 10.20.11.25 80'
    Jan  5 19:05:28.594: INFO: stderr: "+ nc -v -z -w 2 10.20.11.25 80\nConnection to 10.20.11.25 80 port [tcp/http] succeeded!\n"
    Jan  5 19:05:28.594: INFO: stdout: ""
    Jan  5 19:05:28.594: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:28.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2003" for this suite. 01/05/23 19:05:28.614
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:28.621
Jan  5 19:05:28.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:05:28.623
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:28.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:28.639
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jan  5 19:05:28.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:29.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-3884" for this suite. 01/05/23 19:05:29.189
------------------------------
• [0.575 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:28.621
    Jan  5 19:05:28.621: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:05:28.623
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:28.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:28.639
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jan  5 19:05:28.642: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:29.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-3884" for this suite. 01/05/23 19:05:29.189
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:29.2
Jan  5 19:05:29.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 19:05:29.201
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:29.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:29.219
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169
STEP: creating a ConfigMap 01/05/23 19:05:29.221
STEP: fetching the ConfigMap 01/05/23 19:05:29.227
STEP: patching the ConfigMap 01/05/23 19:05:29.23
STEP: listing all ConfigMaps in all namespaces with a label selector 01/05/23 19:05:29.235
STEP: deleting the ConfigMap by collection with a label selector 01/05/23 19:05:29.239
STEP: listing all ConfigMaps in test namespace 01/05/23 19:05:29.244
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:29.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6747" for this suite. 01/05/23 19:05:29.251
------------------------------
• [0.057 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:29.2
    Jan  5 19:05:29.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 19:05:29.201
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:29.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:29.219
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:169
    STEP: creating a ConfigMap 01/05/23 19:05:29.221
    STEP: fetching the ConfigMap 01/05/23 19:05:29.227
    STEP: patching the ConfigMap 01/05/23 19:05:29.23
    STEP: listing all ConfigMaps in all namespaces with a label selector 01/05/23 19:05:29.235
    STEP: deleting the ConfigMap by collection with a label selector 01/05/23 19:05:29.239
    STEP: listing all ConfigMaps in test namespace 01/05/23 19:05:29.244
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:29.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6747" for this suite. 01/05/23 19:05:29.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:29.261
Jan  5 19:05:29.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:05:29.262
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:29.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:29.278
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219
STEP: fetching services 01/05/23 19:05:29.282
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:29.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1416" for this suite. 01/05/23 19:05:29.288
------------------------------
• [0.033 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3219

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:29.261
    Jan  5 19:05:29.261: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:05:29.262
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:29.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:29.278
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3219
    STEP: fetching services 01/05/23 19:05:29.282
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:29.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1416" for this suite. 01/05/23 19:05:29.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:29.295
Jan  5 19:05:29.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-runtime 01/05/23 19:05:29.296
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:29.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:29.315
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:248
STEP: create the container 01/05/23 19:05:29.319
STEP: wait for the container to reach Succeeded 01/05/23 19:05:29.33
STEP: get the container status 01/05/23 19:05:33.354
STEP: the container should be terminated 01/05/23 19:05:33.357
STEP: the termination message should be set 01/05/23 19:05:33.357
Jan  5 19:05:33.357: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 01/05/23 19:05:33.357
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:33.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7820" for this suite. 01/05/23 19:05:33.383
------------------------------
• [4.094 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:29.295
    Jan  5 19:05:29.295: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-runtime 01/05/23 19:05:29.296
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:29.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:29.315
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:248
    STEP: create the container 01/05/23 19:05:29.319
    STEP: wait for the container to reach Succeeded 01/05/23 19:05:29.33
    STEP: get the container status 01/05/23 19:05:33.354
    STEP: the container should be terminated 01/05/23 19:05:33.357
    STEP: the termination message should be set 01/05/23 19:05:33.357
    Jan  5 19:05:33.357: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 01/05/23 19:05:33.357
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:33.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7820" for this suite. 01/05/23 19:05:33.383
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:33.394
Jan  5 19:05:33.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replicaset 01/05/23 19:05:33.396
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:33.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:33.412
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/05/23 19:05:33.415
Jan  5 19:05:33.422: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 19:05:38.431: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 19:05:38.431
STEP: getting scale subresource 01/05/23 19:05:38.431
STEP: updating a scale subresource 01/05/23 19:05:38.435
STEP: verifying the replicaset Spec.Replicas was modified 01/05/23 19:05:38.441
STEP: Patch a scale subresource 01/05/23 19:05:38.444
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:38.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7308" for this suite. 01/05/23 19:05:38.467
------------------------------
• [SLOW TEST] [5.086 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:33.394
    Jan  5 19:05:33.395: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replicaset 01/05/23 19:05:33.396
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:33.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:33.412
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 01/05/23 19:05:33.415
    Jan  5 19:05:33.422: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 19:05:38.431: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 19:05:38.431
    STEP: getting scale subresource 01/05/23 19:05:38.431
    STEP: updating a scale subresource 01/05/23 19:05:38.435
    STEP: verifying the replicaset Spec.Replicas was modified 01/05/23 19:05:38.441
    STEP: Patch a scale subresource 01/05/23 19:05:38.444
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:38.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7308" for this suite. 01/05/23 19:05:38.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:38.486
Jan  5 19:05:38.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename deployment 01/05/23 19:05:38.487
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:38.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:38.512
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jan  5 19:05:38.514: INFO: Creating deployment "test-recreate-deployment"
Jan  5 19:05:38.520: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jan  5 19:05:38.526: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Jan  5 19:05:40.532: INFO: Waiting deployment "test-recreate-deployment" to complete
Jan  5 19:05:40.534: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jan  5 19:05:40.544: INFO: Updating deployment test-recreate-deployment
Jan  5 19:05:40.545: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 19:05:40.630: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-6989  9018fef7-5f9f-4222-a37f-28b1f6c864ef 52087 2 2023-01-05 19:05:38 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004922d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 19:05:40 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-05 19:05:40 +0000 UTC,LastTransitionTime:2023-01-05 19:05:38 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jan  5 19:05:40.633: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-6989  238cbf23-9c4c-45e9-9505-2280e6b41d4a 52085 1 2023-01-05 19:05:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9018fef7-5f9f-4222-a37f-28b1f6c864ef 0xc0049231b0 0xc0049231b1}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9018fef7-5f9f-4222-a37f-28b1f6c864ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004923248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:05:40.633: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jan  5 19:05:40.633: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-6989  927f5508-7ec6-4244-80d1-965d34ee761b 52078 2 2023-01-05 19:05:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9018fef7-5f9f-4222-a37f-28b1f6c864ef 0xc004923097 0xc004923098}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9018fef7-5f9f-4222-a37f-28b1f6c864ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004923148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:05:40.636: INFO: Pod "test-recreate-deployment-cff6dc657-gtdrx" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-gtdrx test-recreate-deployment-cff6dc657- deployment-6989  87f6157e-1b01-49d7-9f22-425afebf9672 52086 0 2023-01-05 19:05:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 238cbf23-9c4c-45e9-9505-2280e6b41d4a 0xc004a346c0 0xc004a346c1}] [] [{kube-controller-manager Update v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"238cbf23-9c4c-45e9-9505-2280e6b41d4a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fprxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fprxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:,StartTime:2023-01-05 19:05:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:40.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-6989" for this suite. 01/05/23 19:05:40.639
------------------------------
• [2.160 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:38.486
    Jan  5 19:05:38.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename deployment 01/05/23 19:05:38.487
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:38.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:38.512
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jan  5 19:05:38.514: INFO: Creating deployment "test-recreate-deployment"
    Jan  5 19:05:38.520: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jan  5 19:05:38.526: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
    Jan  5 19:05:40.532: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jan  5 19:05:40.534: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jan  5 19:05:40.544: INFO: Updating deployment test-recreate-deployment
    Jan  5 19:05:40.545: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 19:05:40.630: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6989  9018fef7-5f9f-4222-a37f-28b1f6c864ef 52087 2 2023-01-05 19:05:38 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004922d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-01-05 19:05:40 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-cff6dc657" is progressing.,LastUpdateTime:2023-01-05 19:05:40 +0000 UTC,LastTransitionTime:2023-01-05 19:05:38 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  5 19:05:40.633: INFO: New ReplicaSet "test-recreate-deployment-cff6dc657" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-cff6dc657  deployment-6989  238cbf23-9c4c-45e9-9505-2280e6b41d4a 52085 1 2023-01-05 19:05:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 9018fef7-5f9f-4222-a37f-28b1f6c864ef 0xc0049231b0 0xc0049231b1}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9018fef7-5f9f-4222-a37f-28b1f6c864ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: cff6dc657,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004923248 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:05:40.633: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jan  5 19:05:40.633: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-795566c5cb  deployment-6989  927f5508-7ec6-4244-80d1-965d34ee761b 52078 2 2023-01-05 19:05:38 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 9018fef7-5f9f-4222-a37f-28b1f6c864ef 0xc004923097 0xc004923098}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9018fef7-5f9f-4222-a37f-28b1f6c864ef\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 795566c5cb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:795566c5cb] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004923148 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:05:40.636: INFO: Pod "test-recreate-deployment-cff6dc657-gtdrx" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-cff6dc657-gtdrx test-recreate-deployment-cff6dc657- deployment-6989  87f6157e-1b01-49d7-9f22-425afebf9672 52086 0 2023-01-05 19:05:40 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:cff6dc657] map[] [{apps/v1 ReplicaSet test-recreate-deployment-cff6dc657 238cbf23-9c4c-45e9-9505-2280e6b41d4a 0xc004a346c0 0xc004a346c1}] [] [{kube-controller-manager Update v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"238cbf23-9c4c-45e9-9505-2280e6b41d4a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fprxm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fprxm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:05:40 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:,StartTime:2023-01-05 19:05:40 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:40.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-6989" for this suite. 01/05/23 19:05:40.639
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:40.648
Jan  5 19:05:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename watch 01/05/23 19:05:40.649
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:40.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:40.672
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 01/05/23 19:05:40.675
STEP: modifying the configmap once 01/05/23 19:05:40.679
STEP: modifying the configmap a second time 01/05/23 19:05:40.687
STEP: deleting the configmap 01/05/23 19:05:40.693
STEP: creating a watch on configmaps from the resource version returned by the first update 01/05/23 19:05:40.697
STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/05/23 19:05:40.698
Jan  5 19:05:40.699: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1296  d34570c9-9b02-45ee-9ae4-0ea150b176ec 52094 0 2023-01-05 19:05:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:05:40.699: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1296  d34570c9-9b02-45ee-9ae4-0ea150b176ec 52095 0 2023-01-05 19:05:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:40.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-1296" for this suite. 01/05/23 19:05:40.702
------------------------------
• [0.059 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:40.648
    Jan  5 19:05:40.648: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename watch 01/05/23 19:05:40.649
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:40.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:40.672
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 01/05/23 19:05:40.675
    STEP: modifying the configmap once 01/05/23 19:05:40.679
    STEP: modifying the configmap a second time 01/05/23 19:05:40.687
    STEP: deleting the configmap 01/05/23 19:05:40.693
    STEP: creating a watch on configmaps from the resource version returned by the first update 01/05/23 19:05:40.697
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 01/05/23 19:05:40.698
    Jan  5 19:05:40.699: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1296  d34570c9-9b02-45ee-9ae4-0ea150b176ec 52094 0 2023-01-05 19:05:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:05:40.699: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1296  d34570c9-9b02-45ee-9ae4-0ea150b176ec 52095 0 2023-01-05 19:05:40 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-01-05 19:05:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:40.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-1296" for this suite. 01/05/23 19:05:40.702
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:40.711
Jan  5 19:05:40.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:05:40.712
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:40.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:40.741
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162
STEP: Creating the pod 01/05/23 19:05:40.745
Jan  5 19:05:40.761: INFO: Waiting up to 5m0s for pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e" in namespace "downward-api-3138" to be "running and ready"
Jan  5 19:05:40.766: INFO: Pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138114ms
Jan  5 19:05:40.767: INFO: The phase of Pod annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:05:42.771: INFO: Pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009982706s
Jan  5 19:05:42.771: INFO: The phase of Pod annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e is Running (Ready = true)
Jan  5 19:05:42.771: INFO: Pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e" satisfied condition "running and ready"
Jan  5 19:05:43.296: INFO: Successfully updated pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:45.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3138" for this suite. 01/05/23 19:05:45.316
------------------------------
• [4.610 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:40.711
    Jan  5 19:05:40.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:05:40.712
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:40.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:40.741
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:162
    STEP: Creating the pod 01/05/23 19:05:40.745
    Jan  5 19:05:40.761: INFO: Waiting up to 5m0s for pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e" in namespace "downward-api-3138" to be "running and ready"
    Jan  5 19:05:40.766: INFO: Pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.138114ms
    Jan  5 19:05:40.767: INFO: The phase of Pod annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:05:42.771: INFO: Pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e": Phase="Running", Reason="", readiness=true. Elapsed: 2.009982706s
    Jan  5 19:05:42.771: INFO: The phase of Pod annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e is Running (Ready = true)
    Jan  5 19:05:42.771: INFO: Pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e" satisfied condition "running and ready"
    Jan  5 19:05:43.296: INFO: Successfully updated pod "annotationupdate64d3d309-94bb-4f22-9d9b-8bd49e2b5b4e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:45.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3138" for this suite. 01/05/23 19:05:45.316
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:45.325
Jan  5 19:05:45.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:05:45.326
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:45.354
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:45.358
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67
STEP: Creating projection with secret that has name projected-secret-test-5d348e40-ff62-4347-919e-8e93b3b6c787 01/05/23 19:05:45.361
STEP: Creating a pod to test consume secrets 01/05/23 19:05:45.365
Jan  5 19:05:45.375: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb" in namespace "projected-3858" to be "Succeeded or Failed"
Jan  5 19:05:45.379: INFO: Pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603221ms
Jan  5 19:05:47.382: INFO: Pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006897263s
Jan  5 19:05:49.383: INFO: Pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00771692s
STEP: Saw pod success 01/05/23 19:05:49.383
Jan  5 19:05:49.384: INFO: Pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb" satisfied condition "Succeeded or Failed"
Jan  5 19:05:49.387: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 19:05:49.393
Jan  5 19:05:49.410: INFO: Waiting for pod pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb to disappear
Jan  5 19:05:49.414: INFO: Pod pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:49.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3858" for this suite. 01/05/23 19:05:49.419
------------------------------
• [4.102 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:45.325
    Jan  5 19:05:45.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:05:45.326
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:45.354
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:45.358
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:67
    STEP: Creating projection with secret that has name projected-secret-test-5d348e40-ff62-4347-919e-8e93b3b6c787 01/05/23 19:05:45.361
    STEP: Creating a pod to test consume secrets 01/05/23 19:05:45.365
    Jan  5 19:05:45.375: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb" in namespace "projected-3858" to be "Succeeded or Failed"
    Jan  5 19:05:45.379: INFO: Pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.603221ms
    Jan  5 19:05:47.382: INFO: Pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006897263s
    Jan  5 19:05:49.383: INFO: Pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00771692s
    STEP: Saw pod success 01/05/23 19:05:49.383
    Jan  5 19:05:49.384: INFO: Pod "pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb" satisfied condition "Succeeded or Failed"
    Jan  5 19:05:49.387: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:05:49.393
    Jan  5 19:05:49.410: INFO: Waiting for pod pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb to disappear
    Jan  5 19:05:49.414: INFO: Pod pod-projected-secrets-b54bc833-2a0e-46fb-9bbd-412c19448adb no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:49.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3858" for this suite. 01/05/23 19:05:49.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
[BeforeEach] [sig-scheduling] LimitRange
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:49.434
Jan  5 19:05:49.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename limitrange 01/05/23 19:05:49.435
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:49.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:49.452
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:31
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61
STEP: Creating a LimitRange 01/05/23 19:05:49.456
STEP: Setting up watch 01/05/23 19:05:49.456
STEP: Submitting a LimitRange 01/05/23 19:05:49.56
STEP: Verifying LimitRange creation was observed 01/05/23 19:05:49.565
STEP: Fetching the LimitRange to ensure it has proper values 01/05/23 19:05:49.566
Jan  5 19:05:49.569: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  5 19:05:49.569: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 01/05/23 19:05:49.57
STEP: Ensuring Pod has resource requirements applied from LimitRange 01/05/23 19:05:49.579
Jan  5 19:05:49.583: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jan  5 19:05:49.583: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 01/05/23 19:05:49.583
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/05/23 19:05:49.592
Jan  5 19:05:49.595: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jan  5 19:05:49.595: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 01/05/23 19:05:49.595
STEP: Failing to create a Pod with more than max resources 01/05/23 19:05:49.6
STEP: Updating a LimitRange 01/05/23 19:05:49.604
STEP: Verifying LimitRange updating is effective 01/05/23 19:05:49.609
STEP: Creating a Pod with less than former min resources 01/05/23 19:05:51.613
STEP: Failing to create a Pod with more than max resources 01/05/23 19:05:51.62
STEP: Deleting a LimitRange 01/05/23 19:05:51.625
STEP: Verifying the LimitRange was deleted 01/05/23 19:05:51.633
Jan  5 19:05:56.636: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 01/05/23 19:05:56.636
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/node/init/init.go:32
Jan  5 19:05:56.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] LimitRange
  tear down framework | framework.go:193
STEP: Destroying namespace "limitrange-8117" for this suite. 01/05/23 19:05:56.652
------------------------------
• [SLOW TEST] [7.224 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:61

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:49.434
    Jan  5 19:05:49.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename limitrange 01/05/23 19:05:49.435
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:49.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:49.452
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:61
    STEP: Creating a LimitRange 01/05/23 19:05:49.456
    STEP: Setting up watch 01/05/23 19:05:49.456
    STEP: Submitting a LimitRange 01/05/23 19:05:49.56
    STEP: Verifying LimitRange creation was observed 01/05/23 19:05:49.565
    STEP: Fetching the LimitRange to ensure it has proper values 01/05/23 19:05:49.566
    Jan  5 19:05:49.569: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  5 19:05:49.569: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 01/05/23 19:05:49.57
    STEP: Ensuring Pod has resource requirements applied from LimitRange 01/05/23 19:05:49.579
    Jan  5 19:05:49.583: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jan  5 19:05:49.583: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 01/05/23 19:05:49.583
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 01/05/23 19:05:49.592
    Jan  5 19:05:49.595: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jan  5 19:05:49.595: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 01/05/23 19:05:49.595
    STEP: Failing to create a Pod with more than max resources 01/05/23 19:05:49.6
    STEP: Updating a LimitRange 01/05/23 19:05:49.604
    STEP: Verifying LimitRange updating is effective 01/05/23 19:05:49.609
    STEP: Creating a Pod with less than former min resources 01/05/23 19:05:51.613
    STEP: Failing to create a Pod with more than max resources 01/05/23 19:05:51.62
    STEP: Deleting a LimitRange 01/05/23 19:05:51.625
    STEP: Verifying the LimitRange was deleted 01/05/23 19:05:51.633
    Jan  5 19:05:56.636: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 01/05/23 19:05:56.636
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:05:56.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] LimitRange
      tear down framework | framework.go:193
    STEP: Destroying namespace "limitrange-8117" for this suite. 01/05/23 19:05:56.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:05:56.67
Jan  5 19:05:56.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replication-controller 01/05/23 19:05:56.672
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:56.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:56.688
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67
STEP: Creating replication controller my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96 01/05/23 19:05:56.691
Jan  5 19:05:56.698: INFO: Pod name my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96: Found 0 pods out of 1
Jan  5 19:06:01.700: INFO: Pod name my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96: Found 1 pods out of 1
Jan  5 19:06:01.700: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96" are running
Jan  5 19:06:01.700: INFO: Waiting up to 5m0s for pod "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r" in namespace "replication-controller-7246" to be "running"
Jan  5 19:06:01.703: INFO: Pod "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.333454ms
Jan  5 19:06:01.703: INFO: Pod "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r" satisfied condition "running"
Jan  5 19:06:01.703: INFO: Pod "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:05:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:05:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:05:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:05:56 +0000 UTC Reason: Message:}])
Jan  5 19:06:01.703: INFO: Trying to dial the pod
Jan  5 19:06:06.716: INFO: Controller my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96: Got expected result from replica 1 [my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r]: "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  5 19:06:06.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7246" for this suite. 01/05/23 19:06:06.72
------------------------------
• [SLOW TEST] [10.055 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:05:56.67
    Jan  5 19:05:56.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replication-controller 01/05/23 19:05:56.672
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:05:56.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:05:56.688
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:67
    STEP: Creating replication controller my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96 01/05/23 19:05:56.691
    Jan  5 19:05:56.698: INFO: Pod name my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96: Found 0 pods out of 1
    Jan  5 19:06:01.700: INFO: Pod name my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96: Found 1 pods out of 1
    Jan  5 19:06:01.700: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96" are running
    Jan  5 19:06:01.700: INFO: Waiting up to 5m0s for pod "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r" in namespace "replication-controller-7246" to be "running"
    Jan  5 19:06:01.703: INFO: Pod "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r": Phase="Running", Reason="", readiness=true. Elapsed: 2.333454ms
    Jan  5 19:06:01.703: INFO: Pod "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r" satisfied condition "running"
    Jan  5 19:06:01.703: INFO: Pod "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:05:56 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:05:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:05:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:05:56 +0000 UTC Reason: Message:}])
    Jan  5 19:06:01.703: INFO: Trying to dial the pod
    Jan  5 19:06:06.716: INFO: Controller my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96: Got expected result from replica 1 [my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r]: "my-hostname-basic-dcfaa0b7-21d5-42be-a4a0-93733c00fc96-bzs8r", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:06:06.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7246" for this suite. 01/05/23 19:06:06.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:06:06.73
Jan  5 19:06:06.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename tables 01/05/23 19:06:06.732
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:06.744
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:06.747
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/node/init/init.go:32
Jan  5 19:06:06.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
  tear down framework | framework.go:193
STEP: Destroying namespace "tables-8809" for this suite. 01/05/23 19:06:06.758
------------------------------
• [0.033 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:06:06.73
    Jan  5 19:06:06.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename tables 01/05/23 19:06:06.732
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:06.744
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:06.747
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:06:06.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Servers with support for Table transformation
      tear down framework | framework.go:193
    STEP: Destroying namespace "tables-8809" for this suite. 01/05/23 19:06:06.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:06:06.765
Jan  5 19:06:06.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:06:06.767
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:06.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:06.783
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/05/23 19:06:06.786
Jan  5 19:06:06.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/05/23 19:06:13.502
Jan  5 19:06:13.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:06:15.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:06:21.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7955" for this suite. 01/05/23 19:06:21.986
------------------------------
• [SLOW TEST] [15.226 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:06:06.765
    Jan  5 19:06:06.766: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:06:06.767
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:06.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:06.783
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:309
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 01/05/23 19:06:06.786
    Jan  5 19:06:06.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 01/05/23 19:06:13.502
    Jan  5 19:06:13.503: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:06:15.128: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:06:21.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7955" for this suite. 01/05/23 19:06:21.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:06:21.993
Jan  5 19:06:21.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:06:21.994
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:22.006
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:22.008
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194
Jan  5 19:06:22.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 19:06:23.563
Jan  5 19:06:23.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 --namespace=crd-publish-openapi-2610 create -f -'
Jan  5 19:06:24.709: INFO: stderr: ""
Jan  5 19:06:24.709: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  5 19:06:24.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 --namespace=crd-publish-openapi-2610 delete e2e-test-crd-publish-openapi-2939-crds test-cr'
Jan  5 19:06:24.794: INFO: stderr: ""
Jan  5 19:06:24.794: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jan  5 19:06:24.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 --namespace=crd-publish-openapi-2610 apply -f -'
Jan  5 19:06:25.054: INFO: stderr: ""
Jan  5 19:06:25.054: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jan  5 19:06:25.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 --namespace=crd-publish-openapi-2610 delete e2e-test-crd-publish-openapi-2939-crds test-cr'
Jan  5 19:06:25.138: INFO: stderr: ""
Jan  5 19:06:25.139: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/05/23 19:06:25.139
Jan  5 19:06:25.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 explain e2e-test-crd-publish-openapi-2939-crds'
Jan  5 19:06:25.382: INFO: stderr: ""
Jan  5 19:06:25.382: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2939-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:06:26.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-2610" for this suite. 01/05/23 19:06:26.928
------------------------------
• [4.941 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:06:21.993
    Jan  5 19:06:21.993: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:06:21.994
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:22.006
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:22.008
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:194
    Jan  5 19:06:22.012: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 19:06:23.563
    Jan  5 19:06:23.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 --namespace=crd-publish-openapi-2610 create -f -'
    Jan  5 19:06:24.709: INFO: stderr: ""
    Jan  5 19:06:24.709: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  5 19:06:24.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 --namespace=crd-publish-openapi-2610 delete e2e-test-crd-publish-openapi-2939-crds test-cr'
    Jan  5 19:06:24.794: INFO: stderr: ""
    Jan  5 19:06:24.794: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jan  5 19:06:24.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 --namespace=crd-publish-openapi-2610 apply -f -'
    Jan  5 19:06:25.054: INFO: stderr: ""
    Jan  5 19:06:25.054: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jan  5 19:06:25.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 --namespace=crd-publish-openapi-2610 delete e2e-test-crd-publish-openapi-2939-crds test-cr'
    Jan  5 19:06:25.138: INFO: stderr: ""
    Jan  5 19:06:25.139: INFO: stdout: "e2e-test-crd-publish-openapi-2939-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/05/23 19:06:25.139
    Jan  5 19:06:25.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-2610 explain e2e-test-crd-publish-openapi-2939-crds'
    Jan  5 19:06:25.382: INFO: stderr: ""
    Jan  5 19:06:25.382: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2939-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:06:26.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-2610" for this suite. 01/05/23 19:06:26.928
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:06:26.934
Jan  5 19:06:26.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:06:26.936
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:26.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:26.952
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57
STEP: Creating secret with name secret-test-71e250ca-3b50-4186-915e-a85d5f9d7b5c 01/05/23 19:06:26.955
STEP: Creating a pod to test consume secrets 01/05/23 19:06:26.959
Jan  5 19:06:26.971: INFO: Waiting up to 5m0s for pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d" in namespace "secrets-1872" to be "Succeeded or Failed"
Jan  5 19:06:26.978: INFO: Pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.690489ms
Jan  5 19:06:28.982: INFO: Pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010212198s
Jan  5 19:06:30.981: INFO: Pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00991235s
STEP: Saw pod success 01/05/23 19:06:30.981
Jan  5 19:06:30.982: INFO: Pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d" satisfied condition "Succeeded or Failed"
Jan  5 19:06:30.984: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 19:06:30.997
Jan  5 19:06:31.012: INFO: Waiting for pod pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d to disappear
Jan  5 19:06:31.015: INFO: Pod pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:06:31.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1872" for this suite. 01/05/23 19:06:31.02
------------------------------
• [4.093 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:06:26.934
    Jan  5 19:06:26.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:06:26.936
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:26.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:26.952
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:57
    STEP: Creating secret with name secret-test-71e250ca-3b50-4186-915e-a85d5f9d7b5c 01/05/23 19:06:26.955
    STEP: Creating a pod to test consume secrets 01/05/23 19:06:26.959
    Jan  5 19:06:26.971: INFO: Waiting up to 5m0s for pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d" in namespace "secrets-1872" to be "Succeeded or Failed"
    Jan  5 19:06:26.978: INFO: Pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.690489ms
    Jan  5 19:06:28.982: INFO: Pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010212198s
    Jan  5 19:06:30.981: INFO: Pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00991235s
    STEP: Saw pod success 01/05/23 19:06:30.981
    Jan  5 19:06:30.982: INFO: Pod "pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d" satisfied condition "Succeeded or Failed"
    Jan  5 19:06:30.984: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:06:30.997
    Jan  5 19:06:31.012: INFO: Waiting for pod pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d to disappear
    Jan  5 19:06:31.015: INFO: Pod pod-secrets-9879e57f-1db8-46a5-82a7-3f8f52dcbb4d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:06:31.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1872" for this suite. 01/05/23 19:06:31.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:06:31.033
Jan  5 19:06:31.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename podtemplate 01/05/23 19:06:31.035
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:31.047
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:31.05
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  5 19:06:31.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8896" for this suite. 01/05/23 19:06:31.081
------------------------------
• [0.054 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:06:31.033
    Jan  5 19:06:31.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename podtemplate 01/05/23 19:06:31.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:31.047
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:31.05
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:06:31.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8896" for this suite. 01/05/23 19:06:31.081
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:06:31.091
Jan  5 19:06:31.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename watch 01/05/23 19:06:31.092
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:31.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:31.109
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 01/05/23 19:06:31.112
STEP: creating a new configmap 01/05/23 19:06:31.114
STEP: modifying the configmap once 01/05/23 19:06:31.117
STEP: changing the label value of the configmap 01/05/23 19:06:31.122
STEP: Expecting to observe a delete notification for the watched object 01/05/23 19:06:31.13
Jan  5 19:06:31.130: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52738 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:31.131: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52739 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:31.131: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52740 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 01/05/23 19:06:31.131
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/05/23 19:06:31.138
STEP: changing the label value of the configmap back 01/05/23 19:06:41.139
STEP: modifying the configmap a third time 01/05/23 19:06:41.147
STEP: deleting the configmap 01/05/23 19:06:41.153
STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/05/23 19:06:41.157
Jan  5 19:06:41.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52843 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:41.158: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52844 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:41.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52845 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  5 19:06:41.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-3713" for this suite. 01/05/23 19:06:41.162
------------------------------
• [SLOW TEST] [10.076 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:06:31.091
    Jan  5 19:06:31.091: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename watch 01/05/23 19:06:31.092
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:31.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:31.109
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 01/05/23 19:06:31.112
    STEP: creating a new configmap 01/05/23 19:06:31.114
    STEP: modifying the configmap once 01/05/23 19:06:31.117
    STEP: changing the label value of the configmap 01/05/23 19:06:31.122
    STEP: Expecting to observe a delete notification for the watched object 01/05/23 19:06:31.13
    Jan  5 19:06:31.130: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52738 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:31.131: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52739 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:31.131: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52740 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 01/05/23 19:06:31.131
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 01/05/23 19:06:31.138
    STEP: changing the label value of the configmap back 01/05/23 19:06:41.139
    STEP: modifying the configmap a third time 01/05/23 19:06:41.147
    STEP: deleting the configmap 01/05/23 19:06:41.153
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 01/05/23 19:06:41.157
    Jan  5 19:06:41.158: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52843 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:41.158: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52844 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:41.158: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3713  a27fdc5f-cd4e-4444-be2b-fab5f136c0c9 52845 0 2023-01-05 19:06:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:06:41.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-3713" for this suite. 01/05/23 19:06:41.162
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:06:41.173
Jan  5 19:06:41.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:06:41.175
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:41.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:41.189
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:06:41.193
Jan  5 19:06:41.203: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91" in namespace "downward-api-6202" to be "Succeeded or Failed"
Jan  5 19:06:41.209: INFO: Pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91": Phase="Pending", Reason="", readiness=false. Elapsed: 5.16322ms
Jan  5 19:06:43.212: INFO: Pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008492915s
Jan  5 19:06:45.214: INFO: Pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011033622s
STEP: Saw pod success 01/05/23 19:06:45.215
Jan  5 19:06:45.215: INFO: Pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91" satisfied condition "Succeeded or Failed"
Jan  5 19:06:45.218: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91 container client-container: <nil>
STEP: delete the pod 01/05/23 19:06:45.228
Jan  5 19:06:45.248: INFO: Waiting for pod downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91 to disappear
Jan  5 19:06:45.252: INFO: Pod downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:06:45.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6202" for this suite. 01/05/23 19:06:45.256
------------------------------
• [4.089 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:06:41.173
    Jan  5 19:06:41.174: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:06:41.175
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:41.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:41.189
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:235
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:06:41.193
    Jan  5 19:06:41.203: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91" in namespace "downward-api-6202" to be "Succeeded or Failed"
    Jan  5 19:06:41.209: INFO: Pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91": Phase="Pending", Reason="", readiness=false. Elapsed: 5.16322ms
    Jan  5 19:06:43.212: INFO: Pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008492915s
    Jan  5 19:06:45.214: INFO: Pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011033622s
    STEP: Saw pod success 01/05/23 19:06:45.215
    Jan  5 19:06:45.215: INFO: Pod "downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91" satisfied condition "Succeeded or Failed"
    Jan  5 19:06:45.218: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:06:45.228
    Jan  5 19:06:45.248: INFO: Waiting for pod downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91 to disappear
    Jan  5 19:06:45.252: INFO: Pod downwardapi-volume-2d951ce2-eabc-495e-bc60-c844f4346e91 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:06:45.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6202" for this suite. 01/05/23 19:06:45.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:06:45.266
Jan  5 19:06:45.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename watch 01/05/23 19:06:45.267
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:45.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:45.301
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 01/05/23 19:06:45.306
STEP: creating a watch on configmaps with label B 01/05/23 19:06:45.307
STEP: creating a watch on configmaps with label A or B 01/05/23 19:06:45.308
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/05/23 19:06:45.311
Jan  5 19:06:45.315: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52895 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:45.315: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52895 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/05/23 19:06:45.315
Jan  5 19:06:45.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52896 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:45.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52896 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/05/23 19:06:45.324
Jan  5 19:06:45.330: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52897 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:45.330: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52897 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/05/23 19:06:45.331
Jan  5 19:06:45.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52898 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:45.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52898 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/05/23 19:06:45.335
Jan  5 19:06:45.339: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8414  b8145cb6-85b0-462b-bbb4-659c1469a229 52899 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:45.339: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8414  b8145cb6-85b0-462b-bbb4-659c1469a229 52899 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/05/23 19:06:55.341
Jan  5 19:06:55.346: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8414  b8145cb6-85b0-462b-bbb4-659c1469a229 52999 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:06:55.347: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8414  b8145cb6-85b0-462b-bbb4-659c1469a229 52999 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:05.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8414" for this suite. 01/05/23 19:07:05.352
------------------------------
• [SLOW TEST] [20.093 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:06:45.266
    Jan  5 19:06:45.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename watch 01/05/23 19:06:45.267
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:06:45.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:06:45.301
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 01/05/23 19:06:45.306
    STEP: creating a watch on configmaps with label B 01/05/23 19:06:45.307
    STEP: creating a watch on configmaps with label A or B 01/05/23 19:06:45.308
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 01/05/23 19:06:45.311
    Jan  5 19:06:45.315: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52895 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:45.315: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52895 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 01/05/23 19:06:45.315
    Jan  5 19:06:45.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52896 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:45.323: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52896 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 01/05/23 19:06:45.324
    Jan  5 19:06:45.330: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52897 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:45.330: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52897 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 01/05/23 19:06:45.331
    Jan  5 19:06:45.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52898 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:45.335: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8414  5e52cc35-061a-44ef-86b5-7c5b7afb1151 52898 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 01/05/23 19:06:45.335
    Jan  5 19:06:45.339: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8414  b8145cb6-85b0-462b-bbb4-659c1469a229 52899 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:45.339: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8414  b8145cb6-85b0-462b-bbb4-659c1469a229 52899 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 01/05/23 19:06:55.341
    Jan  5 19:06:55.346: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8414  b8145cb6-85b0-462b-bbb4-659c1469a229 52999 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:06:55.347: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8414  b8145cb6-85b0-462b-bbb4-659c1469a229 52999 0 2023-01-05 19:06:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-01-05 19:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:05.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8414" for this suite. 01/05/23 19:07:05.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:05.365
Jan  5 19:07:05.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename var-expansion 01/05/23 19:07:05.367
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:05.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:05.385
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112
STEP: Creating a pod to test substitution in volume subpath 01/05/23 19:07:05.388
Jan  5 19:07:05.403: INFO: Waiting up to 5m0s for pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137" in namespace "var-expansion-930" to be "Succeeded or Failed"
Jan  5 19:07:05.406: INFO: Pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.859474ms
Jan  5 19:07:07.409: INFO: Pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006301475s
Jan  5 19:07:09.418: INFO: Pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014438502s
STEP: Saw pod success 01/05/23 19:07:09.418
Jan  5 19:07:09.418: INFO: Pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137" satisfied condition "Succeeded or Failed"
Jan  5 19:07:09.422: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137 container dapi-container: <nil>
STEP: delete the pod 01/05/23 19:07:09.438
Jan  5 19:07:09.457: INFO: Waiting for pod var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137 to disappear
Jan  5 19:07:09.461: INFO: Pod var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:09.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-930" for this suite. 01/05/23 19:07:09.466
------------------------------
• [4.118 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:112

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:05.365
    Jan  5 19:07:05.366: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename var-expansion 01/05/23 19:07:05.367
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:05.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:05.385
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:112
    STEP: Creating a pod to test substitution in volume subpath 01/05/23 19:07:05.388
    Jan  5 19:07:05.403: INFO: Waiting up to 5m0s for pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137" in namespace "var-expansion-930" to be "Succeeded or Failed"
    Jan  5 19:07:05.406: INFO: Pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.859474ms
    Jan  5 19:07:07.409: INFO: Pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006301475s
    Jan  5 19:07:09.418: INFO: Pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014438502s
    STEP: Saw pod success 01/05/23 19:07:09.418
    Jan  5 19:07:09.418: INFO: Pod "var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137" satisfied condition "Succeeded or Failed"
    Jan  5 19:07:09.422: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 19:07:09.438
    Jan  5 19:07:09.457: INFO: Waiting for pod var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137 to disappear
    Jan  5 19:07:09.461: INFO: Pod var-expansion-3ba947aa-785e-44f5-ab40-e0c160067137 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:09.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-930" for this suite. 01/05/23 19:07:09.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:09.489
Jan  5 19:07:09.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:07:09.491
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:09.508
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:09.512
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1812
STEP: Starting the proxy 01/05/23 19:07:09.515
Jan  5 19:07:09.515: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-3756 proxy --unix-socket=/tmp/kubectl-proxy-unix2715819563/test'
STEP: retrieving proxy /api/ output 01/05/23 19:07:09.574
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:09.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-3756" for this suite. 01/05/23 19:07:09.578
------------------------------
• [0.096 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1812

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:09.489
    Jan  5 19:07:09.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:07:09.491
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:09.508
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:09.512
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1812
    STEP: Starting the proxy 01/05/23 19:07:09.515
    Jan  5 19:07:09.515: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-3756 proxy --unix-socket=/tmp/kubectl-proxy-unix2715819563/test'
    STEP: retrieving proxy /api/ output 01/05/23 19:07:09.574
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:09.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-3756" for this suite. 01/05/23 19:07:09.578
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:09.586
Jan  5 19:07:09.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename containers 01/05/23 19:07:09.587
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:09.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:09.607
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39
Jan  5 19:07:09.623: INFO: Waiting up to 5m0s for pod "client-containers-84956d5f-4b27-4fda-8120-c8d435894dbd" in namespace "containers-1832" to be "running"
Jan  5 19:07:09.627: INFO: Pod "client-containers-84956d5f-4b27-4fda-8120-c8d435894dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131559ms
Jan  5 19:07:11.631: INFO: Pod "client-containers-84956d5f-4b27-4fda-8120-c8d435894dbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007890914s
Jan  5 19:07:11.631: INFO: Pod "client-containers-84956d5f-4b27-4fda-8120-c8d435894dbd" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:11.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1832" for this suite. 01/05/23 19:07:11.641
------------------------------
• [2.060 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:09.586
    Jan  5 19:07:09.586: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename containers 01/05/23 19:07:09.587
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:09.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:09.607
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:39
    Jan  5 19:07:09.623: INFO: Waiting up to 5m0s for pod "client-containers-84956d5f-4b27-4fda-8120-c8d435894dbd" in namespace "containers-1832" to be "running"
    Jan  5 19:07:09.627: INFO: Pod "client-containers-84956d5f-4b27-4fda-8120-c8d435894dbd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.131559ms
    Jan  5 19:07:11.631: INFO: Pod "client-containers-84956d5f-4b27-4fda-8120-c8d435894dbd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007890914s
    Jan  5 19:07:11.631: INFO: Pod "client-containers-84956d5f-4b27-4fda-8120-c8d435894dbd" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:11.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1832" for this suite. 01/05/23 19:07:11.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:11.652
Jan  5 19:07:11.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:07:11.654
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:11.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:11.667
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:07:11.67
Jan  5 19:07:11.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e" in namespace "projected-1362" to be "Succeeded or Failed"
Jan  5 19:07:11.685: INFO: Pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.20678ms
Jan  5 19:07:13.689: INFO: Pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007315395s
Jan  5 19:07:15.690: INFO: Pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008341296s
STEP: Saw pod success 01/05/23 19:07:15.69
Jan  5 19:07:15.690: INFO: Pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e" satisfied condition "Succeeded or Failed"
Jan  5 19:07:15.693: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e container client-container: <nil>
STEP: delete the pod 01/05/23 19:07:15.709
Jan  5 19:07:15.721: INFO: Waiting for pod downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e to disappear
Jan  5 19:07:15.724: INFO: Pod downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:15.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1362" for this suite. 01/05/23 19:07:15.729
------------------------------
• [4.082 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:11.652
    Jan  5 19:07:11.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:07:11.654
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:11.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:11.667
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:68
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:07:11.67
    Jan  5 19:07:11.681: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e" in namespace "projected-1362" to be "Succeeded or Failed"
    Jan  5 19:07:11.685: INFO: Pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.20678ms
    Jan  5 19:07:13.689: INFO: Pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007315395s
    Jan  5 19:07:15.690: INFO: Pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008341296s
    STEP: Saw pod success 01/05/23 19:07:15.69
    Jan  5 19:07:15.690: INFO: Pod "downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e" satisfied condition "Succeeded or Failed"
    Jan  5 19:07:15.693: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e container client-container: <nil>
    STEP: delete the pod 01/05/23 19:07:15.709
    Jan  5 19:07:15.721: INFO: Waiting for pod downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e to disappear
    Jan  5 19:07:15.724: INFO: Pod downwardapi-volume-e1548500-edfb-4128-a915-e9da87f4d50e no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:15.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1362" for this suite. 01/05/23 19:07:15.729
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:15.738
Jan  5 19:07:15.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-probe 01/05/23 19:07:15.739
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:15.753
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:15.756
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169
STEP: Creating pod liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49 in namespace container-probe-8372 01/05/23 19:07:15.758
Jan  5 19:07:15.765: INFO: Waiting up to 5m0s for pod "liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49" in namespace "container-probe-8372" to be "not pending"
Jan  5 19:07:15.769: INFO: Pod "liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.54202ms
Jan  5 19:07:17.773: INFO: Pod "liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49": Phase="Running", Reason="", readiness=true. Elapsed: 2.007960684s
Jan  5 19:07:17.774: INFO: Pod "liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49" satisfied condition "not pending"
Jan  5 19:07:17.774: INFO: Started pod liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49 in namespace container-probe-8372
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 19:07:17.775
Jan  5 19:07:17.778: INFO: Initial restart count of pod liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49 is 0
Jan  5 19:07:37.819: INFO: Restart count of pod container-probe-8372/liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49 is now 1 (20.040491858s elapsed)
STEP: deleting the pod 01/05/23 19:07:37.819
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:37.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8372" for this suite. 01/05/23 19:07:37.836
------------------------------
• [SLOW TEST] [22.110 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:169

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:15.738
    Jan  5 19:07:15.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-probe 01/05/23 19:07:15.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:15.753
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:15.756
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:169
    STEP: Creating pod liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49 in namespace container-probe-8372 01/05/23 19:07:15.758
    Jan  5 19:07:15.765: INFO: Waiting up to 5m0s for pod "liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49" in namespace "container-probe-8372" to be "not pending"
    Jan  5 19:07:15.769: INFO: Pod "liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49": Phase="Pending", Reason="", readiness=false. Elapsed: 3.54202ms
    Jan  5 19:07:17.773: INFO: Pod "liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49": Phase="Running", Reason="", readiness=true. Elapsed: 2.007960684s
    Jan  5 19:07:17.774: INFO: Pod "liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49" satisfied condition "not pending"
    Jan  5 19:07:17.774: INFO: Started pod liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49 in namespace container-probe-8372
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 19:07:17.775
    Jan  5 19:07:17.778: INFO: Initial restart count of pod liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49 is 0
    Jan  5 19:07:37.819: INFO: Restart count of pod container-probe-8372/liveness-58ce05c7-7ba3-4c9b-8604-a6e976c81f49 is now 1 (20.040491858s elapsed)
    STEP: deleting the pod 01/05/23 19:07:37.819
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:37.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8372" for this suite. 01/05/23 19:07:37.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:37.859
Jan  5 19:07:37.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:07:37.861
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:37.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:37.88
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 01/05/23 19:07:37.883
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/05/23 19:07:37.884
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 19:07:37.885
STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/05/23 19:07:37.885
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/05/23 19:07:37.887
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 19:07:37.887
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 19:07:37.888
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:37.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-9545" for this suite. 01/05/23 19:07:37.894
------------------------------
• [0.048 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:37.859
    Jan  5 19:07:37.859: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:07:37.861
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:37.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:37.88
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 01/05/23 19:07:37.883
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 01/05/23 19:07:37.884
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 19:07:37.885
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 01/05/23 19:07:37.885
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 01/05/23 19:07:37.887
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 19:07:37.887
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 01/05/23 19:07:37.888
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:37.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-9545" for this suite. 01/05/23 19:07:37.894
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:37.906
Jan  5 19:07:37.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:07:37.911
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:37.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:37.928
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896
STEP: creating a Pod with a static label 01/05/23 19:07:37.937
STEP: watching for Pod to be ready 01/05/23 19:07:37.945
Jan  5 19:07:37.948: INFO: observed Pod pod-test in namespace pods-1762 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jan  5 19:07:37.954: INFO: observed Pod pod-test in namespace pods-1762 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  }]
Jan  5 19:07:37.970: INFO: observed Pod pod-test in namespace pods-1762 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  }]
Jan  5 19:07:39.629: INFO: Found Pod pod-test in namespace pods-1762 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 01/05/23 19:07:39.632
STEP: getting the Pod and ensuring that it's patched 01/05/23 19:07:39.646
STEP: replacing the Pod's status Ready condition to False 01/05/23 19:07:39.648
STEP: check the Pod again to ensure its Ready conditions are False 01/05/23 19:07:39.665
STEP: deleting the Pod via a Collection with a LabelSelector 01/05/23 19:07:39.666
STEP: watching for the Pod to be deleted 01/05/23 19:07:39.677
Jan  5 19:07:39.680: INFO: observed event type MODIFIED
Jan  5 19:07:41.632: INFO: observed event type MODIFIED
Jan  5 19:07:42.633: INFO: observed event type MODIFIED
Jan  5 19:07:42.640: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:42.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1762" for this suite. 01/05/23 19:07:42.652
------------------------------
• [4.750 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:896

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:37.906
    Jan  5 19:07:37.906: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:07:37.911
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:37.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:37.928
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:896
    STEP: creating a Pod with a static label 01/05/23 19:07:37.937
    STEP: watching for Pod to be ready 01/05/23 19:07:37.945
    Jan  5 19:07:37.948: INFO: observed Pod pod-test in namespace pods-1762 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jan  5 19:07:37.954: INFO: observed Pod pod-test in namespace pods-1762 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  }]
    Jan  5 19:07:37.970: INFO: observed Pod pod-test in namespace pods-1762 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  }]
    Jan  5 19:07:39.629: INFO: Found Pod pod-test in namespace pods-1762 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:07:37 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 01/05/23 19:07:39.632
    STEP: getting the Pod and ensuring that it's patched 01/05/23 19:07:39.646
    STEP: replacing the Pod's status Ready condition to False 01/05/23 19:07:39.648
    STEP: check the Pod again to ensure its Ready conditions are False 01/05/23 19:07:39.665
    STEP: deleting the Pod via a Collection with a LabelSelector 01/05/23 19:07:39.666
    STEP: watching for the Pod to be deleted 01/05/23 19:07:39.677
    Jan  5 19:07:39.680: INFO: observed event type MODIFIED
    Jan  5 19:07:41.632: INFO: observed event type MODIFIED
    Jan  5 19:07:42.633: INFO: observed event type MODIFIED
    Jan  5 19:07:42.640: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:42.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1762" for this suite. 01/05/23 19:07:42.652
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:42.66
Jan  5 19:07:42.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replicaset 01/05/23 19:07:42.661
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:42.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:42.678
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 01/05/23 19:07:42.681
STEP: Verify that the required pods have come up 01/05/23 19:07:42.685
Jan  5 19:07:42.688: INFO: Pod name sample-pod: Found 0 pods out of 3
Jan  5 19:07:47.692: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 01/05/23 19:07:47.693
Jan  5 19:07:47.698: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 01/05/23 19:07:47.698
STEP: DeleteCollection of the ReplicaSets 01/05/23 19:07:47.702
STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/05/23 19:07:47.709
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:47.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-5267" for this suite. 01/05/23 19:07:47.72
------------------------------
• [SLOW TEST] [5.069 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:42.66
    Jan  5 19:07:42.661: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replicaset 01/05/23 19:07:42.661
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:42.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:42.678
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 01/05/23 19:07:42.681
    STEP: Verify that the required pods have come up 01/05/23 19:07:42.685
    Jan  5 19:07:42.688: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jan  5 19:07:47.692: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 01/05/23 19:07:47.693
    Jan  5 19:07:47.698: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 01/05/23 19:07:47.698
    STEP: DeleteCollection of the ReplicaSets 01/05/23 19:07:47.702
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 01/05/23 19:07:47.709
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:47.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-5267" for this suite. 01/05/23 19:07:47.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:47.731
Jan  5 19:07:47.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename prestop 01/05/23 19:07:47.733
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:47.765
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:47.769
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-5061 01/05/23 19:07:47.773
STEP: Waiting for pods to come up. 01/05/23 19:07:47.783
Jan  5 19:07:47.783: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5061" to be "running"
Jan  5 19:07:47.790: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124728ms
Jan  5 19:07:49.797: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.013812433s
Jan  5 19:07:49.797: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-5061 01/05/23 19:07:49.8
Jan  5 19:07:49.809: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5061" to be "running"
Jan  5 19:07:49.812: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786797ms
Jan  5 19:07:51.815: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006152376s
Jan  5 19:07:51.815: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 01/05/23 19:07:51.815
Jan  5 19:07:56.829: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 01/05/23 19:07:56.829
[AfterEach] [sig-node] PreStop
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:56.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PreStop
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PreStop
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PreStop
  tear down framework | framework.go:193
STEP: Destroying namespace "prestop-5061" for this suite. 01/05/23 19:07:56.85
------------------------------
• [SLOW TEST] [9.125 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:47.731
    Jan  5 19:07:47.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename prestop 01/05/23 19:07:47.733
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:47.765
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:47.769
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-5061 01/05/23 19:07:47.773
    STEP: Waiting for pods to come up. 01/05/23 19:07:47.783
    Jan  5 19:07:47.783: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-5061" to be "running"
    Jan  5 19:07:47.790: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 7.124728ms
    Jan  5 19:07:49.797: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 2.013812433s
    Jan  5 19:07:49.797: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-5061 01/05/23 19:07:49.8
    Jan  5 19:07:49.809: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-5061" to be "running"
    Jan  5 19:07:49.812: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.786797ms
    Jan  5 19:07:51.815: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006152376s
    Jan  5 19:07:51.815: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 01/05/23 19:07:51.815
    Jan  5 19:07:56.829: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 01/05/23 19:07:56.829
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:56.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PreStop
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PreStop
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PreStop
      tear down framework | framework.go:193
    STEP: Destroying namespace "prestop-5061" for this suite. 01/05/23 19:07:56.85
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:56.87
Jan  5 19:07:56.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:07:56.872
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:56.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:56.888
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140
STEP: Creating projection with secret that has name secret-emptykey-test-865b2c84-4896-4277-9462-bac6d1ad12af 01/05/23 19:07:56.891
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:56.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5821" for this suite. 01/05/23 19:07:56.899
------------------------------
• [0.034 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:56.87
    Jan  5 19:07:56.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:07:56.872
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:56.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:56.888
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:140
    STEP: Creating projection with secret that has name secret-emptykey-test-865b2c84-4896-4277-9462-bac6d1ad12af 01/05/23 19:07:56.891
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:56.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5821" for this suite. 01/05/23 19:07:56.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:56.91
Jan  5 19:07:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:07:56.912
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:56.924
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:56.928
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:07:56.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-2573" for this suite. 01/05/23 19:07:56.983
------------------------------
• [0.081 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:386

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:56.91
    Jan  5 19:07:56.910: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:07:56.912
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:56.924
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:56.928
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:386
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:07:56.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-2573" for this suite. 01/05/23 19:07:56.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:07:57
Jan  5 19:07:57.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:07:57.004
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:57.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:57.031
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/05/23 19:07:57.041
Jan  5 19:07:57.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:07:58.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:08:05.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5793" for this suite. 01/05/23 19:08:05.103
------------------------------
• [SLOW TEST] [8.108 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:357

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:07:57
    Jan  5 19:07:57.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:07:57.004
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:07:57.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:07:57.031
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:357
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 01/05/23 19:07:57.041
    Jan  5 19:07:57.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:07:58.663: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:08:05.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5793" for this suite. 01/05/23 19:08:05.103
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:08:05.112
Jan  5 19:08:05.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:08:05.113
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:05.128
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:05.131
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/05/23 19:08:05.133
Jan  5 19:08:05.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:08:06.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:08:13.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-5982" for this suite. 01/05/23 19:08:13.49
------------------------------
• [SLOW TEST] [8.384 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:08:05.112
    Jan  5 19:08:05.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:08:05.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:05.128
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:05.131
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:276
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 01/05/23 19:08:05.133
    Jan  5 19:08:05.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:08:06.683: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:08:13.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-5982" for this suite. 01/05/23 19:08:13.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:08:13.499
Jan  5 19:08:13.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 19:08:13.501
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:13.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:13.515
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/05/23 19:08:13.52
Jan  5 19:08:13.528: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8546" to be "running and ready"
Jan  5 19:08:13.532: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8929ms
Jan  5 19:08:13.533: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:08:15.536: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007900199s
Jan  5 19:08:15.537: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:08:17.540: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011809023s
Jan  5 19:08:17.541: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 19:08:17.541: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:167
STEP: create the pod with lifecycle hook 01/05/23 19:08:17.546
Jan  5 19:08:17.553: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8546" to be "running and ready"
Jan  5 19:08:17.557: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.113115ms
Jan  5 19:08:17.557: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:08:19.561: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007535108s
Jan  5 19:08:19.561: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jan  5 19:08:19.561: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/05/23 19:08:19.565
STEP: delete the pod with lifecycle hook 01/05/23 19:08:19.572
Jan  5 19:08:19.580: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  5 19:08:19.584: INFO: Pod pod-with-poststart-http-hook still exists
Jan  5 19:08:21.584: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jan  5 19:08:21.587: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  5 19:08:21.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-8546" for this suite. 01/05/23 19:08:21.592
------------------------------
• [SLOW TEST] [8.100 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:08:13.499
    Jan  5 19:08:13.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 19:08:13.501
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:13.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:13.515
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 19:08:13.52
    Jan  5 19:08:13.528: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8546" to be "running and ready"
    Jan  5 19:08:13.532: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.8929ms
    Jan  5 19:08:13.533: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:08:15.536: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007900199s
    Jan  5 19:08:15.537: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:08:17.540: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.011809023s
    Jan  5 19:08:17.541: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 19:08:17.541: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:167
    STEP: create the pod with lifecycle hook 01/05/23 19:08:17.546
    Jan  5 19:08:17.553: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8546" to be "running and ready"
    Jan  5 19:08:17.557: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.113115ms
    Jan  5 19:08:17.557: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:08:19.561: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007535108s
    Jan  5 19:08:19.561: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jan  5 19:08:19.561: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/05/23 19:08:19.565
    STEP: delete the pod with lifecycle hook 01/05/23 19:08:19.572
    Jan  5 19:08:19.580: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  5 19:08:19.584: INFO: Pod pod-with-poststart-http-hook still exists
    Jan  5 19:08:21.584: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jan  5 19:08:21.587: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:08:21.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-8546" for this suite. 01/05/23 19:08:21.592
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:08:21.6
Jan  5 19:08:21.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename security-context-test 01/05/23 19:08:21.601
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:21.617
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:21.62
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:609
Jan  5 19:08:21.632: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13" in namespace "security-context-test-7681" to be "Succeeded or Failed"
Jan  5 19:08:21.636: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124094ms
Jan  5 19:08:23.640: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007769444s
Jan  5 19:08:25.641: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008896571s
Jan  5 19:08:27.642: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010331844s
Jan  5 19:08:27.643: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  5 19:08:27.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-7681" for this suite. 01/05/23 19:08:27.658
------------------------------
• [SLOW TEST] [6.065 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:555
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:609

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:08:21.6
    Jan  5 19:08:21.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename security-context-test 01/05/23 19:08:21.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:21.617
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:21.62
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:609
    Jan  5 19:08:21.632: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13" in namespace "security-context-test-7681" to be "Succeeded or Failed"
    Jan  5 19:08:21.636: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.124094ms
    Jan  5 19:08:23.640: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007769444s
    Jan  5 19:08:25.641: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008896571s
    Jan  5 19:08:27.642: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010331844s
    Jan  5 19:08:27.643: INFO: Pod "alpine-nnp-false-4617f628-a8a7-4193-9af9-ce5641e02f13" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:08:27.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-7681" for this suite. 01/05/23 19:08:27.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:08:27.674
Jan  5 19:08:27.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename subpath 01/05/23 19:08:27.676
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:27.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:27.703
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 19:08:27.707
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-wlvq 01/05/23 19:08:27.732
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 19:08:27.733
Jan  5 19:08:27.751: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wlvq" in namespace "subpath-5765" to be "Succeeded or Failed"
Jan  5 19:08:27.755: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095028ms
Jan  5 19:08:29.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00854688s
Jan  5 19:08:31.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 4.008335592s
Jan  5 19:08:33.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 6.008211929s
Jan  5 19:08:35.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 8.007756218s
Jan  5 19:08:37.760: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 10.009128923s
Jan  5 19:08:39.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 12.008104884s
Jan  5 19:08:41.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 14.008257146s
Jan  5 19:08:43.760: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 16.008795569s
Jan  5 19:08:45.758: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 18.006892592s
Jan  5 19:08:47.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 20.007792s
Jan  5 19:08:49.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=false. Elapsed: 22.007845502s
Jan  5 19:08:51.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007773739s
STEP: Saw pod success 01/05/23 19:08:51.759
Jan  5 19:08:51.759: INFO: Pod "pod-subpath-test-secret-wlvq" satisfied condition "Succeeded or Failed"
Jan  5 19:08:51.761: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-subpath-test-secret-wlvq container test-container-subpath-secret-wlvq: <nil>
STEP: delete the pod 01/05/23 19:08:51.768
Jan  5 19:08:51.778: INFO: Waiting for pod pod-subpath-test-secret-wlvq to disappear
Jan  5 19:08:51.780: INFO: Pod pod-subpath-test-secret-wlvq no longer exists
STEP: Deleting pod pod-subpath-test-secret-wlvq 01/05/23 19:08:51.78
Jan  5 19:08:51.780: INFO: Deleting pod "pod-subpath-test-secret-wlvq" in namespace "subpath-5765"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  5 19:08:51.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-5765" for this suite. 01/05/23 19:08:51.786
------------------------------
• [SLOW TEST] [24.117 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:08:27.674
    Jan  5 19:08:27.674: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename subpath 01/05/23 19:08:27.676
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:27.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:27.703
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 19:08:27.707
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-wlvq 01/05/23 19:08:27.732
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 19:08:27.733
    Jan  5 19:08:27.751: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-wlvq" in namespace "subpath-5765" to be "Succeeded or Failed"
    Jan  5 19:08:27.755: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.095028ms
    Jan  5 19:08:29.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00854688s
    Jan  5 19:08:31.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 4.008335592s
    Jan  5 19:08:33.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 6.008211929s
    Jan  5 19:08:35.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 8.007756218s
    Jan  5 19:08:37.760: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 10.009128923s
    Jan  5 19:08:39.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 12.008104884s
    Jan  5 19:08:41.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 14.008257146s
    Jan  5 19:08:43.760: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 16.008795569s
    Jan  5 19:08:45.758: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 18.006892592s
    Jan  5 19:08:47.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=true. Elapsed: 20.007792s
    Jan  5 19:08:49.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Running", Reason="", readiness=false. Elapsed: 22.007845502s
    Jan  5 19:08:51.759: INFO: Pod "pod-subpath-test-secret-wlvq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007773739s
    STEP: Saw pod success 01/05/23 19:08:51.759
    Jan  5 19:08:51.759: INFO: Pod "pod-subpath-test-secret-wlvq" satisfied condition "Succeeded or Failed"
    Jan  5 19:08:51.761: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-subpath-test-secret-wlvq container test-container-subpath-secret-wlvq: <nil>
    STEP: delete the pod 01/05/23 19:08:51.768
    Jan  5 19:08:51.778: INFO: Waiting for pod pod-subpath-test-secret-wlvq to disappear
    Jan  5 19:08:51.780: INFO: Pod pod-subpath-test-secret-wlvq no longer exists
    STEP: Deleting pod pod-subpath-test-secret-wlvq 01/05/23 19:08:51.78
    Jan  5 19:08:51.780: INFO: Deleting pod "pod-subpath-test-secret-wlvq" in namespace "subpath-5765"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:08:51.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-5765" for this suite. 01/05/23 19:08:51.786
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:08:51.804
Jan  5 19:08:51.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:08:51.805
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:51.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:51.822
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:08:51.836
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:08:52.361
Jan  5 19:08:52.367: INFO: role binding webhook-auth-reader already exists
STEP: Deploying the webhook pod 01/05/23 19:08:52.367
STEP: Wait for the deployment to be ready 01/05/23 19:08:52.379
Jan  5 19:08:52.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 19:08:54.404
STEP: Verifying the service has paired with the endpoint 01/05/23 19:08:54.432
Jan  5 19:08:55.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656
STEP: Listing all of the created validation webhooks 01/05/23 19:08:55.494
STEP: Creating a configMap that should be mutated 01/05/23 19:08:55.51
STEP: Deleting the collection of validation webhooks 01/05/23 19:08:55.552
STEP: Creating a configMap that should not be mutated 01/05/23 19:08:55.585
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:08:55.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9002" for this suite. 01/05/23 19:08:55.633
STEP: Destroying namespace "webhook-9002-markers" for this suite. 01/05/23 19:08:55.64
------------------------------
• [3.845 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:656

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:08:51.804
    Jan  5 19:08:51.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:08:51.805
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:51.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:51.822
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:08:51.836
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:08:52.361
    Jan  5 19:08:52.367: INFO: role binding webhook-auth-reader already exists
    STEP: Deploying the webhook pod 01/05/23 19:08:52.367
    STEP: Wait for the deployment to be ready 01/05/23 19:08:52.379
    Jan  5 19:08:52.386: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 19:08:54.404
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:08:54.432
    Jan  5 19:08:55.432: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:656
    STEP: Listing all of the created validation webhooks 01/05/23 19:08:55.494
    STEP: Creating a configMap that should be mutated 01/05/23 19:08:55.51
    STEP: Deleting the collection of validation webhooks 01/05/23 19:08:55.552
    STEP: Creating a configMap that should not be mutated 01/05/23 19:08:55.585
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:08:55.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9002" for this suite. 01/05/23 19:08:55.633
    STEP: Destroying namespace "webhook-9002-markers" for this suite. 01/05/23 19:08:55.64
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:08:55.655
Jan  5 19:08:55.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename init-container 01/05/23 19:08:55.657
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:55.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:55.68
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334
STEP: creating the pod 01/05/23 19:08:55.684
Jan  5 19:08:55.684: INFO: PodSpec: initContainers in spec.initContainers
Jan  5 19:09:38.985: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-93ec8f91-0259-40ef-9a26-6622afce3ade", GenerateName:"", Namespace:"init-container-2704", SelfLink:"", UID:"7d201a70-dc51-426d-9485-784651c88f17", ResourceVersion:"54881", Generation:0, CreationTimestamp:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"684351587"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00122f2a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 19, 9, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00122f2f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-rsnc9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000dd4c60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rsnc9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rsnc9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rsnc9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0035075f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"gke-gke-1-26-default-pool-05283374-16pz", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009ddce0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003507670)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003507690)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003507698), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00350769c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0015279d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.196.0.39", PodIP:"10.16.2.204", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.16.2.204"}}, StartTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009ddea0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009ddf80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://db159e047944ce6c9bba566132790bb2c288f349a07e927c0a76ecb23039079c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000dd4d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000dd4d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00350771f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:09:38.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-2704" for this suite. 01/05/23 19:09:38.991
------------------------------
• [SLOW TEST] [43.340 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:08:55.655
    Jan  5 19:08:55.656: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename init-container 01/05/23 19:08:55.657
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:08:55.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:08:55.68
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:334
    STEP: creating the pod 01/05/23 19:08:55.684
    Jan  5 19:08:55.684: INFO: PodSpec: initContainers in spec.initContainers
    Jan  5 19:09:38.985: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-93ec8f91-0259-40ef-9a26-6622afce3ade", GenerateName:"", Namespace:"init-container-2704", SelfLink:"", UID:"7d201a70-dc51-426d-9485-784651c88f17", ResourceVersion:"54881", Generation:0, CreationTimestamp:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"684351587"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00122f2a8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.January, 5, 19, 9, 38, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00122f2f0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-rsnc9", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000dd4c60), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rsnc9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rsnc9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-rsnc9", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0035075f0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"gke-gke-1-26-default-pool-05283374-16pz", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009ddce0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003507670)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc003507690)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc003507698), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00350769c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0015279d0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.196.0.39", PodIP:"10.16.2.204", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.16.2.204"}}, StartTime:time.Date(2023, time.January, 5, 19, 8, 55, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009ddea0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009ddf80)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://db159e047944ce6c9bba566132790bb2c288f349a07e927c0a76ecb23039079c", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000dd4d40), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000dd4d20), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc00350771f)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:09:38.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-2704" for this suite. 01/05/23 19:09:38.991
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:09:38.996
Jan  5 19:09:38.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:09:38.998
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:09:39.009
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:09:39.012
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177
STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 19:09:39.015
Jan  5 19:09:39.023: INFO: Waiting up to 5m0s for pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1" in namespace "emptydir-9046" to be "Succeeded or Failed"
Jan  5 19:09:39.027: INFO: Pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304801ms
Jan  5 19:09:41.030: INFO: Pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006631021s
Jan  5 19:09:43.030: INFO: Pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007057769s
STEP: Saw pod success 01/05/23 19:09:43.03
Jan  5 19:09:43.031: INFO: Pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1" satisfied condition "Succeeded or Failed"
Jan  5 19:09:43.033: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-e4eae016-59e8-44d1-910c-17f427873fc1 container test-container: <nil>
STEP: delete the pod 01/05/23 19:09:43.041
Jan  5 19:09:43.051: INFO: Waiting for pod pod-e4eae016-59e8-44d1-910c-17f427873fc1 to disappear
Jan  5 19:09:43.054: INFO: Pod pod-e4eae016-59e8-44d1-910c-17f427873fc1 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:09:43.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9046" for this suite. 01/05/23 19:09:43.058
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:09:38.996
    Jan  5 19:09:38.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:09:38.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:09:39.009
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:09:39.012
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:177
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 19:09:39.015
    Jan  5 19:09:39.023: INFO: Waiting up to 5m0s for pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1" in namespace "emptydir-9046" to be "Succeeded or Failed"
    Jan  5 19:09:39.027: INFO: Pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.304801ms
    Jan  5 19:09:41.030: INFO: Pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006631021s
    Jan  5 19:09:43.030: INFO: Pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007057769s
    STEP: Saw pod success 01/05/23 19:09:43.03
    Jan  5 19:09:43.031: INFO: Pod "pod-e4eae016-59e8-44d1-910c-17f427873fc1" satisfied condition "Succeeded or Failed"
    Jan  5 19:09:43.033: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-e4eae016-59e8-44d1-910c-17f427873fc1 container test-container: <nil>
    STEP: delete the pod 01/05/23 19:09:43.041
    Jan  5 19:09:43.051: INFO: Waiting for pod pod-e4eae016-59e8-44d1-910c-17f427873fc1 to disappear
    Jan  5 19:09:43.054: INFO: Pod pod-e4eae016-59e8-44d1-910c-17f427873fc1 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:09:43.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9046" for this suite. 01/05/23 19:09:43.058
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:09:43.068
Jan  5 19:09:43.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename job 01/05/23 19:09:43.07
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:09:43.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:09:43.085
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507
STEP: Creating a job 01/05/23 19:09:43.089
STEP: Ensuring active pods == parallelism 01/05/23 19:09:43.096
STEP: Orphaning one of the Job's Pods 01/05/23 19:09:45.114
Jan  5 19:09:45.639: INFO: Successfully updated pod "adopt-release-bmpng"
STEP: Checking that the Job readopts the Pod 01/05/23 19:09:45.64
Jan  5 19:09:45.640: INFO: Waiting up to 15m0s for pod "adopt-release-bmpng" in namespace "job-3324" to be "adopted"
Jan  5 19:09:45.644: INFO: Pod "adopt-release-bmpng": Phase="Running", Reason="", readiness=true. Elapsed: 4.226808ms
Jan  5 19:09:47.647: INFO: Pod "adopt-release-bmpng": Phase="Running", Reason="", readiness=true. Elapsed: 2.007349431s
Jan  5 19:09:47.647: INFO: Pod "adopt-release-bmpng" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 01/05/23 19:09:47.647
Jan  5 19:09:48.161: INFO: Successfully updated pod "adopt-release-bmpng"
STEP: Checking that the Job releases the Pod 01/05/23 19:09:48.161
Jan  5 19:09:48.161: INFO: Waiting up to 15m0s for pod "adopt-release-bmpng" in namespace "job-3324" to be "released"
Jan  5 19:09:48.166: INFO: Pod "adopt-release-bmpng": Phase="Running", Reason="", readiness=true. Elapsed: 4.397838ms
Jan  5 19:09:50.170: INFO: Pod "adopt-release-bmpng": Phase="Running", Reason="", readiness=true. Elapsed: 2.008626535s
Jan  5 19:09:50.170: INFO: Pod "adopt-release-bmpng" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  5 19:09:50.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-3324" for this suite. 01/05/23 19:09:50.175
------------------------------
• [SLOW TEST] [7.113 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:09:43.068
    Jan  5 19:09:43.068: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename job 01/05/23 19:09:43.07
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:09:43.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:09:43.085
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:507
    STEP: Creating a job 01/05/23 19:09:43.089
    STEP: Ensuring active pods == parallelism 01/05/23 19:09:43.096
    STEP: Orphaning one of the Job's Pods 01/05/23 19:09:45.114
    Jan  5 19:09:45.639: INFO: Successfully updated pod "adopt-release-bmpng"
    STEP: Checking that the Job readopts the Pod 01/05/23 19:09:45.64
    Jan  5 19:09:45.640: INFO: Waiting up to 15m0s for pod "adopt-release-bmpng" in namespace "job-3324" to be "adopted"
    Jan  5 19:09:45.644: INFO: Pod "adopt-release-bmpng": Phase="Running", Reason="", readiness=true. Elapsed: 4.226808ms
    Jan  5 19:09:47.647: INFO: Pod "adopt-release-bmpng": Phase="Running", Reason="", readiness=true. Elapsed: 2.007349431s
    Jan  5 19:09:47.647: INFO: Pod "adopt-release-bmpng" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 01/05/23 19:09:47.647
    Jan  5 19:09:48.161: INFO: Successfully updated pod "adopt-release-bmpng"
    STEP: Checking that the Job releases the Pod 01/05/23 19:09:48.161
    Jan  5 19:09:48.161: INFO: Waiting up to 15m0s for pod "adopt-release-bmpng" in namespace "job-3324" to be "released"
    Jan  5 19:09:48.166: INFO: Pod "adopt-release-bmpng": Phase="Running", Reason="", readiness=true. Elapsed: 4.397838ms
    Jan  5 19:09:50.170: INFO: Pod "adopt-release-bmpng": Phase="Running", Reason="", readiness=true. Elapsed: 2.008626535s
    Jan  5 19:09:50.170: INFO: Pod "adopt-release-bmpng" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:09:50.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-3324" for this suite. 01/05/23 19:09:50.175
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:09:50.184
Jan  5 19:09:50.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:09:50.186
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:09:50.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:09:50.203
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213
STEP: creating service in namespace services-3290 01/05/23 19:09:50.206
STEP: creating service affinity-clusterip-transition in namespace services-3290 01/05/23 19:09:50.206
STEP: creating replication controller affinity-clusterip-transition in namespace services-3290 01/05/23 19:09:50.217
I0105 19:09:50.224107      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3290, replica count: 3
I0105 19:09:53.277311      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 19:09:53.282: INFO: Creating new exec pod
Jan  5 19:09:53.288: INFO: Waiting up to 5m0s for pod "execpod-affinityqz2b6" in namespace "services-3290" to be "running"
Jan  5 19:09:53.294: INFO: Pod "execpod-affinityqz2b6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.427027ms
Jan  5 19:09:55.297: INFO: Pod "execpod-affinityqz2b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008621491s
Jan  5 19:09:55.297: INFO: Pod "execpod-affinityqz2b6" satisfied condition "running"
Jan  5 19:09:56.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan  5 19:09:56.446: INFO: rc: 1
Jan  5 19:09:56.447: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-clusterip-transition 80
nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:09:57.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan  5 19:09:57.598: INFO: rc: 1
Jan  5 19:09:57.598: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-clusterip-transition 80
nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:09:58.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan  5 19:09:58.578: INFO: rc: 1
Jan  5 19:09:58.578: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-clusterip-transition 80
nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:09:59.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan  5 19:09:59.583: INFO: rc: 1
Jan  5 19:09:59.583: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-clusterip-transition 80
nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:10:00.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
Jan  5 19:10:00.590: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jan  5 19:10:00.590: INFO: stdout: ""
Jan  5 19:10:00.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 10.20.2.244 80'
Jan  5 19:10:00.728: INFO: stderr: "+ nc -v -z -w 2 10.20.2.244 80\nConnection to 10.20.2.244 80 port [tcp/http] succeeded!\n"
Jan  5 19:10:00.728: INFO: stdout: ""
Jan  5 19:10:00.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.20.2.244:80/ ; done'
Jan  5 19:10:01.003: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n"
Jan  5 19:10:01.003: INFO: stdout: "\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n"
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:31.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.20.2.244:80/ ; done'
Jan  5 19:10:31.233: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n"
Jan  5 19:10:31.233: INFO: stdout: "\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-knkrr\naffinity-clusterip-transition-knkrr\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-knkrr\naffinity-clusterip-transition-knkrr\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n"
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-knkrr
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-knkrr
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-knkrr
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-knkrr
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
Jan  5 19:10:31.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.20.2.244:80/ ; done'
Jan  5 19:10:31.511: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n"
Jan  5 19:10:31.511: INFO: stdout: "\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6"
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
Jan  5 19:10:31.511: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3290, will wait for the garbage collector to delete the pods 01/05/23 19:10:31.523
Jan  5 19:10:31.584: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.972975ms
Jan  5 19:10:31.686: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.271201ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:10:33.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3290" for this suite. 01/05/23 19:10:33.403
------------------------------
• [SLOW TEST] [43.224 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2213

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:09:50.184
    Jan  5 19:09:50.185: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:09:50.186
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:09:50.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:09:50.203
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2213
    STEP: creating service in namespace services-3290 01/05/23 19:09:50.206
    STEP: creating service affinity-clusterip-transition in namespace services-3290 01/05/23 19:09:50.206
    STEP: creating replication controller affinity-clusterip-transition in namespace services-3290 01/05/23 19:09:50.217
    I0105 19:09:50.224107      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-3290, replica count: 3
    I0105 19:09:53.277311      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 19:09:53.282: INFO: Creating new exec pod
    Jan  5 19:09:53.288: INFO: Waiting up to 5m0s for pod "execpod-affinityqz2b6" in namespace "services-3290" to be "running"
    Jan  5 19:09:53.294: INFO: Pod "execpod-affinityqz2b6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.427027ms
    Jan  5 19:09:55.297: INFO: Pod "execpod-affinityqz2b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.008621491s
    Jan  5 19:09:55.297: INFO: Pod "execpod-affinityqz2b6" satisfied condition "running"
    Jan  5 19:09:56.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan  5 19:09:56.446: INFO: rc: 1
    Jan  5 19:09:56.447: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-clusterip-transition 80
    nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:09:57.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan  5 19:09:57.598: INFO: rc: 1
    Jan  5 19:09:57.598: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-clusterip-transition 80
    nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:09:58.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan  5 19:09:58.578: INFO: rc: 1
    Jan  5 19:09:58.578: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-clusterip-transition 80
    nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:09:59.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan  5 19:09:59.583: INFO: rc: 1
    Jan  5 19:09:59.583: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-clusterip-transition 80
    nc: connect to affinity-clusterip-transition port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:10:00.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip-transition 80'
    Jan  5 19:10:00.590: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jan  5 19:10:00.590: INFO: stdout: ""
    Jan  5 19:10:00.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c nc -v -z -w 2 10.20.2.244 80'
    Jan  5 19:10:00.728: INFO: stderr: "+ nc -v -z -w 2 10.20.2.244 80\nConnection to 10.20.2.244 80 port [tcp/http] succeeded!\n"
    Jan  5 19:10:00.728: INFO: stdout: ""
    Jan  5 19:10:00.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.20.2.244:80/ ; done'
    Jan  5 19:10:01.003: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n"
    Jan  5 19:10:01.003: INFO: stdout: "\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n"
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:01.003: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:31.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.20.2.244:80/ ; done'
    Jan  5 19:10:31.233: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n"
    Jan  5 19:10:31.233: INFO: stdout: "\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-knkrr\naffinity-clusterip-transition-knkrr\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-knkrr\naffinity-clusterip-transition-knkrr\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-sr76n\naffinity-clusterip-transition-sr76n"
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-knkrr
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-knkrr
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-knkrr
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-knkrr
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:31.233: INFO: Received response from host: affinity-clusterip-transition-sr76n
    Jan  5 19:10:31.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-3290 exec execpod-affinityqz2b6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.20.2.244:80/ ; done'
    Jan  5 19:10:31.511: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.2.244:80/\n"
    Jan  5 19:10:31.511: INFO: stdout: "\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6\naffinity-clusterip-transition-zqpp6"
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Received response from host: affinity-clusterip-transition-zqpp6
    Jan  5 19:10:31.511: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3290, will wait for the garbage collector to delete the pods 01/05/23 19:10:31.523
    Jan  5 19:10:31.584: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.972975ms
    Jan  5 19:10:31.686: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 102.271201ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:10:33.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3290" for this suite. 01/05/23 19:10:33.403
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:10:33.41
Jan  5 19:10:33.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-runtime 01/05/23 19:10:33.414
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:10:33.429
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:10:33.433
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:52
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/05/23 19:10:33.447
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/05/23 19:10:48.504
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/05/23 19:10:48.507
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/05/23 19:10:48.511
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/05/23 19:10:48.511
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/05/23 19:10:48.532
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/05/23 19:10:51.545
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/05/23 19:10:53.554
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/05/23 19:10:53.559
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/05/23 19:10:53.56
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/05/23 19:10:53.585
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/05/23 19:10:54.591
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/05/23 19:10:57.605
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/05/23 19:10:57.61
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/05/23 19:10:57.61
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  5 19:10:57.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-6386" for this suite. 01/05/23 19:10:57.633
------------------------------
• [SLOW TEST] [24.228 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    when starting a container that exits
    test/e2e/common/node/runtime.go:45
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:10:33.41
    Jan  5 19:10:33.410: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-runtime 01/05/23 19:10:33.414
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:10:33.429
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:10:33.433
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:52
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 01/05/23 19:10:33.447
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 01/05/23 19:10:48.504
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 01/05/23 19:10:48.507
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 01/05/23 19:10:48.511
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 01/05/23 19:10:48.511
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 01/05/23 19:10:48.532
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 01/05/23 19:10:51.545
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 01/05/23 19:10:53.554
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 01/05/23 19:10:53.559
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 01/05/23 19:10:53.56
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 01/05/23 19:10:53.585
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 01/05/23 19:10:54.591
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 01/05/23 19:10:57.605
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 01/05/23 19:10:57.61
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 01/05/23 19:10:57.61
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:10:57.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-6386" for this suite. 01/05/23 19:10:57.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:10:57.639
Jan  5 19:10:57.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:10:57.64
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:10:57.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:10:57.657
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:10:57.677
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:10:58.142
STEP: Deploying the webhook pod 01/05/23 19:10:58.149
STEP: Wait for the deployment to be ready 01/05/23 19:10:58.16
Jan  5 19:10:58.165: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 19:11:00.181
STEP: Verifying the service has paired with the endpoint 01/05/23 19:11:00.195
Jan  5 19:11:01.196: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323
Jan  5 19:11:01.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8796-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 19:11:01.714
STEP: Creating a custom resource while v1 is storage version 01/05/23 19:11:01.735
STEP: Patching Custom Resource Definition to set v2 as storage 01/05/23 19:11:03.795
STEP: Patching the custom resource while v2 is storage version 01/05/23 19:11:03.803
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:04.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-2848" for this suite. 01/05/23 19:11:04.423
STEP: Destroying namespace "webhook-2848-markers" for this suite. 01/05/23 19:11:04.428
------------------------------
• [SLOW TEST] [6.798 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:323

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:10:57.639
    Jan  5 19:10:57.639: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:10:57.64
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:10:57.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:10:57.657
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:10:57.677
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:10:58.142
    STEP: Deploying the webhook pod 01/05/23 19:10:58.149
    STEP: Wait for the deployment to be ready 01/05/23 19:10:58.16
    Jan  5 19:10:58.165: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 19:11:00.181
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:11:00.195
    Jan  5 19:11:01.196: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:323
    Jan  5 19:11:01.199: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8796-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 19:11:01.714
    STEP: Creating a custom resource while v1 is storage version 01/05/23 19:11:01.735
    STEP: Patching Custom Resource Definition to set v2 as storage 01/05/23 19:11:03.795
    STEP: Patching the custom resource while v2 is storage version 01/05/23 19:11:03.803
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:04.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-2848" for this suite. 01/05/23 19:11:04.423
    STEP: Destroying namespace "webhook-2848-markers" for this suite. 01/05/23 19:11:04.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:04.449
Jan  5 19:11:04.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:11:04.45
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:04.462
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:04.466
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161
Jan  5 19:11:04.485: INFO: created pod pod-service-account-defaultsa
Jan  5 19:11:04.485: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jan  5 19:11:04.495: INFO: created pod pod-service-account-mountsa
Jan  5 19:11:04.495: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jan  5 19:11:04.517: INFO: created pod pod-service-account-nomountsa
Jan  5 19:11:04.517: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jan  5 19:11:04.532: INFO: created pod pod-service-account-defaultsa-mountspec
Jan  5 19:11:04.534: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jan  5 19:11:04.545: INFO: created pod pod-service-account-mountsa-mountspec
Jan  5 19:11:04.545: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jan  5 19:11:04.559: INFO: created pod pod-service-account-nomountsa-mountspec
Jan  5 19:11:04.559: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jan  5 19:11:04.571: INFO: created pod pod-service-account-defaultsa-nomountspec
Jan  5 19:11:04.571: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jan  5 19:11:04.590: INFO: created pod pod-service-account-mountsa-nomountspec
Jan  5 19:11:04.590: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jan  5 19:11:04.632: INFO: created pod pod-service-account-nomountsa-nomountspec
Jan  5 19:11:04.632: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-8683" for this suite. 01/05/23 19:11:04.656
------------------------------
• [0.217 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:04.449
    Jan  5 19:11:04.449: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:11:04.45
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:04.462
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:04.466
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:161
    Jan  5 19:11:04.485: INFO: created pod pod-service-account-defaultsa
    Jan  5 19:11:04.485: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jan  5 19:11:04.495: INFO: created pod pod-service-account-mountsa
    Jan  5 19:11:04.495: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jan  5 19:11:04.517: INFO: created pod pod-service-account-nomountsa
    Jan  5 19:11:04.517: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jan  5 19:11:04.532: INFO: created pod pod-service-account-defaultsa-mountspec
    Jan  5 19:11:04.534: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jan  5 19:11:04.545: INFO: created pod pod-service-account-mountsa-mountspec
    Jan  5 19:11:04.545: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jan  5 19:11:04.559: INFO: created pod pod-service-account-nomountsa-mountspec
    Jan  5 19:11:04.559: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jan  5 19:11:04.571: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jan  5 19:11:04.571: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jan  5 19:11:04.590: INFO: created pod pod-service-account-mountsa-nomountspec
    Jan  5 19:11:04.590: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jan  5 19:11:04.632: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jan  5 19:11:04.632: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:04.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-8683" for this suite. 01/05/23 19:11:04.656
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:04.668
Jan  5 19:11:04.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename daemonsets 01/05/23 19:11:04.669
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:04.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:04.703
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 19:11:04.738
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:11:04.746
Jan  5 19:11:04.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:11:04.754: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:11:05.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:11:05.773: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:11:06.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 19:11:06.762: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:11:07.764: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 19:11:07.764: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:11:08.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 19:11:08.761: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 01/05/23 19:11:08.764
Jan  5 19:11:08.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 19:11:08.780: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:11:09.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 19:11:09.789: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:11:10.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 19:11:10.788: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:11:11.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 19:11:11.789: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:11:11.792
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5102, will wait for the garbage collector to delete the pods 01/05/23 19:11:11.793
Jan  5 19:11:11.851: INFO: Deleting DaemonSet.extensions daemon-set took: 5.577969ms
Jan  5 19:11:11.952: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.823608ms
Jan  5 19:11:14.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:11:14.456: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 19:11:14.459: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56077"},"items":null}

Jan  5 19:11:14.462: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56077"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:14.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-5102" for this suite. 01/05/23 19:11:14.479
------------------------------
• [SLOW TEST] [9.816 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:04.668
    Jan  5 19:11:04.668: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename daemonsets 01/05/23 19:11:04.669
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:04.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:04.703
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:166
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 19:11:04.738
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:11:04.746
    Jan  5 19:11:04.754: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:11:04.754: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:11:05.773: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:11:05.773: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:11:06.762: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 19:11:06.762: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:11:07.764: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 19:11:07.764: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:11:08.761: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 19:11:08.761: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 01/05/23 19:11:08.764
    Jan  5 19:11:08.779: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 19:11:08.780: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:11:09.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 19:11:09.789: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:11:10.788: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 19:11:10.788: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:11:11.789: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 19:11:11.789: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:11:11.792
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5102, will wait for the garbage collector to delete the pods 01/05/23 19:11:11.793
    Jan  5 19:11:11.851: INFO: Deleting DaemonSet.extensions daemon-set took: 5.577969ms
    Jan  5 19:11:11.952: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.823608ms
    Jan  5 19:11:14.456: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:11:14.456: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 19:11:14.459: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"56077"},"items":null}

    Jan  5 19:11:14.462: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"56077"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:14.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-5102" for this suite. 01/05/23 19:11:14.479
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:14.495
Jan  5 19:11:14.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:11:14.497
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:14.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:14.513
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:11:14.529
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:11:14.796
STEP: Deploying the webhook pod 01/05/23 19:11:14.801
STEP: Wait for the deployment to be ready 01/05/23 19:11:14.813
Jan  5 19:11:14.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 19:11:16.831
STEP: Verifying the service has paired with the endpoint 01/05/23 19:11:16.841
Jan  5 19:11:17.842: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291
Jan  5 19:11:17.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7767-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 19:11:18.365
STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 19:11:18.387
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:20.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6642" for this suite. 01/05/23 19:11:20.998
STEP: Destroying namespace "webhook-6642-markers" for this suite. 01/05/23 19:11:21.006
------------------------------
• [SLOW TEST] [6.520 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:291

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:14.495
    Jan  5 19:11:14.495: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:11:14.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:14.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:14.513
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:11:14.529
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:11:14.796
    STEP: Deploying the webhook pod 01/05/23 19:11:14.801
    STEP: Wait for the deployment to be ready 01/05/23 19:11:14.813
    Jan  5 19:11:14.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 19:11:16.831
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:11:16.841
    Jan  5 19:11:17.842: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:291
    Jan  5 19:11:17.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7767-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 19:11:18.365
    STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 19:11:18.387
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:20.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6642" for this suite. 01/05/23 19:11:20.998
    STEP: Destroying namespace "webhook-6642-markers" for this suite. 01/05/23 19:11:21.006
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:21.033
Jan  5 19:11:21.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:11:21.035
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:21.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:21.061
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:11:21.067
Jan  5 19:11:21.077: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302" in namespace "projected-3948" to be "Succeeded or Failed"
Jan  5 19:11:21.088: INFO: Pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302": Phase="Pending", Reason="", readiness=false. Elapsed: 11.01875ms
Jan  5 19:11:23.093: INFO: Pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015785884s
Jan  5 19:11:25.093: INFO: Pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015396721s
STEP: Saw pod success 01/05/23 19:11:25.093
Jan  5 19:11:25.093: INFO: Pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302" satisfied condition "Succeeded or Failed"
Jan  5 19:11:25.096: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302 container client-container: <nil>
STEP: delete the pod 01/05/23 19:11:25.114
Jan  5 19:11:25.126: INFO: Waiting for pod downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302 to disappear
Jan  5 19:11:25.130: INFO: Pod downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:25.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3948" for this suite. 01/05/23 19:11:25.134
------------------------------
• [4.108 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:21.033
    Jan  5 19:11:21.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:11:21.035
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:21.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:21.061
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:235
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:11:21.067
    Jan  5 19:11:21.077: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302" in namespace "projected-3948" to be "Succeeded or Failed"
    Jan  5 19:11:21.088: INFO: Pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302": Phase="Pending", Reason="", readiness=false. Elapsed: 11.01875ms
    Jan  5 19:11:23.093: INFO: Pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015785884s
    Jan  5 19:11:25.093: INFO: Pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015396721s
    STEP: Saw pod success 01/05/23 19:11:25.093
    Jan  5 19:11:25.093: INFO: Pod "downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302" satisfied condition "Succeeded or Failed"
    Jan  5 19:11:25.096: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:11:25.114
    Jan  5 19:11:25.126: INFO: Waiting for pod downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302 to disappear
    Jan  5 19:11:25.130: INFO: Pod downwardapi-volume-9050ddbf-3ab6-425e-aa4a-7cfdc4196302 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:25.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3948" for this suite. 01/05/23 19:11:25.134
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:25.147
Jan  5 19:11:25.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 19:11:25.148
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:25.164
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:25.167
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/05/23 19:11:25.175
Jan  5 19:11:25.185: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4093" to be "running and ready"
Jan  5 19:11:25.188: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.295099ms
Jan  5 19:11:25.189: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:11:27.192: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006710157s
Jan  5 19:11:27.192: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 19:11:27.192: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:151
STEP: create the pod with lifecycle hook 01/05/23 19:11:27.195
Jan  5 19:11:27.203: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4093" to be "running and ready"
Jan  5 19:11:27.208: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27955ms
Jan  5 19:11:27.208: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:11:29.211: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007782723s
Jan  5 19:11:29.211: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jan  5 19:11:29.211: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/05/23 19:11:29.214
Jan  5 19:11:29.220: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 19:11:29.224: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  5 19:11:31.224: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 19:11:31.231: INFO: Pod pod-with-prestop-exec-hook still exists
Jan  5 19:11:33.224: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jan  5 19:11:33.228: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 01/05/23 19:11:33.228
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:33.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4093" for this suite. 01/05/23 19:11:33.24
------------------------------
• [SLOW TEST] [8.105 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:25.147
    Jan  5 19:11:25.147: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 19:11:25.148
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:25.164
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:25.167
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 19:11:25.175
    Jan  5 19:11:25.185: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4093" to be "running and ready"
    Jan  5 19:11:25.188: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 3.295099ms
    Jan  5 19:11:25.189: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:11:27.192: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.006710157s
    Jan  5 19:11:27.192: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 19:11:27.192: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:151
    STEP: create the pod with lifecycle hook 01/05/23 19:11:27.195
    Jan  5 19:11:27.203: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-4093" to be "running and ready"
    Jan  5 19:11:27.208: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.27955ms
    Jan  5 19:11:27.208: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:11:29.211: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007782723s
    Jan  5 19:11:29.211: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jan  5 19:11:29.211: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/05/23 19:11:29.214
    Jan  5 19:11:29.220: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 19:11:29.224: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  5 19:11:31.224: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 19:11:31.231: INFO: Pod pod-with-prestop-exec-hook still exists
    Jan  5 19:11:33.224: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jan  5 19:11:33.228: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 01/05/23 19:11:33.228
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:33.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4093" for this suite. 01/05/23 19:11:33.24
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:33.255
Jan  5 19:11:33.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename job 01/05/23 19:11:33.256
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:33.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:33.272
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636
STEP: Creating a job 01/05/23 19:11:33.278
STEP: Ensure pods equal to parallelism count is attached to the job 01/05/23 19:11:33.283
STEP: patching /status 01/05/23 19:11:37.287
STEP: updating /status 01/05/23 19:11:37.297
STEP: get /status 01/05/23 19:11:37.308
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:37.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-8017" for this suite. 01/05/23 19:11:37.315
------------------------------
• [4.066 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:636

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:33.255
    Jan  5 19:11:33.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename job 01/05/23 19:11:33.256
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:33.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:33.272
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:636
    STEP: Creating a job 01/05/23 19:11:33.278
    STEP: Ensure pods equal to parallelism count is attached to the job 01/05/23 19:11:33.283
    STEP: patching /status 01/05/23 19:11:37.287
    STEP: updating /status 01/05/23 19:11:37.297
    STEP: get /status 01/05/23 19:11:37.308
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:37.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-8017" for this suite. 01/05/23 19:11:37.315
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:37.327
Jan  5 19:11:37.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:11:37.329
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:37.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:37.345
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1700
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1713
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/05/23 19:11:37.349
Jan  5 19:11:37.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9995 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
Jan  5 19:11:37.443: INFO: stderr: ""
Jan  5 19:11:37.443: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 19:11:37.443
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1704
Jan  5 19:11:37.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9995 delete pods e2e-test-httpd-pod'
Jan  5 19:11:40.500: INFO: stderr: ""
Jan  5 19:11:40.500: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:40.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9995" for this suite. 01/05/23 19:11:40.503
------------------------------
• [3.182 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1697
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1713

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:37.327
    Jan  5 19:11:37.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:11:37.329
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:37.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:37.345
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1700
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1713
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/05/23 19:11:37.349
    Jan  5 19:11:37.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9995 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
    Jan  5 19:11:37.443: INFO: stderr: ""
    Jan  5 19:11:37.443: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 19:11:37.443
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1704
    Jan  5 19:11:37.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9995 delete pods e2e-test-httpd-pod'
    Jan  5 19:11:40.500: INFO: stderr: ""
    Jan  5 19:11:40.500: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:40.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9995" for this suite. 01/05/23 19:11:40.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:40.511
Jan  5 19:11:40.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:11:40.513
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:40.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:40.526
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:11:40.542
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:11:40.808
STEP: Deploying the webhook pod 01/05/23 19:11:40.814
STEP: Wait for the deployment to be ready 01/05/23 19:11:40.828
Jan  5 19:11:40.835: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 19:11:42.846
STEP: Verifying the service has paired with the endpoint 01/05/23 19:11:42.858
Jan  5 19:11:43.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/05/23 19:11:43.864
STEP: create a namespace for the webhook 01/05/23 19:11:43.893
STEP: create a configmap should be unconditionally rejected by the webhook 01/05/23 19:11:43.903
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:43.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-639" for this suite. 01/05/23 19:11:43.978
STEP: Destroying namespace "webhook-639-markers" for this suite. 01/05/23 19:11:43.983
------------------------------
• [3.480 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:40.511
    Jan  5 19:11:40.511: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:11:40.513
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:40.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:40.526
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:11:40.542
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:11:40.808
    STEP: Deploying the webhook pod 01/05/23 19:11:40.814
    STEP: Wait for the deployment to be ready 01/05/23 19:11:40.828
    Jan  5 19:11:40.835: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 19:11:42.846
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:11:42.858
    Jan  5 19:11:43.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:239
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 01/05/23 19:11:43.864
    STEP: create a namespace for the webhook 01/05/23 19:11:43.893
    STEP: create a configmap should be unconditionally rejected by the webhook 01/05/23 19:11:43.903
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:43.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-639" for this suite. 01/05/23 19:11:43.978
    STEP: Destroying namespace "webhook-639-markers" for this suite. 01/05/23 19:11:43.983
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:44.01
Jan  5 19:11:44.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:11:44.013
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:44.046
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:44.055
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392
STEP: Counting existing ResourceQuota 01/05/23 19:11:44.062
STEP: Creating a ResourceQuota 01/05/23 19:11:49.064
STEP: Ensuring resource quota status is calculated 01/05/23 19:11:49.071
STEP: Creating a ReplicationController 01/05/23 19:11:51.075
STEP: Ensuring resource quota status captures replication controller creation 01/05/23 19:11:51.087
STEP: Deleting a ReplicationController 01/05/23 19:11:53.091
STEP: Ensuring resource quota status released usage 01/05/23 19:11:53.096
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:11:55.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4063" for this suite. 01/05/23 19:11:55.105
------------------------------
• [SLOW TEST] [11.101 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:44.01
    Jan  5 19:11:44.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:11:44.013
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:44.046
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:44.055
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:392
    STEP: Counting existing ResourceQuota 01/05/23 19:11:44.062
    STEP: Creating a ResourceQuota 01/05/23 19:11:49.064
    STEP: Ensuring resource quota status is calculated 01/05/23 19:11:49.071
    STEP: Creating a ReplicationController 01/05/23 19:11:51.075
    STEP: Ensuring resource quota status captures replication controller creation 01/05/23 19:11:51.087
    STEP: Deleting a ReplicationController 01/05/23 19:11:53.091
    STEP: Ensuring resource quota status released usage 01/05/23 19:11:53.096
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:11:55.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4063" for this suite. 01/05/23 19:11:55.105
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:11:55.116
Jan  5 19:11:55.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pod-network-test 01/05/23 19:11:55.118
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:55.132
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:55.135
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-5832 01/05/23 19:11:55.138
STEP: creating a selector 01/05/23 19:11:55.138
STEP: Creating the service pods in kubernetes 01/05/23 19:11:55.139
Jan  5 19:11:55.139: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 19:11:55.184: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5832" to be "running and ready"
Jan  5 19:11:55.192: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.445019ms
Jan  5 19:11:55.192: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:11:57.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011014118s
Jan  5 19:11:57.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:11:59.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01098091s
Jan  5 19:11:59.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:01.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01040234s
Jan  5 19:12:01.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:03.196: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011692103s
Jan  5 19:12:03.196: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:05.196: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011843971s
Jan  5 19:12:05.196: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:07.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011248633s
Jan  5 19:12:07.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:09.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010715825s
Jan  5 19:12:09.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:11.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010760532s
Jan  5 19:12:11.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:13.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010967395s
Jan  5 19:12:13.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:15.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013482714s
Jan  5 19:12:15.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:12:17.196: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011662805s
Jan  5 19:12:17.196: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 19:12:17.196: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 19:12:17.198: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5832" to be "running and ready"
Jan  5 19:12:17.200: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014051ms
Jan  5 19:12:17.200: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 19:12:17.200: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  5 19:12:17.203: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5832" to be "running and ready"
Jan  5 19:12:17.205: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.984876ms
Jan  5 19:12:17.205: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  5 19:12:17.205: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 19:12:17.207
Jan  5 19:12:17.216: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5832" to be "running"
Jan  5 19:12:17.220: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624521ms
Jan  5 19:12:19.223: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006797615s
Jan  5 19:12:19.224: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 19:12:19.227: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  5 19:12:19.227: INFO: Breadth first check of 10.16.2.224 on host 10.196.0.39...
Jan  5 19:12:19.230: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.225:9080/dial?request=hostname&protocol=http&host=10.16.2.224&port=8083&tries=1'] Namespace:pod-network-test-5832 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:12:19.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:12:19.232: INFO: ExecWithOptions: Clientset creation
Jan  5 19:12:19.232: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5832/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.16.2.224%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 19:12:19.330: INFO: Waiting for responses: map[]
Jan  5 19:12:19.331: INFO: reached 10.16.2.224 after 0/1 tries
Jan  5 19:12:19.331: INFO: Breadth first check of 10.16.0.71 on host 10.196.0.37...
Jan  5 19:12:19.333: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.225:9080/dial?request=hostname&protocol=http&host=10.16.0.71&port=8083&tries=1'] Namespace:pod-network-test-5832 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:12:19.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:12:19.335: INFO: ExecWithOptions: Clientset creation
Jan  5 19:12:19.335: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5832/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.16.0.71%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 19:12:19.408: INFO: Waiting for responses: map[]
Jan  5 19:12:19.409: INFO: reached 10.16.0.71 after 0/1 tries
Jan  5 19:12:19.411: INFO: Breadth first check of 10.16.1.134 on host 10.196.0.38...
Jan  5 19:12:19.414: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.225:9080/dial?request=hostname&protocol=http&host=10.16.1.134&port=8083&tries=1'] Namespace:pod-network-test-5832 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:12:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:12:19.415: INFO: ExecWithOptions: Clientset creation
Jan  5 19:12:19.416: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5832/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.16.1.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 19:12:19.503: INFO: Waiting for responses: map[]
Jan  5 19:12:19.503: INFO: reached 10.16.1.134 after 0/1 tries
Jan  5 19:12:19.504: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:19.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5832" for this suite. 01/05/23 19:12:19.509
------------------------------
• [SLOW TEST] [24.399 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:11:55.116
    Jan  5 19:11:55.116: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 19:11:55.118
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:11:55.132
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:11:55.135
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-5832 01/05/23 19:11:55.138
    STEP: creating a selector 01/05/23 19:11:55.138
    STEP: Creating the service pods in kubernetes 01/05/23 19:11:55.139
    Jan  5 19:11:55.139: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 19:11:55.184: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5832" to be "running and ready"
    Jan  5 19:11:55.192: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.445019ms
    Jan  5 19:11:55.192: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:11:57.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.011014118s
    Jan  5 19:11:57.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:11:59.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.01098091s
    Jan  5 19:11:59.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:01.194: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.01040234s
    Jan  5 19:12:01.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:03.196: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.011692103s
    Jan  5 19:12:03.196: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:05.196: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.011843971s
    Jan  5 19:12:05.196: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:07.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.011248633s
    Jan  5 19:12:07.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:09.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010715825s
    Jan  5 19:12:09.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:11.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.010760532s
    Jan  5 19:12:11.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:13.195: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.010967395s
    Jan  5 19:12:13.195: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:15.198: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013482714s
    Jan  5 19:12:15.198: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:12:17.196: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.011662805s
    Jan  5 19:12:17.196: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 19:12:17.196: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 19:12:17.198: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5832" to be "running and ready"
    Jan  5 19:12:17.200: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.014051ms
    Jan  5 19:12:17.200: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 19:12:17.200: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  5 19:12:17.203: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5832" to be "running and ready"
    Jan  5 19:12:17.205: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 1.984876ms
    Jan  5 19:12:17.205: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  5 19:12:17.205: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 19:12:17.207
    Jan  5 19:12:17.216: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5832" to be "running"
    Jan  5 19:12:17.220: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.624521ms
    Jan  5 19:12:19.223: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006797615s
    Jan  5 19:12:19.224: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 19:12:19.227: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  5 19:12:19.227: INFO: Breadth first check of 10.16.2.224 on host 10.196.0.39...
    Jan  5 19:12:19.230: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.225:9080/dial?request=hostname&protocol=http&host=10.16.2.224&port=8083&tries=1'] Namespace:pod-network-test-5832 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:12:19.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:12:19.232: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:12:19.232: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5832/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.16.2.224%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 19:12:19.330: INFO: Waiting for responses: map[]
    Jan  5 19:12:19.331: INFO: reached 10.16.2.224 after 0/1 tries
    Jan  5 19:12:19.331: INFO: Breadth first check of 10.16.0.71 on host 10.196.0.37...
    Jan  5 19:12:19.333: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.225:9080/dial?request=hostname&protocol=http&host=10.16.0.71&port=8083&tries=1'] Namespace:pod-network-test-5832 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:12:19.334: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:12:19.335: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:12:19.335: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5832/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.16.0.71%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 19:12:19.408: INFO: Waiting for responses: map[]
    Jan  5 19:12:19.409: INFO: reached 10.16.0.71 after 0/1 tries
    Jan  5 19:12:19.411: INFO: Breadth first check of 10.16.1.134 on host 10.196.0.38...
    Jan  5 19:12:19.414: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.225:9080/dial?request=hostname&protocol=http&host=10.16.1.134&port=8083&tries=1'] Namespace:pod-network-test-5832 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:12:19.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:12:19.415: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:12:19.416: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5832/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.225%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.16.1.134%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 19:12:19.503: INFO: Waiting for responses: map[]
    Jan  5 19:12:19.503: INFO: reached 10.16.1.134 after 0/1 tries
    Jan  5 19:12:19.504: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:19.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5832" for this suite. 01/05/23 19:12:19.509
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:19.52
Jan  5 19:12:19.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename dns 01/05/23 19:12:19.522
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:19.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:19.539
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 01/05/23 19:12:19.555
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2466.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2466.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 165.9.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.9.165_udp@PTR;check="$$(dig +tcp +noall +answer +search 165.9.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.9.165_tcp@PTR;sleep 1; done
 01/05/23 19:12:19.582
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2466.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2466.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 165.9.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.9.165_udp@PTR;check="$$(dig +tcp +noall +answer +search 165.9.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.9.165_tcp@PTR;sleep 1; done
 01/05/23 19:12:19.582
STEP: creating a pod to probe DNS 01/05/23 19:12:19.582
STEP: submitting the pod to kubernetes 01/05/23 19:12:19.583
Jan  5 19:12:19.606: INFO: Waiting up to 15m0s for pod "dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b" in namespace "dns-2466" to be "running"
Jan  5 19:12:19.618: INFO: Pod "dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.724102ms
Jan  5 19:12:21.623: INFO: Pod "dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016651366s
Jan  5 19:12:21.623: INFO: Pod "dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:12:21.623
STEP: looking for the results for each expected name from probers 01/05/23 19:12:21.626
Jan  5 19:12:21.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
Jan  5 19:12:21.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
Jan  5 19:12:21.645: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
Jan  5 19:12:21.648: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
Jan  5 19:12:21.667: INFO: Unable to read jessie_udp@dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
Jan  5 19:12:21.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
Jan  5 19:12:21.675: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
Jan  5 19:12:21.679: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
Jan  5 19:12:21.696: INFO: Lookups using dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b failed for: [wheezy_udp@dns-test-service.dns-2466.svc.cluster.local wheezy_tcp@dns-test-service.dns-2466.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local jessie_udp@dns-test-service.dns-2466.svc.cluster.local jessie_tcp@dns-test-service.dns-2466.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local]

Jan  5 19:12:26.761: INFO: DNS probes using dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b succeeded

STEP: deleting the pod 01/05/23 19:12:26.761
STEP: deleting the test service 01/05/23 19:12:26.784
STEP: deleting the test headless service 01/05/23 19:12:26.809
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:26.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-2466" for this suite. 01/05/23 19:12:26.824
------------------------------
• [SLOW TEST] [7.309 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:19.52
    Jan  5 19:12:19.521: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename dns 01/05/23 19:12:19.522
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:19.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:19.539
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 01/05/23 19:12:19.555
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2466.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2466.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 165.9.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.9.165_udp@PTR;check="$$(dig +tcp +noall +answer +search 165.9.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.9.165_tcp@PTR;sleep 1; done
     01/05/23 19:12:19.582
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2466.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2466.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2466.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2466.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2466.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 165.9.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.9.165_udp@PTR;check="$$(dig +tcp +noall +answer +search 165.9.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.9.165_tcp@PTR;sleep 1; done
     01/05/23 19:12:19.582
    STEP: creating a pod to probe DNS 01/05/23 19:12:19.582
    STEP: submitting the pod to kubernetes 01/05/23 19:12:19.583
    Jan  5 19:12:19.606: INFO: Waiting up to 15m0s for pod "dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b" in namespace "dns-2466" to be "running"
    Jan  5 19:12:19.618: INFO: Pod "dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 11.724102ms
    Jan  5 19:12:21.623: INFO: Pod "dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b": Phase="Running", Reason="", readiness=true. Elapsed: 2.016651366s
    Jan  5 19:12:21.623: INFO: Pod "dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:12:21.623
    STEP: looking for the results for each expected name from probers 01/05/23 19:12:21.626
    Jan  5 19:12:21.634: INFO: Unable to read wheezy_udp@dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
    Jan  5 19:12:21.638: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
    Jan  5 19:12:21.645: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
    Jan  5 19:12:21.648: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
    Jan  5 19:12:21.667: INFO: Unable to read jessie_udp@dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
    Jan  5 19:12:21.672: INFO: Unable to read jessie_tcp@dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
    Jan  5 19:12:21.675: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
    Jan  5 19:12:21.679: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local from pod dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b: the server could not find the requested resource (get pods dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b)
    Jan  5 19:12:21.696: INFO: Lookups using dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b failed for: [wheezy_udp@dns-test-service.dns-2466.svc.cluster.local wheezy_tcp@dns-test-service.dns-2466.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local jessie_udp@dns-test-service.dns-2466.svc.cluster.local jessie_tcp@dns-test-service.dns-2466.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2466.svc.cluster.local]

    Jan  5 19:12:26.761: INFO: DNS probes using dns-2466/dns-test-1d60d39a-6f71-45b9-9648-affb90646d6b succeeded

    STEP: deleting the pod 01/05/23 19:12:26.761
    STEP: deleting the test service 01/05/23 19:12:26.784
    STEP: deleting the test headless service 01/05/23 19:12:26.809
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:26.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-2466" for this suite. 01/05/23 19:12:26.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:26.839
Jan  5 19:12:26.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:12:26.841
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:26.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:26.86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197
STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 19:12:26.864
Jan  5 19:12:26.874: INFO: Waiting up to 5m0s for pod "pod-735313bb-215f-4c38-9687-1eba465519da" in namespace "emptydir-6568" to be "Succeeded or Failed"
Jan  5 19:12:26.878: INFO: Pod "pod-735313bb-215f-4c38-9687-1eba465519da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719215ms
Jan  5 19:12:28.893: INFO: Pod "pod-735313bb-215f-4c38-9687-1eba465519da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018367723s
Jan  5 19:12:30.881: INFO: Pod "pod-735313bb-215f-4c38-9687-1eba465519da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00686601s
STEP: Saw pod success 01/05/23 19:12:30.881
Jan  5 19:12:30.882: INFO: Pod "pod-735313bb-215f-4c38-9687-1eba465519da" satisfied condition "Succeeded or Failed"
Jan  5 19:12:30.884: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-735313bb-215f-4c38-9687-1eba465519da container test-container: <nil>
STEP: delete the pod 01/05/23 19:12:30.891
Jan  5 19:12:30.901: INFO: Waiting for pod pod-735313bb-215f-4c38-9687-1eba465519da to disappear
Jan  5 19:12:30.904: INFO: Pod pod-735313bb-215f-4c38-9687-1eba465519da no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:30.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6568" for this suite. 01/05/23 19:12:30.91
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:26.839
    Jan  5 19:12:26.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:12:26.841
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:26.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:26.86
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:197
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 19:12:26.864
    Jan  5 19:12:26.874: INFO: Waiting up to 5m0s for pod "pod-735313bb-215f-4c38-9687-1eba465519da" in namespace "emptydir-6568" to be "Succeeded or Failed"
    Jan  5 19:12:26.878: INFO: Pod "pod-735313bb-215f-4c38-9687-1eba465519da": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719215ms
    Jan  5 19:12:28.893: INFO: Pod "pod-735313bb-215f-4c38-9687-1eba465519da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018367723s
    Jan  5 19:12:30.881: INFO: Pod "pod-735313bb-215f-4c38-9687-1eba465519da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00686601s
    STEP: Saw pod success 01/05/23 19:12:30.881
    Jan  5 19:12:30.882: INFO: Pod "pod-735313bb-215f-4c38-9687-1eba465519da" satisfied condition "Succeeded or Failed"
    Jan  5 19:12:30.884: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-735313bb-215f-4c38-9687-1eba465519da container test-container: <nil>
    STEP: delete the pod 01/05/23 19:12:30.891
    Jan  5 19:12:30.901: INFO: Waiting for pod pod-735313bb-215f-4c38-9687-1eba465519da to disappear
    Jan  5 19:12:30.904: INFO: Pod pod-735313bb-215f-4c38-9687-1eba465519da no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:30.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6568" for this suite. 01/05/23 19:12:30.91
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:30.918
Jan  5 19:12:30.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:12:30.92
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:30.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:30.942
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083
STEP: Create a pod 01/05/23 19:12:30.946
Jan  5 19:12:30.956: INFO: Waiting up to 5m0s for pod "pod-fpdfh" in namespace "pods-1816" to be "running"
Jan  5 19:12:30.963: INFO: Pod "pod-fpdfh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.503278ms
Jan  5 19:12:32.968: INFO: Pod "pod-fpdfh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01187277s
Jan  5 19:12:32.968: INFO: Pod "pod-fpdfh" satisfied condition "running"
STEP: patching /status 01/05/23 19:12:32.968
Jan  5 19:12:32.979: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:32.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1816" for this suite. 01/05/23 19:12:32.984
------------------------------
• [2.071 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1083

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:30.918
    Jan  5 19:12:30.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:12:30.92
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:30.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:30.942
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1083
    STEP: Create a pod 01/05/23 19:12:30.946
    Jan  5 19:12:30.956: INFO: Waiting up to 5m0s for pod "pod-fpdfh" in namespace "pods-1816" to be "running"
    Jan  5 19:12:30.963: INFO: Pod "pod-fpdfh": Phase="Pending", Reason="", readiness=false. Elapsed: 7.503278ms
    Jan  5 19:12:32.968: INFO: Pod "pod-fpdfh": Phase="Running", Reason="", readiness=true. Elapsed: 2.01187277s
    Jan  5 19:12:32.968: INFO: Pod "pod-fpdfh" satisfied condition "running"
    STEP: patching /status 01/05/23 19:12:32.968
    Jan  5 19:12:32.979: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:32.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1816" for this suite. 01/05/23 19:12:32.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:32.991
Jan  5 19:12:32.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename disruption 01/05/23 19:12:32.992
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:33.004
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:33.007
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164
STEP: Waiting for the pdb to be processed 01/05/23 19:12:33.015
STEP: Updating PodDisruptionBudget status 01/05/23 19:12:35.021
STEP: Waiting for all pods to be running 01/05/23 19:12:35.031
Jan  5 19:12:35.036: INFO: running pods: 0 < 1
STEP: locating a running pod 01/05/23 19:12:37.04
STEP: Waiting for the pdb to be processed 01/05/23 19:12:37.051
STEP: Patching PodDisruptionBudget status 01/05/23 19:12:37.059
STEP: Waiting for the pdb to be processed 01/05/23 19:12:37.07
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:37.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-6702" for this suite. 01/05/23 19:12:37.079
------------------------------
• [4.095 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:32.991
    Jan  5 19:12:32.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename disruption 01/05/23 19:12:32.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:33.004
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:33.007
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:164
    STEP: Waiting for the pdb to be processed 01/05/23 19:12:33.015
    STEP: Updating PodDisruptionBudget status 01/05/23 19:12:35.021
    STEP: Waiting for all pods to be running 01/05/23 19:12:35.031
    Jan  5 19:12:35.036: INFO: running pods: 0 < 1
    STEP: locating a running pod 01/05/23 19:12:37.04
    STEP: Waiting for the pdb to be processed 01/05/23 19:12:37.051
    STEP: Patching PodDisruptionBudget status 01/05/23 19:12:37.059
    STEP: Waiting for the pdb to be processed 01/05/23 19:12:37.07
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:37.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-6702" for this suite. 01/05/23 19:12:37.079
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:37.087
Jan  5 19:12:37.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:12:37.09
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:37.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:37.109
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130
STEP: Creating the pod 01/05/23 19:12:37.111
Jan  5 19:12:37.121: INFO: Waiting up to 5m0s for pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f" in namespace "downward-api-3132" to be "running and ready"
Jan  5 19:12:37.127: INFO: Pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963896ms
Jan  5 19:12:37.127: INFO: The phase of Pod labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:12:39.130: INFO: Pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008895059s
Jan  5 19:12:39.130: INFO: The phase of Pod labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f is Running (Ready = true)
Jan  5 19:12:39.130: INFO: Pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f" satisfied condition "running and ready"
Jan  5 19:12:39.654: INFO: Successfully updated pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:41.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3132" for this suite. 01/05/23 19:12:41.672
------------------------------
• [4.590 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:37.087
    Jan  5 19:12:37.087: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:12:37.09
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:37.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:37.109
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:130
    STEP: Creating the pod 01/05/23 19:12:37.111
    Jan  5 19:12:37.121: INFO: Waiting up to 5m0s for pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f" in namespace "downward-api-3132" to be "running and ready"
    Jan  5 19:12:37.127: INFO: Pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963896ms
    Jan  5 19:12:37.127: INFO: The phase of Pod labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:12:39.130: INFO: Pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008895059s
    Jan  5 19:12:39.130: INFO: The phase of Pod labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f is Running (Ready = true)
    Jan  5 19:12:39.130: INFO: Pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f" satisfied condition "running and ready"
    Jan  5 19:12:39.654: INFO: Successfully updated pod "labelsupdatee9f9ac55-1ead-4464-a743-85010b6d462f"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:41.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3132" for this suite. 01/05/23 19:12:41.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:41.68
Jan  5 19:12:41.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:12:41.682
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:41.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:41.699
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275
STEP: Creating a pod to test service account token:  01/05/23 19:12:41.702
Jan  5 19:12:41.710: INFO: Waiting up to 5m0s for pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e" in namespace "svcaccounts-555" to be "Succeeded or Failed"
Jan  5 19:12:41.713: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12884ms
Jan  5 19:12:43.716: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00633118s
Jan  5 19:12:45.717: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007154741s
Jan  5 19:12:47.718: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007546904s
STEP: Saw pod success 01/05/23 19:12:47.718
Jan  5 19:12:47.719: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e" satisfied condition "Succeeded or Failed"
Jan  5 19:12:47.735: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:12:47.746
Jan  5 19:12:47.760: INFO: Waiting for pod test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e to disappear
Jan  5 19:12:47.763: INFO: Pod test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:47.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-555" for this suite. 01/05/23 19:12:47.778
------------------------------
• [SLOW TEST] [6.103 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:41.68
    Jan  5 19:12:41.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:12:41.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:41.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:41.699
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:275
    STEP: Creating a pod to test service account token:  01/05/23 19:12:41.702
    Jan  5 19:12:41.710: INFO: Waiting up to 5m0s for pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e" in namespace "svcaccounts-555" to be "Succeeded or Failed"
    Jan  5 19:12:41.713: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.12884ms
    Jan  5 19:12:43.716: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00633118s
    Jan  5 19:12:45.717: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007154741s
    Jan  5 19:12:47.718: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.007546904s
    STEP: Saw pod success 01/05/23 19:12:47.718
    Jan  5 19:12:47.719: INFO: Pod "test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e" satisfied condition "Succeeded or Failed"
    Jan  5 19:12:47.735: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:12:47.746
    Jan  5 19:12:47.760: INFO: Waiting for pod test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e to disappear
    Jan  5 19:12:47.763: INFO: Pod test-pod-ec6195cc-09b7-41b7-aafe-06dcf17cff0e no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:47.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-555" for this suite. 01/05/23 19:12:47.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:47.795
Jan  5 19:12:47.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:12:47.797
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:47.813
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:47.817
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205
STEP: Creating secret with name s-test-opt-del-e623ee64-2a32-4d61-8e3f-f242808fb282 01/05/23 19:12:47.825
STEP: Creating secret with name s-test-opt-upd-7884d9d5-be58-4836-aa31-1b762e80c7ef 01/05/23 19:12:47.83
STEP: Creating the pod 01/05/23 19:12:47.834
Jan  5 19:12:47.847: INFO: Waiting up to 5m0s for pod "pod-secrets-d959a597-8916-4d2d-849a-98db39186b67" in namespace "secrets-6501" to be "running and ready"
Jan  5 19:12:47.852: INFO: Pod "pod-secrets-d959a597-8916-4d2d-849a-98db39186b67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168699ms
Jan  5 19:12:47.853: INFO: The phase of Pod pod-secrets-d959a597-8916-4d2d-849a-98db39186b67 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:12:49.871: INFO: Pod "pod-secrets-d959a597-8916-4d2d-849a-98db39186b67": Phase="Running", Reason="", readiness=true. Elapsed: 2.023294559s
Jan  5 19:12:49.875: INFO: The phase of Pod pod-secrets-d959a597-8916-4d2d-849a-98db39186b67 is Running (Ready = true)
Jan  5 19:12:49.875: INFO: Pod "pod-secrets-d959a597-8916-4d2d-849a-98db39186b67" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-e623ee64-2a32-4d61-8e3f-f242808fb282 01/05/23 19:12:49.896
STEP: Updating secret s-test-opt-upd-7884d9d5-be58-4836-aa31-1b762e80c7ef 01/05/23 19:12:49.902
STEP: Creating secret with name s-test-opt-create-db9c0df7-b959-4e5d-82ff-4a466a3414c0 01/05/23 19:12:49.906
STEP: waiting to observe update in volume 01/05/23 19:12:49.91
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:53.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-6501" for this suite. 01/05/23 19:12:53.957
------------------------------
• [SLOW TEST] [6.168 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:47.795
    Jan  5 19:12:47.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:12:47.797
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:47.813
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:47.817
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:205
    STEP: Creating secret with name s-test-opt-del-e623ee64-2a32-4d61-8e3f-f242808fb282 01/05/23 19:12:47.825
    STEP: Creating secret with name s-test-opt-upd-7884d9d5-be58-4836-aa31-1b762e80c7ef 01/05/23 19:12:47.83
    STEP: Creating the pod 01/05/23 19:12:47.834
    Jan  5 19:12:47.847: INFO: Waiting up to 5m0s for pod "pod-secrets-d959a597-8916-4d2d-849a-98db39186b67" in namespace "secrets-6501" to be "running and ready"
    Jan  5 19:12:47.852: INFO: Pod "pod-secrets-d959a597-8916-4d2d-849a-98db39186b67": Phase="Pending", Reason="", readiness=false. Elapsed: 4.168699ms
    Jan  5 19:12:47.853: INFO: The phase of Pod pod-secrets-d959a597-8916-4d2d-849a-98db39186b67 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:12:49.871: INFO: Pod "pod-secrets-d959a597-8916-4d2d-849a-98db39186b67": Phase="Running", Reason="", readiness=true. Elapsed: 2.023294559s
    Jan  5 19:12:49.875: INFO: The phase of Pod pod-secrets-d959a597-8916-4d2d-849a-98db39186b67 is Running (Ready = true)
    Jan  5 19:12:49.875: INFO: Pod "pod-secrets-d959a597-8916-4d2d-849a-98db39186b67" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-e623ee64-2a32-4d61-8e3f-f242808fb282 01/05/23 19:12:49.896
    STEP: Updating secret s-test-opt-upd-7884d9d5-be58-4836-aa31-1b762e80c7ef 01/05/23 19:12:49.902
    STEP: Creating secret with name s-test-opt-create-db9c0df7-b959-4e5d-82ff-4a466a3414c0 01/05/23 19:12:49.906
    STEP: waiting to observe update in volume 01/05/23 19:12:49.91
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:53.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-6501" for this suite. 01/05/23 19:12:53.957
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:53.963
Jan  5 19:12:53.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename csiinlinevolumes 01/05/23 19:12:53.965
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:53.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:53.981
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46
STEP: creating 01/05/23 19:12:53.985
STEP: getting 01/05/23 19:12:54.001
STEP: listing 01/05/23 19:12:54.005
STEP: deleting 01/05/23 19:12:54.007
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:12:54.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-2846" for this suite. 01/05/23 19:12:54.022
------------------------------
• [0.067 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
  test/e2e/storage/csi_inline.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:53.963
    Jan  5 19:12:53.963: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename csiinlinevolumes 01/05/23 19:12:53.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:53.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:53.981
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
      test/e2e/storage/csi_inline.go:46
    STEP: creating 01/05/23 19:12:53.985
    STEP: getting 01/05/23 19:12:54.001
    STEP: listing 01/05/23 19:12:54.005
    STEP: deleting 01/05/23 19:12:54.007
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:12:54.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-2846" for this suite. 01/05/23 19:12:54.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:12:54.035
Jan  5 19:12:54.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename statefulset 01/05/23 19:12:54.036
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:54.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:54.051
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-3393 01/05/23 19:12:54.053
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:697
STEP: Creating stateful set ss in namespace statefulset-3393 01/05/23 19:12:54.061
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3393 01/05/23 19:12:54.069
Jan  5 19:12:54.073: INFO: Found 0 stateful pods, waiting for 1
Jan  5 19:13:04.077: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/05/23 19:13:04.077
Jan  5 19:13:04.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 19:13:04.225: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 19:13:04.225: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 19:13:04.225: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 19:13:04.228: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  5 19:13:14.232: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 19:13:14.232: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 19:13:14.248: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jan  5 19:13:14.248: INFO: ss-0  gke-gke-1-26-default-pool-05283374-dbpc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:12:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:12:54 +0000 UTC  }]
Jan  5 19:13:14.248: INFO: 
Jan  5 19:13:14.248: INFO: StatefulSet ss has not reached scale 3, at 1
Jan  5 19:13:15.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996152246s
Jan  5 19:13:16.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989883575s
Jan  5 19:13:17.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986253771s
Jan  5 19:13:18.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982358498s
Jan  5 19:13:19.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978902588s
Jan  5 19:13:20.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975277459s
Jan  5 19:13:21.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971162063s
Jan  5 19:13:22.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967428975s
Jan  5 19:13:23.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.688405ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3393 01/05/23 19:13:24.285
Jan  5 19:13:24.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 19:13:24.466: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 19:13:24.466: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 19:13:24.466: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 19:13:24.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 19:13:24.607: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  5 19:13:24.607: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 19:13:24.607: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 19:13:24.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 19:13:24.761: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jan  5 19:13:24.761: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 19:13:24.761: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 19:13:24.764: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Jan  5 19:13:34.769: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 19:13:34.769: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 19:13:34.769: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 01/05/23 19:13:34.769
Jan  5 19:13:34.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 19:13:34.939: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 19:13:34.939: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 19:13:34.939: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 19:13:34.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 19:13:35.076: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 19:13:35.076: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 19:13:35.076: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 19:13:35.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 19:13:35.223: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 19:13:35.223: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 19:13:35.223: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 19:13:35.223: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 19:13:35.226: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan  5 19:13:45.240: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 19:13:45.240: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 19:13:45.240: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 19:13:45.260: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jan  5 19:13:45.260: INFO: ss-0  gke-gke-1-26-default-pool-05283374-dbpc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:12:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:12:54 +0000 UTC  }]
Jan  5 19:13:45.260: INFO: ss-1  gke-gke-1-26-default-pool-05283374-16pz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  }]
Jan  5 19:13:45.261: INFO: ss-2  gke-gke-1-26-default-pool-05283374-qmj7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  }]
Jan  5 19:13:45.261: INFO: 
Jan  5 19:13:45.261: INFO: StatefulSet ss has not reached scale 0, at 3
Jan  5 19:13:46.264: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
Jan  5 19:13:46.264: INFO: ss-2  gke-gke-1-26-default-pool-05283374-qmj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  }]
Jan  5 19:13:46.264: INFO: 
Jan  5 19:13:46.264: INFO: StatefulSet ss has not reached scale 0, at 1
Jan  5 19:13:47.267: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989066063s
Jan  5 19:13:48.271: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.985988453s
Jan  5 19:13:49.274: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982698417s
Jan  5 19:13:50.277: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979655472s
Jan  5 19:13:51.281: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.975741242s
Jan  5 19:13:52.285: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.972213021s
Jan  5 19:13:53.288: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.969059876s
Jan  5 19:13:54.299: INFO: Verifying statefulset ss doesn't scale past 0 for another 964.62741ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3393 01/05/23 19:13:55.3
Jan  5 19:13:55.303: INFO: Scaling statefulset ss to 0
Jan  5 19:13:55.312: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  5 19:13:55.315: INFO: Deleting all statefulset in ns statefulset-3393
Jan  5 19:13:55.317: INFO: Scaling statefulset ss to 0
Jan  5 19:13:55.325: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 19:13:55.327: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:13:55.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-3393" for this suite. 01/05/23 19:13:55.342
------------------------------
• [SLOW TEST] [61.314 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:697

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:12:54.035
    Jan  5 19:12:54.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename statefulset 01/05/23 19:12:54.036
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:12:54.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:12:54.051
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-3393 01/05/23 19:12:54.053
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:697
    STEP: Creating stateful set ss in namespace statefulset-3393 01/05/23 19:12:54.061
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3393 01/05/23 19:12:54.069
    Jan  5 19:12:54.073: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 19:13:04.077: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 01/05/23 19:13:04.077
    Jan  5 19:13:04.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 19:13:04.225: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 19:13:04.225: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 19:13:04.225: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 19:13:04.228: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  5 19:13:14.232: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 19:13:14.232: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 19:13:14.248: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
    Jan  5 19:13:14.248: INFO: ss-0  gke-gke-1-26-default-pool-05283374-dbpc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:12:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:04 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:12:54 +0000 UTC  }]
    Jan  5 19:13:14.248: INFO: 
    Jan  5 19:13:14.248: INFO: StatefulSet ss has not reached scale 3, at 1
    Jan  5 19:13:15.253: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996152246s
    Jan  5 19:13:16.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989883575s
    Jan  5 19:13:17.262: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986253771s
    Jan  5 19:13:18.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982358498s
    Jan  5 19:13:19.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978902588s
    Jan  5 19:13:20.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975277459s
    Jan  5 19:13:21.276: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.971162063s
    Jan  5 19:13:22.280: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.967428975s
    Jan  5 19:13:23.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 963.688405ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3393 01/05/23 19:13:24.285
    Jan  5 19:13:24.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 19:13:24.466: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 19:13:24.466: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 19:13:24.466: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 19:13:24.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 19:13:24.607: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  5 19:13:24.607: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 19:13:24.607: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 19:13:24.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 19:13:24.761: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jan  5 19:13:24.761: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 19:13:24.761: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 19:13:24.764: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Jan  5 19:13:34.769: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 19:13:34.769: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 19:13:34.769: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 01/05/23 19:13:34.769
    Jan  5 19:13:34.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 19:13:34.939: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 19:13:34.939: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 19:13:34.939: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 19:13:34.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 19:13:35.076: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 19:13:35.076: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 19:13:35.076: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 19:13:35.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-3393 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 19:13:35.223: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 19:13:35.223: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 19:13:35.223: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 19:13:35.223: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 19:13:35.226: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan  5 19:13:45.240: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 19:13:45.240: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 19:13:45.240: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 19:13:45.260: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
    Jan  5 19:13:45.260: INFO: ss-0  gke-gke-1-26-default-pool-05283374-dbpc  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:12:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:12:54 +0000 UTC  }]
    Jan  5 19:13:45.260: INFO: ss-1  gke-gke-1-26-default-pool-05283374-16pz  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  }]
    Jan  5 19:13:45.261: INFO: ss-2  gke-gke-1-26-default-pool-05283374-qmj7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  }]
    Jan  5 19:13:45.261: INFO: 
    Jan  5 19:13:45.261: INFO: StatefulSet ss has not reached scale 0, at 3
    Jan  5 19:13:46.264: INFO: POD   NODE                                     PHASE    GRACE  CONDITIONS
    Jan  5 19:13:46.264: INFO: ss-2  gke-gke-1-26-default-pool-05283374-qmj7  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:35 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:13:14 +0000 UTC  }]
    Jan  5 19:13:46.264: INFO: 
    Jan  5 19:13:46.264: INFO: StatefulSet ss has not reached scale 0, at 1
    Jan  5 19:13:47.267: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.989066063s
    Jan  5 19:13:48.271: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.985988453s
    Jan  5 19:13:49.274: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.982698417s
    Jan  5 19:13:50.277: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979655472s
    Jan  5 19:13:51.281: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.975741242s
    Jan  5 19:13:52.285: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.972213021s
    Jan  5 19:13:53.288: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.969059876s
    Jan  5 19:13:54.299: INFO: Verifying statefulset ss doesn't scale past 0 for another 964.62741ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3393 01/05/23 19:13:55.3
    Jan  5 19:13:55.303: INFO: Scaling statefulset ss to 0
    Jan  5 19:13:55.312: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  5 19:13:55.315: INFO: Deleting all statefulset in ns statefulset-3393
    Jan  5 19:13:55.317: INFO: Scaling statefulset ss to 0
    Jan  5 19:13:55.325: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 19:13:55.327: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:13:55.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-3393" for this suite. 01/05/23 19:13:55.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:13:55.35
Jan  5 19:13:55.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename ingress 01/05/23 19:13:55.352
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:13:55.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:13:55.422
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:31
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 01/05/23 19:13:55.425
STEP: getting /apis/networking.k8s.io 01/05/23 19:13:55.427
STEP: getting /apis/networking.k8s.iov1 01/05/23 19:13:55.428
STEP: creating 01/05/23 19:13:55.429
STEP: getting 01/05/23 19:13:55.446
STEP: listing 01/05/23 19:13:55.452
STEP: watching 01/05/23 19:13:55.457
Jan  5 19:13:55.457: INFO: starting watch
STEP: cluster-wide listing 01/05/23 19:13:55.46
STEP: cluster-wide watching 01/05/23 19:13:55.462
Jan  5 19:13:55.462: INFO: starting watch
STEP: patching 01/05/23 19:13:55.464
STEP: updating 01/05/23 19:13:55.47
Jan  5 19:13:55.478: INFO: waiting for watch events with expected annotations
Jan  5 19:13:55.479: INFO: saw patched and updated annotations
STEP: patching /status 01/05/23 19:13:55.479
STEP: updating /status 01/05/23 19:13:55.49
STEP: get /status 01/05/23 19:13:55.501
STEP: deleting 01/05/23 19:13:55.505
STEP: deleting a collection 01/05/23 19:13:55.515
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/node/init/init.go:32
Jan  5 19:13:55.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Ingress API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Ingress API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Ingress API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingress-7878" for this suite. 01/05/23 19:13:55.537
------------------------------
• [0.193 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:13:55.35
    Jan  5 19:13:55.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename ingress 01/05/23 19:13:55.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:13:55.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:13:55.422
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:31
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 01/05/23 19:13:55.425
    STEP: getting /apis/networking.k8s.io 01/05/23 19:13:55.427
    STEP: getting /apis/networking.k8s.iov1 01/05/23 19:13:55.428
    STEP: creating 01/05/23 19:13:55.429
    STEP: getting 01/05/23 19:13:55.446
    STEP: listing 01/05/23 19:13:55.452
    STEP: watching 01/05/23 19:13:55.457
    Jan  5 19:13:55.457: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 19:13:55.46
    STEP: cluster-wide watching 01/05/23 19:13:55.462
    Jan  5 19:13:55.462: INFO: starting watch
    STEP: patching 01/05/23 19:13:55.464
    STEP: updating 01/05/23 19:13:55.47
    Jan  5 19:13:55.478: INFO: waiting for watch events with expected annotations
    Jan  5 19:13:55.479: INFO: saw patched and updated annotations
    STEP: patching /status 01/05/23 19:13:55.479
    STEP: updating /status 01/05/23 19:13:55.49
    STEP: get /status 01/05/23 19:13:55.501
    STEP: deleting 01/05/23 19:13:55.505
    STEP: deleting a collection 01/05/23 19:13:55.515
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:13:55.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Ingress API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Ingress API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Ingress API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingress-7878" for this suite. 01/05/23 19:13:55.537
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:13:55.545
Jan  5 19:13:55.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-webhook 01/05/23 19:13:55.547
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:13:55.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:13:55.562
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/05/23 19:13:55.565
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 19:13:56.214
STEP: Deploying the custom resource conversion webhook pod 01/05/23 19:13:56.218
STEP: Wait for the deployment to be ready 01/05/23 19:13:56.229
Jan  5 19:13:56.237: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 19:13:58.245
STEP: Verifying the service has paired with the endpoint 01/05/23 19:13:58.253
Jan  5 19:13:59.253: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jan  5 19:13:59.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Creating a v1 custom resource 01/05/23 19:14:01.85
STEP: v2 custom resource should be converted 01/05/23 19:14:01.855
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:14:02.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-8996" for this suite. 01/05/23 19:14:02.406
------------------------------
• [SLOW TEST] [6.869 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:13:55.545
    Jan  5 19:13:55.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-webhook 01/05/23 19:13:55.547
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:13:55.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:13:55.562
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/05/23 19:13:55.565
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 19:13:56.214
    STEP: Deploying the custom resource conversion webhook pod 01/05/23 19:13:56.218
    STEP: Wait for the deployment to be ready 01/05/23 19:13:56.229
    Jan  5 19:13:56.237: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 19:13:58.245
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:13:58.253
    Jan  5 19:13:59.253: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jan  5 19:13:59.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Creating a v1 custom resource 01/05/23 19:14:01.85
    STEP: v2 custom resource should be converted 01/05/23 19:14:01.855
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:14:02.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-8996" for this suite. 01/05/23 19:14:02.406
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:14:02.427
Jan  5 19:14:02.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:14:02.429
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:02.442
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:02.445
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398
STEP: creating the pod 01/05/23 19:14:02.45
STEP: submitting the pod to kubernetes 01/05/23 19:14:02.451
Jan  5 19:14:02.460: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0" in namespace "pods-2383" to be "running and ready"
Jan  5 19:14:02.463: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.894305ms
Jan  5 19:14:02.463: INFO: The phase of Pod pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:14:04.467: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007191862s
Jan  5 19:14:04.467: INFO: The phase of Pod pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0 is Running (Ready = true)
Jan  5 19:14:04.467: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/05/23 19:14:04.47
STEP: updating the pod 01/05/23 19:14:04.473
Jan  5 19:14:04.992: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0"
Jan  5 19:14:04.993: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0" in namespace "pods-2383" to be "terminated with reason DeadlineExceeded"
Jan  5 19:14:04.996: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Running", Reason="", readiness=true. Elapsed: 3.737714ms
Jan  5 19:14:07.002: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.008832278s
Jan  5 19:14:09.000: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007708997s
Jan  5 19:14:09.000: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:14:09.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2383" for this suite. 01/05/23 19:14:09.004
------------------------------
• [SLOW TEST] [6.582 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:398

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:14:02.427
    Jan  5 19:14:02.428: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:14:02.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:02.442
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:02.445
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:398
    STEP: creating the pod 01/05/23 19:14:02.45
    STEP: submitting the pod to kubernetes 01/05/23 19:14:02.451
    Jan  5 19:14:02.460: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0" in namespace "pods-2383" to be "running and ready"
    Jan  5 19:14:02.463: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.894305ms
    Jan  5 19:14:02.463: INFO: The phase of Pod pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:14:04.467: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007191862s
    Jan  5 19:14:04.467: INFO: The phase of Pod pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0 is Running (Ready = true)
    Jan  5 19:14:04.467: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/05/23 19:14:04.47
    STEP: updating the pod 01/05/23 19:14:04.473
    Jan  5 19:14:04.992: INFO: Successfully updated pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0"
    Jan  5 19:14:04.993: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0" in namespace "pods-2383" to be "terminated with reason DeadlineExceeded"
    Jan  5 19:14:04.996: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Running", Reason="", readiness=true. Elapsed: 3.737714ms
    Jan  5 19:14:07.002: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.008832278s
    Jan  5 19:14:09.000: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.007708997s
    Jan  5 19:14:09.000: INFO: Pod "pod-update-activedeadlineseconds-1d5999fa-1198-444b-9cce-a1994537b5e0" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:14:09.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2383" for this suite. 01/05/23 19:14:09.004
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:14:09.01
Jan  5 19:14:09.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename svc-latency 01/05/23 19:14:09.011
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:09.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:09.029
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:31
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jan  5 19:14:09.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1688 01/05/23 19:14:09.035
I0105 19:14:09.040100      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1688, replica count: 1
I0105 19:14:10.091281      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 19:14:10.205: INFO: Created: latency-svc-fts4s
Jan  5 19:14:10.213: INFO: Got endpoints: latency-svc-fts4s [21.269685ms]
Jan  5 19:14:10.227: INFO: Created: latency-svc-jgj7n
Jan  5 19:14:10.232: INFO: Got endpoints: latency-svc-jgj7n [18.802506ms]
Jan  5 19:14:10.236: INFO: Created: latency-svc-8x4gm
Jan  5 19:14:10.246: INFO: Got endpoints: latency-svc-8x4gm [32.310586ms]
Jan  5 19:14:10.246: INFO: Created: latency-svc-xmn6f
Jan  5 19:14:10.251: INFO: Got endpoints: latency-svc-xmn6f [37.067086ms]
Jan  5 19:14:10.258: INFO: Created: latency-svc-g4zvj
Jan  5 19:14:10.276: INFO: Created: latency-svc-vz66c
Jan  5 19:14:10.281: INFO: Got endpoints: latency-svc-g4zvj [66.97188ms]
Jan  5 19:14:10.285: INFO: Created: latency-svc-2kfh6
Jan  5 19:14:10.289: INFO: Got endpoints: latency-svc-vz66c [75.403241ms]
Jan  5 19:14:10.292: INFO: Got endpoints: latency-svc-2kfh6 [78.7926ms]
Jan  5 19:14:10.293: INFO: Created: latency-svc-6w7s5
Jan  5 19:14:10.303: INFO: Got endpoints: latency-svc-6w7s5 [89.353251ms]
Jan  5 19:14:10.304: INFO: Created: latency-svc-2cttj
Jan  5 19:14:10.308: INFO: Got endpoints: latency-svc-2cttj [94.111349ms]
Jan  5 19:14:10.312: INFO: Created: latency-svc-l5vnh
Jan  5 19:14:10.319: INFO: Created: latency-svc-ntgxx
Jan  5 19:14:10.320: INFO: Got endpoints: latency-svc-l5vnh [105.948503ms]
Jan  5 19:14:10.330: INFO: Got endpoints: latency-svc-ntgxx [116.068756ms]
Jan  5 19:14:10.330: INFO: Created: latency-svc-f7xvs
Jan  5 19:14:10.338: INFO: Created: latency-svc-98czx
Jan  5 19:14:10.339: INFO: Got endpoints: latency-svc-f7xvs [124.2619ms]
Jan  5 19:14:10.346: INFO: Got endpoints: latency-svc-98czx [131.444097ms]
Jan  5 19:14:10.346: INFO: Created: latency-svc-x2wf9
Jan  5 19:14:10.354: INFO: Got endpoints: latency-svc-x2wf9 [139.756615ms]
Jan  5 19:14:10.355: INFO: Created: latency-svc-g4jm8
Jan  5 19:14:10.360: INFO: Got endpoints: latency-svc-g4jm8 [127.664565ms]
Jan  5 19:14:10.365: INFO: Created: latency-svc-h4sd2
Jan  5 19:14:10.371: INFO: Got endpoints: latency-svc-h4sd2 [157.242136ms]
Jan  5 19:14:10.379: INFO: Created: latency-svc-z5rp5
Jan  5 19:14:10.383: INFO: Got endpoints: latency-svc-z5rp5 [169.075601ms]
Jan  5 19:14:10.394: INFO: Created: latency-svc-ljnqr
Jan  5 19:14:10.406: INFO: Got endpoints: latency-svc-ljnqr [160.196294ms]
Jan  5 19:14:10.408: INFO: Created: latency-svc-wcddk
Jan  5 19:14:10.420: INFO: Got endpoints: latency-svc-wcddk [168.918114ms]
Jan  5 19:14:10.421: INFO: Created: latency-svc-h5sv5
Jan  5 19:14:10.425: INFO: Created: latency-svc-jcqt4
Jan  5 19:14:10.433: INFO: Got endpoints: latency-svc-h5sv5 [152.38382ms]
Jan  5 19:14:10.433: INFO: Got endpoints: latency-svc-jcqt4 [140.645015ms]
Jan  5 19:14:10.438: INFO: Created: latency-svc-p4vjm
Jan  5 19:14:10.441: INFO: Created: latency-svc-bd9cg
Jan  5 19:14:10.450: INFO: Got endpoints: latency-svc-p4vjm [160.714888ms]
Jan  5 19:14:10.452: INFO: Got endpoints: latency-svc-bd9cg [149.46399ms]
Jan  5 19:14:10.457: INFO: Created: latency-svc-7fqg7
Jan  5 19:14:10.464: INFO: Got endpoints: latency-svc-7fqg7 [155.395608ms]
Jan  5 19:14:10.465: INFO: Created: latency-svc-fr5ql
Jan  5 19:14:10.475: INFO: Created: latency-svc-x87nk
Jan  5 19:14:10.475: INFO: Got endpoints: latency-svc-fr5ql [155.514092ms]
Jan  5 19:14:10.482: INFO: Got endpoints: latency-svc-x87nk [152.561588ms]
Jan  5 19:14:10.484: INFO: Created: latency-svc-w7nxq
Jan  5 19:14:10.492: INFO: Got endpoints: latency-svc-w7nxq [153.768241ms]
Jan  5 19:14:10.493: INFO: Created: latency-svc-wlwr9
Jan  5 19:14:10.498: INFO: Got endpoints: latency-svc-wlwr9 [143.761447ms]
Jan  5 19:14:10.504: INFO: Created: latency-svc-dckxf
Jan  5 19:14:10.508: INFO: Got endpoints: latency-svc-dckxf [162.76107ms]
Jan  5 19:14:10.511: INFO: Created: latency-svc-v5tfl
Jan  5 19:14:10.519: INFO: Created: latency-svc-lbqs8
Jan  5 19:14:10.520: INFO: Got endpoints: latency-svc-v5tfl [159.992581ms]
Jan  5 19:14:10.528: INFO: Created: latency-svc-9b69w
Jan  5 19:14:10.530: INFO: Got endpoints: latency-svc-lbqs8 [159.888456ms]
Jan  5 19:14:10.537: INFO: Got endpoints: latency-svc-9b69w [152.981499ms]
Jan  5 19:14:10.546: INFO: Created: latency-svc-b7g5p
Jan  5 19:14:10.556: INFO: Created: latency-svc-6dn6h
Jan  5 19:14:10.561: INFO: Got endpoints: latency-svc-b7g5p [155.191789ms]
Jan  5 19:14:10.564: INFO: Got endpoints: latency-svc-6dn6h [130.488093ms]
Jan  5 19:14:10.570: INFO: Created: latency-svc-qjvbf
Jan  5 19:14:10.571: INFO: Got endpoints: latency-svc-qjvbf [151.323776ms]
Jan  5 19:14:10.573: INFO: Created: latency-svc-dsj4x
Jan  5 19:14:10.578: INFO: Got endpoints: latency-svc-dsj4x [127.908642ms]
Jan  5 19:14:10.582: INFO: Created: latency-svc-6mfck
Jan  5 19:14:10.587: INFO: Got endpoints: latency-svc-6mfck [154.05444ms]
Jan  5 19:14:10.589: INFO: Created: latency-svc-5q62r
Jan  5 19:14:10.596: INFO: Created: latency-svc-wzph6
Jan  5 19:14:10.600: INFO: Created: latency-svc-djkfq
Jan  5 19:14:10.605: INFO: Created: latency-svc-zfxpr
Jan  5 19:14:10.610: INFO: Got endpoints: latency-svc-5q62r [158.149534ms]
Jan  5 19:14:10.615: INFO: Created: latency-svc-9wqft
Jan  5 19:14:10.619: INFO: Created: latency-svc-n822l
Jan  5 19:14:10.628: INFO: Created: latency-svc-l7thx
Jan  5 19:14:10.631: INFO: Created: latency-svc-2c6mv
Jan  5 19:14:10.633: INFO: Created: latency-svc-x4w9l
Jan  5 19:14:10.649: INFO: Created: latency-svc-sljgp
Jan  5 19:14:10.657: INFO: Created: latency-svc-w7fg7
Jan  5 19:14:10.662: INFO: Created: latency-svc-6cc4l
Jan  5 19:14:10.667: INFO: Got endpoints: latency-svc-wzph6 [203.43611ms]
Jan  5 19:14:10.678: INFO: Created: latency-svc-tvb48
Jan  5 19:14:10.681: INFO: Created: latency-svc-jxb6h
Jan  5 19:14:10.689: INFO: Created: latency-svc-s8pnv
Jan  5 19:14:10.696: INFO: Created: latency-svc-54dcz
Jan  5 19:14:10.701: INFO: Created: latency-svc-hhm79
Jan  5 19:14:10.709: INFO: Got endpoints: latency-svc-djkfq [233.79317ms]
Jan  5 19:14:10.719: INFO: Created: latency-svc-vr66p
Jan  5 19:14:10.759: INFO: Got endpoints: latency-svc-zfxpr [276.997585ms]
Jan  5 19:14:10.769: INFO: Created: latency-svc-qgb9n
Jan  5 19:14:10.810: INFO: Got endpoints: latency-svc-9wqft [317.544232ms]
Jan  5 19:14:10.819: INFO: Created: latency-svc-5gkkp
Jan  5 19:14:10.862: INFO: Got endpoints: latency-svc-n822l [353.487636ms]
Jan  5 19:14:10.872: INFO: Created: latency-svc-km68z
Jan  5 19:14:10.911: INFO: Got endpoints: latency-svc-l7thx [413.015826ms]
Jan  5 19:14:10.922: INFO: Created: latency-svc-l7cl6
Jan  5 19:14:10.961: INFO: Got endpoints: latency-svc-2c6mv [430.355287ms]
Jan  5 19:14:10.973: INFO: Created: latency-svc-xvh2z
Jan  5 19:14:11.010: INFO: Got endpoints: latency-svc-x4w9l [489.417765ms]
Jan  5 19:14:11.021: INFO: Created: latency-svc-w4rfg
Jan  5 19:14:11.060: INFO: Got endpoints: latency-svc-sljgp [488.565871ms]
Jan  5 19:14:11.070: INFO: Created: latency-svc-94wbf
Jan  5 19:14:11.109: INFO: Got endpoints: latency-svc-w7fg7 [545.4597ms]
Jan  5 19:14:11.120: INFO: Created: latency-svc-dlbth
Jan  5 19:14:11.160: INFO: Got endpoints: latency-svc-6cc4l [622.769072ms]
Jan  5 19:14:11.171: INFO: Created: latency-svc-ghg8m
Jan  5 19:14:11.211: INFO: Got endpoints: latency-svc-tvb48 [649.777976ms]
Jan  5 19:14:11.226: INFO: Created: latency-svc-krvvp
Jan  5 19:14:11.261: INFO: Got endpoints: latency-svc-jxb6h [682.617385ms]
Jan  5 19:14:11.270: INFO: Created: latency-svc-mlfx8
Jan  5 19:14:11.314: INFO: Got endpoints: latency-svc-s8pnv [726.514435ms]
Jan  5 19:14:11.325: INFO: Created: latency-svc-gc96p
Jan  5 19:14:11.359: INFO: Got endpoints: latency-svc-54dcz [748.807551ms]
Jan  5 19:14:11.368: INFO: Created: latency-svc-8l8p7
Jan  5 19:14:11.412: INFO: Got endpoints: latency-svc-hhm79 [744.511005ms]
Jan  5 19:14:11.429: INFO: Created: latency-svc-khm45
Jan  5 19:14:11.460: INFO: Got endpoints: latency-svc-vr66p [750.881463ms]
Jan  5 19:14:11.470: INFO: Created: latency-svc-5nrt6
Jan  5 19:14:11.511: INFO: Got endpoints: latency-svc-qgb9n [751.704422ms]
Jan  5 19:14:11.528: INFO: Created: latency-svc-r6fcr
Jan  5 19:14:11.560: INFO: Got endpoints: latency-svc-5gkkp [749.161764ms]
Jan  5 19:14:11.570: INFO: Created: latency-svc-cc5nm
Jan  5 19:14:11.610: INFO: Got endpoints: latency-svc-km68z [748.358823ms]
Jan  5 19:14:11.620: INFO: Created: latency-svc-thjxh
Jan  5 19:14:11.661: INFO: Got endpoints: latency-svc-l7cl6 [749.536327ms]
Jan  5 19:14:11.674: INFO: Created: latency-svc-frcwx
Jan  5 19:14:11.711: INFO: Got endpoints: latency-svc-xvh2z [749.6075ms]
Jan  5 19:14:11.722: INFO: Created: latency-svc-82tl2
Jan  5 19:14:11.760: INFO: Got endpoints: latency-svc-w4rfg [750.685638ms]
Jan  5 19:14:11.773: INFO: Created: latency-svc-kqf92
Jan  5 19:14:11.810: INFO: Got endpoints: latency-svc-94wbf [749.585444ms]
Jan  5 19:14:11.821: INFO: Created: latency-svc-tz5vk
Jan  5 19:14:11.860: INFO: Got endpoints: latency-svc-dlbth [750.591829ms]
Jan  5 19:14:11.869: INFO: Created: latency-svc-6jfc8
Jan  5 19:14:11.909: INFO: Got endpoints: latency-svc-ghg8m [749.002641ms]
Jan  5 19:14:11.921: INFO: Created: latency-svc-ggth4
Jan  5 19:14:11.959: INFO: Got endpoints: latency-svc-krvvp [747.752738ms]
Jan  5 19:14:11.969: INFO: Created: latency-svc-zhrct
Jan  5 19:14:12.009: INFO: Got endpoints: latency-svc-mlfx8 [748.614907ms]
Jan  5 19:14:12.018: INFO: Created: latency-svc-kjfwq
Jan  5 19:14:12.060: INFO: Got endpoints: latency-svc-gc96p [745.594494ms]
Jan  5 19:14:12.069: INFO: Created: latency-svc-f8cqs
Jan  5 19:14:12.110: INFO: Got endpoints: latency-svc-8l8p7 [750.974639ms]
Jan  5 19:14:12.120: INFO: Created: latency-svc-6fbnt
Jan  5 19:14:12.160: INFO: Got endpoints: latency-svc-khm45 [748.594541ms]
Jan  5 19:14:12.170: INFO: Created: latency-svc-fbplj
Jan  5 19:14:12.212: INFO: Got endpoints: latency-svc-5nrt6 [752.108123ms]
Jan  5 19:14:12.225: INFO: Created: latency-svc-8ktmc
Jan  5 19:14:12.262: INFO: Got endpoints: latency-svc-r6fcr [750.459476ms]
Jan  5 19:14:12.278: INFO: Created: latency-svc-hv9pb
Jan  5 19:14:12.310: INFO: Got endpoints: latency-svc-cc5nm [749.996651ms]
Jan  5 19:14:12.322: INFO: Created: latency-svc-445md
Jan  5 19:14:12.360: INFO: Got endpoints: latency-svc-thjxh [749.769164ms]
Jan  5 19:14:12.371: INFO: Created: latency-svc-vfcbn
Jan  5 19:14:12.410: INFO: Got endpoints: latency-svc-frcwx [749.525356ms]
Jan  5 19:14:12.422: INFO: Created: latency-svc-dqxrb
Jan  5 19:14:12.460: INFO: Got endpoints: latency-svc-82tl2 [749.130036ms]
Jan  5 19:14:12.472: INFO: Created: latency-svc-pcqsp
Jan  5 19:14:12.510: INFO: Got endpoints: latency-svc-kqf92 [749.229317ms]
Jan  5 19:14:12.523: INFO: Created: latency-svc-wt4nv
Jan  5 19:14:12.560: INFO: Got endpoints: latency-svc-tz5vk [749.877473ms]
Jan  5 19:14:12.572: INFO: Created: latency-svc-fp4wd
Jan  5 19:14:12.610: INFO: Got endpoints: latency-svc-6jfc8 [749.903319ms]
Jan  5 19:14:12.621: INFO: Created: latency-svc-l6rlp
Jan  5 19:14:12.660: INFO: Got endpoints: latency-svc-ggth4 [750.710193ms]
Jan  5 19:14:12.675: INFO: Created: latency-svc-tx7nd
Jan  5 19:14:12.710: INFO: Got endpoints: latency-svc-zhrct [750.692058ms]
Jan  5 19:14:12.721: INFO: Created: latency-svc-7pp7d
Jan  5 19:14:12.762: INFO: Got endpoints: latency-svc-kjfwq [753.212918ms]
Jan  5 19:14:12.777: INFO: Created: latency-svc-vkq7p
Jan  5 19:14:12.812: INFO: Got endpoints: latency-svc-f8cqs [752.483174ms]
Jan  5 19:14:12.823: INFO: Created: latency-svc-xgjd9
Jan  5 19:14:12.859: INFO: Got endpoints: latency-svc-6fbnt [748.635609ms]
Jan  5 19:14:12.870: INFO: Created: latency-svc-b55md
Jan  5 19:14:12.910: INFO: Got endpoints: latency-svc-fbplj [749.298083ms]
Jan  5 19:14:12.920: INFO: Created: latency-svc-v58w4
Jan  5 19:14:12.960: INFO: Got endpoints: latency-svc-8ktmc [747.275759ms]
Jan  5 19:14:12.970: INFO: Created: latency-svc-78rpf
Jan  5 19:14:13.013: INFO: Got endpoints: latency-svc-hv9pb [750.941833ms]
Jan  5 19:14:13.027: INFO: Created: latency-svc-x7sp8
Jan  5 19:14:13.060: INFO: Got endpoints: latency-svc-445md [749.870162ms]
Jan  5 19:14:13.070: INFO: Created: latency-svc-5c8kz
Jan  5 19:14:13.111: INFO: Got endpoints: latency-svc-vfcbn [750.354883ms]
Jan  5 19:14:13.121: INFO: Created: latency-svc-9lm9g
Jan  5 19:14:13.162: INFO: Got endpoints: latency-svc-dqxrb [751.67592ms]
Jan  5 19:14:13.172: INFO: Created: latency-svc-fjzts
Jan  5 19:14:13.211: INFO: Got endpoints: latency-svc-pcqsp [750.971782ms]
Jan  5 19:14:13.223: INFO: Created: latency-svc-cts66
Jan  5 19:14:13.260: INFO: Got endpoints: latency-svc-wt4nv [749.570512ms]
Jan  5 19:14:13.270: INFO: Created: latency-svc-b5tb7
Jan  5 19:14:13.312: INFO: Got endpoints: latency-svc-fp4wd [751.549779ms]
Jan  5 19:14:13.321: INFO: Created: latency-svc-bxsrq
Jan  5 19:14:13.362: INFO: Got endpoints: latency-svc-l6rlp [751.878205ms]
Jan  5 19:14:13.371: INFO: Created: latency-svc-qxqr9
Jan  5 19:14:13.412: INFO: Got endpoints: latency-svc-tx7nd [751.669641ms]
Jan  5 19:14:13.422: INFO: Created: latency-svc-xgknk
Jan  5 19:14:13.461: INFO: Got endpoints: latency-svc-7pp7d [750.886161ms]
Jan  5 19:14:13.472: INFO: Created: latency-svc-8cqdv
Jan  5 19:14:13.510: INFO: Got endpoints: latency-svc-vkq7p [747.071756ms]
Jan  5 19:14:13.521: INFO: Created: latency-svc-shlrh
Jan  5 19:14:13.561: INFO: Got endpoints: latency-svc-xgjd9 [748.860279ms]
Jan  5 19:14:13.575: INFO: Created: latency-svc-s5th9
Jan  5 19:14:13.610: INFO: Got endpoints: latency-svc-b55md [750.652638ms]
Jan  5 19:14:13.620: INFO: Created: latency-svc-64fbl
Jan  5 19:14:13.661: INFO: Got endpoints: latency-svc-v58w4 [750.833515ms]
Jan  5 19:14:13.673: INFO: Created: latency-svc-wn5rj
Jan  5 19:14:13.710: INFO: Got endpoints: latency-svc-78rpf [750.241539ms]
Jan  5 19:14:13.720: INFO: Created: latency-svc-sgxrr
Jan  5 19:14:13.760: INFO: Got endpoints: latency-svc-x7sp8 [747.259837ms]
Jan  5 19:14:13.770: INFO: Created: latency-svc-hvnm8
Jan  5 19:14:13.810: INFO: Got endpoints: latency-svc-5c8kz [749.79502ms]
Jan  5 19:14:13.826: INFO: Created: latency-svc-qkbrc
Jan  5 19:14:13.863: INFO: Got endpoints: latency-svc-9lm9g [751.838347ms]
Jan  5 19:14:13.876: INFO: Created: latency-svc-mwpql
Jan  5 19:14:13.911: INFO: Got endpoints: latency-svc-fjzts [749.024559ms]
Jan  5 19:14:13.921: INFO: Created: latency-svc-s2f2h
Jan  5 19:14:13.962: INFO: Got endpoints: latency-svc-cts66 [750.790617ms]
Jan  5 19:14:13.977: INFO: Created: latency-svc-2xhjp
Jan  5 19:14:14.013: INFO: Got endpoints: latency-svc-b5tb7 [753.273936ms]
Jan  5 19:14:14.038: INFO: Created: latency-svc-khstk
Jan  5 19:14:14.060: INFO: Got endpoints: latency-svc-bxsrq [747.778318ms]
Jan  5 19:14:14.073: INFO: Created: latency-svc-c978n
Jan  5 19:14:14.114: INFO: Got endpoints: latency-svc-qxqr9 [751.789472ms]
Jan  5 19:14:14.128: INFO: Created: latency-svc-lfbf8
Jan  5 19:14:14.162: INFO: Got endpoints: latency-svc-xgknk [749.558132ms]
Jan  5 19:14:14.175: INFO: Created: latency-svc-hwlxc
Jan  5 19:14:14.214: INFO: Got endpoints: latency-svc-8cqdv [753.410276ms]
Jan  5 19:14:14.234: INFO: Created: latency-svc-ptvzh
Jan  5 19:14:14.261: INFO: Got endpoints: latency-svc-shlrh [751.379691ms]
Jan  5 19:14:14.277: INFO: Created: latency-svc-vfw6m
Jan  5 19:14:14.313: INFO: Got endpoints: latency-svc-s5th9 [751.501381ms]
Jan  5 19:14:14.328: INFO: Created: latency-svc-srsp6
Jan  5 19:14:14.360: INFO: Got endpoints: latency-svc-64fbl [749.703734ms]
Jan  5 19:14:14.371: INFO: Created: latency-svc-964cj
Jan  5 19:14:14.410: INFO: Got endpoints: latency-svc-wn5rj [749.478069ms]
Jan  5 19:14:14.423: INFO: Created: latency-svc-vmbwj
Jan  5 19:14:14.460: INFO: Got endpoints: latency-svc-sgxrr [749.514808ms]
Jan  5 19:14:14.470: INFO: Created: latency-svc-dhs5b
Jan  5 19:14:14.510: INFO: Got endpoints: latency-svc-hvnm8 [749.657828ms]
Jan  5 19:14:14.527: INFO: Created: latency-svc-jzvxj
Jan  5 19:14:14.559: INFO: Got endpoints: latency-svc-qkbrc [749.657405ms]
Jan  5 19:14:14.573: INFO: Created: latency-svc-vqfpl
Jan  5 19:14:14.611: INFO: Got endpoints: latency-svc-mwpql [747.477555ms]
Jan  5 19:14:14.620: INFO: Created: latency-svc-2f52j
Jan  5 19:14:14.659: INFO: Got endpoints: latency-svc-s2f2h [747.66645ms]
Jan  5 19:14:14.668: INFO: Created: latency-svc-vbmd5
Jan  5 19:14:14.709: INFO: Got endpoints: latency-svc-2xhjp [746.607092ms]
Jan  5 19:14:14.720: INFO: Created: latency-svc-v8zzq
Jan  5 19:14:14.760: INFO: Got endpoints: latency-svc-khstk [747.260289ms]
Jan  5 19:14:14.771: INFO: Created: latency-svc-btg92
Jan  5 19:14:14.810: INFO: Got endpoints: latency-svc-c978n [749.965584ms]
Jan  5 19:14:14.821: INFO: Created: latency-svc-rtvdh
Jan  5 19:14:14.859: INFO: Got endpoints: latency-svc-lfbf8 [745.094974ms]
Jan  5 19:14:14.869: INFO: Created: latency-svc-l8hx8
Jan  5 19:14:14.909: INFO: Got endpoints: latency-svc-hwlxc [747.497669ms]
Jan  5 19:14:14.923: INFO: Created: latency-svc-b8mfp
Jan  5 19:14:14.959: INFO: Got endpoints: latency-svc-ptvzh [744.742311ms]
Jan  5 19:14:14.969: INFO: Created: latency-svc-mjxw7
Jan  5 19:14:15.012: INFO: Got endpoints: latency-svc-vfw6m [751.444967ms]
Jan  5 19:14:15.041: INFO: Created: latency-svc-cjs5f
Jan  5 19:14:15.068: INFO: Got endpoints: latency-svc-srsp6 [755.536729ms]
Jan  5 19:14:15.100: INFO: Created: latency-svc-wqtfm
Jan  5 19:14:15.136: INFO: Got endpoints: latency-svc-964cj [775.81131ms]
Jan  5 19:14:15.168: INFO: Created: latency-svc-2fp56
Jan  5 19:14:15.169: INFO: Got endpoints: latency-svc-vmbwj [758.493915ms]
Jan  5 19:14:15.191: INFO: Created: latency-svc-k4pxb
Jan  5 19:14:15.246: INFO: Got endpoints: latency-svc-dhs5b [785.856742ms]
Jan  5 19:14:15.265: INFO: Got endpoints: latency-svc-jzvxj [755.16695ms]
Jan  5 19:14:15.285: INFO: Created: latency-svc-gkv4l
Jan  5 19:14:15.294: INFO: Created: latency-svc-vgm4l
Jan  5 19:14:15.314: INFO: Got endpoints: latency-svc-vqfpl [754.47342ms]
Jan  5 19:14:15.336: INFO: Created: latency-svc-msh8l
Jan  5 19:14:15.365: INFO: Got endpoints: latency-svc-2f52j [754.628424ms]
Jan  5 19:14:15.378: INFO: Created: latency-svc-xr58q
Jan  5 19:14:15.420: INFO: Got endpoints: latency-svc-vbmd5 [760.980599ms]
Jan  5 19:14:15.430: INFO: Created: latency-svc-s67zf
Jan  5 19:14:15.461: INFO: Got endpoints: latency-svc-v8zzq [752.428391ms]
Jan  5 19:14:15.473: INFO: Created: latency-svc-ftbkz
Jan  5 19:14:15.510: INFO: Got endpoints: latency-svc-btg92 [749.441154ms]
Jan  5 19:14:15.520: INFO: Created: latency-svc-hgl8c
Jan  5 19:14:15.560: INFO: Got endpoints: latency-svc-rtvdh [750.128429ms]
Jan  5 19:14:15.571: INFO: Created: latency-svc-jqczp
Jan  5 19:14:15.611: INFO: Got endpoints: latency-svc-l8hx8 [751.839278ms]
Jan  5 19:14:15.621: INFO: Created: latency-svc-tbsqz
Jan  5 19:14:15.659: INFO: Got endpoints: latency-svc-b8mfp [750.089022ms]
Jan  5 19:14:15.670: INFO: Created: latency-svc-kn4d6
Jan  5 19:14:15.710: INFO: Got endpoints: latency-svc-mjxw7 [750.997503ms]
Jan  5 19:14:15.723: INFO: Created: latency-svc-bz5zt
Jan  5 19:14:15.761: INFO: Got endpoints: latency-svc-cjs5f [748.173234ms]
Jan  5 19:14:15.772: INFO: Created: latency-svc-nwfxw
Jan  5 19:14:15.811: INFO: Got endpoints: latency-svc-wqtfm [743.077544ms]
Jan  5 19:14:15.824: INFO: Created: latency-svc-xvrtc
Jan  5 19:14:15.860: INFO: Got endpoints: latency-svc-2fp56 [723.787964ms]
Jan  5 19:14:15.872: INFO: Created: latency-svc-5d8h5
Jan  5 19:14:15.912: INFO: Got endpoints: latency-svc-k4pxb [742.686454ms]
Jan  5 19:14:15.923: INFO: Created: latency-svc-phfxf
Jan  5 19:14:15.960: INFO: Got endpoints: latency-svc-gkv4l [714.12684ms]
Jan  5 19:14:15.974: INFO: Created: latency-svc-7jvwm
Jan  5 19:14:16.010: INFO: Got endpoints: latency-svc-vgm4l [744.332414ms]
Jan  5 19:14:16.019: INFO: Created: latency-svc-2dpx6
Jan  5 19:14:16.059: INFO: Got endpoints: latency-svc-msh8l [744.772033ms]
Jan  5 19:14:16.068: INFO: Created: latency-svc-5g75t
Jan  5 19:14:16.110: INFO: Got endpoints: latency-svc-xr58q [744.708662ms]
Jan  5 19:14:16.122: INFO: Created: latency-svc-q5sth
Jan  5 19:14:16.161: INFO: Got endpoints: latency-svc-s67zf [740.249001ms]
Jan  5 19:14:16.172: INFO: Created: latency-svc-7q9k9
Jan  5 19:14:16.209: INFO: Got endpoints: latency-svc-ftbkz [747.651177ms]
Jan  5 19:14:16.221: INFO: Created: latency-svc-7522j
Jan  5 19:14:16.261: INFO: Got endpoints: latency-svc-hgl8c [750.764855ms]
Jan  5 19:14:16.270: INFO: Created: latency-svc-95wcq
Jan  5 19:14:16.310: INFO: Got endpoints: latency-svc-jqczp [750.16953ms]
Jan  5 19:14:16.323: INFO: Created: latency-svc-v5skb
Jan  5 19:14:16.360: INFO: Got endpoints: latency-svc-tbsqz [748.816186ms]
Jan  5 19:14:16.372: INFO: Created: latency-svc-mw45k
Jan  5 19:14:16.410: INFO: Got endpoints: latency-svc-kn4d6 [750.260694ms]
Jan  5 19:14:16.420: INFO: Created: latency-svc-jrk6c
Jan  5 19:14:16.459: INFO: Got endpoints: latency-svc-bz5zt [749.104827ms]
Jan  5 19:14:16.472: INFO: Created: latency-svc-cqx5c
Jan  5 19:14:16.510: INFO: Got endpoints: latency-svc-nwfxw [748.779316ms]
Jan  5 19:14:16.520: INFO: Created: latency-svc-8gkwp
Jan  5 19:14:16.559: INFO: Got endpoints: latency-svc-xvrtc [747.500474ms]
Jan  5 19:14:16.571: INFO: Created: latency-svc-gcbx9
Jan  5 19:14:16.610: INFO: Got endpoints: latency-svc-5d8h5 [749.980078ms]
Jan  5 19:14:16.620: INFO: Created: latency-svc-zxwhf
Jan  5 19:14:16.660: INFO: Got endpoints: latency-svc-phfxf [748.028921ms]
Jan  5 19:14:16.675: INFO: Created: latency-svc-c8w6p
Jan  5 19:14:16.710: INFO: Got endpoints: latency-svc-7jvwm [749.854238ms]
Jan  5 19:14:16.721: INFO: Created: latency-svc-s6n2w
Jan  5 19:14:16.760: INFO: Got endpoints: latency-svc-2dpx6 [749.953828ms]
Jan  5 19:14:16.772: INFO: Created: latency-svc-mz2vs
Jan  5 19:14:16.809: INFO: Got endpoints: latency-svc-5g75t [750.241342ms]
Jan  5 19:14:16.821: INFO: Created: latency-svc-k6847
Jan  5 19:14:16.860: INFO: Got endpoints: latency-svc-q5sth [749.361408ms]
Jan  5 19:14:16.872: INFO: Created: latency-svc-8dqmz
Jan  5 19:14:16.910: INFO: Got endpoints: latency-svc-7q9k9 [749.14788ms]
Jan  5 19:14:16.919: INFO: Created: latency-svc-2jwjj
Jan  5 19:14:16.960: INFO: Got endpoints: latency-svc-7522j [750.933121ms]
Jan  5 19:14:16.970: INFO: Created: latency-svc-klss2
Jan  5 19:14:17.009: INFO: Got endpoints: latency-svc-95wcq [748.235398ms]
Jan  5 19:14:17.024: INFO: Created: latency-svc-2bpk6
Jan  5 19:14:17.060: INFO: Got endpoints: latency-svc-v5skb [749.554809ms]
Jan  5 19:14:17.071: INFO: Created: latency-svc-72thw
Jan  5 19:14:17.110: INFO: Got endpoints: latency-svc-mw45k [749.698656ms]
Jan  5 19:14:17.121: INFO: Created: latency-svc-jrlkb
Jan  5 19:14:17.160: INFO: Got endpoints: latency-svc-jrk6c [750.522702ms]
Jan  5 19:14:17.169: INFO: Created: latency-svc-c22nw
Jan  5 19:14:17.212: INFO: Got endpoints: latency-svc-cqx5c [752.584234ms]
Jan  5 19:14:17.221: INFO: Created: latency-svc-t6gvr
Jan  5 19:14:17.261: INFO: Got endpoints: latency-svc-8gkwp [751.646007ms]
Jan  5 19:14:17.284: INFO: Created: latency-svc-ps59r
Jan  5 19:14:17.311: INFO: Got endpoints: latency-svc-gcbx9 [751.508555ms]
Jan  5 19:14:17.322: INFO: Created: latency-svc-54jws
Jan  5 19:14:17.360: INFO: Got endpoints: latency-svc-zxwhf [749.487964ms]
Jan  5 19:14:17.370: INFO: Created: latency-svc-sh6hd
Jan  5 19:14:17.410: INFO: Got endpoints: latency-svc-c8w6p [750.237542ms]
Jan  5 19:14:17.421: INFO: Created: latency-svc-fthfb
Jan  5 19:14:17.462: INFO: Got endpoints: latency-svc-s6n2w [751.554825ms]
Jan  5 19:14:17.475: INFO: Created: latency-svc-bpdjk
Jan  5 19:14:17.510: INFO: Got endpoints: latency-svc-mz2vs [750.592992ms]
Jan  5 19:14:17.521: INFO: Created: latency-svc-lrqdf
Jan  5 19:14:17.560: INFO: Got endpoints: latency-svc-k6847 [750.975328ms]
Jan  5 19:14:17.572: INFO: Created: latency-svc-swlch
Jan  5 19:14:17.611: INFO: Got endpoints: latency-svc-8dqmz [751.674681ms]
Jan  5 19:14:17.622: INFO: Created: latency-svc-fbp5l
Jan  5 19:14:17.659: INFO: Got endpoints: latency-svc-2jwjj [749.413947ms]
Jan  5 19:14:17.672: INFO: Created: latency-svc-9cbpm
Jan  5 19:14:17.710: INFO: Got endpoints: latency-svc-klss2 [749.112165ms]
Jan  5 19:14:17.722: INFO: Created: latency-svc-64xct
Jan  5 19:14:17.759: INFO: Got endpoints: latency-svc-2bpk6 [750.155089ms]
Jan  5 19:14:17.771: INFO: Created: latency-svc-xsgmf
Jan  5 19:14:17.810: INFO: Got endpoints: latency-svc-72thw [749.754173ms]
Jan  5 19:14:17.832: INFO: Created: latency-svc-cr8vz
Jan  5 19:14:17.860: INFO: Got endpoints: latency-svc-jrlkb [750.172998ms]
Jan  5 19:14:17.874: INFO: Created: latency-svc-rg9ls
Jan  5 19:14:17.910: INFO: Got endpoints: latency-svc-c22nw [749.613234ms]
Jan  5 19:14:17.928: INFO: Created: latency-svc-vsmp6
Jan  5 19:14:17.961: INFO: Got endpoints: latency-svc-t6gvr [748.714793ms]
Jan  5 19:14:17.975: INFO: Created: latency-svc-blb5c
Jan  5 19:14:18.014: INFO: Got endpoints: latency-svc-ps59r [752.154207ms]
Jan  5 19:14:18.028: INFO: Created: latency-svc-gl58z
Jan  5 19:14:18.060: INFO: Got endpoints: latency-svc-54jws [749.073681ms]
Jan  5 19:14:18.110: INFO: Got endpoints: latency-svc-sh6hd [750.720526ms]
Jan  5 19:14:18.160: INFO: Got endpoints: latency-svc-fthfb [750.091726ms]
Jan  5 19:14:18.211: INFO: Got endpoints: latency-svc-bpdjk [749.334808ms]
Jan  5 19:14:18.260: INFO: Got endpoints: latency-svc-lrqdf [750.121445ms]
Jan  5 19:14:18.310: INFO: Got endpoints: latency-svc-swlch [749.90889ms]
Jan  5 19:14:18.359: INFO: Got endpoints: latency-svc-fbp5l [747.902336ms]
Jan  5 19:14:18.410: INFO: Got endpoints: latency-svc-9cbpm [750.181109ms]
Jan  5 19:14:18.459: INFO: Got endpoints: latency-svc-64xct [749.379373ms]
Jan  5 19:14:18.511: INFO: Got endpoints: latency-svc-xsgmf [751.626374ms]
Jan  5 19:14:18.559: INFO: Got endpoints: latency-svc-cr8vz [749.135639ms]
Jan  5 19:14:18.610: INFO: Got endpoints: latency-svc-rg9ls [748.51261ms]
Jan  5 19:14:18.659: INFO: Got endpoints: latency-svc-vsmp6 [749.233335ms]
Jan  5 19:14:18.710: INFO: Got endpoints: latency-svc-blb5c [749.285808ms]
Jan  5 19:14:18.760: INFO: Got endpoints: latency-svc-gl58z [746.116333ms]
Jan  5 19:14:18.760: INFO: Latencies: [18.802506ms 32.310586ms 37.067086ms 66.97188ms 75.403241ms 78.7926ms 89.353251ms 94.111349ms 105.948503ms 116.068756ms 124.2619ms 127.664565ms 127.908642ms 130.488093ms 131.444097ms 139.756615ms 140.645015ms 143.761447ms 149.46399ms 151.323776ms 152.38382ms 152.561588ms 152.981499ms 153.768241ms 154.05444ms 155.191789ms 155.395608ms 155.514092ms 157.242136ms 158.149534ms 159.888456ms 159.992581ms 160.196294ms 160.714888ms 162.76107ms 168.918114ms 169.075601ms 203.43611ms 233.79317ms 276.997585ms 317.544232ms 353.487636ms 413.015826ms 430.355287ms 488.565871ms 489.417765ms 545.4597ms 622.769072ms 649.777976ms 682.617385ms 714.12684ms 723.787964ms 726.514435ms 740.249001ms 742.686454ms 743.077544ms 744.332414ms 744.511005ms 744.708662ms 744.742311ms 744.772033ms 745.094974ms 745.594494ms 746.116333ms 746.607092ms 747.071756ms 747.259837ms 747.260289ms 747.275759ms 747.477555ms 747.497669ms 747.500474ms 747.651177ms 747.66645ms 747.752738ms 747.778318ms 747.902336ms 748.028921ms 748.173234ms 748.235398ms 748.358823ms 748.51261ms 748.594541ms 748.614907ms 748.635609ms 748.714793ms 748.779316ms 748.807551ms 748.816186ms 748.860279ms 749.002641ms 749.024559ms 749.073681ms 749.104827ms 749.112165ms 749.130036ms 749.135639ms 749.14788ms 749.161764ms 749.229317ms 749.233335ms 749.285808ms 749.298083ms 749.334808ms 749.361408ms 749.379373ms 749.413947ms 749.441154ms 749.478069ms 749.487964ms 749.514808ms 749.525356ms 749.536327ms 749.554809ms 749.558132ms 749.570512ms 749.585444ms 749.6075ms 749.613234ms 749.657405ms 749.657828ms 749.698656ms 749.703734ms 749.754173ms 749.769164ms 749.79502ms 749.854238ms 749.870162ms 749.877473ms 749.903319ms 749.90889ms 749.953828ms 749.965584ms 749.980078ms 749.996651ms 750.089022ms 750.091726ms 750.121445ms 750.128429ms 750.155089ms 750.16953ms 750.172998ms 750.181109ms 750.237542ms 750.241342ms 750.241539ms 750.260694ms 750.354883ms 750.459476ms 750.522702ms 750.591829ms 750.592992ms 750.652638ms 750.685638ms 750.692058ms 750.710193ms 750.720526ms 750.764855ms 750.790617ms 750.833515ms 750.881463ms 750.886161ms 750.933121ms 750.941833ms 750.971782ms 750.974639ms 750.975328ms 750.997503ms 751.379691ms 751.444967ms 751.501381ms 751.508555ms 751.549779ms 751.554825ms 751.626374ms 751.646007ms 751.669641ms 751.674681ms 751.67592ms 751.704422ms 751.789472ms 751.838347ms 751.839278ms 751.878205ms 752.108123ms 752.154207ms 752.428391ms 752.483174ms 752.584234ms 753.212918ms 753.273936ms 753.410276ms 754.47342ms 754.628424ms 755.16695ms 755.536729ms 758.493915ms 760.980599ms 775.81131ms 785.856742ms]
Jan  5 19:14:18.761: INFO: 50 %ile: 749.233335ms
Jan  5 19:14:18.761: INFO: 90 %ile: 751.789472ms
Jan  5 19:14:18.761: INFO: 99 %ile: 775.81131ms
Jan  5 19:14:18.761: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/node/init/init.go:32
Jan  5 19:14:18.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Service endpoints latency
  tear down framework | framework.go:193
STEP: Destroying namespace "svc-latency-1688" for this suite. 01/05/23 19:14:18.768
------------------------------
• [SLOW TEST] [9.764 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:14:09.01
    Jan  5 19:14:09.010: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename svc-latency 01/05/23 19:14:09.011
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:09.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:09.029
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jan  5 19:14:09.034: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1688 01/05/23 19:14:09.035
    I0105 19:14:09.040100      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1688, replica count: 1
    I0105 19:14:10.091281      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 19:14:10.205: INFO: Created: latency-svc-fts4s
    Jan  5 19:14:10.213: INFO: Got endpoints: latency-svc-fts4s [21.269685ms]
    Jan  5 19:14:10.227: INFO: Created: latency-svc-jgj7n
    Jan  5 19:14:10.232: INFO: Got endpoints: latency-svc-jgj7n [18.802506ms]
    Jan  5 19:14:10.236: INFO: Created: latency-svc-8x4gm
    Jan  5 19:14:10.246: INFO: Got endpoints: latency-svc-8x4gm [32.310586ms]
    Jan  5 19:14:10.246: INFO: Created: latency-svc-xmn6f
    Jan  5 19:14:10.251: INFO: Got endpoints: latency-svc-xmn6f [37.067086ms]
    Jan  5 19:14:10.258: INFO: Created: latency-svc-g4zvj
    Jan  5 19:14:10.276: INFO: Created: latency-svc-vz66c
    Jan  5 19:14:10.281: INFO: Got endpoints: latency-svc-g4zvj [66.97188ms]
    Jan  5 19:14:10.285: INFO: Created: latency-svc-2kfh6
    Jan  5 19:14:10.289: INFO: Got endpoints: latency-svc-vz66c [75.403241ms]
    Jan  5 19:14:10.292: INFO: Got endpoints: latency-svc-2kfh6 [78.7926ms]
    Jan  5 19:14:10.293: INFO: Created: latency-svc-6w7s5
    Jan  5 19:14:10.303: INFO: Got endpoints: latency-svc-6w7s5 [89.353251ms]
    Jan  5 19:14:10.304: INFO: Created: latency-svc-2cttj
    Jan  5 19:14:10.308: INFO: Got endpoints: latency-svc-2cttj [94.111349ms]
    Jan  5 19:14:10.312: INFO: Created: latency-svc-l5vnh
    Jan  5 19:14:10.319: INFO: Created: latency-svc-ntgxx
    Jan  5 19:14:10.320: INFO: Got endpoints: latency-svc-l5vnh [105.948503ms]
    Jan  5 19:14:10.330: INFO: Got endpoints: latency-svc-ntgxx [116.068756ms]
    Jan  5 19:14:10.330: INFO: Created: latency-svc-f7xvs
    Jan  5 19:14:10.338: INFO: Created: latency-svc-98czx
    Jan  5 19:14:10.339: INFO: Got endpoints: latency-svc-f7xvs [124.2619ms]
    Jan  5 19:14:10.346: INFO: Got endpoints: latency-svc-98czx [131.444097ms]
    Jan  5 19:14:10.346: INFO: Created: latency-svc-x2wf9
    Jan  5 19:14:10.354: INFO: Got endpoints: latency-svc-x2wf9 [139.756615ms]
    Jan  5 19:14:10.355: INFO: Created: latency-svc-g4jm8
    Jan  5 19:14:10.360: INFO: Got endpoints: latency-svc-g4jm8 [127.664565ms]
    Jan  5 19:14:10.365: INFO: Created: latency-svc-h4sd2
    Jan  5 19:14:10.371: INFO: Got endpoints: latency-svc-h4sd2 [157.242136ms]
    Jan  5 19:14:10.379: INFO: Created: latency-svc-z5rp5
    Jan  5 19:14:10.383: INFO: Got endpoints: latency-svc-z5rp5 [169.075601ms]
    Jan  5 19:14:10.394: INFO: Created: latency-svc-ljnqr
    Jan  5 19:14:10.406: INFO: Got endpoints: latency-svc-ljnqr [160.196294ms]
    Jan  5 19:14:10.408: INFO: Created: latency-svc-wcddk
    Jan  5 19:14:10.420: INFO: Got endpoints: latency-svc-wcddk [168.918114ms]
    Jan  5 19:14:10.421: INFO: Created: latency-svc-h5sv5
    Jan  5 19:14:10.425: INFO: Created: latency-svc-jcqt4
    Jan  5 19:14:10.433: INFO: Got endpoints: latency-svc-h5sv5 [152.38382ms]
    Jan  5 19:14:10.433: INFO: Got endpoints: latency-svc-jcqt4 [140.645015ms]
    Jan  5 19:14:10.438: INFO: Created: latency-svc-p4vjm
    Jan  5 19:14:10.441: INFO: Created: latency-svc-bd9cg
    Jan  5 19:14:10.450: INFO: Got endpoints: latency-svc-p4vjm [160.714888ms]
    Jan  5 19:14:10.452: INFO: Got endpoints: latency-svc-bd9cg [149.46399ms]
    Jan  5 19:14:10.457: INFO: Created: latency-svc-7fqg7
    Jan  5 19:14:10.464: INFO: Got endpoints: latency-svc-7fqg7 [155.395608ms]
    Jan  5 19:14:10.465: INFO: Created: latency-svc-fr5ql
    Jan  5 19:14:10.475: INFO: Created: latency-svc-x87nk
    Jan  5 19:14:10.475: INFO: Got endpoints: latency-svc-fr5ql [155.514092ms]
    Jan  5 19:14:10.482: INFO: Got endpoints: latency-svc-x87nk [152.561588ms]
    Jan  5 19:14:10.484: INFO: Created: latency-svc-w7nxq
    Jan  5 19:14:10.492: INFO: Got endpoints: latency-svc-w7nxq [153.768241ms]
    Jan  5 19:14:10.493: INFO: Created: latency-svc-wlwr9
    Jan  5 19:14:10.498: INFO: Got endpoints: latency-svc-wlwr9 [143.761447ms]
    Jan  5 19:14:10.504: INFO: Created: latency-svc-dckxf
    Jan  5 19:14:10.508: INFO: Got endpoints: latency-svc-dckxf [162.76107ms]
    Jan  5 19:14:10.511: INFO: Created: latency-svc-v5tfl
    Jan  5 19:14:10.519: INFO: Created: latency-svc-lbqs8
    Jan  5 19:14:10.520: INFO: Got endpoints: latency-svc-v5tfl [159.992581ms]
    Jan  5 19:14:10.528: INFO: Created: latency-svc-9b69w
    Jan  5 19:14:10.530: INFO: Got endpoints: latency-svc-lbqs8 [159.888456ms]
    Jan  5 19:14:10.537: INFO: Got endpoints: latency-svc-9b69w [152.981499ms]
    Jan  5 19:14:10.546: INFO: Created: latency-svc-b7g5p
    Jan  5 19:14:10.556: INFO: Created: latency-svc-6dn6h
    Jan  5 19:14:10.561: INFO: Got endpoints: latency-svc-b7g5p [155.191789ms]
    Jan  5 19:14:10.564: INFO: Got endpoints: latency-svc-6dn6h [130.488093ms]
    Jan  5 19:14:10.570: INFO: Created: latency-svc-qjvbf
    Jan  5 19:14:10.571: INFO: Got endpoints: latency-svc-qjvbf [151.323776ms]
    Jan  5 19:14:10.573: INFO: Created: latency-svc-dsj4x
    Jan  5 19:14:10.578: INFO: Got endpoints: latency-svc-dsj4x [127.908642ms]
    Jan  5 19:14:10.582: INFO: Created: latency-svc-6mfck
    Jan  5 19:14:10.587: INFO: Got endpoints: latency-svc-6mfck [154.05444ms]
    Jan  5 19:14:10.589: INFO: Created: latency-svc-5q62r
    Jan  5 19:14:10.596: INFO: Created: latency-svc-wzph6
    Jan  5 19:14:10.600: INFO: Created: latency-svc-djkfq
    Jan  5 19:14:10.605: INFO: Created: latency-svc-zfxpr
    Jan  5 19:14:10.610: INFO: Got endpoints: latency-svc-5q62r [158.149534ms]
    Jan  5 19:14:10.615: INFO: Created: latency-svc-9wqft
    Jan  5 19:14:10.619: INFO: Created: latency-svc-n822l
    Jan  5 19:14:10.628: INFO: Created: latency-svc-l7thx
    Jan  5 19:14:10.631: INFO: Created: latency-svc-2c6mv
    Jan  5 19:14:10.633: INFO: Created: latency-svc-x4w9l
    Jan  5 19:14:10.649: INFO: Created: latency-svc-sljgp
    Jan  5 19:14:10.657: INFO: Created: latency-svc-w7fg7
    Jan  5 19:14:10.662: INFO: Created: latency-svc-6cc4l
    Jan  5 19:14:10.667: INFO: Got endpoints: latency-svc-wzph6 [203.43611ms]
    Jan  5 19:14:10.678: INFO: Created: latency-svc-tvb48
    Jan  5 19:14:10.681: INFO: Created: latency-svc-jxb6h
    Jan  5 19:14:10.689: INFO: Created: latency-svc-s8pnv
    Jan  5 19:14:10.696: INFO: Created: latency-svc-54dcz
    Jan  5 19:14:10.701: INFO: Created: latency-svc-hhm79
    Jan  5 19:14:10.709: INFO: Got endpoints: latency-svc-djkfq [233.79317ms]
    Jan  5 19:14:10.719: INFO: Created: latency-svc-vr66p
    Jan  5 19:14:10.759: INFO: Got endpoints: latency-svc-zfxpr [276.997585ms]
    Jan  5 19:14:10.769: INFO: Created: latency-svc-qgb9n
    Jan  5 19:14:10.810: INFO: Got endpoints: latency-svc-9wqft [317.544232ms]
    Jan  5 19:14:10.819: INFO: Created: latency-svc-5gkkp
    Jan  5 19:14:10.862: INFO: Got endpoints: latency-svc-n822l [353.487636ms]
    Jan  5 19:14:10.872: INFO: Created: latency-svc-km68z
    Jan  5 19:14:10.911: INFO: Got endpoints: latency-svc-l7thx [413.015826ms]
    Jan  5 19:14:10.922: INFO: Created: latency-svc-l7cl6
    Jan  5 19:14:10.961: INFO: Got endpoints: latency-svc-2c6mv [430.355287ms]
    Jan  5 19:14:10.973: INFO: Created: latency-svc-xvh2z
    Jan  5 19:14:11.010: INFO: Got endpoints: latency-svc-x4w9l [489.417765ms]
    Jan  5 19:14:11.021: INFO: Created: latency-svc-w4rfg
    Jan  5 19:14:11.060: INFO: Got endpoints: latency-svc-sljgp [488.565871ms]
    Jan  5 19:14:11.070: INFO: Created: latency-svc-94wbf
    Jan  5 19:14:11.109: INFO: Got endpoints: latency-svc-w7fg7 [545.4597ms]
    Jan  5 19:14:11.120: INFO: Created: latency-svc-dlbth
    Jan  5 19:14:11.160: INFO: Got endpoints: latency-svc-6cc4l [622.769072ms]
    Jan  5 19:14:11.171: INFO: Created: latency-svc-ghg8m
    Jan  5 19:14:11.211: INFO: Got endpoints: latency-svc-tvb48 [649.777976ms]
    Jan  5 19:14:11.226: INFO: Created: latency-svc-krvvp
    Jan  5 19:14:11.261: INFO: Got endpoints: latency-svc-jxb6h [682.617385ms]
    Jan  5 19:14:11.270: INFO: Created: latency-svc-mlfx8
    Jan  5 19:14:11.314: INFO: Got endpoints: latency-svc-s8pnv [726.514435ms]
    Jan  5 19:14:11.325: INFO: Created: latency-svc-gc96p
    Jan  5 19:14:11.359: INFO: Got endpoints: latency-svc-54dcz [748.807551ms]
    Jan  5 19:14:11.368: INFO: Created: latency-svc-8l8p7
    Jan  5 19:14:11.412: INFO: Got endpoints: latency-svc-hhm79 [744.511005ms]
    Jan  5 19:14:11.429: INFO: Created: latency-svc-khm45
    Jan  5 19:14:11.460: INFO: Got endpoints: latency-svc-vr66p [750.881463ms]
    Jan  5 19:14:11.470: INFO: Created: latency-svc-5nrt6
    Jan  5 19:14:11.511: INFO: Got endpoints: latency-svc-qgb9n [751.704422ms]
    Jan  5 19:14:11.528: INFO: Created: latency-svc-r6fcr
    Jan  5 19:14:11.560: INFO: Got endpoints: latency-svc-5gkkp [749.161764ms]
    Jan  5 19:14:11.570: INFO: Created: latency-svc-cc5nm
    Jan  5 19:14:11.610: INFO: Got endpoints: latency-svc-km68z [748.358823ms]
    Jan  5 19:14:11.620: INFO: Created: latency-svc-thjxh
    Jan  5 19:14:11.661: INFO: Got endpoints: latency-svc-l7cl6 [749.536327ms]
    Jan  5 19:14:11.674: INFO: Created: latency-svc-frcwx
    Jan  5 19:14:11.711: INFO: Got endpoints: latency-svc-xvh2z [749.6075ms]
    Jan  5 19:14:11.722: INFO: Created: latency-svc-82tl2
    Jan  5 19:14:11.760: INFO: Got endpoints: latency-svc-w4rfg [750.685638ms]
    Jan  5 19:14:11.773: INFO: Created: latency-svc-kqf92
    Jan  5 19:14:11.810: INFO: Got endpoints: latency-svc-94wbf [749.585444ms]
    Jan  5 19:14:11.821: INFO: Created: latency-svc-tz5vk
    Jan  5 19:14:11.860: INFO: Got endpoints: latency-svc-dlbth [750.591829ms]
    Jan  5 19:14:11.869: INFO: Created: latency-svc-6jfc8
    Jan  5 19:14:11.909: INFO: Got endpoints: latency-svc-ghg8m [749.002641ms]
    Jan  5 19:14:11.921: INFO: Created: latency-svc-ggth4
    Jan  5 19:14:11.959: INFO: Got endpoints: latency-svc-krvvp [747.752738ms]
    Jan  5 19:14:11.969: INFO: Created: latency-svc-zhrct
    Jan  5 19:14:12.009: INFO: Got endpoints: latency-svc-mlfx8 [748.614907ms]
    Jan  5 19:14:12.018: INFO: Created: latency-svc-kjfwq
    Jan  5 19:14:12.060: INFO: Got endpoints: latency-svc-gc96p [745.594494ms]
    Jan  5 19:14:12.069: INFO: Created: latency-svc-f8cqs
    Jan  5 19:14:12.110: INFO: Got endpoints: latency-svc-8l8p7 [750.974639ms]
    Jan  5 19:14:12.120: INFO: Created: latency-svc-6fbnt
    Jan  5 19:14:12.160: INFO: Got endpoints: latency-svc-khm45 [748.594541ms]
    Jan  5 19:14:12.170: INFO: Created: latency-svc-fbplj
    Jan  5 19:14:12.212: INFO: Got endpoints: latency-svc-5nrt6 [752.108123ms]
    Jan  5 19:14:12.225: INFO: Created: latency-svc-8ktmc
    Jan  5 19:14:12.262: INFO: Got endpoints: latency-svc-r6fcr [750.459476ms]
    Jan  5 19:14:12.278: INFO: Created: latency-svc-hv9pb
    Jan  5 19:14:12.310: INFO: Got endpoints: latency-svc-cc5nm [749.996651ms]
    Jan  5 19:14:12.322: INFO: Created: latency-svc-445md
    Jan  5 19:14:12.360: INFO: Got endpoints: latency-svc-thjxh [749.769164ms]
    Jan  5 19:14:12.371: INFO: Created: latency-svc-vfcbn
    Jan  5 19:14:12.410: INFO: Got endpoints: latency-svc-frcwx [749.525356ms]
    Jan  5 19:14:12.422: INFO: Created: latency-svc-dqxrb
    Jan  5 19:14:12.460: INFO: Got endpoints: latency-svc-82tl2 [749.130036ms]
    Jan  5 19:14:12.472: INFO: Created: latency-svc-pcqsp
    Jan  5 19:14:12.510: INFO: Got endpoints: latency-svc-kqf92 [749.229317ms]
    Jan  5 19:14:12.523: INFO: Created: latency-svc-wt4nv
    Jan  5 19:14:12.560: INFO: Got endpoints: latency-svc-tz5vk [749.877473ms]
    Jan  5 19:14:12.572: INFO: Created: latency-svc-fp4wd
    Jan  5 19:14:12.610: INFO: Got endpoints: latency-svc-6jfc8 [749.903319ms]
    Jan  5 19:14:12.621: INFO: Created: latency-svc-l6rlp
    Jan  5 19:14:12.660: INFO: Got endpoints: latency-svc-ggth4 [750.710193ms]
    Jan  5 19:14:12.675: INFO: Created: latency-svc-tx7nd
    Jan  5 19:14:12.710: INFO: Got endpoints: latency-svc-zhrct [750.692058ms]
    Jan  5 19:14:12.721: INFO: Created: latency-svc-7pp7d
    Jan  5 19:14:12.762: INFO: Got endpoints: latency-svc-kjfwq [753.212918ms]
    Jan  5 19:14:12.777: INFO: Created: latency-svc-vkq7p
    Jan  5 19:14:12.812: INFO: Got endpoints: latency-svc-f8cqs [752.483174ms]
    Jan  5 19:14:12.823: INFO: Created: latency-svc-xgjd9
    Jan  5 19:14:12.859: INFO: Got endpoints: latency-svc-6fbnt [748.635609ms]
    Jan  5 19:14:12.870: INFO: Created: latency-svc-b55md
    Jan  5 19:14:12.910: INFO: Got endpoints: latency-svc-fbplj [749.298083ms]
    Jan  5 19:14:12.920: INFO: Created: latency-svc-v58w4
    Jan  5 19:14:12.960: INFO: Got endpoints: latency-svc-8ktmc [747.275759ms]
    Jan  5 19:14:12.970: INFO: Created: latency-svc-78rpf
    Jan  5 19:14:13.013: INFO: Got endpoints: latency-svc-hv9pb [750.941833ms]
    Jan  5 19:14:13.027: INFO: Created: latency-svc-x7sp8
    Jan  5 19:14:13.060: INFO: Got endpoints: latency-svc-445md [749.870162ms]
    Jan  5 19:14:13.070: INFO: Created: latency-svc-5c8kz
    Jan  5 19:14:13.111: INFO: Got endpoints: latency-svc-vfcbn [750.354883ms]
    Jan  5 19:14:13.121: INFO: Created: latency-svc-9lm9g
    Jan  5 19:14:13.162: INFO: Got endpoints: latency-svc-dqxrb [751.67592ms]
    Jan  5 19:14:13.172: INFO: Created: latency-svc-fjzts
    Jan  5 19:14:13.211: INFO: Got endpoints: latency-svc-pcqsp [750.971782ms]
    Jan  5 19:14:13.223: INFO: Created: latency-svc-cts66
    Jan  5 19:14:13.260: INFO: Got endpoints: latency-svc-wt4nv [749.570512ms]
    Jan  5 19:14:13.270: INFO: Created: latency-svc-b5tb7
    Jan  5 19:14:13.312: INFO: Got endpoints: latency-svc-fp4wd [751.549779ms]
    Jan  5 19:14:13.321: INFO: Created: latency-svc-bxsrq
    Jan  5 19:14:13.362: INFO: Got endpoints: latency-svc-l6rlp [751.878205ms]
    Jan  5 19:14:13.371: INFO: Created: latency-svc-qxqr9
    Jan  5 19:14:13.412: INFO: Got endpoints: latency-svc-tx7nd [751.669641ms]
    Jan  5 19:14:13.422: INFO: Created: latency-svc-xgknk
    Jan  5 19:14:13.461: INFO: Got endpoints: latency-svc-7pp7d [750.886161ms]
    Jan  5 19:14:13.472: INFO: Created: latency-svc-8cqdv
    Jan  5 19:14:13.510: INFO: Got endpoints: latency-svc-vkq7p [747.071756ms]
    Jan  5 19:14:13.521: INFO: Created: latency-svc-shlrh
    Jan  5 19:14:13.561: INFO: Got endpoints: latency-svc-xgjd9 [748.860279ms]
    Jan  5 19:14:13.575: INFO: Created: latency-svc-s5th9
    Jan  5 19:14:13.610: INFO: Got endpoints: latency-svc-b55md [750.652638ms]
    Jan  5 19:14:13.620: INFO: Created: latency-svc-64fbl
    Jan  5 19:14:13.661: INFO: Got endpoints: latency-svc-v58w4 [750.833515ms]
    Jan  5 19:14:13.673: INFO: Created: latency-svc-wn5rj
    Jan  5 19:14:13.710: INFO: Got endpoints: latency-svc-78rpf [750.241539ms]
    Jan  5 19:14:13.720: INFO: Created: latency-svc-sgxrr
    Jan  5 19:14:13.760: INFO: Got endpoints: latency-svc-x7sp8 [747.259837ms]
    Jan  5 19:14:13.770: INFO: Created: latency-svc-hvnm8
    Jan  5 19:14:13.810: INFO: Got endpoints: latency-svc-5c8kz [749.79502ms]
    Jan  5 19:14:13.826: INFO: Created: latency-svc-qkbrc
    Jan  5 19:14:13.863: INFO: Got endpoints: latency-svc-9lm9g [751.838347ms]
    Jan  5 19:14:13.876: INFO: Created: latency-svc-mwpql
    Jan  5 19:14:13.911: INFO: Got endpoints: latency-svc-fjzts [749.024559ms]
    Jan  5 19:14:13.921: INFO: Created: latency-svc-s2f2h
    Jan  5 19:14:13.962: INFO: Got endpoints: latency-svc-cts66 [750.790617ms]
    Jan  5 19:14:13.977: INFO: Created: latency-svc-2xhjp
    Jan  5 19:14:14.013: INFO: Got endpoints: latency-svc-b5tb7 [753.273936ms]
    Jan  5 19:14:14.038: INFO: Created: latency-svc-khstk
    Jan  5 19:14:14.060: INFO: Got endpoints: latency-svc-bxsrq [747.778318ms]
    Jan  5 19:14:14.073: INFO: Created: latency-svc-c978n
    Jan  5 19:14:14.114: INFO: Got endpoints: latency-svc-qxqr9 [751.789472ms]
    Jan  5 19:14:14.128: INFO: Created: latency-svc-lfbf8
    Jan  5 19:14:14.162: INFO: Got endpoints: latency-svc-xgknk [749.558132ms]
    Jan  5 19:14:14.175: INFO: Created: latency-svc-hwlxc
    Jan  5 19:14:14.214: INFO: Got endpoints: latency-svc-8cqdv [753.410276ms]
    Jan  5 19:14:14.234: INFO: Created: latency-svc-ptvzh
    Jan  5 19:14:14.261: INFO: Got endpoints: latency-svc-shlrh [751.379691ms]
    Jan  5 19:14:14.277: INFO: Created: latency-svc-vfw6m
    Jan  5 19:14:14.313: INFO: Got endpoints: latency-svc-s5th9 [751.501381ms]
    Jan  5 19:14:14.328: INFO: Created: latency-svc-srsp6
    Jan  5 19:14:14.360: INFO: Got endpoints: latency-svc-64fbl [749.703734ms]
    Jan  5 19:14:14.371: INFO: Created: latency-svc-964cj
    Jan  5 19:14:14.410: INFO: Got endpoints: latency-svc-wn5rj [749.478069ms]
    Jan  5 19:14:14.423: INFO: Created: latency-svc-vmbwj
    Jan  5 19:14:14.460: INFO: Got endpoints: latency-svc-sgxrr [749.514808ms]
    Jan  5 19:14:14.470: INFO: Created: latency-svc-dhs5b
    Jan  5 19:14:14.510: INFO: Got endpoints: latency-svc-hvnm8 [749.657828ms]
    Jan  5 19:14:14.527: INFO: Created: latency-svc-jzvxj
    Jan  5 19:14:14.559: INFO: Got endpoints: latency-svc-qkbrc [749.657405ms]
    Jan  5 19:14:14.573: INFO: Created: latency-svc-vqfpl
    Jan  5 19:14:14.611: INFO: Got endpoints: latency-svc-mwpql [747.477555ms]
    Jan  5 19:14:14.620: INFO: Created: latency-svc-2f52j
    Jan  5 19:14:14.659: INFO: Got endpoints: latency-svc-s2f2h [747.66645ms]
    Jan  5 19:14:14.668: INFO: Created: latency-svc-vbmd5
    Jan  5 19:14:14.709: INFO: Got endpoints: latency-svc-2xhjp [746.607092ms]
    Jan  5 19:14:14.720: INFO: Created: latency-svc-v8zzq
    Jan  5 19:14:14.760: INFO: Got endpoints: latency-svc-khstk [747.260289ms]
    Jan  5 19:14:14.771: INFO: Created: latency-svc-btg92
    Jan  5 19:14:14.810: INFO: Got endpoints: latency-svc-c978n [749.965584ms]
    Jan  5 19:14:14.821: INFO: Created: latency-svc-rtvdh
    Jan  5 19:14:14.859: INFO: Got endpoints: latency-svc-lfbf8 [745.094974ms]
    Jan  5 19:14:14.869: INFO: Created: latency-svc-l8hx8
    Jan  5 19:14:14.909: INFO: Got endpoints: latency-svc-hwlxc [747.497669ms]
    Jan  5 19:14:14.923: INFO: Created: latency-svc-b8mfp
    Jan  5 19:14:14.959: INFO: Got endpoints: latency-svc-ptvzh [744.742311ms]
    Jan  5 19:14:14.969: INFO: Created: latency-svc-mjxw7
    Jan  5 19:14:15.012: INFO: Got endpoints: latency-svc-vfw6m [751.444967ms]
    Jan  5 19:14:15.041: INFO: Created: latency-svc-cjs5f
    Jan  5 19:14:15.068: INFO: Got endpoints: latency-svc-srsp6 [755.536729ms]
    Jan  5 19:14:15.100: INFO: Created: latency-svc-wqtfm
    Jan  5 19:14:15.136: INFO: Got endpoints: latency-svc-964cj [775.81131ms]
    Jan  5 19:14:15.168: INFO: Created: latency-svc-2fp56
    Jan  5 19:14:15.169: INFO: Got endpoints: latency-svc-vmbwj [758.493915ms]
    Jan  5 19:14:15.191: INFO: Created: latency-svc-k4pxb
    Jan  5 19:14:15.246: INFO: Got endpoints: latency-svc-dhs5b [785.856742ms]
    Jan  5 19:14:15.265: INFO: Got endpoints: latency-svc-jzvxj [755.16695ms]
    Jan  5 19:14:15.285: INFO: Created: latency-svc-gkv4l
    Jan  5 19:14:15.294: INFO: Created: latency-svc-vgm4l
    Jan  5 19:14:15.314: INFO: Got endpoints: latency-svc-vqfpl [754.47342ms]
    Jan  5 19:14:15.336: INFO: Created: latency-svc-msh8l
    Jan  5 19:14:15.365: INFO: Got endpoints: latency-svc-2f52j [754.628424ms]
    Jan  5 19:14:15.378: INFO: Created: latency-svc-xr58q
    Jan  5 19:14:15.420: INFO: Got endpoints: latency-svc-vbmd5 [760.980599ms]
    Jan  5 19:14:15.430: INFO: Created: latency-svc-s67zf
    Jan  5 19:14:15.461: INFO: Got endpoints: latency-svc-v8zzq [752.428391ms]
    Jan  5 19:14:15.473: INFO: Created: latency-svc-ftbkz
    Jan  5 19:14:15.510: INFO: Got endpoints: latency-svc-btg92 [749.441154ms]
    Jan  5 19:14:15.520: INFO: Created: latency-svc-hgl8c
    Jan  5 19:14:15.560: INFO: Got endpoints: latency-svc-rtvdh [750.128429ms]
    Jan  5 19:14:15.571: INFO: Created: latency-svc-jqczp
    Jan  5 19:14:15.611: INFO: Got endpoints: latency-svc-l8hx8 [751.839278ms]
    Jan  5 19:14:15.621: INFO: Created: latency-svc-tbsqz
    Jan  5 19:14:15.659: INFO: Got endpoints: latency-svc-b8mfp [750.089022ms]
    Jan  5 19:14:15.670: INFO: Created: latency-svc-kn4d6
    Jan  5 19:14:15.710: INFO: Got endpoints: latency-svc-mjxw7 [750.997503ms]
    Jan  5 19:14:15.723: INFO: Created: latency-svc-bz5zt
    Jan  5 19:14:15.761: INFO: Got endpoints: latency-svc-cjs5f [748.173234ms]
    Jan  5 19:14:15.772: INFO: Created: latency-svc-nwfxw
    Jan  5 19:14:15.811: INFO: Got endpoints: latency-svc-wqtfm [743.077544ms]
    Jan  5 19:14:15.824: INFO: Created: latency-svc-xvrtc
    Jan  5 19:14:15.860: INFO: Got endpoints: latency-svc-2fp56 [723.787964ms]
    Jan  5 19:14:15.872: INFO: Created: latency-svc-5d8h5
    Jan  5 19:14:15.912: INFO: Got endpoints: latency-svc-k4pxb [742.686454ms]
    Jan  5 19:14:15.923: INFO: Created: latency-svc-phfxf
    Jan  5 19:14:15.960: INFO: Got endpoints: latency-svc-gkv4l [714.12684ms]
    Jan  5 19:14:15.974: INFO: Created: latency-svc-7jvwm
    Jan  5 19:14:16.010: INFO: Got endpoints: latency-svc-vgm4l [744.332414ms]
    Jan  5 19:14:16.019: INFO: Created: latency-svc-2dpx6
    Jan  5 19:14:16.059: INFO: Got endpoints: latency-svc-msh8l [744.772033ms]
    Jan  5 19:14:16.068: INFO: Created: latency-svc-5g75t
    Jan  5 19:14:16.110: INFO: Got endpoints: latency-svc-xr58q [744.708662ms]
    Jan  5 19:14:16.122: INFO: Created: latency-svc-q5sth
    Jan  5 19:14:16.161: INFO: Got endpoints: latency-svc-s67zf [740.249001ms]
    Jan  5 19:14:16.172: INFO: Created: latency-svc-7q9k9
    Jan  5 19:14:16.209: INFO: Got endpoints: latency-svc-ftbkz [747.651177ms]
    Jan  5 19:14:16.221: INFO: Created: latency-svc-7522j
    Jan  5 19:14:16.261: INFO: Got endpoints: latency-svc-hgl8c [750.764855ms]
    Jan  5 19:14:16.270: INFO: Created: latency-svc-95wcq
    Jan  5 19:14:16.310: INFO: Got endpoints: latency-svc-jqczp [750.16953ms]
    Jan  5 19:14:16.323: INFO: Created: latency-svc-v5skb
    Jan  5 19:14:16.360: INFO: Got endpoints: latency-svc-tbsqz [748.816186ms]
    Jan  5 19:14:16.372: INFO: Created: latency-svc-mw45k
    Jan  5 19:14:16.410: INFO: Got endpoints: latency-svc-kn4d6 [750.260694ms]
    Jan  5 19:14:16.420: INFO: Created: latency-svc-jrk6c
    Jan  5 19:14:16.459: INFO: Got endpoints: latency-svc-bz5zt [749.104827ms]
    Jan  5 19:14:16.472: INFO: Created: latency-svc-cqx5c
    Jan  5 19:14:16.510: INFO: Got endpoints: latency-svc-nwfxw [748.779316ms]
    Jan  5 19:14:16.520: INFO: Created: latency-svc-8gkwp
    Jan  5 19:14:16.559: INFO: Got endpoints: latency-svc-xvrtc [747.500474ms]
    Jan  5 19:14:16.571: INFO: Created: latency-svc-gcbx9
    Jan  5 19:14:16.610: INFO: Got endpoints: latency-svc-5d8h5 [749.980078ms]
    Jan  5 19:14:16.620: INFO: Created: latency-svc-zxwhf
    Jan  5 19:14:16.660: INFO: Got endpoints: latency-svc-phfxf [748.028921ms]
    Jan  5 19:14:16.675: INFO: Created: latency-svc-c8w6p
    Jan  5 19:14:16.710: INFO: Got endpoints: latency-svc-7jvwm [749.854238ms]
    Jan  5 19:14:16.721: INFO: Created: latency-svc-s6n2w
    Jan  5 19:14:16.760: INFO: Got endpoints: latency-svc-2dpx6 [749.953828ms]
    Jan  5 19:14:16.772: INFO: Created: latency-svc-mz2vs
    Jan  5 19:14:16.809: INFO: Got endpoints: latency-svc-5g75t [750.241342ms]
    Jan  5 19:14:16.821: INFO: Created: latency-svc-k6847
    Jan  5 19:14:16.860: INFO: Got endpoints: latency-svc-q5sth [749.361408ms]
    Jan  5 19:14:16.872: INFO: Created: latency-svc-8dqmz
    Jan  5 19:14:16.910: INFO: Got endpoints: latency-svc-7q9k9 [749.14788ms]
    Jan  5 19:14:16.919: INFO: Created: latency-svc-2jwjj
    Jan  5 19:14:16.960: INFO: Got endpoints: latency-svc-7522j [750.933121ms]
    Jan  5 19:14:16.970: INFO: Created: latency-svc-klss2
    Jan  5 19:14:17.009: INFO: Got endpoints: latency-svc-95wcq [748.235398ms]
    Jan  5 19:14:17.024: INFO: Created: latency-svc-2bpk6
    Jan  5 19:14:17.060: INFO: Got endpoints: latency-svc-v5skb [749.554809ms]
    Jan  5 19:14:17.071: INFO: Created: latency-svc-72thw
    Jan  5 19:14:17.110: INFO: Got endpoints: latency-svc-mw45k [749.698656ms]
    Jan  5 19:14:17.121: INFO: Created: latency-svc-jrlkb
    Jan  5 19:14:17.160: INFO: Got endpoints: latency-svc-jrk6c [750.522702ms]
    Jan  5 19:14:17.169: INFO: Created: latency-svc-c22nw
    Jan  5 19:14:17.212: INFO: Got endpoints: latency-svc-cqx5c [752.584234ms]
    Jan  5 19:14:17.221: INFO: Created: latency-svc-t6gvr
    Jan  5 19:14:17.261: INFO: Got endpoints: latency-svc-8gkwp [751.646007ms]
    Jan  5 19:14:17.284: INFO: Created: latency-svc-ps59r
    Jan  5 19:14:17.311: INFO: Got endpoints: latency-svc-gcbx9 [751.508555ms]
    Jan  5 19:14:17.322: INFO: Created: latency-svc-54jws
    Jan  5 19:14:17.360: INFO: Got endpoints: latency-svc-zxwhf [749.487964ms]
    Jan  5 19:14:17.370: INFO: Created: latency-svc-sh6hd
    Jan  5 19:14:17.410: INFO: Got endpoints: latency-svc-c8w6p [750.237542ms]
    Jan  5 19:14:17.421: INFO: Created: latency-svc-fthfb
    Jan  5 19:14:17.462: INFO: Got endpoints: latency-svc-s6n2w [751.554825ms]
    Jan  5 19:14:17.475: INFO: Created: latency-svc-bpdjk
    Jan  5 19:14:17.510: INFO: Got endpoints: latency-svc-mz2vs [750.592992ms]
    Jan  5 19:14:17.521: INFO: Created: latency-svc-lrqdf
    Jan  5 19:14:17.560: INFO: Got endpoints: latency-svc-k6847 [750.975328ms]
    Jan  5 19:14:17.572: INFO: Created: latency-svc-swlch
    Jan  5 19:14:17.611: INFO: Got endpoints: latency-svc-8dqmz [751.674681ms]
    Jan  5 19:14:17.622: INFO: Created: latency-svc-fbp5l
    Jan  5 19:14:17.659: INFO: Got endpoints: latency-svc-2jwjj [749.413947ms]
    Jan  5 19:14:17.672: INFO: Created: latency-svc-9cbpm
    Jan  5 19:14:17.710: INFO: Got endpoints: latency-svc-klss2 [749.112165ms]
    Jan  5 19:14:17.722: INFO: Created: latency-svc-64xct
    Jan  5 19:14:17.759: INFO: Got endpoints: latency-svc-2bpk6 [750.155089ms]
    Jan  5 19:14:17.771: INFO: Created: latency-svc-xsgmf
    Jan  5 19:14:17.810: INFO: Got endpoints: latency-svc-72thw [749.754173ms]
    Jan  5 19:14:17.832: INFO: Created: latency-svc-cr8vz
    Jan  5 19:14:17.860: INFO: Got endpoints: latency-svc-jrlkb [750.172998ms]
    Jan  5 19:14:17.874: INFO: Created: latency-svc-rg9ls
    Jan  5 19:14:17.910: INFO: Got endpoints: latency-svc-c22nw [749.613234ms]
    Jan  5 19:14:17.928: INFO: Created: latency-svc-vsmp6
    Jan  5 19:14:17.961: INFO: Got endpoints: latency-svc-t6gvr [748.714793ms]
    Jan  5 19:14:17.975: INFO: Created: latency-svc-blb5c
    Jan  5 19:14:18.014: INFO: Got endpoints: latency-svc-ps59r [752.154207ms]
    Jan  5 19:14:18.028: INFO: Created: latency-svc-gl58z
    Jan  5 19:14:18.060: INFO: Got endpoints: latency-svc-54jws [749.073681ms]
    Jan  5 19:14:18.110: INFO: Got endpoints: latency-svc-sh6hd [750.720526ms]
    Jan  5 19:14:18.160: INFO: Got endpoints: latency-svc-fthfb [750.091726ms]
    Jan  5 19:14:18.211: INFO: Got endpoints: latency-svc-bpdjk [749.334808ms]
    Jan  5 19:14:18.260: INFO: Got endpoints: latency-svc-lrqdf [750.121445ms]
    Jan  5 19:14:18.310: INFO: Got endpoints: latency-svc-swlch [749.90889ms]
    Jan  5 19:14:18.359: INFO: Got endpoints: latency-svc-fbp5l [747.902336ms]
    Jan  5 19:14:18.410: INFO: Got endpoints: latency-svc-9cbpm [750.181109ms]
    Jan  5 19:14:18.459: INFO: Got endpoints: latency-svc-64xct [749.379373ms]
    Jan  5 19:14:18.511: INFO: Got endpoints: latency-svc-xsgmf [751.626374ms]
    Jan  5 19:14:18.559: INFO: Got endpoints: latency-svc-cr8vz [749.135639ms]
    Jan  5 19:14:18.610: INFO: Got endpoints: latency-svc-rg9ls [748.51261ms]
    Jan  5 19:14:18.659: INFO: Got endpoints: latency-svc-vsmp6 [749.233335ms]
    Jan  5 19:14:18.710: INFO: Got endpoints: latency-svc-blb5c [749.285808ms]
    Jan  5 19:14:18.760: INFO: Got endpoints: latency-svc-gl58z [746.116333ms]
    Jan  5 19:14:18.760: INFO: Latencies: [18.802506ms 32.310586ms 37.067086ms 66.97188ms 75.403241ms 78.7926ms 89.353251ms 94.111349ms 105.948503ms 116.068756ms 124.2619ms 127.664565ms 127.908642ms 130.488093ms 131.444097ms 139.756615ms 140.645015ms 143.761447ms 149.46399ms 151.323776ms 152.38382ms 152.561588ms 152.981499ms 153.768241ms 154.05444ms 155.191789ms 155.395608ms 155.514092ms 157.242136ms 158.149534ms 159.888456ms 159.992581ms 160.196294ms 160.714888ms 162.76107ms 168.918114ms 169.075601ms 203.43611ms 233.79317ms 276.997585ms 317.544232ms 353.487636ms 413.015826ms 430.355287ms 488.565871ms 489.417765ms 545.4597ms 622.769072ms 649.777976ms 682.617385ms 714.12684ms 723.787964ms 726.514435ms 740.249001ms 742.686454ms 743.077544ms 744.332414ms 744.511005ms 744.708662ms 744.742311ms 744.772033ms 745.094974ms 745.594494ms 746.116333ms 746.607092ms 747.071756ms 747.259837ms 747.260289ms 747.275759ms 747.477555ms 747.497669ms 747.500474ms 747.651177ms 747.66645ms 747.752738ms 747.778318ms 747.902336ms 748.028921ms 748.173234ms 748.235398ms 748.358823ms 748.51261ms 748.594541ms 748.614907ms 748.635609ms 748.714793ms 748.779316ms 748.807551ms 748.816186ms 748.860279ms 749.002641ms 749.024559ms 749.073681ms 749.104827ms 749.112165ms 749.130036ms 749.135639ms 749.14788ms 749.161764ms 749.229317ms 749.233335ms 749.285808ms 749.298083ms 749.334808ms 749.361408ms 749.379373ms 749.413947ms 749.441154ms 749.478069ms 749.487964ms 749.514808ms 749.525356ms 749.536327ms 749.554809ms 749.558132ms 749.570512ms 749.585444ms 749.6075ms 749.613234ms 749.657405ms 749.657828ms 749.698656ms 749.703734ms 749.754173ms 749.769164ms 749.79502ms 749.854238ms 749.870162ms 749.877473ms 749.903319ms 749.90889ms 749.953828ms 749.965584ms 749.980078ms 749.996651ms 750.089022ms 750.091726ms 750.121445ms 750.128429ms 750.155089ms 750.16953ms 750.172998ms 750.181109ms 750.237542ms 750.241342ms 750.241539ms 750.260694ms 750.354883ms 750.459476ms 750.522702ms 750.591829ms 750.592992ms 750.652638ms 750.685638ms 750.692058ms 750.710193ms 750.720526ms 750.764855ms 750.790617ms 750.833515ms 750.881463ms 750.886161ms 750.933121ms 750.941833ms 750.971782ms 750.974639ms 750.975328ms 750.997503ms 751.379691ms 751.444967ms 751.501381ms 751.508555ms 751.549779ms 751.554825ms 751.626374ms 751.646007ms 751.669641ms 751.674681ms 751.67592ms 751.704422ms 751.789472ms 751.838347ms 751.839278ms 751.878205ms 752.108123ms 752.154207ms 752.428391ms 752.483174ms 752.584234ms 753.212918ms 753.273936ms 753.410276ms 754.47342ms 754.628424ms 755.16695ms 755.536729ms 758.493915ms 760.980599ms 775.81131ms 785.856742ms]
    Jan  5 19:14:18.761: INFO: 50 %ile: 749.233335ms
    Jan  5 19:14:18.761: INFO: 90 %ile: 751.789472ms
    Jan  5 19:14:18.761: INFO: 99 %ile: 775.81131ms
    Jan  5 19:14:18.761: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:14:18.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Service endpoints latency
      tear down framework | framework.go:193
    STEP: Destroying namespace "svc-latency-1688" for this suite. 01/05/23 19:14:18.768
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:14:18.791
Jan  5 19:14:18.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:14:18.793
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:18.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:18.809
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 19:14:18.813
Jan  5 19:14:18.823: INFO: Waiting up to 5m0s for pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822" in namespace "emptydir-3068" to be "Succeeded or Failed"
Jan  5 19:14:18.826: INFO: Pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.879718ms
Jan  5 19:14:20.830: INFO: Pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007020747s
Jan  5 19:14:22.829: INFO: Pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005964993s
STEP: Saw pod success 01/05/23 19:14:22.829
Jan  5 19:14:22.829: INFO: Pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822" satisfied condition "Succeeded or Failed"
Jan  5 19:14:22.832: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-c0d900f3-7b15-4519-98f2-f85369efb822 container test-container: <nil>
STEP: delete the pod 01/05/23 19:14:22.85
Jan  5 19:14:22.863: INFO: Waiting for pod pod-c0d900f3-7b15-4519-98f2-f85369efb822 to disappear
Jan  5 19:14:22.865: INFO: Pod pod-c0d900f3-7b15-4519-98f2-f85369efb822 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:14:22.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-3068" for this suite. 01/05/23 19:14:22.869
------------------------------
• [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:127

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:14:18.791
    Jan  5 19:14:18.792: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:14:18.793
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:18.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:18.809
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:127
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 19:14:18.813
    Jan  5 19:14:18.823: INFO: Waiting up to 5m0s for pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822" in namespace "emptydir-3068" to be "Succeeded or Failed"
    Jan  5 19:14:18.826: INFO: Pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.879718ms
    Jan  5 19:14:20.830: INFO: Pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007020747s
    Jan  5 19:14:22.829: INFO: Pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005964993s
    STEP: Saw pod success 01/05/23 19:14:22.829
    Jan  5 19:14:22.829: INFO: Pod "pod-c0d900f3-7b15-4519-98f2-f85369efb822" satisfied condition "Succeeded or Failed"
    Jan  5 19:14:22.832: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-c0d900f3-7b15-4519-98f2-f85369efb822 container test-container: <nil>
    STEP: delete the pod 01/05/23 19:14:22.85
    Jan  5 19:14:22.863: INFO: Waiting for pod pod-c0d900f3-7b15-4519-98f2-f85369efb822 to disappear
    Jan  5 19:14:22.865: INFO: Pod pod-c0d900f3-7b15-4519-98f2-f85369efb822 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:14:22.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-3068" for this suite. 01/05/23 19:14:22.869
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:14:22.881
Jan  5 19:14:22.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename daemonsets 01/05/23 19:14:22.883
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:22.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:22.903
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432
Jan  5 19:14:22.926: INFO: Create a RollingUpdate DaemonSet
Jan  5 19:14:22.938: INFO: Check that daemon pods launch on every node of the cluster
Jan  5 19:14:22.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:14:22.946: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:14:23.958: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:14:23.958: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:14:24.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 19:14:24.954: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jan  5 19:14:24.954: INFO: Update the DaemonSet to trigger a rollout
Jan  5 19:14:24.967: INFO: Updating DaemonSet daemon-set
Jan  5 19:14:27.985: INFO: Roll back the DaemonSet before rollout is complete
Jan  5 19:14:28.002: INFO: Updating DaemonSet daemon-set
Jan  5 19:14:28.002: INFO: Make sure DaemonSet rollback is complete
Jan  5 19:14:28.008: INFO: Wrong image for pod: daemon-set-2rgl6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
Jan  5 19:14:28.008: INFO: Pod daemon-set-2rgl6 is not available
Jan  5 19:14:31.017: INFO: Pod daemon-set-l7tfj is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:14:31.031
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2089, will wait for the garbage collector to delete the pods 01/05/23 19:14:31.031
Jan  5 19:14:31.106: INFO: Deleting DaemonSet.extensions daemon-set took: 10.56396ms
Jan  5 19:14:31.209: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.430701ms
Jan  5 19:14:34.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:14:34.114: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 19:14:34.116: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60192"},"items":null}

Jan  5 19:14:34.119: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60192"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:14:34.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2089" for this suite. 01/05/23 19:14:34.132
------------------------------
• [SLOW TEST] [11.256 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:432

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:14:22.881
    Jan  5 19:14:22.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename daemonsets 01/05/23 19:14:22.883
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:22.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:22.903
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:432
    Jan  5 19:14:22.926: INFO: Create a RollingUpdate DaemonSet
    Jan  5 19:14:22.938: INFO: Check that daemon pods launch on every node of the cluster
    Jan  5 19:14:22.946: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:14:22.946: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:14:23.958: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:14:23.958: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:14:24.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 19:14:24.954: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jan  5 19:14:24.954: INFO: Update the DaemonSet to trigger a rollout
    Jan  5 19:14:24.967: INFO: Updating DaemonSet daemon-set
    Jan  5 19:14:27.985: INFO: Roll back the DaemonSet before rollout is complete
    Jan  5 19:14:28.002: INFO: Updating DaemonSet daemon-set
    Jan  5 19:14:28.002: INFO: Make sure DaemonSet rollback is complete
    Jan  5 19:14:28.008: INFO: Wrong image for pod: daemon-set-2rgl6. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
    Jan  5 19:14:28.008: INFO: Pod daemon-set-2rgl6 is not available
    Jan  5 19:14:31.017: INFO: Pod daemon-set-l7tfj is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:14:31.031
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2089, will wait for the garbage collector to delete the pods 01/05/23 19:14:31.031
    Jan  5 19:14:31.106: INFO: Deleting DaemonSet.extensions daemon-set took: 10.56396ms
    Jan  5 19:14:31.209: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.430701ms
    Jan  5 19:14:34.114: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:14:34.114: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 19:14:34.116: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"60192"},"items":null}

    Jan  5 19:14:34.119: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"60192"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:14:34.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2089" for this suite. 01/05/23 19:14:34.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:14:34.142
Jan  5 19:14:34.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-runtime 01/05/23 19:14:34.144
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:34.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:34.161
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:232
STEP: create the container 01/05/23 19:14:34.165
STEP: wait for the container to reach Succeeded 01/05/23 19:14:34.174
STEP: get the container status 01/05/23 19:14:38.19
STEP: the container should be terminated 01/05/23 19:14:38.192
STEP: the termination message should be set 01/05/23 19:14:38.192
Jan  5 19:14:38.192: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 01/05/23 19:14:38.192
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  5 19:14:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-7896" for this suite. 01/05/23 19:14:38.212
------------------------------
• [4.074 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:14:34.142
    Jan  5 19:14:34.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-runtime 01/05/23 19:14:34.144
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:34.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:34.161
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:232
    STEP: create the container 01/05/23 19:14:34.165
    STEP: wait for the container to reach Succeeded 01/05/23 19:14:34.174
    STEP: get the container status 01/05/23 19:14:38.19
    STEP: the container should be terminated 01/05/23 19:14:38.192
    STEP: the termination message should be set 01/05/23 19:14:38.192
    Jan  5 19:14:38.192: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 01/05/23 19:14:38.192
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:14:38.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-7896" for this suite. 01/05/23 19:14:38.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:14:38.225
Jan  5 19:14:38.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:14:38.226
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:38.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:38.241
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:14:38.255
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:14:38.428
STEP: Deploying the webhook pod 01/05/23 19:14:38.434
STEP: Wait for the deployment to be ready 01/05/23 19:14:38.446
Jan  5 19:14:38.453: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 19:14:40.462
STEP: Verifying the service has paired with the endpoint 01/05/23 19:14:40.472
Jan  5 19:14:41.473: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/05/23 19:14:41.476
STEP: create a configmap that should be updated by the webhook 01/05/23 19:14:41.497
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:14:41.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-400" for this suite. 01/05/23 19:14:41.554
STEP: Destroying namespace "webhook-400-markers" for this suite. 01/05/23 19:14:41.56
------------------------------
• [3.345 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:252

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:14:38.225
    Jan  5 19:14:38.225: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:14:38.226
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:38.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:38.241
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:14:38.255
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:14:38.428
    STEP: Deploying the webhook pod 01/05/23 19:14:38.434
    STEP: Wait for the deployment to be ready 01/05/23 19:14:38.446
    Jan  5 19:14:38.453: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 19:14:40.462
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:14:40.472
    Jan  5 19:14:41.473: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:252
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 01/05/23 19:14:41.476
    STEP: create a configmap that should be updated by the webhook 01/05/23 19:14:41.497
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:14:41.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-400" for this suite. 01/05/23 19:14:41.554
    STEP: Destroying namespace "webhook-400-markers" for this suite. 01/05/23 19:14:41.56
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:14:41.572
Jan  5 19:14:41.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 19:14:41.573
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:41.587
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:41.589
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 01/05/23 19:14:41.592
STEP: Creating RC which spawns configmap-volume pods 01/05/23 19:14:41.828
Jan  5 19:14:41.931: INFO: Pod name wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da: Found 2 pods out of 5
Jan  5 19:14:46.938: INFO: Pod name wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 19:14:46.938
Jan  5 19:14:46.938: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:14:46.941: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.805732ms
Jan  5 19:14:48.946: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007398061s
Jan  5 19:14:50.945: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006612547s
Jan  5 19:14:52.946: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00728112s
Jan  5 19:14:54.947: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008608225s
Jan  5 19:14:56.945: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Running", Reason="", readiness=true. Elapsed: 10.006944022s
Jan  5 19:14:56.945: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57" satisfied condition "running"
Jan  5 19:14:56.945: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-4vh77" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:14:56.948: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-4vh77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794945ms
Jan  5 19:14:58.953: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-4vh77": Phase="Running", Reason="", readiness=true. Elapsed: 2.007605299s
Jan  5 19:14:58.953: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-4vh77" satisfied condition "running"
Jan  5 19:14:58.953: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-vdkx2" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:14:58.956: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-vdkx2": Phase="Running", Reason="", readiness=true. Elapsed: 2.882959ms
Jan  5 19:14:58.956: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-vdkx2" satisfied condition "running"
Jan  5 19:14:58.956: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-ww9cr" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:14:58.959: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-ww9cr": Phase="Running", Reason="", readiness=true. Elapsed: 2.600093ms
Jan  5 19:14:58.959: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-ww9cr" satisfied condition "running"
Jan  5 19:14:58.959: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-x65dx" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:14:58.962: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-x65dx": Phase="Running", Reason="", readiness=true. Elapsed: 2.88938ms
Jan  5 19:14:58.962: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-x65dx" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da in namespace emptydir-wrapper-6766, will wait for the garbage collector to delete the pods 01/05/23 19:14:58.962
Jan  5 19:14:59.022: INFO: Deleting ReplicationController wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da took: 5.483862ms
Jan  5 19:14:59.122: INFO: Terminating ReplicationController wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da pods took: 100.204178ms
STEP: Creating RC which spawns configmap-volume pods 01/05/23 19:15:01.834
Jan  5 19:15:01.848: INFO: Pod name wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3: Found 0 pods out of 5
Jan  5 19:15:06.854: INFO: Pod name wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 19:15:06.854
Jan  5 19:15:06.854: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:06.858: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119152ms
Jan  5 19:15:08.861: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006815163s
Jan  5 19:15:10.863: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008499589s
Jan  5 19:15:12.863: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008098651s
Jan  5 19:15:14.863: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008361679s
Jan  5 19:15:16.861: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Running", Reason="", readiness=true. Elapsed: 10.006968046s
Jan  5 19:15:16.861: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4" satisfied condition "running"
Jan  5 19:15:16.862: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-9tnwh" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:16.865: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-9tnwh": Phase="Running", Reason="", readiness=true. Elapsed: 3.09519ms
Jan  5 19:15:16.865: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-9tnwh" satisfied condition "running"
Jan  5 19:15:16.865: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-khsx5" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:16.868: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-khsx5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.199949ms
Jan  5 19:15:18.872: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-khsx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007616409s
Jan  5 19:15:18.873: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-khsx5" satisfied condition "running"
Jan  5 19:15:18.873: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-r798l" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:18.876: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-r798l": Phase="Running", Reason="", readiness=true. Elapsed: 3.18701ms
Jan  5 19:15:18.877: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-r798l" satisfied condition "running"
Jan  5 19:15:18.877: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-v9bmr" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:18.880: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-v9bmr": Phase="Running", Reason="", readiness=true. Elapsed: 2.97301ms
Jan  5 19:15:18.881: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-v9bmr" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3 in namespace emptydir-wrapper-6766, will wait for the garbage collector to delete the pods 01/05/23 19:15:18.881
Jan  5 19:15:18.939: INFO: Deleting ReplicationController wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3 took: 4.634662ms
Jan  5 19:15:19.040: INFO: Terminating ReplicationController wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3 pods took: 101.109009ms
STEP: Creating RC which spawns configmap-volume pods 01/05/23 19:15:22.15
Jan  5 19:15:22.167: INFO: Pod name wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54: Found 0 pods out of 5
Jan  5 19:15:27.172: INFO: Pod name wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54: Found 5 pods out of 5
STEP: Ensuring each pod is running 01/05/23 19:15:27.173
Jan  5 19:15:27.173: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:27.176: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170888ms
Jan  5 19:15:29.180: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006955629s
Jan  5 19:15:31.180: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007607761s
Jan  5 19:15:33.181: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008307767s
Jan  5 19:15:35.181: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00783043s
Jan  5 19:15:37.181: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007764896s
Jan  5 19:15:39.180: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Running", Reason="", readiness=true. Elapsed: 12.006959696s
Jan  5 19:15:39.180: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8" satisfied condition "running"
Jan  5 19:15:39.180: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-9sg9l" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:39.183: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-9sg9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.968007ms
Jan  5 19:15:39.183: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-9sg9l" satisfied condition "running"
Jan  5 19:15:39.183: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-jvw9r" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:39.185: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-jvw9r": Phase="Running", Reason="", readiness=true. Elapsed: 2.355459ms
Jan  5 19:15:39.185: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-jvw9r" satisfied condition "running"
Jan  5 19:15:39.186: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-kfw75" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:39.189: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-kfw75": Phase="Running", Reason="", readiness=true. Elapsed: 3.158596ms
Jan  5 19:15:39.189: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-kfw75" satisfied condition "running"
Jan  5 19:15:39.189: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-r9xdh" in namespace "emptydir-wrapper-6766" to be "running"
Jan  5 19:15:39.192: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-r9xdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.663877ms
Jan  5 19:15:39.192: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-r9xdh" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54 in namespace emptydir-wrapper-6766, will wait for the garbage collector to delete the pods 01/05/23 19:15:39.193
Jan  5 19:15:39.251: INFO: Deleting ReplicationController wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54 took: 6.031084ms
Jan  5 19:15:39.352: INFO: Terminating ReplicationController wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54 pods took: 100.901617ms
STEP: Cleaning up the configMaps 01/05/23 19:15:42.953
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:15:43.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-6766" for this suite. 01/05/23 19:15:43.192
------------------------------
• [SLOW TEST] [61.627 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:14:41.572
    Jan  5 19:14:41.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 19:14:41.573
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:14:41.587
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:14:41.589
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 01/05/23 19:14:41.592
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 19:14:41.828
    Jan  5 19:14:41.931: INFO: Pod name wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da: Found 2 pods out of 5
    Jan  5 19:14:46.938: INFO: Pod name wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 19:14:46.938
    Jan  5 19:14:46.938: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:14:46.941: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.805732ms
    Jan  5 19:14:48.946: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007398061s
    Jan  5 19:14:50.945: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006612547s
    Jan  5 19:14:52.946: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 6.00728112s
    Jan  5 19:14:54.947: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008608225s
    Jan  5 19:14:56.945: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57": Phase="Running", Reason="", readiness=true. Elapsed: 10.006944022s
    Jan  5 19:14:56.945: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-24v57" satisfied condition "running"
    Jan  5 19:14:56.945: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-4vh77" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:14:56.948: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-4vh77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.794945ms
    Jan  5 19:14:58.953: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-4vh77": Phase="Running", Reason="", readiness=true. Elapsed: 2.007605299s
    Jan  5 19:14:58.953: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-4vh77" satisfied condition "running"
    Jan  5 19:14:58.953: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-vdkx2" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:14:58.956: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-vdkx2": Phase="Running", Reason="", readiness=true. Elapsed: 2.882959ms
    Jan  5 19:14:58.956: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-vdkx2" satisfied condition "running"
    Jan  5 19:14:58.956: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-ww9cr" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:14:58.959: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-ww9cr": Phase="Running", Reason="", readiness=true. Elapsed: 2.600093ms
    Jan  5 19:14:58.959: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-ww9cr" satisfied condition "running"
    Jan  5 19:14:58.959: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-x65dx" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:14:58.962: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-x65dx": Phase="Running", Reason="", readiness=true. Elapsed: 2.88938ms
    Jan  5 19:14:58.962: INFO: Pod "wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da-x65dx" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da in namespace emptydir-wrapper-6766, will wait for the garbage collector to delete the pods 01/05/23 19:14:58.962
    Jan  5 19:14:59.022: INFO: Deleting ReplicationController wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da took: 5.483862ms
    Jan  5 19:14:59.122: INFO: Terminating ReplicationController wrapped-volume-race-e3e16058-f6b1-404d-abc3-bac19f6980da pods took: 100.204178ms
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 19:15:01.834
    Jan  5 19:15:01.848: INFO: Pod name wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3: Found 0 pods out of 5
    Jan  5 19:15:06.854: INFO: Pod name wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 19:15:06.854
    Jan  5 19:15:06.854: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:06.858: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.119152ms
    Jan  5 19:15:08.861: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006815163s
    Jan  5 19:15:10.863: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008499589s
    Jan  5 19:15:12.863: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008098651s
    Jan  5 19:15:14.863: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008361679s
    Jan  5 19:15:16.861: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4": Phase="Running", Reason="", readiness=true. Elapsed: 10.006968046s
    Jan  5 19:15:16.861: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-4xxb4" satisfied condition "running"
    Jan  5 19:15:16.862: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-9tnwh" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:16.865: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-9tnwh": Phase="Running", Reason="", readiness=true. Elapsed: 3.09519ms
    Jan  5 19:15:16.865: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-9tnwh" satisfied condition "running"
    Jan  5 19:15:16.865: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-khsx5" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:16.868: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-khsx5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.199949ms
    Jan  5 19:15:18.872: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-khsx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.007616409s
    Jan  5 19:15:18.873: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-khsx5" satisfied condition "running"
    Jan  5 19:15:18.873: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-r798l" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:18.876: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-r798l": Phase="Running", Reason="", readiness=true. Elapsed: 3.18701ms
    Jan  5 19:15:18.877: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-r798l" satisfied condition "running"
    Jan  5 19:15:18.877: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-v9bmr" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:18.880: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-v9bmr": Phase="Running", Reason="", readiness=true. Elapsed: 2.97301ms
    Jan  5 19:15:18.881: INFO: Pod "wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3-v9bmr" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3 in namespace emptydir-wrapper-6766, will wait for the garbage collector to delete the pods 01/05/23 19:15:18.881
    Jan  5 19:15:18.939: INFO: Deleting ReplicationController wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3 took: 4.634662ms
    Jan  5 19:15:19.040: INFO: Terminating ReplicationController wrapped-volume-race-83cb6ef3-7e39-4925-a20d-ec75610428b3 pods took: 101.109009ms
    STEP: Creating RC which spawns configmap-volume pods 01/05/23 19:15:22.15
    Jan  5 19:15:22.167: INFO: Pod name wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54: Found 0 pods out of 5
    Jan  5 19:15:27.172: INFO: Pod name wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54: Found 5 pods out of 5
    STEP: Ensuring each pod is running 01/05/23 19:15:27.173
    Jan  5 19:15:27.173: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:27.176: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.170888ms
    Jan  5 19:15:29.180: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006955629s
    Jan  5 19:15:31.180: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007607761s
    Jan  5 19:15:33.181: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008307767s
    Jan  5 19:15:35.181: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.00783043s
    Jan  5 19:15:37.181: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007764896s
    Jan  5 19:15:39.180: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8": Phase="Running", Reason="", readiness=true. Elapsed: 12.006959696s
    Jan  5 19:15:39.180: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-6tvc8" satisfied condition "running"
    Jan  5 19:15:39.180: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-9sg9l" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:39.183: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-9sg9l": Phase="Running", Reason="", readiness=true. Elapsed: 2.968007ms
    Jan  5 19:15:39.183: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-9sg9l" satisfied condition "running"
    Jan  5 19:15:39.183: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-jvw9r" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:39.185: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-jvw9r": Phase="Running", Reason="", readiness=true. Elapsed: 2.355459ms
    Jan  5 19:15:39.185: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-jvw9r" satisfied condition "running"
    Jan  5 19:15:39.186: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-kfw75" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:39.189: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-kfw75": Phase="Running", Reason="", readiness=true. Elapsed: 3.158596ms
    Jan  5 19:15:39.189: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-kfw75" satisfied condition "running"
    Jan  5 19:15:39.189: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-r9xdh" in namespace "emptydir-wrapper-6766" to be "running"
    Jan  5 19:15:39.192: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-r9xdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.663877ms
    Jan  5 19:15:39.192: INFO: Pod "wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54-r9xdh" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54 in namespace emptydir-wrapper-6766, will wait for the garbage collector to delete the pods 01/05/23 19:15:39.193
    Jan  5 19:15:39.251: INFO: Deleting ReplicationController wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54 took: 6.031084ms
    Jan  5 19:15:39.352: INFO: Terminating ReplicationController wrapped-volume-race-b778a6b0-8652-46ca-82e3-d13966868c54 pods took: 100.901617ms
    STEP: Cleaning up the configMaps 01/05/23 19:15:42.953
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:15:43.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-6766" for this suite. 01/05/23 19:15:43.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:15:43.198
Jan  5 19:15:43.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename var-expansion 01/05/23 19:15:43.2
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:43.213
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:43.216
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73
STEP: Creating a pod to test substitution in container's command 01/05/23 19:15:43.22
Jan  5 19:15:43.228: INFO: Waiting up to 5m0s for pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1" in namespace "var-expansion-4437" to be "Succeeded or Failed"
Jan  5 19:15:43.232: INFO: Pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062532ms
Jan  5 19:15:45.238: INFO: Pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00976064s
Jan  5 19:15:47.236: INFO: Pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007443802s
STEP: Saw pod success 01/05/23 19:15:47.236
Jan  5 19:15:47.236: INFO: Pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1" satisfied condition "Succeeded or Failed"
Jan  5 19:15:47.239: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1 container dapi-container: <nil>
STEP: delete the pod 01/05/23 19:15:47.254
Jan  5 19:15:47.266: INFO: Waiting for pod var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1 to disappear
Jan  5 19:15:47.269: INFO: Pod var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  5 19:15:47.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-4437" for this suite. 01/05/23 19:15:47.273
------------------------------
• [4.081 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:15:43.198
    Jan  5 19:15:43.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename var-expansion 01/05/23 19:15:43.2
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:43.213
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:43.216
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:73
    STEP: Creating a pod to test substitution in container's command 01/05/23 19:15:43.22
    Jan  5 19:15:43.228: INFO: Waiting up to 5m0s for pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1" in namespace "var-expansion-4437" to be "Succeeded or Failed"
    Jan  5 19:15:43.232: INFO: Pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062532ms
    Jan  5 19:15:45.238: INFO: Pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.00976064s
    Jan  5 19:15:47.236: INFO: Pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007443802s
    STEP: Saw pod success 01/05/23 19:15:47.236
    Jan  5 19:15:47.236: INFO: Pod "var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1" satisfied condition "Succeeded or Failed"
    Jan  5 19:15:47.239: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 19:15:47.254
    Jan  5 19:15:47.266: INFO: Waiting for pod var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1 to disappear
    Jan  5 19:15:47.269: INFO: Pod var-expansion-1f7a1dc7-3e21-437e-b0cd-f2c978dee4b1 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:15:47.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-4437" for this suite. 01/05/23 19:15:47.273
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:15:47.282
Jan  5 19:15:47.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:15:47.283
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:47.297
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:47.303
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810
STEP: Creating ServiceAccount "e2e-sa-cvqwv"  01/05/23 19:15:47.306
Jan  5 19:15:47.310: INFO: AutomountServiceAccountToken: false
STEP: Updating ServiceAccount "e2e-sa-cvqwv"  01/05/23 19:15:47.31
Jan  5 19:15:47.316: INFO: AutomountServiceAccountToken: true
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  5 19:15:47.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-6935" for this suite. 01/05/23 19:15:47.32
------------------------------
• [0.043 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should update a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:15:47.282
    Jan  5 19:15:47.282: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:15:47.283
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:47.297
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:47.303
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should update a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:810
    STEP: Creating ServiceAccount "e2e-sa-cvqwv"  01/05/23 19:15:47.306
    Jan  5 19:15:47.310: INFO: AutomountServiceAccountToken: false
    STEP: Updating ServiceAccount "e2e-sa-cvqwv"  01/05/23 19:15:47.31
    Jan  5 19:15:47.316: INFO: AutomountServiceAccountToken: true
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:15:47.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-6935" for this suite. 01/05/23 19:15:47.32
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:15:47.33
Jan  5 19:15:47.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename endpointslice 01/05/23 19:15:47.331
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:47.342
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:47.345
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66
Jan  5 19:15:47.354: INFO: Endpoints addresses: [10.196.0.36] , ports: [443]
Jan  5 19:15:47.354: INFO: EndpointSlices addresses: [10.196.0.36] , ports: [443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  5 19:15:47.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-3848" for this suite. 01/05/23 19:15:47.358
------------------------------
• [0.034 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:15:47.33
    Jan  5 19:15:47.330: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename endpointslice 01/05/23 19:15:47.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:47.342
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:47.345
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:66
    Jan  5 19:15:47.354: INFO: Endpoints addresses: [10.196.0.36] , ports: [443]
    Jan  5 19:15:47.354: INFO: EndpointSlices addresses: [10.196.0.36] , ports: [443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:15:47.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-3848" for this suite. 01/05/23 19:15:47.358
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:15:47.369
Jan  5 19:15:47.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename var-expansion 01/05/23 19:15:47.37
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:47.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:47.385
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44
STEP: Creating a pod to test env composition 01/05/23 19:15:47.388
Jan  5 19:15:47.399: INFO: Waiting up to 5m0s for pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398" in namespace "var-expansion-2580" to be "Succeeded or Failed"
Jan  5 19:15:47.402: INFO: Pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.678373ms
Jan  5 19:15:49.406: INFO: Pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006990585s
Jan  5 19:15:51.405: INFO: Pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006202675s
STEP: Saw pod success 01/05/23 19:15:51.405
Jan  5 19:15:51.406: INFO: Pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398" satisfied condition "Succeeded or Failed"
Jan  5 19:15:51.408: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod var-expansion-0d490948-1442-4848-9433-a07c99b23398 container dapi-container: <nil>
STEP: delete the pod 01/05/23 19:15:51.414
Jan  5 19:15:51.425: INFO: Waiting for pod var-expansion-0d490948-1442-4848-9433-a07c99b23398 to disappear
Jan  5 19:15:51.427: INFO: Pod var-expansion-0d490948-1442-4848-9433-a07c99b23398 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  5 19:15:51.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2580" for this suite. 01/05/23 19:15:51.432
------------------------------
• [4.069 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:15:47.369
    Jan  5 19:15:47.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename var-expansion 01/05/23 19:15:47.37
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:47.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:47.385
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:44
    STEP: Creating a pod to test env composition 01/05/23 19:15:47.388
    Jan  5 19:15:47.399: INFO: Waiting up to 5m0s for pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398" in namespace "var-expansion-2580" to be "Succeeded or Failed"
    Jan  5 19:15:47.402: INFO: Pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.678373ms
    Jan  5 19:15:49.406: INFO: Pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006990585s
    Jan  5 19:15:51.405: INFO: Pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006202675s
    STEP: Saw pod success 01/05/23 19:15:51.405
    Jan  5 19:15:51.406: INFO: Pod "var-expansion-0d490948-1442-4848-9433-a07c99b23398" satisfied condition "Succeeded or Failed"
    Jan  5 19:15:51.408: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod var-expansion-0d490948-1442-4848-9433-a07c99b23398 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 19:15:51.414
    Jan  5 19:15:51.425: INFO: Waiting for pod var-expansion-0d490948-1442-4848-9433-a07c99b23398 to disappear
    Jan  5 19:15:51.427: INFO: Pod var-expansion-0d490948-1442-4848-9433-a07c99b23398 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:15:51.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2580" for this suite. 01/05/23 19:15:51.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:15:51.442
Jan  5 19:15:51.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubelet-test 01/05/23 19:15:51.443
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:51.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:51.458
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jan  5 19:15:51.471: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf" in namespace "kubelet-test-8423" to be "running and ready"
Jan  5 19:15:51.477: INFO: Pod "busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.814389ms
Jan  5 19:15:51.478: INFO: The phase of Pod busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:15:53.482: INFO: Pod "busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009904672s
Jan  5 19:15:53.482: INFO: The phase of Pod busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf is Running (Ready = true)
Jan  5 19:15:53.483: INFO: Pod "busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:15:53.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-8423" for this suite. 01/05/23 19:15:53.503
------------------------------
• [2.069 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:15:51.442
    Jan  5 19:15:51.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 19:15:51.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:51.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:51.458
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jan  5 19:15:51.471: INFO: Waiting up to 5m0s for pod "busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf" in namespace "kubelet-test-8423" to be "running and ready"
    Jan  5 19:15:51.477: INFO: Pod "busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.814389ms
    Jan  5 19:15:51.478: INFO: The phase of Pod busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:15:53.482: INFO: Pod "busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf": Phase="Running", Reason="", readiness=true. Elapsed: 2.009904672s
    Jan  5 19:15:53.482: INFO: The phase of Pod busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf is Running (Ready = true)
    Jan  5 19:15:53.483: INFO: Pod "busybox-readonly-fsc4f5c53d-38af-4695-80d9-f53271ee0daf" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:15:53.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-8423" for this suite. 01/05/23 19:15:53.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:15:53.521
Jan  5 19:15:53.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:15:53.528
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:53.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:53.549
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344
STEP: creating the pod 01/05/23 19:15:53.552
STEP: submitting the pod to kubernetes 01/05/23 19:15:53.552
Jan  5 19:15:53.563: INFO: Waiting up to 5m0s for pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698" in namespace "pods-5363" to be "running and ready"
Jan  5 19:15:53.566: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015709ms
Jan  5 19:15:53.567: INFO: The phase of Pod pod-update-d91817a7-e127-43f3-b17a-91464312a698 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:15:55.570: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006965099s
Jan  5 19:15:55.570: INFO: The phase of Pod pod-update-d91817a7-e127-43f3-b17a-91464312a698 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:15:57.571: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698": Phase="Running", Reason="", readiness=true. Elapsed: 4.007553287s
Jan  5 19:15:57.571: INFO: The phase of Pod pod-update-d91817a7-e127-43f3-b17a-91464312a698 is Running (Ready = true)
Jan  5 19:15:57.572: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 01/05/23 19:15:57.575
STEP: updating the pod 01/05/23 19:15:57.578
Jan  5 19:15:58.090: INFO: Successfully updated pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698"
Jan  5 19:15:58.090: INFO: Waiting up to 5m0s for pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698" in namespace "pods-5363" to be "running"
Jan  5 19:15:58.093: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698": Phase="Running", Reason="", readiness=true. Elapsed: 3.419026ms
Jan  5 19:15:58.094: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 01/05/23 19:15:58.094
Jan  5 19:15:58.097: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:15:58.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5363" for this suite. 01/05/23 19:15:58.101
------------------------------
• [4.586 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:344

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:15:53.521
    Jan  5 19:15:53.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:15:53.528
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:53.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:53.549
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:344
    STEP: creating the pod 01/05/23 19:15:53.552
    STEP: submitting the pod to kubernetes 01/05/23 19:15:53.552
    Jan  5 19:15:53.563: INFO: Waiting up to 5m0s for pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698" in namespace "pods-5363" to be "running and ready"
    Jan  5 19:15:53.566: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698": Phase="Pending", Reason="", readiness=false. Elapsed: 3.015709ms
    Jan  5 19:15:53.567: INFO: The phase of Pod pod-update-d91817a7-e127-43f3-b17a-91464312a698 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:15:55.570: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006965099s
    Jan  5 19:15:55.570: INFO: The phase of Pod pod-update-d91817a7-e127-43f3-b17a-91464312a698 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:15:57.571: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698": Phase="Running", Reason="", readiness=true. Elapsed: 4.007553287s
    Jan  5 19:15:57.571: INFO: The phase of Pod pod-update-d91817a7-e127-43f3-b17a-91464312a698 is Running (Ready = true)
    Jan  5 19:15:57.572: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 01/05/23 19:15:57.575
    STEP: updating the pod 01/05/23 19:15:57.578
    Jan  5 19:15:58.090: INFO: Successfully updated pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698"
    Jan  5 19:15:58.090: INFO: Waiting up to 5m0s for pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698" in namespace "pods-5363" to be "running"
    Jan  5 19:15:58.093: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698": Phase="Running", Reason="", readiness=true. Elapsed: 3.419026ms
    Jan  5 19:15:58.094: INFO: Pod "pod-update-d91817a7-e127-43f3-b17a-91464312a698" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 01/05/23 19:15:58.094
    Jan  5 19:15:58.097: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:15:58.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5363" for this suite. 01/05/23 19:15:58.101
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:15:58.111
Jan  5 19:15:58.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:15:58.112
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:58.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:58.126
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391
STEP: set up a multi version CRD 01/05/23 19:15:58.129
Jan  5 19:15:58.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: rename a version 01/05/23 19:16:02.016
STEP: check the new version name is served 01/05/23 19:16:02.03
STEP: check the old version name is removed 01/05/23 19:16:03.071
STEP: check the other version is not changed 01/05/23 19:16:03.843
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:16:06.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-3947" for this suite. 01/05/23 19:16:06.925
------------------------------
• [SLOW TEST] [8.819 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:391

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:15:58.111
    Jan  5 19:15:58.111: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:15:58.112
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:15:58.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:15:58.126
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:391
    STEP: set up a multi version CRD 01/05/23 19:15:58.129
    Jan  5 19:15:58.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: rename a version 01/05/23 19:16:02.016
    STEP: check the new version name is served 01/05/23 19:16:02.03
    STEP: check the old version name is removed 01/05/23 19:16:03.071
    STEP: check the other version is not changed 01/05/23 19:16:03.843
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:16:06.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-3947" for this suite. 01/05/23 19:16:06.925
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:16:06.933
Jan  5 19:16:06.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename cronjob 01/05/23 19:16:06.935
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:06.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:06.95
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 01/05/23 19:16:06.954
STEP: creating 01/05/23 19:16:06.954
STEP: getting 01/05/23 19:16:06.96
STEP: listing 01/05/23 19:16:06.963
STEP: watching 01/05/23 19:16:06.965
Jan  5 19:16:06.965: INFO: starting watch
STEP: cluster-wide listing 01/05/23 19:16:06.967
STEP: cluster-wide watching 01/05/23 19:16:06.969
Jan  5 19:16:06.969: INFO: starting watch
STEP: patching 01/05/23 19:16:06.97
STEP: updating 01/05/23 19:16:06.978
Jan  5 19:16:06.987: INFO: waiting for watch events with expected annotations
Jan  5 19:16:06.988: INFO: saw patched and updated annotations
STEP: patching /status 01/05/23 19:16:06.988
STEP: updating /status 01/05/23 19:16:06.996
STEP: get /status 01/05/23 19:16:07.002
STEP: deleting 01/05/23 19:16:07.004
STEP: deleting a collection 01/05/23 19:16:07.016
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  5 19:16:07.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-1892" for this suite. 01/05/23 19:16:07.028
------------------------------
• [0.100 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:16:06.933
    Jan  5 19:16:06.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename cronjob 01/05/23 19:16:06.935
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:06.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:06.95
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 01/05/23 19:16:06.954
    STEP: creating 01/05/23 19:16:06.954
    STEP: getting 01/05/23 19:16:06.96
    STEP: listing 01/05/23 19:16:06.963
    STEP: watching 01/05/23 19:16:06.965
    Jan  5 19:16:06.965: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 19:16:06.967
    STEP: cluster-wide watching 01/05/23 19:16:06.969
    Jan  5 19:16:06.969: INFO: starting watch
    STEP: patching 01/05/23 19:16:06.97
    STEP: updating 01/05/23 19:16:06.978
    Jan  5 19:16:06.987: INFO: waiting for watch events with expected annotations
    Jan  5 19:16:06.988: INFO: saw patched and updated annotations
    STEP: patching /status 01/05/23 19:16:06.988
    STEP: updating /status 01/05/23 19:16:06.996
    STEP: get /status 01/05/23 19:16:07.002
    STEP: deleting 01/05/23 19:16:07.004
    STEP: deleting a collection 01/05/23 19:16:07.016
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:16:07.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-1892" for this suite. 01/05/23 19:16:07.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:16:07.04
Jan  5 19:16:07.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:16:07.041
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:07.051
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:07.054
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88
STEP: Creating projection with secret that has name projected-secret-test-map-5ef813e7-f336-4627-8e23-6d84c78a55b7 01/05/23 19:16:07.057
STEP: Creating a pod to test consume secrets 01/05/23 19:16:07.063
Jan  5 19:16:07.070: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade" in namespace "projected-1081" to be "Succeeded or Failed"
Jan  5 19:16:07.074: INFO: Pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade": Phase="Pending", Reason="", readiness=false. Elapsed: 3.196828ms
Jan  5 19:16:09.077: INFO: Pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006636032s
Jan  5 19:16:11.077: INFO: Pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006664184s
STEP: Saw pod success 01/05/23 19:16:11.077
Jan  5 19:16:11.078: INFO: Pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade" satisfied condition "Succeeded or Failed"
Jan  5 19:16:11.080: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 19:16:11.094
Jan  5 19:16:11.110: INFO: Waiting for pod pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade to disappear
Jan  5 19:16:11.113: INFO: Pod pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  5 19:16:11.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1081" for this suite. 01/05/23 19:16:11.118
------------------------------
• [4.083 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:16:07.04
    Jan  5 19:16:07.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:16:07.041
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:07.051
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:07.054
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:88
    STEP: Creating projection with secret that has name projected-secret-test-map-5ef813e7-f336-4627-8e23-6d84c78a55b7 01/05/23 19:16:07.057
    STEP: Creating a pod to test consume secrets 01/05/23 19:16:07.063
    Jan  5 19:16:07.070: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade" in namespace "projected-1081" to be "Succeeded or Failed"
    Jan  5 19:16:07.074: INFO: Pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade": Phase="Pending", Reason="", readiness=false. Elapsed: 3.196828ms
    Jan  5 19:16:09.077: INFO: Pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006636032s
    Jan  5 19:16:11.077: INFO: Pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006664184s
    STEP: Saw pod success 01/05/23 19:16:11.077
    Jan  5 19:16:11.078: INFO: Pod "pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade" satisfied condition "Succeeded or Failed"
    Jan  5 19:16:11.080: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:16:11.094
    Jan  5 19:16:11.110: INFO: Waiting for pod pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade to disappear
    Jan  5 19:16:11.113: INFO: Pod pod-projected-secrets-9efefd25-031e-4aa7-82fb-2e58b6c3fade no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:16:11.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1081" for this suite. 01/05/23 19:16:11.118
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:16:11.126
Jan  5 19:16:11.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename watch 01/05/23 19:16:11.127
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:11.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:11.146
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 01/05/23 19:16:11.149
STEP: creating a new configmap 01/05/23 19:16:11.151
STEP: modifying the configmap once 01/05/23 19:16:11.155
STEP: closing the watch once it receives two notifications 01/05/23 19:16:11.164
Jan  5 19:16:11.164: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5556  558bd644-fc81-4a50-9d32-c6633406e4a2 61499 0 2023-01-05 19:16:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 19:16:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:16:11.164: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5556  558bd644-fc81-4a50-9d32-c6633406e4a2 61500 0 2023-01-05 19:16:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 19:16:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 01/05/23 19:16:11.165
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/05/23 19:16:11.173
STEP: deleting the configmap 01/05/23 19:16:11.175
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/05/23 19:16:11.185
Jan  5 19:16:11.185: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5556  558bd644-fc81-4a50-9d32-c6633406e4a2 61501 0 2023-01-05 19:16:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 19:16:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jan  5 19:16:11.186: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5556  558bd644-fc81-4a50-9d32-c6633406e4a2 61502 0 2023-01-05 19:16:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 19:16:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  5 19:16:11.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-5556" for this suite. 01/05/23 19:16:11.19
------------------------------
• [0.070 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:16:11.126
    Jan  5 19:16:11.126: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename watch 01/05/23 19:16:11.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:11.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:11.146
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 01/05/23 19:16:11.149
    STEP: creating a new configmap 01/05/23 19:16:11.151
    STEP: modifying the configmap once 01/05/23 19:16:11.155
    STEP: closing the watch once it receives two notifications 01/05/23 19:16:11.164
    Jan  5 19:16:11.164: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5556  558bd644-fc81-4a50-9d32-c6633406e4a2 61499 0 2023-01-05 19:16:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 19:16:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:16:11.164: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5556  558bd644-fc81-4a50-9d32-c6633406e4a2 61500 0 2023-01-05 19:16:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 19:16:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 01/05/23 19:16:11.165
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 01/05/23 19:16:11.173
    STEP: deleting the configmap 01/05/23 19:16:11.175
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 01/05/23 19:16:11.185
    Jan  5 19:16:11.185: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5556  558bd644-fc81-4a50-9d32-c6633406e4a2 61501 0 2023-01-05 19:16:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 19:16:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jan  5 19:16:11.186: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-5556  558bd644-fc81-4a50-9d32-c6633406e4a2 61502 0 2023-01-05 19:16:11 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-01-05 19:16:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:16:11.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-5556" for this suite. 01/05/23 19:16:11.19
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:16:11.196
Jan  5 19:16:11.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename endpointslice 01/05/23 19:16:11.199
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:11.212
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:11.216
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205
STEP: referencing a single matching pod 01/05/23 19:16:16.299
STEP: referencing matching pods with named port 01/05/23 19:16:21.308
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/05/23 19:16:26.318
STEP: recreating EndpointSlices after they've been deleted 01/05/23 19:16:31.324
Jan  5 19:16:31.338: INFO: EndpointSlice for Service endpointslice-2563/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  5 19:16:41.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2563" for this suite. 01/05/23 19:16:41.35
------------------------------
• [SLOW TEST] [30.159 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:205

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:16:11.196
    Jan  5 19:16:11.198: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename endpointslice 01/05/23 19:16:11.199
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:11.212
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:11.216
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:205
    STEP: referencing a single matching pod 01/05/23 19:16:16.299
    STEP: referencing matching pods with named port 01/05/23 19:16:21.308
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 01/05/23 19:16:26.318
    STEP: recreating EndpointSlices after they've been deleted 01/05/23 19:16:31.324
    Jan  5 19:16:31.338: INFO: EndpointSlice for Service endpointslice-2563/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:16:41.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2563" for this suite. 01/05/23 19:16:41.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:16:41.36
Jan  5 19:16:41.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename events 01/05/23 19:16:41.361
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:41.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:41.375
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 01/05/23 19:16:41.379
STEP: listing all events in all namespaces 01/05/23 19:16:41.384
STEP: patching the test event 01/05/23 19:16:41.389
STEP: fetching the test event 01/05/23 19:16:41.395
STEP: updating the test event 01/05/23 19:16:41.397
STEP: getting the test event 01/05/23 19:16:41.407
STEP: deleting the test event 01/05/23 19:16:41.41
STEP: listing all events in all namespaces 01/05/23 19:16:41.417
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan  5 19:16:41.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-7437" for this suite. 01/05/23 19:16:41.428
------------------------------
• [0.074 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:16:41.36
    Jan  5 19:16:41.360: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename events 01/05/23 19:16:41.361
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:41.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:41.375
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 01/05/23 19:16:41.379
    STEP: listing all events in all namespaces 01/05/23 19:16:41.384
    STEP: patching the test event 01/05/23 19:16:41.389
    STEP: fetching the test event 01/05/23 19:16:41.395
    STEP: updating the test event 01/05/23 19:16:41.397
    STEP: getting the test event 01/05/23 19:16:41.407
    STEP: deleting the test event 01/05/23 19:16:41.41
    STEP: listing all events in all namespaces 01/05/23 19:16:41.417
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:16:41.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-7437" for this suite. 01/05/23 19:16:41.428
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:16:41.434
Jan  5 19:16:41.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename job 01/05/23 19:16:41.436
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:41.449
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:41.452
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426
STEP: Creating a job 01/05/23 19:16:41.455
STEP: Ensuring job reaches completions 01/05/23 19:16:41.46
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  5 19:16:53.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-5481" for this suite. 01/05/23 19:16:53.468
------------------------------
• [SLOW TEST] [12.039 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:426

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:16:41.434
    Jan  5 19:16:41.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename job 01/05/23 19:16:41.436
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:41.449
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:41.452
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:426
    STEP: Creating a job 01/05/23 19:16:41.455
    STEP: Ensuring job reaches completions 01/05/23 19:16:41.46
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:16:53.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-5481" for this suite. 01/05/23 19:16:53.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:16:53.477
Jan  5 19:16:53.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename init-container 01/05/23 19:16:53.478
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:53.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:53.498
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255
STEP: creating the pod 01/05/23 19:16:53.504
Jan  5 19:16:53.504: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:16:57.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-3686" for this suite. 01/05/23 19:16:57.48
------------------------------
• [4.008 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:255

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:16:53.477
    Jan  5 19:16:53.477: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename init-container 01/05/23 19:16:53.478
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:53.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:53.498
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:255
    STEP: creating the pod 01/05/23 19:16:53.504
    Jan  5 19:16:53.504: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:16:57.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-3686" for this suite. 01/05/23 19:16:57.48
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:16:57.487
Jan  5 19:16:57.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-probe 01/05/23 19:16:57.489
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:57.499
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:57.502
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152
STEP: Creating pod busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0 in namespace container-probe-2913 01/05/23 19:16:57.506
Jan  5 19:16:57.513: INFO: Waiting up to 5m0s for pod "busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0" in namespace "container-probe-2913" to be "not pending"
Jan  5 19:16:57.516: INFO: Pod "busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.527805ms
Jan  5 19:16:59.520: INFO: Pod "busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00680604s
Jan  5 19:16:59.520: INFO: Pod "busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0" satisfied condition "not pending"
Jan  5 19:16:59.520: INFO: Started pod busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0 in namespace container-probe-2913
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 19:16:59.52
Jan  5 19:16:59.523: INFO: Initial restart count of pod busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0 is 0
STEP: deleting the pod 01/05/23 19:21:00.007
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:00.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2913" for this suite. 01/05/23 19:21:00.034
------------------------------
• [SLOW TEST] [242.575 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:16:57.487
    Jan  5 19:16:57.487: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-probe 01/05/23 19:16:57.489
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:16:57.499
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:16:57.502
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:152
    STEP: Creating pod busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0 in namespace container-probe-2913 01/05/23 19:16:57.506
    Jan  5 19:16:57.513: INFO: Waiting up to 5m0s for pod "busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0" in namespace "container-probe-2913" to be "not pending"
    Jan  5 19:16:57.516: INFO: Pod "busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.527805ms
    Jan  5 19:16:59.520: INFO: Pod "busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0": Phase="Running", Reason="", readiness=true. Elapsed: 2.00680604s
    Jan  5 19:16:59.520: INFO: Pod "busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0" satisfied condition "not pending"
    Jan  5 19:16:59.520: INFO: Started pod busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0 in namespace container-probe-2913
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 19:16:59.52
    Jan  5 19:16:59.523: INFO: Initial restart count of pod busybox-2be914ac-572b-4f9f-b1b6-6f4cca376fb0 is 0
    STEP: deleting the pod 01/05/23 19:21:00.007
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:00.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2913" for this suite. 01/05/23 19:21:00.034
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:00.062
Jan  5 19:21:00.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename ingressclass 01/05/23 19:21:00.064
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:00.099
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:00.103
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 01/05/23 19:21:00.108
STEP: getting /apis/networking.k8s.io 01/05/23 19:21:00.117
STEP: getting /apis/networking.k8s.iov1 01/05/23 19:21:00.121
STEP: creating 01/05/23 19:21:00.13
STEP: getting 01/05/23 19:21:00.18
STEP: listing 01/05/23 19:21:00.194
STEP: watching 01/05/23 19:21:00.197
Jan  5 19:21:00.197: INFO: starting watch
STEP: patching 01/05/23 19:21:00.198
STEP: updating 01/05/23 19:21:00.208
Jan  5 19:21:00.218: INFO: waiting for watch events with expected annotations
Jan  5 19:21:00.219: INFO: saw patched and updated annotations
STEP: deleting 01/05/23 19:21:00.219
STEP: deleting a collection 01/05/23 19:21:00.23
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:00.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] IngressClass API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] IngressClass API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] IngressClass API
  tear down framework | framework.go:193
STEP: Destroying namespace "ingressclass-2087" for this suite. 01/05/23 19:21:00.258
------------------------------
• [0.215 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:00.062
    Jan  5 19:21:00.062: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename ingressclass 01/05/23 19:21:00.064
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:00.099
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:00.103
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 01/05/23 19:21:00.108
    STEP: getting /apis/networking.k8s.io 01/05/23 19:21:00.117
    STEP: getting /apis/networking.k8s.iov1 01/05/23 19:21:00.121
    STEP: creating 01/05/23 19:21:00.13
    STEP: getting 01/05/23 19:21:00.18
    STEP: listing 01/05/23 19:21:00.194
    STEP: watching 01/05/23 19:21:00.197
    Jan  5 19:21:00.197: INFO: starting watch
    STEP: patching 01/05/23 19:21:00.198
    STEP: updating 01/05/23 19:21:00.208
    Jan  5 19:21:00.218: INFO: waiting for watch events with expected annotations
    Jan  5 19:21:00.219: INFO: saw patched and updated annotations
    STEP: deleting 01/05/23 19:21:00.219
    STEP: deleting a collection 01/05/23 19:21:00.23
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:00.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] IngressClass API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] IngressClass API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] IngressClass API
      tear down framework | framework.go:193
    STEP: Destroying namespace "ingressclass-2087" for this suite. 01/05/23 19:21:00.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:00.285
Jan  5 19:21:00.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:21:00.287
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:00.309
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:00.312
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1494
STEP: creating the pod 01/05/23 19:21:00.317
Jan  5 19:21:00.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 create -f -'
Jan  5 19:21:01.228: INFO: stderr: ""
Jan  5 19:21:01.228: INFO: stdout: "pod/pause created\n"
Jan  5 19:21:01.228: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jan  5 19:21:01.228: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-437" to be "running and ready"
Jan  5 19:21:01.231: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.946619ms
Jan  5 19:21:01.231: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'gke-gke-1-26-default-pool-05283374-16pz' to be 'Running' but was 'Pending'
Jan  5 19:21:03.234: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006306498s
Jan  5 19:21:03.234: INFO: Pod "pause" satisfied condition "running and ready"
Jan  5 19:21:03.234: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1509
STEP: adding the label testing-label with value testing-label-value to a pod 01/05/23 19:21:03.234
Jan  5 19:21:03.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 label pods pause testing-label=testing-label-value'
Jan  5 19:21:03.332: INFO: stderr: ""
Jan  5 19:21:03.332: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 01/05/23 19:21:03.332
Jan  5 19:21:03.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 get pod pause -L testing-label'
Jan  5 19:21:03.419: INFO: stderr: ""
Jan  5 19:21:03.419: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 01/05/23 19:21:03.419
Jan  5 19:21:03.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 label pods pause testing-label-'
Jan  5 19:21:03.509: INFO: stderr: ""
Jan  5 19:21:03.509: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 01/05/23 19:21:03.509
Jan  5 19:21:03.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 get pod pause -L testing-label'
Jan  5 19:21:03.582: INFO: stderr: ""
Jan  5 19:21:03.582: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1500
STEP: using delete to clean up resources 01/05/23 19:21:03.582
Jan  5 19:21:03.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 delete --grace-period=0 --force -f -'
Jan  5 19:21:03.664: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 19:21:03.664: INFO: stdout: "pod \"pause\" force deleted\n"
Jan  5 19:21:03.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 get rc,svc -l name=pause --no-headers'
Jan  5 19:21:03.747: INFO: stderr: "No resources found in kubectl-437 namespace.\n"
Jan  5 19:21:03.747: INFO: stdout: ""
Jan  5 19:21:03.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 19:21:03.822: INFO: stderr: ""
Jan  5 19:21:03.822: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:03.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-437" for this suite. 01/05/23 19:21:03.825
------------------------------
• [3.545 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1492
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1509

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:00.285
    Jan  5 19:21:00.286: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:21:00.287
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:00.309
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:00.312
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1494
    STEP: creating the pod 01/05/23 19:21:00.317
    Jan  5 19:21:00.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 create -f -'
    Jan  5 19:21:01.228: INFO: stderr: ""
    Jan  5 19:21:01.228: INFO: stdout: "pod/pause created\n"
    Jan  5 19:21:01.228: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jan  5 19:21:01.228: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-437" to be "running and ready"
    Jan  5 19:21:01.231: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.946619ms
    Jan  5 19:21:01.231: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'gke-gke-1-26-default-pool-05283374-16pz' to be 'Running' but was 'Pending'
    Jan  5 19:21:03.234: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006306498s
    Jan  5 19:21:03.234: INFO: Pod "pause" satisfied condition "running and ready"
    Jan  5 19:21:03.234: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1509
    STEP: adding the label testing-label with value testing-label-value to a pod 01/05/23 19:21:03.234
    Jan  5 19:21:03.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 label pods pause testing-label=testing-label-value'
    Jan  5 19:21:03.332: INFO: stderr: ""
    Jan  5 19:21:03.332: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 01/05/23 19:21:03.332
    Jan  5 19:21:03.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 get pod pause -L testing-label'
    Jan  5 19:21:03.419: INFO: stderr: ""
    Jan  5 19:21:03.419: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 01/05/23 19:21:03.419
    Jan  5 19:21:03.419: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 label pods pause testing-label-'
    Jan  5 19:21:03.509: INFO: stderr: ""
    Jan  5 19:21:03.509: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 01/05/23 19:21:03.509
    Jan  5 19:21:03.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 get pod pause -L testing-label'
    Jan  5 19:21:03.582: INFO: stderr: ""
    Jan  5 19:21:03.582: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1500
    STEP: using delete to clean up resources 01/05/23 19:21:03.582
    Jan  5 19:21:03.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 delete --grace-period=0 --force -f -'
    Jan  5 19:21:03.664: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 19:21:03.664: INFO: stdout: "pod \"pause\" force deleted\n"
    Jan  5 19:21:03.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 get rc,svc -l name=pause --no-headers'
    Jan  5 19:21:03.747: INFO: stderr: "No resources found in kubectl-437 namespace.\n"
    Jan  5 19:21:03.747: INFO: stdout: ""
    Jan  5 19:21:03.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-437 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 19:21:03.822: INFO: stderr: ""
    Jan  5 19:21:03.822: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:03.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-437" for this suite. 01/05/23 19:21:03.825
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:03.832
Jan  5 19:21:03.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:21:03.833
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:03.848
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:03.85
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227
STEP: Creating Pod 01/05/23 19:21:03.853
Jan  5 19:21:03.864: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6" in namespace "emptydir-4385" to be "running"
Jan  5 19:21:03.867: INFO: Pod "pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05915ms
Jan  5 19:21:05.870: INFO: Pod "pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6": Phase="Running", Reason="", readiness=false. Elapsed: 2.006253254s
Jan  5 19:21:05.870: INFO: Pod "pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6" satisfied condition "running"
STEP: Reading file content from the nginx-container 01/05/23 19:21:05.87
Jan  5 19:21:05.870: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4385 PodName:pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:21:05.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:21:05.871: INFO: ExecWithOptions: Clientset creation
Jan  5 19:21:05.871: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/emptydir-4385/pods/pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jan  5 19:21:05.945: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:05.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4385" for this suite. 01/05/23 19:21:05.951
------------------------------
• [2.125 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:227

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:03.832
    Jan  5 19:21:03.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:21:03.833
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:03.848
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:03.85
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:227
    STEP: Creating Pod 01/05/23 19:21:03.853
    Jan  5 19:21:03.864: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6" in namespace "emptydir-4385" to be "running"
    Jan  5 19:21:03.867: INFO: Pod "pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.05915ms
    Jan  5 19:21:05.870: INFO: Pod "pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6": Phase="Running", Reason="", readiness=false. Elapsed: 2.006253254s
    Jan  5 19:21:05.870: INFO: Pod "pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6" satisfied condition "running"
    STEP: Reading file content from the nginx-container 01/05/23 19:21:05.87
    Jan  5 19:21:05.870: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-4385 PodName:pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:21:05.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:21:05.871: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:21:05.871: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/emptydir-4385/pods/pod-sharedvolume-88eaeabd-c447-4abb-8ae9-4cd2d00176d6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jan  5 19:21:05.945: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:05.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4385" for this suite. 01/05/23 19:21:05.951
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:05.961
Jan  5 19:21:05.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:21:05.962
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:05.977
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:05.98
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477
STEP: creating a service externalname-service with the type=ExternalName in namespace services-9714 01/05/23 19:21:05.983
STEP: changing the ExternalName service to type=NodePort 01/05/23 19:21:05.997
STEP: creating replication controller externalname-service in namespace services-9714 01/05/23 19:21:06.015
I0105 19:21:06.023560      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9714, replica count: 2
I0105 19:21:09.074409      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 19:21:09.074: INFO: Creating new exec pod
Jan  5 19:21:09.084: INFO: Waiting up to 5m0s for pod "execpodv8h77" in namespace "services-9714" to be "running"
Jan  5 19:21:09.088: INFO: Pod "execpodv8h77": Phase="Pending", Reason="", readiness=false. Elapsed: 3.243273ms
Jan  5 19:21:11.092: INFO: Pod "execpodv8h77": Phase="Running", Reason="", readiness=true. Elapsed: 2.007134871s
Jan  5 19:21:11.092: INFO: Pod "execpodv8h77" satisfied condition "running"
Jan  5 19:21:12.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9714 exec execpodv8h77 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
Jan  5 19:21:12.356: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jan  5 19:21:12.356: INFO: stdout: ""
Jan  5 19:21:12.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9714 exec execpodv8h77 -- /bin/sh -x -c nc -v -z -w 2 10.20.7.55 80'
Jan  5 19:21:12.514: INFO: stderr: "+ nc -v -z -w 2 10.20.7.55 80\nConnection to 10.20.7.55 80 port [tcp/http] succeeded!\n"
Jan  5 19:21:12.514: INFO: stdout: ""
Jan  5 19:21:12.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9714 exec execpodv8h77 -- /bin/sh -x -c nc -v -z -w 2 10.196.0.37 31899'
Jan  5 19:21:12.660: INFO: stderr: "+ nc -v -z -w 2 10.196.0.37 31899\nConnection to 10.196.0.37 31899 port [tcp/*] succeeded!\n"
Jan  5 19:21:12.660: INFO: stdout: ""
Jan  5 19:21:12.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9714 exec execpodv8h77 -- /bin/sh -x -c nc -v -z -w 2 10.196.0.38 31899'
Jan  5 19:21:12.790: INFO: stderr: "+ nc -v -z -w 2 10.196.0.38 31899\nConnection to 10.196.0.38 31899 port [tcp/*] succeeded!\n"
Jan  5 19:21:12.790: INFO: stdout: ""
Jan  5 19:21:12.790: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:12.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9714" for this suite. 01/05/23 19:21:12.821
------------------------------
• [SLOW TEST] [6.866 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1477

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:05.961
    Jan  5 19:21:05.961: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:21:05.962
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:05.977
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:05.98
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1477
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-9714 01/05/23 19:21:05.983
    STEP: changing the ExternalName service to type=NodePort 01/05/23 19:21:05.997
    STEP: creating replication controller externalname-service in namespace services-9714 01/05/23 19:21:06.015
    I0105 19:21:06.023560      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-9714, replica count: 2
    I0105 19:21:09.074409      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 19:21:09.074: INFO: Creating new exec pod
    Jan  5 19:21:09.084: INFO: Waiting up to 5m0s for pod "execpodv8h77" in namespace "services-9714" to be "running"
    Jan  5 19:21:09.088: INFO: Pod "execpodv8h77": Phase="Pending", Reason="", readiness=false. Elapsed: 3.243273ms
    Jan  5 19:21:11.092: INFO: Pod "execpodv8h77": Phase="Running", Reason="", readiness=true. Elapsed: 2.007134871s
    Jan  5 19:21:11.092: INFO: Pod "execpodv8h77" satisfied condition "running"
    Jan  5 19:21:12.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9714 exec execpodv8h77 -- /bin/sh -x -c nc -v -z -w 2 externalname-service 80'
    Jan  5 19:21:12.356: INFO: stderr: "+ nc -v -z -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jan  5 19:21:12.356: INFO: stdout: ""
    Jan  5 19:21:12.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9714 exec execpodv8h77 -- /bin/sh -x -c nc -v -z -w 2 10.20.7.55 80'
    Jan  5 19:21:12.514: INFO: stderr: "+ nc -v -z -w 2 10.20.7.55 80\nConnection to 10.20.7.55 80 port [tcp/http] succeeded!\n"
    Jan  5 19:21:12.514: INFO: stdout: ""
    Jan  5 19:21:12.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9714 exec execpodv8h77 -- /bin/sh -x -c nc -v -z -w 2 10.196.0.37 31899'
    Jan  5 19:21:12.660: INFO: stderr: "+ nc -v -z -w 2 10.196.0.37 31899\nConnection to 10.196.0.37 31899 port [tcp/*] succeeded!\n"
    Jan  5 19:21:12.660: INFO: stdout: ""
    Jan  5 19:21:12.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9714 exec execpodv8h77 -- /bin/sh -x -c nc -v -z -w 2 10.196.0.38 31899'
    Jan  5 19:21:12.790: INFO: stderr: "+ nc -v -z -w 2 10.196.0.38 31899\nConnection to 10.196.0.38 31899 port [tcp/*] succeeded!\n"
    Jan  5 19:21:12.790: INFO: stdout: ""
    Jan  5 19:21:12.790: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:12.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9714" for this suite. 01/05/23 19:21:12.821
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:12.828
Jan  5 19:21:12.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:21:12.829
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:12.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:12.845
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157
STEP: Creating a pod to test emptydir volume type on node default medium 01/05/23 19:21:12.848
Jan  5 19:21:12.857: INFO: Waiting up to 5m0s for pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f" in namespace "emptydir-1307" to be "Succeeded or Failed"
Jan  5 19:21:12.863: INFO: Pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.325261ms
Jan  5 19:21:14.866: INFO: Pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00888264s
Jan  5 19:21:16.867: INFO: Pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009544219s
STEP: Saw pod success 01/05/23 19:21:16.867
Jan  5 19:21:16.867: INFO: Pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f" satisfied condition "Succeeded or Failed"
Jan  5 19:21:16.870: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f container test-container: <nil>
STEP: delete the pod 01/05/23 19:21:16.886
Jan  5 19:21:16.898: INFO: Waiting for pod pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f to disappear
Jan  5 19:21:16.901: INFO: Pod pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:16.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1307" for this suite. 01/05/23 19:21:16.906
------------------------------
• [4.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:12.828
    Jan  5 19:21:12.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:21:12.829
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:12.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:12.845
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:157
    STEP: Creating a pod to test emptydir volume type on node default medium 01/05/23 19:21:12.848
    Jan  5 19:21:12.857: INFO: Waiting up to 5m0s for pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f" in namespace "emptydir-1307" to be "Succeeded or Failed"
    Jan  5 19:21:12.863: INFO: Pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.325261ms
    Jan  5 19:21:14.866: INFO: Pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00888264s
    Jan  5 19:21:16.867: INFO: Pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009544219s
    STEP: Saw pod success 01/05/23 19:21:16.867
    Jan  5 19:21:16.867: INFO: Pod "pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f" satisfied condition "Succeeded or Failed"
    Jan  5 19:21:16.870: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f container test-container: <nil>
    STEP: delete the pod 01/05/23 19:21:16.886
    Jan  5 19:21:16.898: INFO: Waiting for pod pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f to disappear
    Jan  5 19:21:16.901: INFO: Pod pod-1bd45d3a-a9c6-4d0c-b54d-5bdbebb3367f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:16.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1307" for this suite. 01/05/23 19:21:16.906
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:16.912
Jan  5 19:21:16.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename runtimeclass 01/05/23 19:21:16.914
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:16.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:16.931
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jan  5 19:21:16.955: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2211 to be scheduled
Jan  5 19:21:16.962: INFO: 1 pods are not scheduled: [runtimeclass-2211/test-runtimeclass-runtimeclass-2211-preconfigured-handler-tzvt9(c3040fd4-00dc-44de-8b1a-9969e81f1c25)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:18.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2211" for this suite. 01/05/23 19:21:18.981
------------------------------
• [2.077 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:16.912
    Jan  5 19:21:16.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 19:21:16.914
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:16.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:16.931
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jan  5 19:21:16.955: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2211 to be scheduled
    Jan  5 19:21:16.962: INFO: 1 pods are not scheduled: [runtimeclass-2211/test-runtimeclass-runtimeclass-2211-preconfigured-handler-tzvt9(c3040fd4-00dc-44de-8b1a-9969e81f1c25)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:18.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2211" for this suite. 01/05/23 19:21:18.981
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:18.991
Jan  5 19:21:18.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replication-controller 01/05/23 19:21:18.992
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:19.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:19.009
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92
STEP: Given a Pod with a 'name' label pod-adoption is created 01/05/23 19:21:19.013
Jan  5 19:21:19.025: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-852" to be "running and ready"
Jan  5 19:21:19.031: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080005ms
Jan  5 19:21:19.031: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:21:21.035: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.009522217s
Jan  5 19:21:21.035: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jan  5 19:21:21.036: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 01/05/23 19:21:21.038
STEP: Then the orphan pod is adopted 01/05/23 19:21:21.043
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:22.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-852" for this suite. 01/05/23 19:21:22.054
------------------------------
• [3.069 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:18.991
    Jan  5 19:21:18.991: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replication-controller 01/05/23 19:21:18.992
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:19.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:19.009
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:92
    STEP: Given a Pod with a 'name' label pod-adoption is created 01/05/23 19:21:19.013
    Jan  5 19:21:19.025: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-852" to be "running and ready"
    Jan  5 19:21:19.031: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.080005ms
    Jan  5 19:21:19.031: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:21:21.035: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.009522217s
    Jan  5 19:21:21.035: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jan  5 19:21:21.036: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 01/05/23 19:21:21.038
    STEP: Then the orphan pod is adopted 01/05/23 19:21:21.043
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:22.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-852" for this suite. 01/05/23 19:21:22.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:22.06
Jan  5 19:21:22.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename proxy 01/05/23 19:21:22.061
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:22.072
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:22.076
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jan  5 19:21:22.079: INFO: Creating pod...
Jan  5 19:21:22.090: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-411" to be "running"
Jan  5 19:21:22.094: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536209ms
Jan  5 19:21:24.097: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007312081s
Jan  5 19:21:24.098: INFO: Pod "agnhost" satisfied condition "running"
Jan  5 19:21:24.098: INFO: Creating service...
Jan  5 19:21:24.111: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=DELETE
Jan  5 19:21:24.124: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 19:21:24.124: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=OPTIONS
Jan  5 19:21:24.128: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 19:21:24.128: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=PATCH
Jan  5 19:21:24.132: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 19:21:24.132: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=POST
Jan  5 19:21:24.136: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 19:21:24.136: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=PUT
Jan  5 19:21:24.141: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 19:21:24.141: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=DELETE
Jan  5 19:21:24.148: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 19:21:24.148: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jan  5 19:21:24.157: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 19:21:24.157: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=PATCH
Jan  5 19:21:24.161: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 19:21:24.161: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=POST
Jan  5 19:21:24.170: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 19:21:24.170: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=PUT
Jan  5 19:21:24.179: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 19:21:24.179: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=GET
Jan  5 19:21:24.181: INFO: http.Client request:GET StatusCode:301
Jan  5 19:21:24.181: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=GET
Jan  5 19:21:24.184: INFO: http.Client request:GET StatusCode:301
Jan  5 19:21:24.184: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=HEAD
Jan  5 19:21:24.186: INFO: http.Client request:HEAD StatusCode:301
Jan  5 19:21:24.186: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=HEAD
Jan  5 19:21:24.189: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:24.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-411" for this suite. 01/05/23 19:21:24.194
------------------------------
• [2.143 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:22.06
    Jan  5 19:21:22.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename proxy 01/05/23 19:21:22.061
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:22.072
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:22.076
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jan  5 19:21:22.079: INFO: Creating pod...
    Jan  5 19:21:22.090: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-411" to be "running"
    Jan  5 19:21:22.094: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.536209ms
    Jan  5 19:21:24.097: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007312081s
    Jan  5 19:21:24.098: INFO: Pod "agnhost" satisfied condition "running"
    Jan  5 19:21:24.098: INFO: Creating service...
    Jan  5 19:21:24.111: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=DELETE
    Jan  5 19:21:24.124: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 19:21:24.124: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=OPTIONS
    Jan  5 19:21:24.128: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 19:21:24.128: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=PATCH
    Jan  5 19:21:24.132: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 19:21:24.132: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=POST
    Jan  5 19:21:24.136: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 19:21:24.136: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=PUT
    Jan  5 19:21:24.141: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 19:21:24.141: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=DELETE
    Jan  5 19:21:24.148: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 19:21:24.148: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jan  5 19:21:24.157: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 19:21:24.157: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=PATCH
    Jan  5 19:21:24.161: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 19:21:24.161: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=POST
    Jan  5 19:21:24.170: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 19:21:24.170: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=PUT
    Jan  5 19:21:24.179: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 19:21:24.179: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=GET
    Jan  5 19:21:24.181: INFO: http.Client request:GET StatusCode:301
    Jan  5 19:21:24.181: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=GET
    Jan  5 19:21:24.184: INFO: http.Client request:GET StatusCode:301
    Jan  5 19:21:24.184: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/pods/agnhost/proxy?method=HEAD
    Jan  5 19:21:24.186: INFO: http.Client request:HEAD StatusCode:301
    Jan  5 19:21:24.186: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-411/services/e2e-proxy-test-service/proxy?method=HEAD
    Jan  5 19:21:24.189: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:24.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-411" for this suite. 01/05/23 19:21:24.194
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:24.206
Jan  5 19:21:24.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename gc 01/05/23 19:21:24.208
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:24.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:24.228
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 01/05/23 19:21:24.231
STEP: delete the rc 01/05/23 19:21:29.245
STEP: wait for all pods to be garbage collected 01/05/23 19:21:29.252
STEP: Gathering metrics 01/05/23 19:21:34.258
W0105 19:21:34.267858      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 19:21:34.267: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:34.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9512" for this suite. 01/05/23 19:21:34.271
------------------------------
• [SLOW TEST] [10.070 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:24.206
    Jan  5 19:21:24.206: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename gc 01/05/23 19:21:24.208
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:24.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:24.228
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 01/05/23 19:21:24.231
    STEP: delete the rc 01/05/23 19:21:29.245
    STEP: wait for all pods to be garbage collected 01/05/23 19:21:29.252
    STEP: Gathering metrics 01/05/23 19:21:34.258
    W0105 19:21:34.267858      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 19:21:34.267: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:34.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9512" for this suite. 01/05/23 19:21:34.271
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:34.276
Jan  5 19:21:34.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:21:34.278
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:34.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:34.293
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:21:34.307
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:21:34.746
STEP: Deploying the webhook pod 01/05/23 19:21:34.751
STEP: Wait for the deployment to be ready 01/05/23 19:21:34.762
Jan  5 19:21:34.774: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 19:21:36.783
STEP: Verifying the service has paired with the endpoint 01/05/23 19:21:36.794
Jan  5 19:21:37.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582
STEP: Listing all of the created validation webhooks 01/05/23 19:21:37.871
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:21:37.959
STEP: Deleting the collection of validation webhooks 01/05/23 19:21:38.015
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:21:38.056
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:38.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-6590" for this suite. 01/05/23 19:21:38.103
STEP: Destroying namespace "webhook-6590-markers" for this suite. 01/05/23 19:21:38.114
------------------------------
• [3.846 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:582

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:34.276
    Jan  5 19:21:34.277: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:21:34.278
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:34.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:34.293
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:21:34.307
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:21:34.746
    STEP: Deploying the webhook pod 01/05/23 19:21:34.751
    STEP: Wait for the deployment to be ready 01/05/23 19:21:34.762
    Jan  5 19:21:34.774: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 19:21:36.783
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:21:36.794
    Jan  5 19:21:37.795: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:582
    STEP: Listing all of the created validation webhooks 01/05/23 19:21:37.871
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:21:37.959
    STEP: Deleting the collection of validation webhooks 01/05/23 19:21:38.015
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:21:38.056
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:38.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-6590" for this suite. 01/05/23 19:21:38.103
    STEP: Destroying namespace "webhook-6590-markers" for this suite. 01/05/23 19:21:38.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:38.124
Jan  5 19:21:38.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:21:38.126
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:38.159
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:38.163
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160
STEP: Discovering how many secrets are in namespace by default 01/05/23 19:21:38.166
STEP: Counting existing ResourceQuota 01/05/23 19:21:43.168
STEP: Creating a ResourceQuota 01/05/23 19:21:48.172
STEP: Ensuring resource quota status is calculated 01/05/23 19:21:48.177
STEP: Creating a Secret 01/05/23 19:21:50.181
STEP: Ensuring resource quota status captures secret creation 01/05/23 19:21:50.195
STEP: Deleting a secret 01/05/23 19:21:52.199
STEP: Ensuring resource quota status released usage 01/05/23 19:21:52.205
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:21:54.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4928" for this suite. 01/05/23 19:21:54.213
------------------------------
• [SLOW TEST] [16.095 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:38.124
    Jan  5 19:21:38.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:21:38.126
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:38.159
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:38.163
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:160
    STEP: Discovering how many secrets are in namespace by default 01/05/23 19:21:38.166
    STEP: Counting existing ResourceQuota 01/05/23 19:21:43.168
    STEP: Creating a ResourceQuota 01/05/23 19:21:48.172
    STEP: Ensuring resource quota status is calculated 01/05/23 19:21:48.177
    STEP: Creating a Secret 01/05/23 19:21:50.181
    STEP: Ensuring resource quota status captures secret creation 01/05/23 19:21:50.195
    STEP: Deleting a secret 01/05/23 19:21:52.199
    STEP: Ensuring resource quota status released usage 01/05/23 19:21:52.205
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:21:54.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4928" for this suite. 01/05/23 19:21:54.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:21:54.223
Jan  5 19:21:54.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename cronjob 01/05/23 19:21:54.225
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:54.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:54.24
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 01/05/23 19:21:54.243
STEP: Ensuring a job is scheduled 01/05/23 19:21:54.25
STEP: Ensuring exactly one is scheduled 01/05/23 19:22:00.254
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 19:22:00.257
STEP: Ensuring no more jobs are scheduled 01/05/23 19:22:00.259
STEP: Removing cronjob 01/05/23 19:27:00.266
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:00.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-4469" for this suite. 01/05/23 19:27:00.292
------------------------------
• [SLOW TEST] [306.116 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:21:54.223
    Jan  5 19:21:54.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename cronjob 01/05/23 19:21:54.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:21:54.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:21:54.24
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 01/05/23 19:21:54.243
    STEP: Ensuring a job is scheduled 01/05/23 19:21:54.25
    STEP: Ensuring exactly one is scheduled 01/05/23 19:22:00.254
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 19:22:00.257
    STEP: Ensuring no more jobs are scheduled 01/05/23 19:22:00.259
    STEP: Removing cronjob 01/05/23 19:27:00.266
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:00.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-4469" for this suite. 01/05/23 19:27:00.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:00.34
Jan  5 19:27:00.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename dns 01/05/23 19:27:00.341
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:00.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:00.394
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 01/05/23 19:27:00.397
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8483.svc.cluster.local;sleep 1; done
 01/05/23 19:27:00.434
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8483.svc.cluster.local;sleep 1; done
 01/05/23 19:27:00.434
STEP: creating a pod to probe DNS 01/05/23 19:27:00.434
STEP: submitting the pod to kubernetes 01/05/23 19:27:00.434
Jan  5 19:27:00.463: INFO: Waiting up to 15m0s for pod "dns-test-e17fec8f-ee59-49bd-aede-906486256b27" in namespace "dns-8483" to be "running"
Jan  5 19:27:00.468: INFO: Pod "dns-test-e17fec8f-ee59-49bd-aede-906486256b27": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101137ms
Jan  5 19:27:02.471: INFO: Pod "dns-test-e17fec8f-ee59-49bd-aede-906486256b27": Phase="Running", Reason="", readiness=true. Elapsed: 2.00831747s
Jan  5 19:27:02.471: INFO: Pod "dns-test-e17fec8f-ee59-49bd-aede-906486256b27" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:27:02.471
STEP: looking for the results for each expected name from probers 01/05/23 19:27:02.474
Jan  5 19:27:02.483: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
Jan  5 19:27:02.486: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
Jan  5 19:27:02.490: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
Jan  5 19:27:02.494: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
Jan  5 19:27:02.499: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
Jan  5 19:27:02.504: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
Jan  5 19:27:02.507: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
Jan  5 19:27:02.512: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
Jan  5 19:27:02.512: INFO: Lookups using dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8483.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8483.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local jessie_udp@dns-test-service-2.dns-8483.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8483.svc.cluster.local]

Jan  5 19:27:07.551: INFO: DNS probes using dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27 succeeded

STEP: deleting the pod 01/05/23 19:27:07.552
STEP: deleting the test headless service 01/05/23 19:27:07.566
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:07.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-8483" for this suite. 01/05/23 19:27:07.603
------------------------------
• [SLOW TEST] [7.275 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:00.34
    Jan  5 19:27:00.340: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename dns 01/05/23 19:27:00.341
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:00.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:00.394
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 01/05/23 19:27:00.397
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8483.svc.cluster.local;sleep 1; done
     01/05/23 19:27:00.434
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8483.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8483.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8483.svc.cluster.local;sleep 1; done
     01/05/23 19:27:00.434
    STEP: creating a pod to probe DNS 01/05/23 19:27:00.434
    STEP: submitting the pod to kubernetes 01/05/23 19:27:00.434
    Jan  5 19:27:00.463: INFO: Waiting up to 15m0s for pod "dns-test-e17fec8f-ee59-49bd-aede-906486256b27" in namespace "dns-8483" to be "running"
    Jan  5 19:27:00.468: INFO: Pod "dns-test-e17fec8f-ee59-49bd-aede-906486256b27": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101137ms
    Jan  5 19:27:02.471: INFO: Pod "dns-test-e17fec8f-ee59-49bd-aede-906486256b27": Phase="Running", Reason="", readiness=true. Elapsed: 2.00831747s
    Jan  5 19:27:02.471: INFO: Pod "dns-test-e17fec8f-ee59-49bd-aede-906486256b27" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:27:02.471
    STEP: looking for the results for each expected name from probers 01/05/23 19:27:02.474
    Jan  5 19:27:02.483: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
    Jan  5 19:27:02.486: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
    Jan  5 19:27:02.490: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
    Jan  5 19:27:02.494: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
    Jan  5 19:27:02.499: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
    Jan  5 19:27:02.504: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
    Jan  5 19:27:02.507: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
    Jan  5 19:27:02.512: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8483.svc.cluster.local from pod dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27: the server could not find the requested resource (get pods dns-test-e17fec8f-ee59-49bd-aede-906486256b27)
    Jan  5 19:27:02.512: INFO: Lookups using dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8483.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8483.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8483.svc.cluster.local jessie_udp@dns-test-service-2.dns-8483.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8483.svc.cluster.local]

    Jan  5 19:27:07.551: INFO: DNS probes using dns-8483/dns-test-e17fec8f-ee59-49bd-aede-906486256b27 succeeded

    STEP: deleting the pod 01/05/23 19:27:07.552
    STEP: deleting the test headless service 01/05/23 19:27:07.566
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:07.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-8483" for this suite. 01/05/23 19:27:07.603
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:07.627
Jan  5 19:27:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 19:27:07.629
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:07.646
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:07.65
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:07.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-7361" for this suite. 01/05/23 19:27:07.705
------------------------------
• [0.084 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:504

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:07.627
    Jan  5 19:27:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 19:27:07.629
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:07.646
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:07.65
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:504
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:07.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-7361" for this suite. 01/05/23 19:27:07.705
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:07.716
Jan  5 19:27:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-webhook 01/05/23 19:27:07.718
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:07.732
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:07.735
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 01/05/23 19:27:07.739
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 19:27:08.335
STEP: Deploying the custom resource conversion webhook pod 01/05/23 19:27:08.341
STEP: Wait for the deployment to be ready 01/05/23 19:27:08.352
Jan  5 19:27:08.357: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 19:27:10.366
STEP: Verifying the service has paired with the endpoint 01/05/23 19:27:10.377
Jan  5 19:27:11.378: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jan  5 19:27:11.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Creating a v1 custom resource 01/05/23 19:27:13.982
STEP: Create a v2 custom resource 01/05/23 19:27:13.997
STEP: List CRs in v1 01/05/23 19:27:14.002
STEP: List CRs in v2 01/05/23 19:27:14.109
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:14.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-webhook-7395" for this suite. 01/05/23 19:27:14.672
------------------------------
• [SLOW TEST] [6.964 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:07.716
    Jan  5 19:27:07.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-webhook 01/05/23 19:27:07.718
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:07.732
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:07.735
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 01/05/23 19:27:07.739
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 01/05/23 19:27:08.335
    STEP: Deploying the custom resource conversion webhook pod 01/05/23 19:27:08.341
    STEP: Wait for the deployment to be ready 01/05/23 19:27:08.352
    Jan  5 19:27:08.357: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 19:27:10.366
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:27:10.377
    Jan  5 19:27:11.378: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jan  5 19:27:11.381: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Creating a v1 custom resource 01/05/23 19:27:13.982
    STEP: Create a v2 custom resource 01/05/23 19:27:13.997
    STEP: List CRs in v1 01/05/23 19:27:14.002
    STEP: List CRs in v2 01/05/23 19:27:14.109
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:14.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-webhook-7395" for this suite. 01/05/23 19:27:14.672
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:14.68
Jan  5 19:27:14.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:27:14.682
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:14.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:14.711
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207
STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 19:27:14.716
Jan  5 19:27:14.731: INFO: Waiting up to 5m0s for pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a" in namespace "emptydir-4128" to be "Succeeded or Failed"
Jan  5 19:27:14.734: INFO: Pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802559ms
Jan  5 19:27:16.737: INFO: Pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006134602s
Jan  5 19:27:18.737: INFO: Pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006230994s
STEP: Saw pod success 01/05/23 19:27:18.737
Jan  5 19:27:18.737: INFO: Pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a" satisfied condition "Succeeded or Failed"
Jan  5 19:27:18.740: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-7275843b-6312-4a29-977e-a476cfd0e55a container test-container: <nil>
STEP: delete the pod 01/05/23 19:27:18.758
Jan  5 19:27:18.769: INFO: Waiting for pod pod-7275843b-6312-4a29-977e-a476cfd0e55a to disappear
Jan  5 19:27:18.772: INFO: Pod pod-7275843b-6312-4a29-977e-a476cfd0e55a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:18.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4128" for this suite. 01/05/23 19:27:18.777
------------------------------
• [4.102 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:14.68
    Jan  5 19:27:14.680: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:27:14.682
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:14.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:14.711
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:207
    STEP: Creating a pod to test emptydir 0666 on node default medium 01/05/23 19:27:14.716
    Jan  5 19:27:14.731: INFO: Waiting up to 5m0s for pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a" in namespace "emptydir-4128" to be "Succeeded or Failed"
    Jan  5 19:27:14.734: INFO: Pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.802559ms
    Jan  5 19:27:16.737: INFO: Pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006134602s
    Jan  5 19:27:18.737: INFO: Pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006230994s
    STEP: Saw pod success 01/05/23 19:27:18.737
    Jan  5 19:27:18.737: INFO: Pod "pod-7275843b-6312-4a29-977e-a476cfd0e55a" satisfied condition "Succeeded or Failed"
    Jan  5 19:27:18.740: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-7275843b-6312-4a29-977e-a476cfd0e55a container test-container: <nil>
    STEP: delete the pod 01/05/23 19:27:18.758
    Jan  5 19:27:18.769: INFO: Waiting for pod pod-7275843b-6312-4a29-977e-a476cfd0e55a to disappear
    Jan  5 19:27:18.772: INFO: Pod pod-7275843b-6312-4a29-977e-a476cfd0e55a no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:18.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4128" for this suite. 01/05/23 19:27:18.777
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:18.789
Jan  5 19:27:18.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/05/23 19:27:18.791
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:18.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:18.806
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:31
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 01/05/23 19:27:18.809
STEP: Creating hostNetwork=false pod 01/05/23 19:27:18.809
Jan  5 19:27:18.819: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9178" to be "running and ready"
Jan  5 19:27:18.824: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359506ms
Jan  5 19:27:18.824: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:27:20.828: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008194552s
Jan  5 19:27:20.828: INFO: The phase of Pod test-pod is Running (Ready = true)
Jan  5 19:27:20.828: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 01/05/23 19:27:20.83
Jan  5 19:27:20.837: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9178" to be "running and ready"
Jan  5 19:27:20.839: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657944ms
Jan  5 19:27:20.840: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:27:22.844: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006870597s
Jan  5 19:27:22.844: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jan  5 19:27:22.844: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 01/05/23 19:27:22.846
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/05/23 19:27:22.846
Jan  5 19:27:22.846: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:22.847: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:22.847: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 19:27:22.915: INFO: Exec stderr: ""
Jan  5 19:27:22.916: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:22.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:22.916: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:22.916: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 19:27:22.981: INFO: Exec stderr: ""
Jan  5 19:27:22.981: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:22.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:22.982: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:22.982: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 19:27:23.040: INFO: Exec stderr: ""
Jan  5 19:27:23.040: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:23.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:23.042: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:23.042: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 19:27:23.100: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/05/23 19:27:23.1
Jan  5 19:27:23.101: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:23.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:23.102: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:23.102: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  5 19:27:23.163: INFO: Exec stderr: ""
Jan  5 19:27:23.163: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:23.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:23.164: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:23.164: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jan  5 19:27:23.222: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/05/23 19:27:23.222
Jan  5 19:27:23.223: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:23.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:23.224: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:23.224: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 19:27:23.283: INFO: Exec stderr: ""
Jan  5 19:27:23.283: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:23.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:23.285: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:23.285: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jan  5 19:27:23.354: INFO: Exec stderr: ""
Jan  5 19:27:23.354: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:23.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:23.355: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:23.355: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 19:27:23.412: INFO: Exec stderr: ""
Jan  5 19:27:23.412: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:27:23.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:27:23.413: INFO: ExecWithOptions: Clientset creation
Jan  5 19:27:23.414: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jan  5 19:27:23.477: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:23.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
  tear down framework | framework.go:193
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9178" for this suite. 01/05/23 19:27:23.482
------------------------------
• [4.698 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:18.789
    Jan  5 19:27:18.790: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 01/05/23 19:27:18.791
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:18.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:18.806
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:31
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 01/05/23 19:27:18.809
    STEP: Creating hostNetwork=false pod 01/05/23 19:27:18.809
    Jan  5 19:27:18.819: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-9178" to be "running and ready"
    Jan  5 19:27:18.824: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.359506ms
    Jan  5 19:27:18.824: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:27:20.828: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.008194552s
    Jan  5 19:27:20.828: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jan  5 19:27:20.828: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 01/05/23 19:27:20.83
    Jan  5 19:27:20.837: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-9178" to be "running and ready"
    Jan  5 19:27:20.839: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657944ms
    Jan  5 19:27:20.840: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:27:22.844: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006870597s
    Jan  5 19:27:22.844: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jan  5 19:27:22.844: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 01/05/23 19:27:22.846
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 01/05/23 19:27:22.846
    Jan  5 19:27:22.846: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:22.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:22.847: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:22.847: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 19:27:22.915: INFO: Exec stderr: ""
    Jan  5 19:27:22.916: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:22.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:22.916: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:22.916: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 19:27:22.981: INFO: Exec stderr: ""
    Jan  5 19:27:22.981: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:22.981: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:22.982: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:22.982: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 19:27:23.040: INFO: Exec stderr: ""
    Jan  5 19:27:23.040: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:23.041: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:23.042: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:23.042: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 19:27:23.100: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 01/05/23 19:27:23.1
    Jan  5 19:27:23.101: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:23.101: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:23.102: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:23.102: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  5 19:27:23.163: INFO: Exec stderr: ""
    Jan  5 19:27:23.163: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:23.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:23.164: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:23.164: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jan  5 19:27:23.222: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 01/05/23 19:27:23.222
    Jan  5 19:27:23.223: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:23.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:23.224: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:23.224: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 19:27:23.283: INFO: Exec stderr: ""
    Jan  5 19:27:23.283: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:23.283: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:23.285: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:23.285: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jan  5 19:27:23.354: INFO: Exec stderr: ""
    Jan  5 19:27:23.354: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:23.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:23.355: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:23.355: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 19:27:23.412: INFO: Exec stderr: ""
    Jan  5 19:27:23.412: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9178 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:27:23.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:27:23.413: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:27:23.414: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9178/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jan  5 19:27:23.477: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:23.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] KubeletManagedEtcHosts
      tear down framework | framework.go:193
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-9178" for this suite. 01/05/23 19:27:23.482
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:23.491
Jan  5 19:27:23.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:27:23.493
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:23.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:23.512
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1734
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1747
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/05/23 19:27:23.515
Jan  5 19:27:23.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-1799 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jan  5 19:27:23.605: INFO: stderr: ""
Jan  5 19:27:23.605: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 01/05/23 19:27:23.605
STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 19:27:28.661
Jan  5 19:27:28.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-1799 get pod e2e-test-httpd-pod -o json'
Jan  5 19:27:28.850: INFO: stderr: ""
Jan  5 19:27:28.850: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-05T19:27:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1799\",\n        \"resourceVersion\": \"68146\",\n        \"uid\": \"4a88cb5e-20e8-4afc-a025-d8a0a6fc63e5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-kj62q\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"gke-gke-1-26-default-pool-05283374-16pz\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-kj62q\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T19:27:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T19:27:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T19:27:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T19:27:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e041448c5c1ac8a5ec7c04ac02e5b0037ba491d1fa3412601ef0f6ab0f2edb08\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-05T19:27:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.196.0.39\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.16.2.8\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.16.2.8\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-05T19:27:23Z\"\n    }\n}\n"
STEP: replace the image in the pod 01/05/23 19:27:28.85
Jan  5 19:27:28.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-1799 replace -f -'
Jan  5 19:27:29.927: INFO: stderr: ""
Jan  5 19:27:29.927: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/05/23 19:27:29.927
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1738
Jan  5 19:27:29.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-1799 delete pods e2e-test-httpd-pod'
Jan  5 19:27:32.151: INFO: stderr: ""
Jan  5 19:27:32.151: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:32.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-1799" for this suite. 01/05/23 19:27:32.158
------------------------------
• [SLOW TEST] [8.690 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1731
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1747

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:23.491
    Jan  5 19:27:23.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:27:23.493
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:23.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:23.512
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1734
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1747
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 01/05/23 19:27:23.515
    Jan  5 19:27:23.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-1799 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jan  5 19:27:23.605: INFO: stderr: ""
    Jan  5 19:27:23.605: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 01/05/23 19:27:23.605
    STEP: verifying the pod e2e-test-httpd-pod was created 01/05/23 19:27:28.661
    Jan  5 19:27:28.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-1799 get pod e2e-test-httpd-pod -o json'
    Jan  5 19:27:28.850: INFO: stderr: ""
    Jan  5 19:27:28.850: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-01-05T19:27:23Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-1799\",\n        \"resourceVersion\": \"68146\",\n        \"uid\": \"4a88cb5e-20e8-4afc-a025-d8a0a6fc63e5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-kj62q\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"gke-gke-1-26-default-pool-05283374-16pz\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-kj62q\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T19:27:23Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T19:27:25Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T19:27:25Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-01-05T19:27:23Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://e041448c5c1ac8a5ec7c04ac02e5b0037ba491d1fa3412601ef0f6ab0f2edb08\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-01-05T19:27:24Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.196.0.39\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.16.2.8\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.16.2.8\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-01-05T19:27:23Z\"\n    }\n}\n"
    STEP: replace the image in the pod 01/05/23 19:27:28.85
    Jan  5 19:27:28.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-1799 replace -f -'
    Jan  5 19:27:29.927: INFO: stderr: ""
    Jan  5 19:27:29.927: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 01/05/23 19:27:29.927
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1738
    Jan  5 19:27:29.930: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-1799 delete pods e2e-test-httpd-pod'
    Jan  5 19:27:32.151: INFO: stderr: ""
    Jan  5 19:27:32.151: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:32.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-1799" for this suite. 01/05/23 19:27:32.158
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:32.183
Jan  5 19:27:32.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:27:32.184
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:32.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:32.199
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99
STEP: Creating configMap with name projected-configmap-test-volume-map-a61ea9b2-703a-4161-99f1-56860d0dc434 01/05/23 19:27:32.202
STEP: Creating a pod to test consume configMaps 01/05/23 19:27:32.206
Jan  5 19:27:32.217: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47" in namespace "projected-4966" to be "Succeeded or Failed"
Jan  5 19:27:32.221: INFO: Pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807604ms
Jan  5 19:27:34.224: INFO: Pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006730464s
Jan  5 19:27:36.225: INFO: Pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007385958s
STEP: Saw pod success 01/05/23 19:27:36.225
Jan  5 19:27:36.225: INFO: Pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47" satisfied condition "Succeeded or Failed"
Jan  5 19:27:36.227: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:27:36.234
Jan  5 19:27:36.246: INFO: Waiting for pod pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47 to disappear
Jan  5 19:27:36.251: INFO: Pod pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:36.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4966" for this suite. 01/05/23 19:27:36.255
------------------------------
• [4.079 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:32.183
    Jan  5 19:27:32.183: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:27:32.184
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:32.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:32.199
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:99
    STEP: Creating configMap with name projected-configmap-test-volume-map-a61ea9b2-703a-4161-99f1-56860d0dc434 01/05/23 19:27:32.202
    STEP: Creating a pod to test consume configMaps 01/05/23 19:27:32.206
    Jan  5 19:27:32.217: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47" in namespace "projected-4966" to be "Succeeded or Failed"
    Jan  5 19:27:32.221: INFO: Pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807604ms
    Jan  5 19:27:34.224: INFO: Pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006730464s
    Jan  5 19:27:36.225: INFO: Pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007385958s
    STEP: Saw pod success 01/05/23 19:27:36.225
    Jan  5 19:27:36.225: INFO: Pod "pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47" satisfied condition "Succeeded or Failed"
    Jan  5 19:27:36.227: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:27:36.234
    Jan  5 19:27:36.246: INFO: Waiting for pod pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47 to disappear
    Jan  5 19:27:36.251: INFO: Pod pod-projected-configmaps-05538f1b-d035-4f23-a46d-6572f6607c47 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:36.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4966" for this suite. 01/05/23 19:27:36.255
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:36.266
Jan  5 19:27:36.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 19:27:36.268
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:36.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:36.28
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89
STEP: Creating configMap with name configmap-test-volume-map-7c9df453-e2dc-4069-b40d-c8f8b02c0a18 01/05/23 19:27:36.283
STEP: Creating a pod to test consume configMaps 01/05/23 19:27:36.287
Jan  5 19:27:36.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385" in namespace "configmap-8831" to be "Succeeded or Failed"
Jan  5 19:27:36.303: INFO: Pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385": Phase="Pending", Reason="", readiness=false. Elapsed: 5.856822ms
Jan  5 19:27:38.306: INFO: Pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00913937s
Jan  5 19:27:40.307: INFO: Pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009954791s
STEP: Saw pod success 01/05/23 19:27:40.307
Jan  5 19:27:40.307: INFO: Pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385" satisfied condition "Succeeded or Failed"
Jan  5 19:27:40.310: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:27:40.317
Jan  5 19:27:40.333: INFO: Waiting for pod pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385 to disappear
Jan  5 19:27:40.339: INFO: Pod pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:40.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8831" for this suite. 01/05/23 19:27:40.345
------------------------------
• [4.085 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:36.266
    Jan  5 19:27:36.266: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 19:27:36.268
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:36.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:36.28
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:89
    STEP: Creating configMap with name configmap-test-volume-map-7c9df453-e2dc-4069-b40d-c8f8b02c0a18 01/05/23 19:27:36.283
    STEP: Creating a pod to test consume configMaps 01/05/23 19:27:36.287
    Jan  5 19:27:36.297: INFO: Waiting up to 5m0s for pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385" in namespace "configmap-8831" to be "Succeeded or Failed"
    Jan  5 19:27:36.303: INFO: Pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385": Phase="Pending", Reason="", readiness=false. Elapsed: 5.856822ms
    Jan  5 19:27:38.306: INFO: Pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00913937s
    Jan  5 19:27:40.307: INFO: Pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009954791s
    STEP: Saw pod success 01/05/23 19:27:40.307
    Jan  5 19:27:40.307: INFO: Pod "pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385" satisfied condition "Succeeded or Failed"
    Jan  5 19:27:40.310: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:27:40.317
    Jan  5 19:27:40.333: INFO: Waiting for pod pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385 to disappear
    Jan  5 19:27:40.339: INFO: Pod pod-configmaps-286f1c7b-3c33-4c57-bf65-069283a51385 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:40.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8831" for this suite. 01/05/23 19:27:40.345
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:40.36
Jan  5 19:27:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:27:40.363
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:40.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:40.378
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:27:40.384
Jan  5 19:27:40.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6" in namespace "projected-8287" to be "Succeeded or Failed"
Jan  5 19:27:40.401: INFO: Pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.038324ms
Jan  5 19:27:42.404: INFO: Pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006523134s
Jan  5 19:27:44.405: INFO: Pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007295288s
STEP: Saw pod success 01/05/23 19:27:44.405
Jan  5 19:27:44.405: INFO: Pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6" satisfied condition "Succeeded or Failed"
Jan  5 19:27:44.408: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6 container client-container: <nil>
STEP: delete the pod 01/05/23 19:27:44.414
Jan  5 19:27:44.426: INFO: Waiting for pod downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6 to disappear
Jan  5 19:27:44.429: INFO: Pod downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:44.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8287" for this suite. 01/05/23 19:27:44.434
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:40.36
    Jan  5 19:27:40.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:27:40.363
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:40.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:40.378
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:193
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:27:40.384
    Jan  5 19:27:40.398: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6" in namespace "projected-8287" to be "Succeeded or Failed"
    Jan  5 19:27:40.401: INFO: Pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.038324ms
    Jan  5 19:27:42.404: INFO: Pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006523134s
    Jan  5 19:27:44.405: INFO: Pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007295288s
    STEP: Saw pod success 01/05/23 19:27:44.405
    Jan  5 19:27:44.405: INFO: Pod "downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6" satisfied condition "Succeeded or Failed"
    Jan  5 19:27:44.408: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:27:44.414
    Jan  5 19:27:44.426: INFO: Waiting for pod downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6 to disappear
    Jan  5 19:27:44.429: INFO: Pod downwardapi-volume-5c80b2e7-09ae-4e12-962a-1036c18003b6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:44.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8287" for this suite. 01/05/23 19:27:44.434
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:44.448
Jan  5 19:27:44.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 19:27:44.449
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:44.464
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:44.467
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175
STEP: Creating configMap with name configmap-test-upd-f5343eb1-b896-432c-9d5d-abd920baef0d 01/05/23 19:27:44.472
STEP: Creating the pod 01/05/23 19:27:44.476
Jan  5 19:27:44.485: INFO: Waiting up to 5m0s for pod "pod-configmaps-320e31ee-c109-46b5-b4cc-7ef178b12ebf" in namespace "configmap-4146" to be "running"
Jan  5 19:27:44.489: INFO: Pod "pod-configmaps-320e31ee-c109-46b5-b4cc-7ef178b12ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.503271ms
Jan  5 19:27:46.494: INFO: Pod "pod-configmaps-320e31ee-c109-46b5-b4cc-7ef178b12ebf": Phase="Running", Reason="", readiness=false. Elapsed: 2.008313212s
Jan  5 19:27:46.494: INFO: Pod "pod-configmaps-320e31ee-c109-46b5-b4cc-7ef178b12ebf" satisfied condition "running"
STEP: Waiting for pod with text data 01/05/23 19:27:46.494
STEP: Waiting for pod with binary data 01/05/23 19:27:46.5
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:46.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4146" for this suite. 01/05/23 19:27:46.508
------------------------------
• [2.065 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:44.448
    Jan  5 19:27:44.448: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 19:27:44.449
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:44.464
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:44.467
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:175
    STEP: Creating configMap with name configmap-test-upd-f5343eb1-b896-432c-9d5d-abd920baef0d 01/05/23 19:27:44.472
    STEP: Creating the pod 01/05/23 19:27:44.476
    Jan  5 19:27:44.485: INFO: Waiting up to 5m0s for pod "pod-configmaps-320e31ee-c109-46b5-b4cc-7ef178b12ebf" in namespace "configmap-4146" to be "running"
    Jan  5 19:27:44.489: INFO: Pod "pod-configmaps-320e31ee-c109-46b5-b4cc-7ef178b12ebf": Phase="Pending", Reason="", readiness=false. Elapsed: 3.503271ms
    Jan  5 19:27:46.494: INFO: Pod "pod-configmaps-320e31ee-c109-46b5-b4cc-7ef178b12ebf": Phase="Running", Reason="", readiness=false. Elapsed: 2.008313212s
    Jan  5 19:27:46.494: INFO: Pod "pod-configmaps-320e31ee-c109-46b5-b4cc-7ef178b12ebf" satisfied condition "running"
    STEP: Waiting for pod with text data 01/05/23 19:27:46.494
    STEP: Waiting for pod with binary data 01/05/23 19:27:46.5
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:46.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4146" for this suite. 01/05/23 19:27:46.508
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:46.54
Jan  5 19:27:46.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:27:46.542
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:46.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:46.563
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69
Jan  5 19:27:46.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/05/23 19:27:48.186
Jan  5 19:27:48.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 create -f -'
Jan  5 19:27:48.991: INFO: stderr: ""
Jan  5 19:27:48.991: INFO: stdout: "e2e-test-crd-publish-openapi-7313-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  5 19:27:48.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 delete e2e-test-crd-publish-openapi-7313-crds test-foo'
Jan  5 19:27:49.084: INFO: stderr: ""
Jan  5 19:27:49.084: INFO: stdout: "e2e-test-crd-publish-openapi-7313-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jan  5 19:27:49.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 apply -f -'
Jan  5 19:27:49.372: INFO: stderr: ""
Jan  5 19:27:49.372: INFO: stdout: "e2e-test-crd-publish-openapi-7313-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jan  5 19:27:49.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 delete e2e-test-crd-publish-openapi-7313-crds test-foo'
Jan  5 19:27:49.469: INFO: stderr: ""
Jan  5 19:27:49.470: INFO: stdout: "e2e-test-crd-publish-openapi-7313-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/05/23 19:27:49.47
Jan  5 19:27:49.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 create -f -'
Jan  5 19:27:49.726: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/05/23 19:27:49.726
Jan  5 19:27:49.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 create -f -'
Jan  5 19:27:49.979: INFO: rc: 1
Jan  5 19:27:49.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 apply -f -'
Jan  5 19:27:50.247: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/05/23 19:27:50.247
Jan  5 19:27:50.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 create -f -'
Jan  5 19:27:50.513: INFO: rc: 1
Jan  5 19:27:50.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 apply -f -'
Jan  5 19:27:50.818: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 01/05/23 19:27:50.818
Jan  5 19:27:50.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds'
Jan  5 19:27:51.068: INFO: stderr: ""
Jan  5 19:27:51.068: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 01/05/23 19:27:51.069
Jan  5 19:27:51.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds.metadata'
Jan  5 19:27:51.319: INFO: stderr: ""
Jan  5 19:27:51.319: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jan  5 19:27:51.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds.spec'
Jan  5 19:27:51.609: INFO: stderr: ""
Jan  5 19:27:51.609: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jan  5 19:27:51.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds.spec.bars'
Jan  5 19:27:51.997: INFO: stderr: ""
Jan  5 19:27:51.997: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/05/23 19:27:51.998
Jan  5 19:27:51.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds.spec.bars2'
Jan  5 19:27:52.287: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:53.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-9630" for this suite. 01/05/23 19:27:53.851
------------------------------
• [SLOW TEST] [7.321 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:46.54
    Jan  5 19:27:46.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:27:46.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:46.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:46.563
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:69
    Jan  5 19:27:46.567: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 01/05/23 19:27:48.186
    Jan  5 19:27:48.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 create -f -'
    Jan  5 19:27:48.991: INFO: stderr: ""
    Jan  5 19:27:48.991: INFO: stdout: "e2e-test-crd-publish-openapi-7313-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  5 19:27:48.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 delete e2e-test-crd-publish-openapi-7313-crds test-foo'
    Jan  5 19:27:49.084: INFO: stderr: ""
    Jan  5 19:27:49.084: INFO: stdout: "e2e-test-crd-publish-openapi-7313-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jan  5 19:27:49.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 apply -f -'
    Jan  5 19:27:49.372: INFO: stderr: ""
    Jan  5 19:27:49.372: INFO: stdout: "e2e-test-crd-publish-openapi-7313-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jan  5 19:27:49.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 delete e2e-test-crd-publish-openapi-7313-crds test-foo'
    Jan  5 19:27:49.469: INFO: stderr: ""
    Jan  5 19:27:49.470: INFO: stdout: "e2e-test-crd-publish-openapi-7313-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 01/05/23 19:27:49.47
    Jan  5 19:27:49.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 create -f -'
    Jan  5 19:27:49.726: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 01/05/23 19:27:49.726
    Jan  5 19:27:49.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 create -f -'
    Jan  5 19:27:49.979: INFO: rc: 1
    Jan  5 19:27:49.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 apply -f -'
    Jan  5 19:27:50.247: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 01/05/23 19:27:50.247
    Jan  5 19:27:50.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 create -f -'
    Jan  5 19:27:50.513: INFO: rc: 1
    Jan  5 19:27:50.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 --namespace=crd-publish-openapi-9630 apply -f -'
    Jan  5 19:27:50.818: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 01/05/23 19:27:50.818
    Jan  5 19:27:50.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds'
    Jan  5 19:27:51.068: INFO: stderr: ""
    Jan  5 19:27:51.068: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 01/05/23 19:27:51.069
    Jan  5 19:27:51.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds.metadata'
    Jan  5 19:27:51.319: INFO: stderr: ""
    Jan  5 19:27:51.319: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jan  5 19:27:51.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds.spec'
    Jan  5 19:27:51.609: INFO: stderr: ""
    Jan  5 19:27:51.609: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jan  5 19:27:51.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds.spec.bars'
    Jan  5 19:27:51.997: INFO: stderr: ""
    Jan  5 19:27:51.997: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7313-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 01/05/23 19:27:51.998
    Jan  5 19:27:51.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-9630 explain e2e-test-crd-publish-openapi-7313-crds.spec.bars2'
    Jan  5 19:27:52.287: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:53.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-9630" for this suite. 01/05/23 19:27:53.851
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:53.867
Jan  5 19:27:53.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replication-controller 01/05/23 19:27:53.875
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:53.898
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:53.901
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110
STEP: creating a ReplicationController 01/05/23 19:27:53.909
STEP: waiting for RC to be added 01/05/23 19:27:53.915
STEP: waiting for available Replicas 01/05/23 19:27:53.915
STEP: patching ReplicationController 01/05/23 19:27:55.253
STEP: waiting for RC to be modified 01/05/23 19:27:55.263
STEP: patching ReplicationController status 01/05/23 19:27:55.263
STEP: waiting for RC to be modified 01/05/23 19:27:55.268
STEP: waiting for available Replicas 01/05/23 19:27:55.268
STEP: fetching ReplicationController status 01/05/23 19:27:55.275
STEP: patching ReplicationController scale 01/05/23 19:27:55.278
STEP: waiting for RC to be modified 01/05/23 19:27:55.284
STEP: waiting for ReplicationController's scale to be the max amount 01/05/23 19:27:55.284
STEP: fetching ReplicationController; ensuring that it's patched 01/05/23 19:27:56.405
STEP: updating ReplicationController status 01/05/23 19:27:56.408
STEP: waiting for RC to be modified 01/05/23 19:27:56.414
STEP: listing all ReplicationControllers 01/05/23 19:27:56.414
STEP: checking that ReplicationController has expected values 01/05/23 19:27:56.423
STEP: deleting ReplicationControllers by collection 01/05/23 19:27:56.423
STEP: waiting for ReplicationController to have a DELETED watchEvent 01/05/23 19:27:56.435
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  5 19:27:56.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-9070" for this suite. 01/05/23 19:27:56.526
------------------------------
• [2.666 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:53.867
    Jan  5 19:27:53.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replication-controller 01/05/23 19:27:53.875
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:53.898
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:53.901
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:110
    STEP: creating a ReplicationController 01/05/23 19:27:53.909
    STEP: waiting for RC to be added 01/05/23 19:27:53.915
    STEP: waiting for available Replicas 01/05/23 19:27:53.915
    STEP: patching ReplicationController 01/05/23 19:27:55.253
    STEP: waiting for RC to be modified 01/05/23 19:27:55.263
    STEP: patching ReplicationController status 01/05/23 19:27:55.263
    STEP: waiting for RC to be modified 01/05/23 19:27:55.268
    STEP: waiting for available Replicas 01/05/23 19:27:55.268
    STEP: fetching ReplicationController status 01/05/23 19:27:55.275
    STEP: patching ReplicationController scale 01/05/23 19:27:55.278
    STEP: waiting for RC to be modified 01/05/23 19:27:55.284
    STEP: waiting for ReplicationController's scale to be the max amount 01/05/23 19:27:55.284
    STEP: fetching ReplicationController; ensuring that it's patched 01/05/23 19:27:56.405
    STEP: updating ReplicationController status 01/05/23 19:27:56.408
    STEP: waiting for RC to be modified 01/05/23 19:27:56.414
    STEP: listing all ReplicationControllers 01/05/23 19:27:56.414
    STEP: checking that ReplicationController has expected values 01/05/23 19:27:56.423
    STEP: deleting ReplicationControllers by collection 01/05/23 19:27:56.423
    STEP: waiting for ReplicationController to have a DELETED watchEvent 01/05/23 19:27:56.435
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:27:56.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-9070" for this suite. 01/05/23 19:27:56.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:27:56.533
Jan  5 19:27:56.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename gc 01/05/23 19:27:56.534
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:56.547
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:56.551
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 01/05/23 19:27:56.558
STEP: create the rc2 01/05/23 19:27:56.568
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/05/23 19:28:01.625
STEP: delete the rc simpletest-rc-to-be-deleted 01/05/23 19:28:03.118
STEP: wait for the rc to be deleted 01/05/23 19:28:03.157
Jan  5 19:28:08.183: INFO: 68 pods remaining
Jan  5 19:28:08.321: INFO: 68 pods has nil DeletionTimestamp
Jan  5 19:28:08.348: INFO: 
STEP: Gathering metrics 01/05/23 19:28:13.187
W0105 19:28:13.413266      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 19:28:13.413: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  5 19:28:13.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-2sckg" in namespace "gc-9245"
Jan  5 19:28:13.426: INFO: Deleting pod "simpletest-rc-to-be-deleted-4d5bk" in namespace "gc-9245"
Jan  5 19:28:13.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pg4l" in namespace "gc-9245"
Jan  5 19:28:13.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q79s" in namespace "gc-9245"
Jan  5 19:28:13.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cn5d" in namespace "gc-9245"
Jan  5 19:28:13.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dghj" in namespace "gc-9245"
Jan  5 19:28:13.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lf8g" in namespace "gc-9245"
Jan  5 19:28:13.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qkjz" in namespace "gc-9245"
Jan  5 19:28:13.633: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xnj7" in namespace "gc-9245"
Jan  5 19:28:13.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-69gkm" in namespace "gc-9245"
Jan  5 19:28:13.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-75prl" in namespace "gc-9245"
Jan  5 19:28:13.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-76xw6" in namespace "gc-9245"
Jan  5 19:28:13.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-779hb" in namespace "gc-9245"
Jan  5 19:28:13.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gbw8" in namespace "gc-9245"
Jan  5 19:28:13.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qnnm" in namespace "gc-9245"
Jan  5 19:28:13.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zfmz" in namespace "gc-9245"
Jan  5 19:28:13.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-89959" in namespace "gc-9245"
Jan  5 19:28:14.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fcfc" in namespace "gc-9245"
Jan  5 19:28:14.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nsxj" in namespace "gc-9245"
Jan  5 19:28:14.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sqn8" in namespace "gc-9245"
Jan  5 19:28:14.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-95rx2" in namespace "gc-9245"
Jan  5 19:28:14.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-99fxk" in namespace "gc-9245"
Jan  5 19:28:14.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cdl4" in namespace "gc-9245"
Jan  5 19:28:14.256: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jjss" in namespace "gc-9245"
Jan  5 19:28:14.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pt8h" in namespace "gc-9245"
Jan  5 19:28:14.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sw85" in namespace "gc-9245"
Jan  5 19:28:14.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9khv" in namespace "gc-9245"
Jan  5 19:28:14.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-bb2k8" in namespace "gc-9245"
Jan  5 19:28:14.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsc8j" in namespace "gc-9245"
Jan  5 19:28:14.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-chzsl" in namespace "gc-9245"
Jan  5 19:28:14.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-clwcm" in namespace "gc-9245"
Jan  5 19:28:14.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqdxk" in namespace "gc-9245"
Jan  5 19:28:14.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcrl9" in namespace "gc-9245"
Jan  5 19:28:14.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-dltld" in namespace "gc-9245"
Jan  5 19:28:14.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwrjq" in namespace "gc-9245"
Jan  5 19:28:14.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmtzv" in namespace "gc-9245"
Jan  5 19:28:14.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvttt" in namespace "gc-9245"
Jan  5 19:28:14.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx5gt" in namespace "gc-9245"
Jan  5 19:28:14.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkg6l" in namespace "gc-9245"
Jan  5 19:28:14.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9lcb" in namespace "gc-9245"
Jan  5 19:28:14.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzvkw" in namespace "gc-9245"
Jan  5 19:28:15.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-j94vd" in namespace "gc-9245"
Jan  5 19:28:15.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgdjn" in namespace "gc-9245"
Jan  5 19:28:15.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgdkv" in namespace "gc-9245"
Jan  5 19:28:15.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-jpmzm" in namespace "gc-9245"
Jan  5 19:28:15.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqxrg" in namespace "gc-9245"
Jan  5 19:28:15.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-k29c6" in namespace "gc-9245"
Jan  5 19:28:15.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4p2l" in namespace "gc-9245"
Jan  5 19:28:15.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4xhc" in namespace "gc-9245"
Jan  5 19:28:15.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-kzhzs" in namespace "gc-9245"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  5 19:28:15.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-9245" for this suite. 01/05/23 19:28:15.604
------------------------------
• [SLOW TEST] [19.152 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:27:56.533
    Jan  5 19:27:56.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename gc 01/05/23 19:27:56.534
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:27:56.547
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:27:56.551
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 01/05/23 19:27:56.558
    STEP: create the rc2 01/05/23 19:27:56.568
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 01/05/23 19:28:01.625
    STEP: delete the rc simpletest-rc-to-be-deleted 01/05/23 19:28:03.118
    STEP: wait for the rc to be deleted 01/05/23 19:28:03.157
    Jan  5 19:28:08.183: INFO: 68 pods remaining
    Jan  5 19:28:08.321: INFO: 68 pods has nil DeletionTimestamp
    Jan  5 19:28:08.348: INFO: 
    STEP: Gathering metrics 01/05/23 19:28:13.187
    W0105 19:28:13.413266      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 19:28:13.413: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  5 19:28:13.413: INFO: Deleting pod "simpletest-rc-to-be-deleted-2sckg" in namespace "gc-9245"
    Jan  5 19:28:13.426: INFO: Deleting pod "simpletest-rc-to-be-deleted-4d5bk" in namespace "gc-9245"
    Jan  5 19:28:13.457: INFO: Deleting pod "simpletest-rc-to-be-deleted-4pg4l" in namespace "gc-9245"
    Jan  5 19:28:13.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-4q79s" in namespace "gc-9245"
    Jan  5 19:28:13.542: INFO: Deleting pod "simpletest-rc-to-be-deleted-5cn5d" in namespace "gc-9245"
    Jan  5 19:28:13.571: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dghj" in namespace "gc-9245"
    Jan  5 19:28:13.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-5lf8g" in namespace "gc-9245"
    Jan  5 19:28:13.615: INFO: Deleting pod "simpletest-rc-to-be-deleted-5qkjz" in namespace "gc-9245"
    Jan  5 19:28:13.633: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xnj7" in namespace "gc-9245"
    Jan  5 19:28:13.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-69gkm" in namespace "gc-9245"
    Jan  5 19:28:13.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-75prl" in namespace "gc-9245"
    Jan  5 19:28:13.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-76xw6" in namespace "gc-9245"
    Jan  5 19:28:13.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-779hb" in namespace "gc-9245"
    Jan  5 19:28:13.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-7gbw8" in namespace "gc-9245"
    Jan  5 19:28:13.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qnnm" in namespace "gc-9245"
    Jan  5 19:28:13.879: INFO: Deleting pod "simpletest-rc-to-be-deleted-7zfmz" in namespace "gc-9245"
    Jan  5 19:28:13.918: INFO: Deleting pod "simpletest-rc-to-be-deleted-89959" in namespace "gc-9245"
    Jan  5 19:28:14.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-8fcfc" in namespace "gc-9245"
    Jan  5 19:28:14.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-8nsxj" in namespace "gc-9245"
    Jan  5 19:28:14.061: INFO: Deleting pod "simpletest-rc-to-be-deleted-8sqn8" in namespace "gc-9245"
    Jan  5 19:28:14.099: INFO: Deleting pod "simpletest-rc-to-be-deleted-95rx2" in namespace "gc-9245"
    Jan  5 19:28:14.137: INFO: Deleting pod "simpletest-rc-to-be-deleted-99fxk" in namespace "gc-9245"
    Jan  5 19:28:14.241: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cdl4" in namespace "gc-9245"
    Jan  5 19:28:14.256: INFO: Deleting pod "simpletest-rc-to-be-deleted-9jjss" in namespace "gc-9245"
    Jan  5 19:28:14.289: INFO: Deleting pod "simpletest-rc-to-be-deleted-9pt8h" in namespace "gc-9245"
    Jan  5 19:28:14.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sw85" in namespace "gc-9245"
    Jan  5 19:28:14.380: INFO: Deleting pod "simpletest-rc-to-be-deleted-b9khv" in namespace "gc-9245"
    Jan  5 19:28:14.400: INFO: Deleting pod "simpletest-rc-to-be-deleted-bb2k8" in namespace "gc-9245"
    Jan  5 19:28:14.453: INFO: Deleting pod "simpletest-rc-to-be-deleted-bsc8j" in namespace "gc-9245"
    Jan  5 19:28:14.466: INFO: Deleting pod "simpletest-rc-to-be-deleted-chzsl" in namespace "gc-9245"
    Jan  5 19:28:14.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-clwcm" in namespace "gc-9245"
    Jan  5 19:28:14.490: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqdxk" in namespace "gc-9245"
    Jan  5 19:28:14.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-dcrl9" in namespace "gc-9245"
    Jan  5 19:28:14.610: INFO: Deleting pod "simpletest-rc-to-be-deleted-dltld" in namespace "gc-9245"
    Jan  5 19:28:14.655: INFO: Deleting pod "simpletest-rc-to-be-deleted-dwrjq" in namespace "gc-9245"
    Jan  5 19:28:14.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-fmtzv" in namespace "gc-9245"
    Jan  5 19:28:14.715: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvttt" in namespace "gc-9245"
    Jan  5 19:28:14.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx5gt" in namespace "gc-9245"
    Jan  5 19:28:14.854: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkg6l" in namespace "gc-9245"
    Jan  5 19:28:14.913: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9lcb" in namespace "gc-9245"
    Jan  5 19:28:14.975: INFO: Deleting pod "simpletest-rc-to-be-deleted-hzvkw" in namespace "gc-9245"
    Jan  5 19:28:15.011: INFO: Deleting pod "simpletest-rc-to-be-deleted-j94vd" in namespace "gc-9245"
    Jan  5 19:28:15.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgdjn" in namespace "gc-9245"
    Jan  5 19:28:15.173: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgdkv" in namespace "gc-9245"
    Jan  5 19:28:15.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-jpmzm" in namespace "gc-9245"
    Jan  5 19:28:15.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-jqxrg" in namespace "gc-9245"
    Jan  5 19:28:15.366: INFO: Deleting pod "simpletest-rc-to-be-deleted-k29c6" in namespace "gc-9245"
    Jan  5 19:28:15.476: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4p2l" in namespace "gc-9245"
    Jan  5 19:28:15.514: INFO: Deleting pod "simpletest-rc-to-be-deleted-k4xhc" in namespace "gc-9245"
    Jan  5 19:28:15.559: INFO: Deleting pod "simpletest-rc-to-be-deleted-kzhzs" in namespace "gc-9245"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:28:15.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-9245" for this suite. 01/05/23 19:28:15.604
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:28:15.691
Jan  5 19:28:15.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename security-context 01/05/23 19:28:15.693
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:28:15.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:28:15.768
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 19:28:15.775
Jan  5 19:28:15.790: INFO: Waiting up to 5m0s for pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e" in namespace "security-context-5318" to be "Succeeded or Failed"
Jan  5 19:28:15.798: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.248346ms
Jan  5 19:28:17.876: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085041025s
Jan  5 19:28:19.814: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022888348s
Jan  5 19:28:21.809: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018286665s
Jan  5 19:28:23.811: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019768276s
Jan  5 19:28:25.809: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018311305s
Jan  5 19:28:27.809: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.018476675s
STEP: Saw pod success 01/05/23 19:28:27.81
Jan  5 19:28:27.811: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e" satisfied condition "Succeeded or Failed"
Jan  5 19:28:27.814: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod security-context-b15deec8-2538-443b-89a8-78db8678fd4e container test-container: <nil>
STEP: delete the pod 01/05/23 19:28:27.821
Jan  5 19:28:27.833: INFO: Waiting for pod security-context-b15deec8-2538-443b-89a8-78db8678fd4e to disappear
Jan  5 19:28:27.838: INFO: Pod security-context-b15deec8-2538-443b-89a8-78db8678fd4e no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  5 19:28:27.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5318" for this suite. 01/05/23 19:28:27.845
------------------------------
• [SLOW TEST] [12.159 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:164

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:28:15.691
    Jan  5 19:28:15.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename security-context 01/05/23 19:28:15.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:28:15.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:28:15.768
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:164
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 19:28:15.775
    Jan  5 19:28:15.790: INFO: Waiting up to 5m0s for pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e" in namespace "security-context-5318" to be "Succeeded or Failed"
    Jan  5 19:28:15.798: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.248346ms
    Jan  5 19:28:17.876: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085041025s
    Jan  5 19:28:19.814: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022888348s
    Jan  5 19:28:21.809: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.018286665s
    Jan  5 19:28:23.811: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.019768276s
    Jan  5 19:28:25.809: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.018311305s
    Jan  5 19:28:27.809: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.018476675s
    STEP: Saw pod success 01/05/23 19:28:27.81
    Jan  5 19:28:27.811: INFO: Pod "security-context-b15deec8-2538-443b-89a8-78db8678fd4e" satisfied condition "Succeeded or Failed"
    Jan  5 19:28:27.814: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod security-context-b15deec8-2538-443b-89a8-78db8678fd4e container test-container: <nil>
    STEP: delete the pod 01/05/23 19:28:27.821
    Jan  5 19:28:27.833: INFO: Waiting for pod security-context-b15deec8-2538-443b-89a8-78db8678fd4e to disappear
    Jan  5 19:28:27.838: INFO: Pod security-context-b15deec8-2538-443b-89a8-78db8678fd4e no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:28:27.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5318" for this suite. 01/05/23 19:28:27.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:28:27.863
Jan  5 19:28:27.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename job 01/05/23 19:28:27.866
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:28:27.879
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:28:27.884
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366
STEP: Creating Indexed job 01/05/23 19:28:27.888
STEP: Ensuring job reaches completions 01/05/23 19:28:27.894
STEP: Ensuring pods with index for job exist 01/05/23 19:28:37.901
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  5 19:28:37.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4750" for this suite. 01/05/23 19:28:37.907
------------------------------
• [SLOW TEST] [10.049 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:28:27.863
    Jan  5 19:28:27.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename job 01/05/23 19:28:27.866
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:28:27.879
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:28:27.884
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:366
    STEP: Creating Indexed job 01/05/23 19:28:27.888
    STEP: Ensuring job reaches completions 01/05/23 19:28:27.894
    STEP: Ensuring pods with index for job exist 01/05/23 19:28:37.901
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:28:37.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4750" for this suite. 01/05/23 19:28:37.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:28:37.915
Jan  5 19:28:37.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:28:37.917
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:28:37.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:28:37.932
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:28:37.945
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:28:38.946
STEP: Deploying the webhook pod 01/05/23 19:28:38.951
STEP: Wait for the deployment to be ready 01/05/23 19:28:38.963
Jan  5 19:28:38.967: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 19:28:40.977
STEP: Verifying the service has paired with the endpoint 01/05/23 19:28:40.994
Jan  5 19:28:41.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381
STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/05/23 19:28:41.999
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 19:28:41.999
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/05/23 19:28:42.021
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/05/23 19:28:43.031
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 19:28:43.032
STEP: Having no error when timeout is longer than webhook latency 01/05/23 19:28:44.061
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 19:28:44.062
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/05/23 19:28:49.097
STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 19:28:49.098
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:28:54.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-641" for this suite. 01/05/23 19:28:54.182
STEP: Destroying namespace "webhook-641-markers" for this suite. 01/05/23 19:28:54.195
------------------------------
• [SLOW TEST] [16.294 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:381

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:28:37.915
    Jan  5 19:28:37.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:28:37.917
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:28:37.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:28:37.932
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:28:37.945
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:28:38.946
    STEP: Deploying the webhook pod 01/05/23 19:28:38.951
    STEP: Wait for the deployment to be ready 01/05/23 19:28:38.963
    Jan  5 19:28:38.967: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 19:28:40.977
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:28:40.994
    Jan  5 19:28:41.995: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:381
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 01/05/23 19:28:41.999
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 19:28:41.999
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 01/05/23 19:28:42.021
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 01/05/23 19:28:43.031
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 19:28:43.032
    STEP: Having no error when timeout is longer than webhook latency 01/05/23 19:28:44.061
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 19:28:44.062
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 01/05/23 19:28:49.097
    STEP: Registering slow webhook via the AdmissionRegistration API 01/05/23 19:28:49.098
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:28:54.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-641" for this suite. 01/05/23 19:28:54.182
    STEP: Destroying namespace "webhook-641-markers" for this suite. 01/05/23 19:28:54.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:28:54.216
Jan  5 19:28:54.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:28:54.217
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:28:54.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:28:54.24
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531
Jan  5 19:28:54.258: INFO: created pod
Jan  5 19:28:54.258: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1005" to be "Succeeded or Failed"
Jan  5 19:28:54.261: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.53438ms
Jan  5 19:28:56.264: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005762315s
Jan  5 19:28:58.264: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005306468s
STEP: Saw pod success 01/05/23 19:28:58.264
Jan  5 19:28:58.264: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jan  5 19:29:28.264: INFO: polling logs
Jan  5 19:29:28.274: INFO: Pod logs: 
I0105 19:28:55.179901       1 log.go:198] OK: Got token
I0105 19:28:55.179991       1 log.go:198] validating with in-cluster discovery
I0105 19:28:55.180425       1 log.go:198] OK: got issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26
I0105 19:28:55.180515       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26", Subject:"system:serviceaccount:svcaccounts-1005:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672947534, NotBefore:1672946934, IssuedAt:1672946934, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1005", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4d1f6092-caf4-4df3-b6ff-ad74632a2b4b"}}}
I0105 19:28:55.208847       1 log.go:198] failed to validate with in-cluster discovery: Get "https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26/.well-known/openid-configuration": x509: certificate signed by unknown authority
I0105 19:28:55.208867       1 log.go:198] falling back to validating with external discovery
I0105 19:28:55.209113       1 log.go:198] OK: got issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26
I0105 19:28:55.209223       1 log.go:198] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26", Subject:"system:serviceaccount:svcaccounts-1005:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672947534, NotBefore:1672946934, IssuedAt:1672946934, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1005", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4d1f6092-caf4-4df3-b6ff-ad74632a2b4b"}}}
I0105 19:28:55.279594       1 log.go:198] OK: Constructed OIDC provider for issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26
I0105 19:28:55.296427       1 log.go:198] OK: Validated signature on JWT
I0105 19:28:55.296650       1 log.go:198] OK: Got valid claims from token!
I0105 19:28:55.296745       1 log.go:198] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26", Subject:"system:serviceaccount:svcaccounts-1005:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672947534, NotBefore:1672946934, IssuedAt:1672946934, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1005", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4d1f6092-caf4-4df3-b6ff-ad74632a2b4b"}}}

Jan  5 19:29:28.274: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  5 19:29:28.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-1005" for this suite. 01/05/23 19:29:28.283
------------------------------
• [SLOW TEST] [34.072 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:28:54.216
    Jan  5 19:28:54.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:28:54.217
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:28:54.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:28:54.24
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:531
    Jan  5 19:28:54.258: INFO: created pod
    Jan  5 19:28:54.258: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-1005" to be "Succeeded or Failed"
    Jan  5 19:28:54.261: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.53438ms
    Jan  5 19:28:56.264: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005762315s
    Jan  5 19:28:58.264: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005306468s
    STEP: Saw pod success 01/05/23 19:28:58.264
    Jan  5 19:28:58.264: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jan  5 19:29:28.264: INFO: polling logs
    Jan  5 19:29:28.274: INFO: Pod logs: 
    I0105 19:28:55.179901       1 log.go:198] OK: Got token
    I0105 19:28:55.179991       1 log.go:198] validating with in-cluster discovery
    I0105 19:28:55.180425       1 log.go:198] OK: got issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26
    I0105 19:28:55.180515       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26", Subject:"system:serviceaccount:svcaccounts-1005:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672947534, NotBefore:1672946934, IssuedAt:1672946934, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1005", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4d1f6092-caf4-4df3-b6ff-ad74632a2b4b"}}}
    I0105 19:28:55.208847       1 log.go:198] failed to validate with in-cluster discovery: Get "https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26/.well-known/openid-configuration": x509: certificate signed by unknown authority
    I0105 19:28:55.208867       1 log.go:198] falling back to validating with external discovery
    I0105 19:28:55.209113       1 log.go:198] OK: got issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26
    I0105 19:28:55.209223       1 log.go:198] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26", Subject:"system:serviceaccount:svcaccounts-1005:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672947534, NotBefore:1672946934, IssuedAt:1672946934, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1005", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4d1f6092-caf4-4df3-b6ff-ad74632a2b4b"}}}
    I0105 19:28:55.279594       1 log.go:198] OK: Constructed OIDC provider for issuer https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26
    I0105 19:28:55.296427       1 log.go:198] OK: Validated signature on JWT
    I0105 19:28:55.296650       1 log.go:198] OK: Got valid claims from token!
    I0105 19:28:55.296745       1 log.go:198] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://container.googleapis.com/v1/projects/liggitt-gke-dev/locations/us-east7-c/clusters/gke-1-26", Subject:"system:serviceaccount:svcaccounts-1005:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1672947534, NotBefore:1672946934, IssuedAt:1672946934, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1005", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"4d1f6092-caf4-4df3-b6ff-ad74632a2b4b"}}}

    Jan  5 19:29:28.274: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:29:28.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-1005" for this suite. 01/05/23 19:29:28.283
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:29:28.295
Jan  5 19:29:28.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename events 01/05/23 19:29:28.297
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:28.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:28.313
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 01/05/23 19:29:28.362
Jan  5 19:29:28.368: INFO: created test-event-1
Jan  5 19:29:28.371: INFO: created test-event-2
Jan  5 19:29:28.375: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 01/05/23 19:29:28.375
STEP: delete collection of events 01/05/23 19:29:28.377
Jan  5 19:29:28.378: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/05/23 19:29:28.389
Jan  5 19:29:28.390: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/node/init/init.go:32
Jan  5 19:29:28.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events
  tear down framework | framework.go:193
STEP: Destroying namespace "events-6028" for this suite. 01/05/23 19:29:28.396
------------------------------
• [0.105 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:29:28.295
    Jan  5 19:29:28.296: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename events 01/05/23 19:29:28.297
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:28.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:28.313
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 01/05/23 19:29:28.362
    Jan  5 19:29:28.368: INFO: created test-event-1
    Jan  5 19:29:28.371: INFO: created test-event-2
    Jan  5 19:29:28.375: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 01/05/23 19:29:28.375
    STEP: delete collection of events 01/05/23 19:29:28.377
    Jan  5 19:29:28.378: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/05/23 19:29:28.389
    Jan  5 19:29:28.390: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:29:28.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-6028" for this suite. 01/05/23 19:29:28.396
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:29:28.406
Jan  5 19:29:28.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replicaset 01/05/23 19:29:28.407
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:28.42
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:28.423
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jan  5 19:29:28.435: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 19:29:33.439: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 19:29:33.439
STEP: Scaling up "test-rs" replicaset  01/05/23 19:29:33.439
Jan  5 19:29:33.450: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 01/05/23 19:29:33.45
W0105 19:29:33.465690      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 19:29:33.468: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 19:29:33.492: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 19:29:33.513: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 19:29:33.522: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 1, AvailableReplicas 1
Jan  5 19:29:34.548: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 2, AvailableReplicas 2
Jan  5 19:29:35.429: INFO: observed Replicaset test-rs in namespace replicaset-7715 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:29:35.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-7715" for this suite. 01/05/23 19:29:35.433
------------------------------
• [SLOW TEST] [7.031 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:29:28.406
    Jan  5 19:29:28.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replicaset 01/05/23 19:29:28.407
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:28.42
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:28.423
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jan  5 19:29:28.435: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 19:29:33.439: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 19:29:33.439
    STEP: Scaling up "test-rs" replicaset  01/05/23 19:29:33.439
    Jan  5 19:29:33.450: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 01/05/23 19:29:33.45
    W0105 19:29:33.465690      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 19:29:33.468: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 19:29:33.492: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 19:29:33.513: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 19:29:33.522: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 1, AvailableReplicas 1
    Jan  5 19:29:34.548: INFO: observed ReplicaSet test-rs in namespace replicaset-7715 with ReadyReplicas 2, AvailableReplicas 2
    Jan  5 19:29:35.429: INFO: observed Replicaset test-rs in namespace replicaset-7715 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:29:35.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-7715" for this suite. 01/05/23 19:29:35.433
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:29:35.439
Jan  5 19:29:35.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename containers 01/05/23 19:29:35.44
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:35.456
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:35.459
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59
STEP: Creating a pod to test override arguments 01/05/23 19:29:35.461
Jan  5 19:29:35.469: INFO: Waiting up to 5m0s for pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af" in namespace "containers-1256" to be "Succeeded or Failed"
Jan  5 19:29:35.475: INFO: Pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.61336ms
Jan  5 19:29:37.478: INFO: Pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008804186s
Jan  5 19:29:39.479: INFO: Pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009806725s
STEP: Saw pod success 01/05/23 19:29:39.479
Jan  5 19:29:39.479: INFO: Pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af" satisfied condition "Succeeded or Failed"
Jan  5 19:29:39.482: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:29:39.488
Jan  5 19:29:39.500: INFO: Waiting for pod client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af to disappear
Jan  5 19:29:39.503: INFO: Pod client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  5 19:29:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-1256" for this suite. 01/05/23 19:29:39.51
------------------------------
• [4.079 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:29:35.439
    Jan  5 19:29:35.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename containers 01/05/23 19:29:35.44
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:35.456
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:35.459
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:59
    STEP: Creating a pod to test override arguments 01/05/23 19:29:35.461
    Jan  5 19:29:35.469: INFO: Waiting up to 5m0s for pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af" in namespace "containers-1256" to be "Succeeded or Failed"
    Jan  5 19:29:35.475: INFO: Pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af": Phase="Pending", Reason="", readiness=false. Elapsed: 5.61336ms
    Jan  5 19:29:37.478: INFO: Pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008804186s
    Jan  5 19:29:39.479: INFO: Pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009806725s
    STEP: Saw pod success 01/05/23 19:29:39.479
    Jan  5 19:29:39.479: INFO: Pod "client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af" satisfied condition "Succeeded or Failed"
    Jan  5 19:29:39.482: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:29:39.488
    Jan  5 19:29:39.500: INFO: Waiting for pod client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af to disappear
    Jan  5 19:29:39.503: INFO: Pod client-containers-87883acd-f51e-4bbe-be4f-bc1f69a515af no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:29:39.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-1256" for this suite. 01/05/23 19:29:39.51
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:29:39.519
Jan  5 19:29:39.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename daemonsets 01/05/23 19:29:39.52
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:39.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:39.554
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 19:29:39.587
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:29:39.592
Jan  5 19:29:39.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:29:39.606: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:29:40.628: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 19:29:40.629: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:29:41.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 19:29:41.612: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 01/05/23 19:29:41.615
Jan  5 19:29:41.617: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 01/05/23 19:29:41.618
Jan  5 19:29:41.627: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 01/05/23 19:29:41.627
Jan  5 19:29:41.629: INFO: Observed &DaemonSet event: ADDED
Jan  5 19:29:41.629: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.630: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.630: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.630: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.631: INFO: Found daemon set daemon-set in namespace daemonsets-3094 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 19:29:41.631: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 01/05/23 19:29:41.631
STEP: watching for the daemon set status to be patched 01/05/23 19:29:41.637
Jan  5 19:29:41.639: INFO: Observed &DaemonSet event: ADDED
Jan  5 19:29:41.639: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.639: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.640: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.640: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.640: INFO: Observed daemon set daemon-set in namespace daemonsets-3094 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 19:29:41.641: INFO: Observed &DaemonSet event: MODIFIED
Jan  5 19:29:41.641: INFO: Found daemon set daemon-set in namespace daemonsets-3094 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jan  5 19:29:41.641: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:29:41.644
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3094, will wait for the garbage collector to delete the pods 01/05/23 19:29:41.645
Jan  5 19:29:41.703: INFO: Deleting DaemonSet.extensions daemon-set took: 5.519841ms
Jan  5 19:29:41.804: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.941702ms
Jan  5 19:29:44.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:29:44.508: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 19:29:44.512: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70527"},"items":null}

Jan  5 19:29:44.515: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70527"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:29:44.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3094" for this suite. 01/05/23 19:29:44.536
------------------------------
• [SLOW TEST] [5.022 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:862

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:29:39.519
    Jan  5 19:29:39.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename daemonsets 01/05/23 19:29:39.52
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:39.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:39.554
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:862
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 19:29:39.587
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:29:39.592
    Jan  5 19:29:39.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:29:39.606: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:29:40.628: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 19:29:40.629: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:29:41.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 19:29:41.612: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 01/05/23 19:29:41.615
    Jan  5 19:29:41.617: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 01/05/23 19:29:41.618
    Jan  5 19:29:41.627: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 01/05/23 19:29:41.627
    Jan  5 19:29:41.629: INFO: Observed &DaemonSet event: ADDED
    Jan  5 19:29:41.629: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.630: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.630: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.630: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.631: INFO: Found daemon set daemon-set in namespace daemonsets-3094 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 19:29:41.631: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 01/05/23 19:29:41.631
    STEP: watching for the daemon set status to be patched 01/05/23 19:29:41.637
    Jan  5 19:29:41.639: INFO: Observed &DaemonSet event: ADDED
    Jan  5 19:29:41.639: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.639: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.640: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.640: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.640: INFO: Observed daemon set daemon-set in namespace daemonsets-3094 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 19:29:41.641: INFO: Observed &DaemonSet event: MODIFIED
    Jan  5 19:29:41.641: INFO: Found daemon set daemon-set in namespace daemonsets-3094 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jan  5 19:29:41.641: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:29:41.644
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3094, will wait for the garbage collector to delete the pods 01/05/23 19:29:41.645
    Jan  5 19:29:41.703: INFO: Deleting DaemonSet.extensions daemon-set took: 5.519841ms
    Jan  5 19:29:41.804: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.941702ms
    Jan  5 19:29:44.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:29:44.508: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 19:29:44.512: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"70527"},"items":null}

    Jan  5 19:29:44.515: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"70527"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:29:44.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3094" for this suite. 01/05/23 19:29:44.536
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:29:44.553
Jan  5 19:29:44.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename gc 01/05/23 19:29:44.554
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:44.568
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:44.571
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 01/05/23 19:29:44.577
STEP: delete the rc 01/05/23 19:29:49.587
STEP: wait for the rc to be deleted 01/05/23 19:29:49.595
Jan  5 19:29:50.860: INFO: 80 pods remaining
Jan  5 19:29:50.860: INFO: 80 pods has nil DeletionTimestamp
Jan  5 19:29:50.860: INFO: 
Jan  5 19:29:51.621: INFO: 71 pods remaining
Jan  5 19:29:51.621: INFO: 71 pods has nil DeletionTimestamp
Jan  5 19:29:51.621: INFO: 
Jan  5 19:29:52.677: INFO: 60 pods remaining
Jan  5 19:29:52.677: INFO: 60 pods has nil DeletionTimestamp
Jan  5 19:29:52.677: INFO: 
Jan  5 19:29:53.656: INFO: 40 pods remaining
Jan  5 19:29:53.656: INFO: 40 pods has nil DeletionTimestamp
Jan  5 19:29:53.656: INFO: 
Jan  5 19:29:54.634: INFO: 31 pods remaining
Jan  5 19:29:54.634: INFO: 31 pods has nil DeletionTimestamp
Jan  5 19:29:54.634: INFO: 
Jan  5 19:29:55.620: INFO: 20 pods remaining
Jan  5 19:29:55.633: INFO: 20 pods has nil DeletionTimestamp
Jan  5 19:29:55.646: INFO: 
STEP: Gathering metrics 01/05/23 19:29:56.607
W0105 19:29:56.621884      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 19:29:56.624: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  5 19:29:56.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1381" for this suite. 01/05/23 19:29:56.632
------------------------------
• [SLOW TEST] [12.092 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:29:44.553
    Jan  5 19:29:44.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename gc 01/05/23 19:29:44.554
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:44.568
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:44.571
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 01/05/23 19:29:44.577
    STEP: delete the rc 01/05/23 19:29:49.587
    STEP: wait for the rc to be deleted 01/05/23 19:29:49.595
    Jan  5 19:29:50.860: INFO: 80 pods remaining
    Jan  5 19:29:50.860: INFO: 80 pods has nil DeletionTimestamp
    Jan  5 19:29:50.860: INFO: 
    Jan  5 19:29:51.621: INFO: 71 pods remaining
    Jan  5 19:29:51.621: INFO: 71 pods has nil DeletionTimestamp
    Jan  5 19:29:51.621: INFO: 
    Jan  5 19:29:52.677: INFO: 60 pods remaining
    Jan  5 19:29:52.677: INFO: 60 pods has nil DeletionTimestamp
    Jan  5 19:29:52.677: INFO: 
    Jan  5 19:29:53.656: INFO: 40 pods remaining
    Jan  5 19:29:53.656: INFO: 40 pods has nil DeletionTimestamp
    Jan  5 19:29:53.656: INFO: 
    Jan  5 19:29:54.634: INFO: 31 pods remaining
    Jan  5 19:29:54.634: INFO: 31 pods has nil DeletionTimestamp
    Jan  5 19:29:54.634: INFO: 
    Jan  5 19:29:55.620: INFO: 20 pods remaining
    Jan  5 19:29:55.633: INFO: 20 pods has nil DeletionTimestamp
    Jan  5 19:29:55.646: INFO: 
    STEP: Gathering metrics 01/05/23 19:29:56.607
    W0105 19:29:56.621884      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 19:29:56.624: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:29:56.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1381" for this suite. 01/05/23 19:29:56.632
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:29:56.748
Jan  5 19:29:56.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sysctl 01/05/23 19:29:56.772
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:56.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:56.808
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/05/23 19:29:56.82
STEP: Watching for error events or started pod 01/05/23 19:29:56.867
STEP: Waiting for pod completion 01/05/23 19:30:22.91
Jan  5 19:30:22.910: INFO: Waiting up to 3m0s for pod "sysctl-81e058f6-3b51-4960-a467-e9308d0cfba6" in namespace "sysctl-4306" to be "completed"
Jan  5 19:30:22.913: INFO: Pod "sysctl-81e058f6-3b51-4960-a467-e9308d0cfba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.360187ms
Jan  5 19:30:22.913: INFO: Pod "sysctl-81e058f6-3b51-4960-a467-e9308d0cfba6" satisfied condition "completed"
STEP: Checking that the pod succeeded 01/05/23 19:30:22.916
STEP: Getting logs from the pod 01/05/23 19:30:22.916
STEP: Checking that the sysctl is actually updated 01/05/23 19:30:22.922
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:30:22.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4306" for this suite. 01/05/23 19:30:22.927
------------------------------
• [SLOW TEST] [26.189 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:29:56.748
    Jan  5 19:29:56.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sysctl 01/05/23 19:29:56.772
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:29:56.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:29:56.808
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 01/05/23 19:29:56.82
    STEP: Watching for error events or started pod 01/05/23 19:29:56.867
    STEP: Waiting for pod completion 01/05/23 19:30:22.91
    Jan  5 19:30:22.910: INFO: Waiting up to 3m0s for pod "sysctl-81e058f6-3b51-4960-a467-e9308d0cfba6" in namespace "sysctl-4306" to be "completed"
    Jan  5 19:30:22.913: INFO: Pod "sysctl-81e058f6-3b51-4960-a467-e9308d0cfba6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.360187ms
    Jan  5 19:30:22.913: INFO: Pod "sysctl-81e058f6-3b51-4960-a467-e9308d0cfba6" satisfied condition "completed"
    STEP: Checking that the pod succeeded 01/05/23 19:30:22.916
    STEP: Getting logs from the pod 01/05/23 19:30:22.916
    STEP: Checking that the sysctl is actually updated 01/05/23 19:30:22.922
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:30:22.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4306" for this suite. 01/05/23 19:30:22.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:30:22.94
Jan  5 19:30:22.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:30:22.941
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:30:22.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:30:22.956
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:30:22.977
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:30:23.788
STEP: Deploying the webhook pod 01/05/23 19:30:23.793
STEP: Wait for the deployment to be ready 01/05/23 19:30:23.805
Jan  5 19:30:23.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 19:30:25.819
STEP: Verifying the service has paired with the endpoint 01/05/23 19:30:25.835
Jan  5 19:30:26.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308
STEP: Registering the crd webhook via the AdmissionRegistration API 01/05/23 19:30:26.838
STEP: Creating a custom resource definition that should be denied by the webhook 01/05/23 19:30:26.858
Jan  5 19:30:26.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:30:26.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3549" for this suite. 01/05/23 19:30:26.923
STEP: Destroying namespace "webhook-3549-markers" for this suite. 01/05/23 19:30:26.931
------------------------------
• [3.999 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:30:22.94
    Jan  5 19:30:22.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:30:22.941
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:30:22.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:30:22.956
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:30:22.977
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:30:23.788
    STEP: Deploying the webhook pod 01/05/23 19:30:23.793
    STEP: Wait for the deployment to be ready 01/05/23 19:30:23.805
    Jan  5 19:30:23.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 19:30:25.819
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:30:25.835
    Jan  5 19:30:26.835: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:308
    STEP: Registering the crd webhook via the AdmissionRegistration API 01/05/23 19:30:26.838
    STEP: Creating a custom resource definition that should be denied by the webhook 01/05/23 19:30:26.858
    Jan  5 19:30:26.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:30:26.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3549" for this suite. 01/05/23 19:30:26.923
    STEP: Destroying namespace "webhook-3549-markers" for this suite. 01/05/23 19:30:26.931
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:30:26.938
Jan  5 19:30:26.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:30:26.939
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:30:26.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:30:26.957
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230
STEP: Counting existing ResourceQuota 01/05/23 19:30:26.962
STEP: Creating a ResourceQuota 01/05/23 19:30:31.968
STEP: Ensuring resource quota status is calculated 01/05/23 19:30:31.974
STEP: Creating a Pod that fits quota 01/05/23 19:30:33.977
STEP: Ensuring ResourceQuota status captures the pod usage 01/05/23 19:30:33.995
STEP: Not allowing a pod to be created that exceeds remaining quota 01/05/23 19:30:35.999
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/05/23 19:30:36.004
STEP: Ensuring a pod cannot update its resource requirements 01/05/23 19:30:36.011
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/05/23 19:30:36.015
STEP: Deleting the pod 01/05/23 19:30:38.018
STEP: Ensuring resource quota status released the pod usage 01/05/23 19:30:38.03
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:30:40.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-2735" for this suite. 01/05/23 19:30:40.039
------------------------------
• [SLOW TEST] [13.105 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:230

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:30:26.938
    Jan  5 19:30:26.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:30:26.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:30:26.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:30:26.957
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:230
    STEP: Counting existing ResourceQuota 01/05/23 19:30:26.962
    STEP: Creating a ResourceQuota 01/05/23 19:30:31.968
    STEP: Ensuring resource quota status is calculated 01/05/23 19:30:31.974
    STEP: Creating a Pod that fits quota 01/05/23 19:30:33.977
    STEP: Ensuring ResourceQuota status captures the pod usage 01/05/23 19:30:33.995
    STEP: Not allowing a pod to be created that exceeds remaining quota 01/05/23 19:30:35.999
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 01/05/23 19:30:36.004
    STEP: Ensuring a pod cannot update its resource requirements 01/05/23 19:30:36.011
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 01/05/23 19:30:36.015
    STEP: Deleting the pod 01/05/23 19:30:38.018
    STEP: Ensuring resource quota status released the pod usage 01/05/23 19:30:38.03
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:30:40.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-2735" for this suite. 01/05/23 19:30:40.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:30:40.048
Jan  5 19:30:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pod-network-test 01/05/23 19:30:40.05
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:30:40.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:30:40.066
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-746 01/05/23 19:30:40.07
STEP: creating a selector 01/05/23 19:30:40.07
STEP: Creating the service pods in kubernetes 01/05/23 19:30:40.07
Jan  5 19:30:40.070: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 19:30:40.123: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-746" to be "running and ready"
Jan  5 19:30:40.126: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.093535ms
Jan  5 19:30:40.126: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:30:42.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00774707s
Jan  5 19:30:42.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:30:44.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006330774s
Jan  5 19:30:44.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:30:46.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00824622s
Jan  5 19:30:46.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:30:48.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007653343s
Jan  5 19:30:48.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:30:50.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007069412s
Jan  5 19:30:50.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:30:52.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007363145s
Jan  5 19:30:52.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:30:54.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008281997s
Jan  5 19:30:54.132: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:30:56.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006684686s
Jan  5 19:30:56.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:30:58.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007148526s
Jan  5 19:30:58.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:31:00.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013723484s
Jan  5 19:31:00.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 19:31:02.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007433013s
Jan  5 19:31:02.131: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 19:31:02.131: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 19:31:02.133: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-746" to be "running and ready"
Jan  5 19:31:02.135: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.313486ms
Jan  5 19:31:02.136: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 19:31:02.136: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  5 19:31:02.138: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-746" to be "running and ready"
Jan  5 19:31:02.140: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.216181ms
Jan  5 19:31:02.140: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  5 19:31:02.141: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 19:31:02.143
Jan  5 19:31:02.150: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-746" to be "running"
Jan  5 19:31:02.153: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111925ms
Jan  5 19:31:04.158: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007408213s
Jan  5 19:31:04.158: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 19:31:04.161: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  5 19:31:04.161: INFO: Breadth first check of 10.16.2.95 on host 10.196.0.39...
Jan  5 19:31:04.164: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.96:9080/dial?request=hostname&protocol=udp&host=10.16.2.95&port=8081&tries=1'] Namespace:pod-network-test-746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:31:04.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:31:04.164: INFO: ExecWithOptions: Clientset creation
Jan  5 19:31:04.165: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-746/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.96%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.16.2.95%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 19:31:04.230: INFO: Waiting for responses: map[]
Jan  5 19:31:04.230: INFO: reached 10.16.2.95 after 0/1 tries
Jan  5 19:31:04.230: INFO: Breadth first check of 10.16.0.154 on host 10.196.0.37...
Jan  5 19:31:04.233: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.96:9080/dial?request=hostname&protocol=udp&host=10.16.0.154&port=8081&tries=1'] Namespace:pod-network-test-746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:31:04.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:31:04.234: INFO: ExecWithOptions: Clientset creation
Jan  5 19:31:04.234: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-746/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.96%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.16.0.154%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 19:31:04.298: INFO: Waiting for responses: map[]
Jan  5 19:31:04.299: INFO: reached 10.16.0.154 after 0/1 tries
Jan  5 19:31:04.299: INFO: Breadth first check of 10.16.1.213 on host 10.196.0.38...
Jan  5 19:31:04.304: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.96:9080/dial?request=hostname&protocol=udp&host=10.16.1.213&port=8081&tries=1'] Namespace:pod-network-test-746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:31:04.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:31:04.304: INFO: ExecWithOptions: Clientset creation
Jan  5 19:31:04.305: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-746/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.96%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.16.1.213%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jan  5 19:31:04.377: INFO: Waiting for responses: map[]
Jan  5 19:31:04.377: INFO: reached 10.16.1.213 after 0/1 tries
Jan  5 19:31:04.377: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  5 19:31:04.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-746" for this suite. 01/05/23 19:31:04.382
------------------------------
• [SLOW TEST] [24.339 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:30:40.048
    Jan  5 19:30:40.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 19:30:40.05
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:30:40.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:30:40.066
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-746 01/05/23 19:30:40.07
    STEP: creating a selector 01/05/23 19:30:40.07
    STEP: Creating the service pods in kubernetes 01/05/23 19:30:40.07
    Jan  5 19:30:40.070: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 19:30:40.123: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-746" to be "running and ready"
    Jan  5 19:30:40.126: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.093535ms
    Jan  5 19:30:40.126: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:30:42.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.00774707s
    Jan  5 19:30:42.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:30:44.129: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.006330774s
    Jan  5 19:30:44.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:30:46.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.00824622s
    Jan  5 19:30:46.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:30:48.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.007653343s
    Jan  5 19:30:48.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:30:50.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.007069412s
    Jan  5 19:30:50.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:30:52.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.007363145s
    Jan  5 19:30:52.131: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:30:54.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.008281997s
    Jan  5 19:30:54.132: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:30:56.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.006684686s
    Jan  5 19:30:56.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:30:58.130: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.007148526s
    Jan  5 19:30:58.130: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:31:00.137: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.013723484s
    Jan  5 19:31:00.137: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 19:31:02.131: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.007433013s
    Jan  5 19:31:02.131: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 19:31:02.131: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 19:31:02.133: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-746" to be "running and ready"
    Jan  5 19:31:02.135: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 2.313486ms
    Jan  5 19:31:02.136: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 19:31:02.136: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  5 19:31:02.138: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-746" to be "running and ready"
    Jan  5 19:31:02.140: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.216181ms
    Jan  5 19:31:02.140: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  5 19:31:02.141: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 19:31:02.143
    Jan  5 19:31:02.150: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-746" to be "running"
    Jan  5 19:31:02.153: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.111925ms
    Jan  5 19:31:04.158: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007408213s
    Jan  5 19:31:04.158: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 19:31:04.161: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  5 19:31:04.161: INFO: Breadth first check of 10.16.2.95 on host 10.196.0.39...
    Jan  5 19:31:04.164: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.96:9080/dial?request=hostname&protocol=udp&host=10.16.2.95&port=8081&tries=1'] Namespace:pod-network-test-746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:31:04.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:31:04.164: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:31:04.165: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-746/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.96%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.16.2.95%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 19:31:04.230: INFO: Waiting for responses: map[]
    Jan  5 19:31:04.230: INFO: reached 10.16.2.95 after 0/1 tries
    Jan  5 19:31:04.230: INFO: Breadth first check of 10.16.0.154 on host 10.196.0.37...
    Jan  5 19:31:04.233: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.96:9080/dial?request=hostname&protocol=udp&host=10.16.0.154&port=8081&tries=1'] Namespace:pod-network-test-746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:31:04.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:31:04.234: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:31:04.234: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-746/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.96%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.16.0.154%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 19:31:04.298: INFO: Waiting for responses: map[]
    Jan  5 19:31:04.299: INFO: reached 10.16.0.154 after 0/1 tries
    Jan  5 19:31:04.299: INFO: Breadth first check of 10.16.1.213 on host 10.196.0.38...
    Jan  5 19:31:04.304: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.16.2.96:9080/dial?request=hostname&protocol=udp&host=10.16.1.213&port=8081&tries=1'] Namespace:pod-network-test-746 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:31:04.304: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:31:04.304: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:31:04.305: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-746/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.16.2.96%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.16.1.213%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jan  5 19:31:04.377: INFO: Waiting for responses: map[]
    Jan  5 19:31:04.377: INFO: reached 10.16.1.213 after 0/1 tries
    Jan  5 19:31:04.377: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:31:04.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-746" for this suite. 01/05/23 19:31:04.382
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:31:04.389
Jan  5 19:31:04.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:31:04.391
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:04.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:04.407
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162
STEP: Creating the pod 01/05/23 19:31:04.41
Jan  5 19:31:04.422: INFO: Waiting up to 5m0s for pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1" in namespace "projected-6606" to be "running and ready"
Jan  5 19:31:04.426: INFO: Pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021793ms
Jan  5 19:31:04.426: INFO: The phase of Pod annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:31:06.429: INFO: Pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006600804s
Jan  5 19:31:06.430: INFO: The phase of Pod annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1 is Running (Ready = true)
Jan  5 19:31:06.430: INFO: Pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1" satisfied condition "running and ready"
Jan  5 19:31:06.958: INFO: Successfully updated pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:31:10.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6606" for this suite. 01/05/23 19:31:10.984
------------------------------
• [SLOW TEST] [6.600 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:162

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:31:04.389
    Jan  5 19:31:04.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:31:04.391
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:04.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:04.407
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:162
    STEP: Creating the pod 01/05/23 19:31:04.41
    Jan  5 19:31:04.422: INFO: Waiting up to 5m0s for pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1" in namespace "projected-6606" to be "running and ready"
    Jan  5 19:31:04.426: INFO: Pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021793ms
    Jan  5 19:31:04.426: INFO: The phase of Pod annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:31:06.429: INFO: Pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1": Phase="Running", Reason="", readiness=true. Elapsed: 2.006600804s
    Jan  5 19:31:06.430: INFO: The phase of Pod annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1 is Running (Ready = true)
    Jan  5 19:31:06.430: INFO: Pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1" satisfied condition "running and ready"
    Jan  5 19:31:06.958: INFO: Successfully updated pod "annotationupdatea87d4aea-bd2b-457a-81ff-cfd122df33c1"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:31:10.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6606" for this suite. 01/05/23 19:31:10.984
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:31:10.996
Jan  5 19:31:10.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename job 01/05/23 19:31:10.998
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:11.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:11.014
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703
STEP: Creating a suspended job 01/05/23 19:31:11.019
STEP: Patching the Job 01/05/23 19:31:11.025
STEP: Watching for Job to be patched 01/05/23 19:31:11.044
Jan  5 19:31:11.045: INFO: Event ADDED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  5 19:31:11.045: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking:]
Jan  5 19:31:11.046: INFO: Event MODIFIED found for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 01/05/23 19:31:11.046
STEP: Watching for Job to be updated 01/05/23 19:31:11.056
Jan  5 19:31:11.057: INFO: Event MODIFIED found for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:11.057: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 01/05/23 19:31:11.058
Jan  5 19:31:11.060: INFO: Job: e2e-dkwxb as labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb]
STEP: Waiting for job to complete 01/05/23 19:31:11.06
STEP: Delete a job collection with a labelselector 01/05/23 19:31:23.063
STEP: Watching for Job to be deleted 01/05/23 19:31:23.069
Jan  5 19:31:23.071: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:23.071: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:23.072: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:23.072: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:23.072: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:23.072: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:23.073: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:23.073: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jan  5 19:31:23.073: INFO: Event DELETED found for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 01/05/23 19:31:23.073
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  5 19:31:23.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-4308" for this suite. 01/05/23 19:31:23.083
------------------------------
• [SLOW TEST] [12.093 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:703

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:31:10.996
    Jan  5 19:31:10.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename job 01/05/23 19:31:10.998
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:11.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:11.014
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:703
    STEP: Creating a suspended job 01/05/23 19:31:11.019
    STEP: Patching the Job 01/05/23 19:31:11.025
    STEP: Watching for Job to be patched 01/05/23 19:31:11.044
    Jan  5 19:31:11.045: INFO: Event ADDED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  5 19:31:11.045: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jan  5 19:31:11.046: INFO: Event MODIFIED found for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 01/05/23 19:31:11.046
    STEP: Watching for Job to be updated 01/05/23 19:31:11.056
    Jan  5 19:31:11.057: INFO: Event MODIFIED found for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:11.057: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 01/05/23 19:31:11.058
    Jan  5 19:31:11.060: INFO: Job: e2e-dkwxb as labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb]
    STEP: Waiting for job to complete 01/05/23 19:31:11.06
    STEP: Delete a job collection with a labelselector 01/05/23 19:31:23.063
    STEP: Watching for Job to be deleted 01/05/23 19:31:23.069
    Jan  5 19:31:23.071: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:23.071: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:23.072: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:23.072: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:23.072: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:23.072: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:23.073: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:23.073: INFO: Event MODIFIED observed for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jan  5 19:31:23.073: INFO: Event DELETED found for Job e2e-dkwxb in namespace job-4308 with labels: map[e2e-dkwxb:patched e2e-job-label:e2e-dkwxb] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 01/05/23 19:31:23.073
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:31:23.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-4308" for this suite. 01/05/23 19:31:23.083
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:31:23.096
Jan  5 19:31:23.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:31:23.098
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:23.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:23.125
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166
STEP: Creating a pod to test downward api env vars 01/05/23 19:31:23.129
Jan  5 19:31:23.139: INFO: Waiting up to 5m0s for pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521" in namespace "downward-api-5081" to be "Succeeded or Failed"
Jan  5 19:31:23.143: INFO: Pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798296ms
Jan  5 19:31:25.147: INFO: Pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007532822s
Jan  5 19:31:27.146: INFO: Pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007060816s
STEP: Saw pod success 01/05/23 19:31:27.146
Jan  5 19:31:27.146: INFO: Pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521" satisfied condition "Succeeded or Failed"
Jan  5 19:31:27.149: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521 container dapi-container: <nil>
STEP: delete the pod 01/05/23 19:31:27.157
Jan  5 19:31:27.169: INFO: Waiting for pod downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521 to disappear
Jan  5 19:31:27.172: INFO: Pod downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  5 19:31:27.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-5081" for this suite. 01/05/23 19:31:27.177
------------------------------
• [4.086 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:31:23.096
    Jan  5 19:31:23.096: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:31:23.098
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:23.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:23.125
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:166
    STEP: Creating a pod to test downward api env vars 01/05/23 19:31:23.129
    Jan  5 19:31:23.139: INFO: Waiting up to 5m0s for pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521" in namespace "downward-api-5081" to be "Succeeded or Failed"
    Jan  5 19:31:23.143: INFO: Pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521": Phase="Pending", Reason="", readiness=false. Elapsed: 3.798296ms
    Jan  5 19:31:25.147: INFO: Pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007532822s
    Jan  5 19:31:27.146: INFO: Pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007060816s
    STEP: Saw pod success 01/05/23 19:31:27.146
    Jan  5 19:31:27.146: INFO: Pod "downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521" satisfied condition "Succeeded or Failed"
    Jan  5 19:31:27.149: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 19:31:27.157
    Jan  5 19:31:27.169: INFO: Waiting for pod downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521 to disappear
    Jan  5 19:31:27.172: INFO: Pod downward-api-0365b449-9e3f-4af7-a39f-ed78359aa521 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:31:27.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-5081" for this suite. 01/05/23 19:31:27.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:31:27.186
Jan  5 19:31:27.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:31:27.188
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:27.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:27.201
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845
STEP: Create set of pods 01/05/23 19:31:27.204
Jan  5 19:31:27.215: INFO: created test-pod-1
Jan  5 19:31:27.224: INFO: created test-pod-2
Jan  5 19:31:27.236: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 01/05/23 19:31:27.236
Jan  5 19:31:27.236: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1123' to be running and ready
Jan  5 19:31:27.248: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 19:31:27.248: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 19:31:27.248: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 19:31:27.248: INFO: 0 / 3 pods in namespace 'pods-1123' are running and ready (0 seconds elapsed)
Jan  5 19:31:27.248: INFO: expected 0 pod replicas in namespace 'pods-1123', 0 are Running and Ready.
Jan  5 19:31:27.248: INFO: POD         NODE                                     PHASE    GRACE  CONDITIONS
Jan  5 19:31:27.248: INFO: test-pod-1  gke-gke-1-26-default-pool-05283374-16pz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
Jan  5 19:31:27.248: INFO: test-pod-2  gke-gke-1-26-default-pool-05283374-qmj7  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
Jan  5 19:31:27.248: INFO: test-pod-3  gke-gke-1-26-default-pool-05283374-16pz  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
Jan  5 19:31:27.248: INFO: 
Jan  5 19:31:29.257: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 19:31:29.257: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 19:31:29.257: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jan  5 19:31:29.257: INFO: 0 / 3 pods in namespace 'pods-1123' are running and ready (2 seconds elapsed)
Jan  5 19:31:29.257: INFO: expected 0 pod replicas in namespace 'pods-1123', 0 are Running and Ready.
Jan  5 19:31:29.257: INFO: POD         NODE                                     PHASE    GRACE  CONDITIONS
Jan  5 19:31:29.257: INFO: test-pod-1  gke-gke-1-26-default-pool-05283374-16pz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
Jan  5 19:31:29.257: INFO: test-pod-2  gke-gke-1-26-default-pool-05283374-qmj7  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
Jan  5 19:31:29.257: INFO: test-pod-3  gke-gke-1-26-default-pool-05283374-16pz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
Jan  5 19:31:29.257: INFO: 
Jan  5 19:31:31.255: INFO: 3 / 3 pods in namespace 'pods-1123' are running and ready (4 seconds elapsed)
Jan  5 19:31:31.255: INFO: expected 0 pod replicas in namespace 'pods-1123', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 01/05/23 19:31:31.271
Jan  5 19:31:31.274: INFO: Pod quantity 3 is different from expected quantity 0
Jan  5 19:31:32.278: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:31:33.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-1123" for this suite. 01/05/23 19:31:33.28
------------------------------
• [SLOW TEST] [6.098 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:845

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:31:27.186
    Jan  5 19:31:27.186: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:31:27.188
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:27.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:27.201
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:845
    STEP: Create set of pods 01/05/23 19:31:27.204
    Jan  5 19:31:27.215: INFO: created test-pod-1
    Jan  5 19:31:27.224: INFO: created test-pod-2
    Jan  5 19:31:27.236: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 01/05/23 19:31:27.236
    Jan  5 19:31:27.236: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1123' to be running and ready
    Jan  5 19:31:27.248: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 19:31:27.248: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 19:31:27.248: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 19:31:27.248: INFO: 0 / 3 pods in namespace 'pods-1123' are running and ready (0 seconds elapsed)
    Jan  5 19:31:27.248: INFO: expected 0 pod replicas in namespace 'pods-1123', 0 are Running and Ready.
    Jan  5 19:31:27.248: INFO: POD         NODE                                     PHASE    GRACE  CONDITIONS
    Jan  5 19:31:27.248: INFO: test-pod-1  gke-gke-1-26-default-pool-05283374-16pz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
    Jan  5 19:31:27.248: INFO: test-pod-2  gke-gke-1-26-default-pool-05283374-qmj7  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
    Jan  5 19:31:27.248: INFO: test-pod-3  gke-gke-1-26-default-pool-05283374-16pz  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
    Jan  5 19:31:27.248: INFO: 
    Jan  5 19:31:29.257: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 19:31:29.257: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 19:31:29.257: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jan  5 19:31:29.257: INFO: 0 / 3 pods in namespace 'pods-1123' are running and ready (2 seconds elapsed)
    Jan  5 19:31:29.257: INFO: expected 0 pod replicas in namespace 'pods-1123', 0 are Running and Ready.
    Jan  5 19:31:29.257: INFO: POD         NODE                                     PHASE    GRACE  CONDITIONS
    Jan  5 19:31:29.257: INFO: test-pod-1  gke-gke-1-26-default-pool-05283374-16pz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
    Jan  5 19:31:29.257: INFO: test-pod-2  gke-gke-1-26-default-pool-05283374-qmj7  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
    Jan  5 19:31:29.257: INFO: test-pod-3  gke-gke-1-26-default-pool-05283374-16pz  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-01-05 19:31:27 +0000 UTC  }]
    Jan  5 19:31:29.257: INFO: 
    Jan  5 19:31:31.255: INFO: 3 / 3 pods in namespace 'pods-1123' are running and ready (4 seconds elapsed)
    Jan  5 19:31:31.255: INFO: expected 0 pod replicas in namespace 'pods-1123', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 01/05/23 19:31:31.271
    Jan  5 19:31:31.274: INFO: Pod quantity 3 is different from expected quantity 0
    Jan  5 19:31:32.278: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:31:33.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-1123" for this suite. 01/05/23 19:31:33.28
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:31:33.287
Jan  5 19:31:33.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-preemption 01/05/23 19:31:33.288
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:33.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:33.305
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan  5 19:31:33.318: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 19:32:33.348: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:32:33.351
Jan  5 19:32:33.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 19:32:33.352
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:32:33.365
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:32:33.368
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:569
STEP: Finding an available node 01/05/23 19:32:33.37
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 19:32:33.37
Jan  5 19:32:33.379: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8870" to be "running"
Jan  5 19:32:33.382: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024937ms
Jan  5 19:32:35.386: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006945855s
Jan  5 19:32:37.385: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.006313851s
Jan  5 19:32:37.386: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 19:32:37.388
Jan  5 19:32:37.402: INFO: found a healthy node: gke-gke-1-26-default-pool-05283374-16pz
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:616
Jan  5 19:32:49.466: INFO: pods created so far: [1 1 1]
Jan  5 19:32:49.466: INFO: length of pods created so far: 3
Jan  5 19:32:51.477: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/node/init/init.go:32
Jan  5 19:32:58.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:543
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:32:58.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PreemptionExecutionPath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PreemptionExecutionPath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PreemptionExecutionPath
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8870" for this suite. 01/05/23 19:32:58.58
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7698" for this suite. 01/05/23 19:32:58.587
------------------------------
• [SLOW TEST] [85.305 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:531
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:616

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:31:33.287
    Jan  5 19:31:33.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 19:31:33.288
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:31:33.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:31:33.305
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan  5 19:31:33.318: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 19:32:33.348: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:32:33.351
    Jan  5 19:32:33.351: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 19:32:33.352
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:32:33.365
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:32:33.368
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:569
    STEP: Finding an available node 01/05/23 19:32:33.37
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 19:32:33.37
    Jan  5 19:32:33.379: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-8870" to be "running"
    Jan  5 19:32:33.382: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.024937ms
    Jan  5 19:32:35.386: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006945855s
    Jan  5 19:32:37.385: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.006313851s
    Jan  5 19:32:37.386: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 19:32:37.388
    Jan  5 19:32:37.402: INFO: found a healthy node: gke-gke-1-26-default-pool-05283374-16pz
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:616
    Jan  5 19:32:49.466: INFO: pods created so far: [1 1 1]
    Jan  5 19:32:49.466: INFO: length of pods created so far: 3
    Jan  5 19:32:51.477: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:32:58.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:543
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:32:58.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PreemptionExecutionPath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PreemptionExecutionPath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PreemptionExecutionPath
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8870" for this suite. 01/05/23 19:32:58.58
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7698" for this suite. 01/05/23 19:32:58.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:32:58.6
Jan  5 19:32:58.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:32:58.601
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:32:58.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:32:58.614
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:326
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:352
STEP: creating a replication controller 01/05/23 19:32:58.617
Jan  5 19:32:58.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 create -f -'
Jan  5 19:32:59.355: INFO: stderr: ""
Jan  5 19:32:59.355: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 19:32:59.355
Jan  5 19:32:59.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 19:32:59.450: INFO: stderr: ""
Jan  5 19:32:59.450: INFO: stdout: "update-demo-nautilus-lmw88 update-demo-nautilus-q45gm "
Jan  5 19:32:59.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-lmw88 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:32:59.524: INFO: stderr: ""
Jan  5 19:32:59.524: INFO: stdout: ""
Jan  5 19:32:59.524: INFO: update-demo-nautilus-lmw88 is created but not running
Jan  5 19:33:04.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 19:33:04.607: INFO: stderr: ""
Jan  5 19:33:04.607: INFO: stdout: "update-demo-nautilus-lmw88 update-demo-nautilus-q45gm "
Jan  5 19:33:04.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-lmw88 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:33:04.700: INFO: stderr: ""
Jan  5 19:33:04.700: INFO: stdout: "true"
Jan  5 19:33:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-lmw88 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 19:33:04.814: INFO: stderr: ""
Jan  5 19:33:04.814: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  5 19:33:04.814: INFO: validating pod update-demo-nautilus-lmw88
Jan  5 19:33:04.827: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 19:33:04.827: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 19:33:04.827: INFO: update-demo-nautilus-lmw88 is verified up and running
Jan  5 19:33:04.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:33:04.907: INFO: stderr: ""
Jan  5 19:33:04.907: INFO: stdout: "true"
Jan  5 19:33:04.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 19:33:04.990: INFO: stderr: ""
Jan  5 19:33:04.990: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  5 19:33:04.990: INFO: validating pod update-demo-nautilus-q45gm
Jan  5 19:33:04.999: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 19:33:05.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 19:33:05.000: INFO: update-demo-nautilus-q45gm is verified up and running
STEP: scaling down the replication controller 01/05/23 19:33:05
Jan  5 19:33:05.003: INFO: scanned /root for discovery docs: <nil>
Jan  5 19:33:05.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jan  5 19:33:06.113: INFO: stderr: ""
Jan  5 19:33:06.113: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 19:33:06.113
Jan  5 19:33:06.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 19:33:06.234: INFO: stderr: ""
Jan  5 19:33:06.234: INFO: stdout: "update-demo-nautilus-q45gm "
Jan  5 19:33:06.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:33:06.323: INFO: stderr: ""
Jan  5 19:33:06.323: INFO: stdout: "true"
Jan  5 19:33:06.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 19:33:06.418: INFO: stderr: ""
Jan  5 19:33:06.418: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  5 19:33:06.418: INFO: validating pod update-demo-nautilus-q45gm
Jan  5 19:33:06.431: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 19:33:06.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 19:33:06.431: INFO: update-demo-nautilus-q45gm is verified up and running
STEP: scaling up the replication controller 01/05/23 19:33:06.431
Jan  5 19:33:06.433: INFO: scanned /root for discovery docs: <nil>
Jan  5 19:33:06.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jan  5 19:33:07.548: INFO: stderr: ""
Jan  5 19:33:07.548: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 19:33:07.548
Jan  5 19:33:07.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jan  5 19:33:07.705: INFO: stderr: ""
Jan  5 19:33:07.705: INFO: stdout: "update-demo-nautilus-k42fs update-demo-nautilus-q45gm "
Jan  5 19:33:07.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-k42fs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:33:07.830: INFO: stderr: ""
Jan  5 19:33:07.830: INFO: stdout: "true"
Jan  5 19:33:07.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-k42fs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 19:33:07.964: INFO: stderr: ""
Jan  5 19:33:07.964: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  5 19:33:07.964: INFO: validating pod update-demo-nautilus-k42fs
Jan  5 19:33:07.975: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 19:33:07.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 19:33:07.975: INFO: update-demo-nautilus-k42fs is verified up and running
Jan  5 19:33:07.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jan  5 19:33:08.100: INFO: stderr: ""
Jan  5 19:33:08.100: INFO: stdout: "true"
Jan  5 19:33:08.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jan  5 19:33:08.176: INFO: stderr: ""
Jan  5 19:33:08.176: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
Jan  5 19:33:08.176: INFO: validating pod update-demo-nautilus-q45gm
Jan  5 19:33:08.179: INFO: got data: {
  "image": "nautilus.jpg"
}

Jan  5 19:33:08.179: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jan  5 19:33:08.179: INFO: update-demo-nautilus-q45gm is verified up and running
STEP: using delete to clean up resources 01/05/23 19:33:08.179
Jan  5 19:33:08.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 delete --grace-period=0 --force -f -'
Jan  5 19:33:08.259: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 19:33:08.259: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jan  5 19:33:08.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get rc,svc -l name=update-demo --no-headers'
Jan  5 19:33:08.405: INFO: stderr: "No resources found in kubectl-6313 namespace.\n"
Jan  5 19:33:08.405: INFO: stdout: ""
Jan  5 19:33:08.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jan  5 19:33:08.551: INFO: stderr: ""
Jan  5 19:33:08.551: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:33:08.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6313" for this suite. 01/05/23 19:33:08.556
------------------------------
• [SLOW TEST] [9.962 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:324
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:32:58.6
    Jan  5 19:32:58.600: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:32:58.601
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:32:58.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:32:58.614
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:326
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:352
    STEP: creating a replication controller 01/05/23 19:32:58.617
    Jan  5 19:32:58.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 create -f -'
    Jan  5 19:32:59.355: INFO: stderr: ""
    Jan  5 19:32:59.355: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 19:32:59.355
    Jan  5 19:32:59.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 19:32:59.450: INFO: stderr: ""
    Jan  5 19:32:59.450: INFO: stdout: "update-demo-nautilus-lmw88 update-demo-nautilus-q45gm "
    Jan  5 19:32:59.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-lmw88 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:32:59.524: INFO: stderr: ""
    Jan  5 19:32:59.524: INFO: stdout: ""
    Jan  5 19:32:59.524: INFO: update-demo-nautilus-lmw88 is created but not running
    Jan  5 19:33:04.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 19:33:04.607: INFO: stderr: ""
    Jan  5 19:33:04.607: INFO: stdout: "update-demo-nautilus-lmw88 update-demo-nautilus-q45gm "
    Jan  5 19:33:04.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-lmw88 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:33:04.700: INFO: stderr: ""
    Jan  5 19:33:04.700: INFO: stdout: "true"
    Jan  5 19:33:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-lmw88 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 19:33:04.814: INFO: stderr: ""
    Jan  5 19:33:04.814: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  5 19:33:04.814: INFO: validating pod update-demo-nautilus-lmw88
    Jan  5 19:33:04.827: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 19:33:04.827: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 19:33:04.827: INFO: update-demo-nautilus-lmw88 is verified up and running
    Jan  5 19:33:04.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:33:04.907: INFO: stderr: ""
    Jan  5 19:33:04.907: INFO: stdout: "true"
    Jan  5 19:33:04.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 19:33:04.990: INFO: stderr: ""
    Jan  5 19:33:04.990: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  5 19:33:04.990: INFO: validating pod update-demo-nautilus-q45gm
    Jan  5 19:33:04.999: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 19:33:05.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 19:33:05.000: INFO: update-demo-nautilus-q45gm is verified up and running
    STEP: scaling down the replication controller 01/05/23 19:33:05
    Jan  5 19:33:05.003: INFO: scanned /root for discovery docs: <nil>
    Jan  5 19:33:05.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jan  5 19:33:06.113: INFO: stderr: ""
    Jan  5 19:33:06.113: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 19:33:06.113
    Jan  5 19:33:06.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 19:33:06.234: INFO: stderr: ""
    Jan  5 19:33:06.234: INFO: stdout: "update-demo-nautilus-q45gm "
    Jan  5 19:33:06.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:33:06.323: INFO: stderr: ""
    Jan  5 19:33:06.323: INFO: stdout: "true"
    Jan  5 19:33:06.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 19:33:06.418: INFO: stderr: ""
    Jan  5 19:33:06.418: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  5 19:33:06.418: INFO: validating pod update-demo-nautilus-q45gm
    Jan  5 19:33:06.431: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 19:33:06.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 19:33:06.431: INFO: update-demo-nautilus-q45gm is verified up and running
    STEP: scaling up the replication controller 01/05/23 19:33:06.431
    Jan  5 19:33:06.433: INFO: scanned /root for discovery docs: <nil>
    Jan  5 19:33:06.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jan  5 19:33:07.548: INFO: stderr: ""
    Jan  5 19:33:07.548: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 01/05/23 19:33:07.548
    Jan  5 19:33:07.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jan  5 19:33:07.705: INFO: stderr: ""
    Jan  5 19:33:07.705: INFO: stdout: "update-demo-nautilus-k42fs update-demo-nautilus-q45gm "
    Jan  5 19:33:07.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-k42fs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:33:07.830: INFO: stderr: ""
    Jan  5 19:33:07.830: INFO: stdout: "true"
    Jan  5 19:33:07.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-k42fs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 19:33:07.964: INFO: stderr: ""
    Jan  5 19:33:07.964: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  5 19:33:07.964: INFO: validating pod update-demo-nautilus-k42fs
    Jan  5 19:33:07.975: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 19:33:07.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 19:33:07.975: INFO: update-demo-nautilus-k42fs is verified up and running
    Jan  5 19:33:07.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jan  5 19:33:08.100: INFO: stderr: ""
    Jan  5 19:33:08.100: INFO: stdout: "true"
    Jan  5 19:33:08.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods update-demo-nautilus-q45gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jan  5 19:33:08.176: INFO: stderr: ""
    Jan  5 19:33:08.176: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
    Jan  5 19:33:08.176: INFO: validating pod update-demo-nautilus-q45gm
    Jan  5 19:33:08.179: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jan  5 19:33:08.179: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jan  5 19:33:08.179: INFO: update-demo-nautilus-q45gm is verified up and running
    STEP: using delete to clean up resources 01/05/23 19:33:08.179
    Jan  5 19:33:08.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 delete --grace-period=0 --force -f -'
    Jan  5 19:33:08.259: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 19:33:08.259: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jan  5 19:33:08.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get rc,svc -l name=update-demo --no-headers'
    Jan  5 19:33:08.405: INFO: stderr: "No resources found in kubectl-6313 namespace.\n"
    Jan  5 19:33:08.405: INFO: stdout: ""
    Jan  5 19:33:08.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6313 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jan  5 19:33:08.551: INFO: stderr: ""
    Jan  5 19:33:08.551: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:33:08.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6313" for this suite. 01/05/23 19:33:08.556
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:33:08.562
Jan  5 19:33:08.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:33:08.563
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:08.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:08.578
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167
STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 19:33:08.58
Jan  5 19:33:08.588: INFO: Waiting up to 5m0s for pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211" in namespace "emptydir-1649" to be "Succeeded or Failed"
Jan  5 19:33:08.592: INFO: Pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211": Phase="Pending", Reason="", readiness=false. Elapsed: 4.299036ms
Jan  5 19:33:10.596: INFO: Pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008084328s
Jan  5 19:33:12.596: INFO: Pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007828995s
STEP: Saw pod success 01/05/23 19:33:12.596
Jan  5 19:33:12.596: INFO: Pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211" satisfied condition "Succeeded or Failed"
Jan  5 19:33:12.599: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-f09ab353-9d85-484d-9a83-e8ad1a220211 container test-container: <nil>
STEP: delete the pod 01/05/23 19:33:12.613
Jan  5 19:33:12.626: INFO: Waiting for pod pod-f09ab353-9d85-484d-9a83-e8ad1a220211 to disappear
Jan  5 19:33:12.629: INFO: Pod pod-f09ab353-9d85-484d-9a83-e8ad1a220211 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:33:12.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-1649" for this suite. 01/05/23 19:33:12.633
------------------------------
• [4.076 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:167

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:33:08.562
    Jan  5 19:33:08.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:33:08.563
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:08.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:08.578
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:167
    STEP: Creating a pod to test emptydir 0644 on node default medium 01/05/23 19:33:08.58
    Jan  5 19:33:08.588: INFO: Waiting up to 5m0s for pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211" in namespace "emptydir-1649" to be "Succeeded or Failed"
    Jan  5 19:33:08.592: INFO: Pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211": Phase="Pending", Reason="", readiness=false. Elapsed: 4.299036ms
    Jan  5 19:33:10.596: INFO: Pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008084328s
    Jan  5 19:33:12.596: INFO: Pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007828995s
    STEP: Saw pod success 01/05/23 19:33:12.596
    Jan  5 19:33:12.596: INFO: Pod "pod-f09ab353-9d85-484d-9a83-e8ad1a220211" satisfied condition "Succeeded or Failed"
    Jan  5 19:33:12.599: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-f09ab353-9d85-484d-9a83-e8ad1a220211 container test-container: <nil>
    STEP: delete the pod 01/05/23 19:33:12.613
    Jan  5 19:33:12.626: INFO: Waiting for pod pod-f09ab353-9d85-484d-9a83-e8ad1a220211 to disappear
    Jan  5 19:33:12.629: INFO: Pod pod-f09ab353-9d85-484d-9a83-e8ad1a220211 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:33:12.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-1649" for this suite. 01/05/23 19:33:12.633
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:33:12.646
Jan  5 19:33:12.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:33:12.648
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:12.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:12.662
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326
STEP: Counting existing ResourceQuota 01/05/23 19:33:29.668
STEP: Creating a ResourceQuota 01/05/23 19:33:34.671
STEP: Ensuring resource quota status is calculated 01/05/23 19:33:34.676
STEP: Creating a ConfigMap 01/05/23 19:33:36.68
STEP: Ensuring resource quota status captures configMap creation 01/05/23 19:33:36.692
STEP: Deleting a ConfigMap 01/05/23 19:33:38.696
STEP: Ensuring resource quota status released usage 01/05/23 19:33:38.701
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:33:40.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-66" for this suite. 01/05/23 19:33:40.709
------------------------------
• [SLOW TEST] [28.067 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:33:12.646
    Jan  5 19:33:12.647: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:33:12.648
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:12.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:12.662
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:326
    STEP: Counting existing ResourceQuota 01/05/23 19:33:29.668
    STEP: Creating a ResourceQuota 01/05/23 19:33:34.671
    STEP: Ensuring resource quota status is calculated 01/05/23 19:33:34.676
    STEP: Creating a ConfigMap 01/05/23 19:33:36.68
    STEP: Ensuring resource quota status captures configMap creation 01/05/23 19:33:36.692
    STEP: Deleting a ConfigMap 01/05/23 19:33:38.696
    STEP: Ensuring resource quota status released usage 01/05/23 19:33:38.701
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:33:40.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-66" for this suite. 01/05/23 19:33:40.709
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:33:40.72
Jan  5 19:33:40.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:33:40.722
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:40.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:40.742
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1685
Jan  5 19:33:40.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-606 version'
Jan  5 19:33:40.812: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jan  5 19:33:40.812: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0-gke.1500\", GitCommit:\"c91157dfd0d025c4cc99027e9c89fd48e578b5dd\", GitTreeState:\"clean\", BuildDate:\"2022-12-28T09:35:54Z\", GoVersion:\"go1.19.4 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:33:40.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-606" for this suite. 01/05/23 19:33:40.816
------------------------------
• [0.101 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1679
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1685

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:33:40.72
    Jan  5 19:33:40.721: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:33:40.722
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:40.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:40.742
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1685
    Jan  5 19:33:40.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-606 version'
    Jan  5 19:33:40.812: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jan  5 19:33:40.812: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0\", GitCommit:\"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d\", GitTreeState:\"clean\", BuildDate:\"2022-12-08T19:58:30Z\", GoVersion:\"go1.19.4\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"26\", GitVersion:\"v1.26.0-gke.1500\", GitCommit:\"c91157dfd0d025c4cc99027e9c89fd48e578b5dd\", GitTreeState:\"clean\", BuildDate:\"2022-12-28T09:35:54Z\", GoVersion:\"go1.19.4 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:33:40.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-606" for this suite. 01/05/23 19:33:40.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:33:40.824
Jan  5 19:33:40.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replicaset 01/05/23 19:33:40.825
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:40.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:40.842
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/05/23 19:33:40.844
Jan  5 19:33:40.851: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9436" to be "running and ready"
Jan  5 19:33:40.855: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112834ms
Jan  5 19:33:40.855: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:33:42.859: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.00802077s
Jan  5 19:33:42.859: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jan  5 19:33:42.859: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 01/05/23 19:33:42.861
STEP: Then the orphan pod is adopted 01/05/23 19:33:42.866
STEP: When the matched label of one of its pods change 01/05/23 19:33:43.875
Jan  5 19:33:43.879: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 01/05/23 19:33:43.892
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:33:44.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-9436" for this suite. 01/05/23 19:33:44.902
------------------------------
• [4.084 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:33:40.824
    Jan  5 19:33:40.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replicaset 01/05/23 19:33:40.825
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:40.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:40.842
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 01/05/23 19:33:40.844
    Jan  5 19:33:40.851: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-9436" to be "running and ready"
    Jan  5 19:33:40.855: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112834ms
    Jan  5 19:33:40.855: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:33:42.859: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.00802077s
    Jan  5 19:33:42.859: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jan  5 19:33:42.859: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 01/05/23 19:33:42.861
    STEP: Then the orphan pod is adopted 01/05/23 19:33:42.866
    STEP: When the matched label of one of its pods change 01/05/23 19:33:43.875
    Jan  5 19:33:43.879: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/05/23 19:33:43.892
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:33:44.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-9436" for this suite. 01/05/23 19:33:44.902
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:33:44.916
Jan  5 19:33:44.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:33:44.918
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:44.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:44.936
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250
STEP: creating service in namespace services-9510 01/05/23 19:33:44.938
STEP: creating service affinity-nodeport-transition in namespace services-9510 01/05/23 19:33:44.939
STEP: creating replication controller affinity-nodeport-transition in namespace services-9510 01/05/23 19:33:44.956
I0105 19:33:44.965051      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9510, replica count: 3
I0105 19:33:48.016196      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 19:33:48.023: INFO: Creating new exec pod
Jan  5 19:33:48.030: INFO: Waiting up to 5m0s for pod "execpod-affinity92vcq" in namespace "services-9510" to be "running"
Jan  5 19:33:48.036: INFO: Pod "execpod-affinity92vcq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.592136ms
Jan  5 19:33:50.044: INFO: Pod "execpod-affinity92vcq": Phase="Running", Reason="", readiness=true. Elapsed: 2.013139799s
Jan  5 19:33:50.044: INFO: Pod "execpod-affinity92vcq" satisfied condition "running"
Jan  5 19:33:51.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan  5 19:33:51.210: INFO: rc: 1
Jan  5 19:33:51.210: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:33:52.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan  5 19:33:52.362: INFO: rc: 1
Jan  5 19:33:52.362: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:33:53.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan  5 19:33:53.357: INFO: rc: 1
Jan  5 19:33:53.357: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:33:54.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan  5 19:33:54.469: INFO: rc: 1
Jan  5 19:33:54.469: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-nodeport-transition 80
nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:33:55.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
Jan  5 19:33:55.356: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jan  5 19:33:55.356: INFO: stdout: ""
Jan  5 19:33:55.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 10.20.4.168 80'
Jan  5 19:33:55.499: INFO: stderr: "+ nc -v -z -w 2 10.20.4.168 80\nConnection to 10.20.4.168 80 port [tcp/http] succeeded!\n"
Jan  5 19:33:55.499: INFO: stdout: ""
Jan  5 19:33:55.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 10.196.0.38 31587'
Jan  5 19:33:55.636: INFO: stderr: "+ nc -v -z -w 2 10.196.0.38 31587\nConnection to 10.196.0.38 31587 port [tcp/*] succeeded!\n"
Jan  5 19:33:55.636: INFO: stdout: ""
Jan  5 19:33:55.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 10.196.0.39 31587'
Jan  5 19:33:55.773: INFO: stderr: "+ nc -v -z -w 2 10.196.0.39 31587\nConnection to 10.196.0.39 31587 port [tcp/*] succeeded!\n"
Jan  5 19:33:55.773: INFO: stdout: ""
Jan  5 19:33:55.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.196.0.39:31587/ ; done'
Jan  5 19:33:56.009: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n"
Jan  5 19:33:56.009: INFO: stdout: "\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w"
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.196.0.39:31587/ ; done'
Jan  5 19:34:26.238: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n"
Jan  5 19:34:26.238: INFO: stdout: "\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-c4bts"
Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-t5pbp
Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-t5pbp
Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-t5pbp
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-t5pbp
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-t5pbp
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.196.0.39:31587/ ; done'
Jan  5 19:34:26.490: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n"
Jan  5 19:34:26.490: INFO: stdout: "\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts"
Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
Jan  5 19:34:26.491: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9510, will wait for the garbage collector to delete the pods 01/05/23 19:34:26.5
Jan  5 19:34:26.562: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.934663ms
Jan  5 19:34:26.662: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.415036ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:34:28.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-9510" for this suite. 01/05/23 19:34:28.387
------------------------------
• [SLOW TEST] [43.475 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:33:44.916
    Jan  5 19:33:44.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:33:44.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:33:44.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:33:44.936
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2250
    STEP: creating service in namespace services-9510 01/05/23 19:33:44.938
    STEP: creating service affinity-nodeport-transition in namespace services-9510 01/05/23 19:33:44.939
    STEP: creating replication controller affinity-nodeport-transition in namespace services-9510 01/05/23 19:33:44.956
    I0105 19:33:44.965051      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-9510, replica count: 3
    I0105 19:33:48.016196      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 19:33:48.023: INFO: Creating new exec pod
    Jan  5 19:33:48.030: INFO: Waiting up to 5m0s for pod "execpod-affinity92vcq" in namespace "services-9510" to be "running"
    Jan  5 19:33:48.036: INFO: Pod "execpod-affinity92vcq": Phase="Pending", Reason="", readiness=false. Elapsed: 5.592136ms
    Jan  5 19:33:50.044: INFO: Pod "execpod-affinity92vcq": Phase="Running", Reason="", readiness=true. Elapsed: 2.013139799s
    Jan  5 19:33:50.044: INFO: Pod "execpod-affinity92vcq" satisfied condition "running"
    Jan  5 19:33:51.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan  5 19:33:51.210: INFO: rc: 1
    Jan  5 19:33:51.210: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-nodeport-transition 80
    nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:33:52.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan  5 19:33:52.362: INFO: rc: 1
    Jan  5 19:33:52.362: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-nodeport-transition 80
    nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:33:53.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan  5 19:33:53.357: INFO: rc: 1
    Jan  5 19:33:53.357: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-nodeport-transition 80
    nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:33:54.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan  5 19:33:54.469: INFO: rc: 1
    Jan  5 19:33:54.469: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-nodeport-transition 80
    nc: connect to affinity-nodeport-transition port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:33:55.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport-transition 80'
    Jan  5 19:33:55.356: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jan  5 19:33:55.356: INFO: stdout: ""
    Jan  5 19:33:55.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 10.20.4.168 80'
    Jan  5 19:33:55.499: INFO: stderr: "+ nc -v -z -w 2 10.20.4.168 80\nConnection to 10.20.4.168 80 port [tcp/http] succeeded!\n"
    Jan  5 19:33:55.499: INFO: stdout: ""
    Jan  5 19:33:55.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 10.196.0.38 31587'
    Jan  5 19:33:55.636: INFO: stderr: "+ nc -v -z -w 2 10.196.0.38 31587\nConnection to 10.196.0.38 31587 port [tcp/*] succeeded!\n"
    Jan  5 19:33:55.636: INFO: stdout: ""
    Jan  5 19:33:55.636: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c nc -v -z -w 2 10.196.0.39 31587'
    Jan  5 19:33:55.773: INFO: stderr: "+ nc -v -z -w 2 10.196.0.39 31587\nConnection to 10.196.0.39 31587 port [tcp/*] succeeded!\n"
    Jan  5 19:33:55.773: INFO: stdout: ""
    Jan  5 19:33:55.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.196.0.39:31587/ ; done'
    Jan  5 19:33:56.009: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n"
    Jan  5 19:33:56.009: INFO: stdout: "\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w"
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:33:56.009: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.196.0.39:31587/ ; done'
    Jan  5 19:34:26.238: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n"
    Jan  5 19:34:26.238: INFO: stdout: "\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-t5pbp\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-hgk6w\naffinity-nodeport-transition-c4bts"
    Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-t5pbp
    Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-t5pbp
    Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.238: INFO: Received response from host: affinity-nodeport-transition-t5pbp
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-t5pbp
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-t5pbp
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-hgk6w
    Jan  5 19:34:26.239: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-9510 exec execpod-affinity92vcq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.196.0.39:31587/ ; done'
    Jan  5 19:34:26.490: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31587/\n"
    Jan  5 19:34:26.490: INFO: stdout: "\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts\naffinity-nodeport-transition-c4bts"
    Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.490: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Received response from host: affinity-nodeport-transition-c4bts
    Jan  5 19:34:26.491: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-9510, will wait for the garbage collector to delete the pods 01/05/23 19:34:26.5
    Jan  5 19:34:26.562: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.934663ms
    Jan  5 19:34:26.662: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.415036ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:34:28.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-9510" for this suite. 01/05/23 19:34:28.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:34:28.392
Jan  5 19:34:28.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:34:28.394
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:28.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:28.412
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884
STEP: Creating a ResourceQuota 01/05/23 19:34:28.415
STEP: Getting a ResourceQuota 01/05/23 19:34:28.42
STEP: Updating a ResourceQuota 01/05/23 19:34:28.422
STEP: Verifying a ResourceQuota was modified 01/05/23 19:34:28.431
STEP: Deleting a ResourceQuota 01/05/23 19:34:28.434
STEP: Verifying the deleted ResourceQuota 01/05/23 19:34:28.439
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:34:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-9484" for this suite. 01/05/23 19:34:28.444
------------------------------
• [0.058 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:884

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:34:28.392
    Jan  5 19:34:28.392: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:34:28.394
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:28.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:28.412
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:884
    STEP: Creating a ResourceQuota 01/05/23 19:34:28.415
    STEP: Getting a ResourceQuota 01/05/23 19:34:28.42
    STEP: Updating a ResourceQuota 01/05/23 19:34:28.422
    STEP: Verifying a ResourceQuota was modified 01/05/23 19:34:28.431
    STEP: Deleting a ResourceQuota 01/05/23 19:34:28.434
    STEP: Verifying the deleted ResourceQuota 01/05/23 19:34:28.439
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:34:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-9484" for this suite. 01/05/23 19:34:28.444
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:34:28.455
Jan  5 19:34:28.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename daemonsets 01/05/23 19:34:28.456
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:28.472
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:28.475
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294
STEP: Creating a simple DaemonSet "daemon-set" 01/05/23 19:34:28.494
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:34:28.499
Jan  5 19:34:28.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:34:28.505: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:34:29.512: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:34:29.512: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:34:30.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 19:34:30.513: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:34:31.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 19:34:31.514: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/05/23 19:34:31.518
Jan  5 19:34:31.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 19:34:31.540: INFO: Node gke-gke-1-26-default-pool-05283374-dbpc is running 0 daemon pod, expected 1
Jan  5 19:34:32.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 19:34:32.547: INFO: Node gke-gke-1-26-default-pool-05283374-dbpc is running 0 daemon pod, expected 1
Jan  5 19:34:33.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 19:34:33.548: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 01/05/23 19:34:33.549
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:34:33.553
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1958, will wait for the garbage collector to delete the pods 01/05/23 19:34:33.554
Jan  5 19:34:33.611: INFO: Deleting DaemonSet.extensions daemon-set took: 4.35836ms
Jan  5 19:34:33.714: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.620728ms
Jan  5 19:34:36.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:34:36.418: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 19:34:36.420: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74259"},"items":null}

Jan  5 19:34:36.422: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74259"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:34:36.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-1958" for this suite. 01/05/23 19:34:36.434
------------------------------
• [SLOW TEST] [7.984 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:294

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:34:28.455
    Jan  5 19:34:28.455: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename daemonsets 01/05/23 19:34:28.456
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:28.472
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:28.475
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:294
    STEP: Creating a simple DaemonSet "daemon-set" 01/05/23 19:34:28.494
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:34:28.499
    Jan  5 19:34:28.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:34:28.505: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:34:29.512: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:34:29.512: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:34:30.513: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 19:34:30.513: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:34:31.514: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 19:34:31.514: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 01/05/23 19:34:31.518
    Jan  5 19:34:31.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 19:34:31.540: INFO: Node gke-gke-1-26-default-pool-05283374-dbpc is running 0 daemon pod, expected 1
    Jan  5 19:34:32.547: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 19:34:32.547: INFO: Node gke-gke-1-26-default-pool-05283374-dbpc is running 0 daemon pod, expected 1
    Jan  5 19:34:33.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 19:34:33.548: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 01/05/23 19:34:33.549
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:34:33.553
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1958, will wait for the garbage collector to delete the pods 01/05/23 19:34:33.554
    Jan  5 19:34:33.611: INFO: Deleting DaemonSet.extensions daemon-set took: 4.35836ms
    Jan  5 19:34:33.714: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.620728ms
    Jan  5 19:34:36.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:34:36.418: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 19:34:36.420: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74259"},"items":null}

    Jan  5 19:34:36.422: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74259"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:34:36.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-1958" for this suite. 01/05/23 19:34:36.434
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:34:36.439
Jan  5 19:34:36.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:34:36.441
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:36.45
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:36.454
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jan  5 19:34:36.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:34:42.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-1083" for this suite. 01/05/23 19:34:42.721
------------------------------
• [SLOW TEST] [6.287 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:34:36.439
    Jan  5 19:34:36.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:34:36.441
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:36.45
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:36.454
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jan  5 19:34:36.456: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:34:42.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-1083" for this suite. 01/05/23 19:34:42.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:34:42.734
Jan  5 19:34:42.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:34:42.735
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:42.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:42.749
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jan  5 19:34:42.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:34:45.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-7554" for this suite. 01/05/23 19:34:45.991
------------------------------
• [3.262 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:34:42.734
    Jan  5 19:34:42.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:34:42.735
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:42.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:42.749
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jan  5 19:34:42.752: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:34:45.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-7554" for this suite. 01/05/23 19:34:45.991
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:34:45.999
Jan  5 19:34:46.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:34:46.001
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:46.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:46.017
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226
STEP: creating the pod 01/05/23 19:34:46.019
STEP: setting up watch 01/05/23 19:34:46.019
STEP: submitting the pod to kubernetes 01/05/23 19:34:46.123
STEP: verifying the pod is in kubernetes 01/05/23 19:34:46.135
STEP: verifying pod creation was observed 01/05/23 19:34:46.14
Jan  5 19:34:46.140: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ea727073-7972-4330-b300-a74f846c4b16" in namespace "pods-8151" to be "running"
Jan  5 19:34:46.143: INFO: Pod "pod-submit-remove-ea727073-7972-4330-b300-a74f846c4b16": Phase="Pending", Reason="", readiness=false. Elapsed: 3.056903ms
Jan  5 19:34:48.147: INFO: Pod "pod-submit-remove-ea727073-7972-4330-b300-a74f846c4b16": Phase="Running", Reason="", readiness=true. Elapsed: 2.006889687s
Jan  5 19:34:48.147: INFO: Pod "pod-submit-remove-ea727073-7972-4330-b300-a74f846c4b16" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 19:34:48.15
STEP: verifying pod deletion was observed 01/05/23 19:34:48.158
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:34:51.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8151" for this suite. 01/05/23 19:34:51.093
------------------------------
• [SLOW TEST] [5.099 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:34:45.999
    Jan  5 19:34:46.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:34:46.001
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:46.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:46.017
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:226
    STEP: creating the pod 01/05/23 19:34:46.019
    STEP: setting up watch 01/05/23 19:34:46.019
    STEP: submitting the pod to kubernetes 01/05/23 19:34:46.123
    STEP: verifying the pod is in kubernetes 01/05/23 19:34:46.135
    STEP: verifying pod creation was observed 01/05/23 19:34:46.14
    Jan  5 19:34:46.140: INFO: Waiting up to 5m0s for pod "pod-submit-remove-ea727073-7972-4330-b300-a74f846c4b16" in namespace "pods-8151" to be "running"
    Jan  5 19:34:46.143: INFO: Pod "pod-submit-remove-ea727073-7972-4330-b300-a74f846c4b16": Phase="Pending", Reason="", readiness=false. Elapsed: 3.056903ms
    Jan  5 19:34:48.147: INFO: Pod "pod-submit-remove-ea727073-7972-4330-b300-a74f846c4b16": Phase="Running", Reason="", readiness=true. Elapsed: 2.006889687s
    Jan  5 19:34:48.147: INFO: Pod "pod-submit-remove-ea727073-7972-4330-b300-a74f846c4b16" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 19:34:48.15
    STEP: verifying pod deletion was observed 01/05/23 19:34:48.158
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:34:51.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8151" for this suite. 01/05/23 19:34:51.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:34:51.103
Jan  5 19:34:51.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename deployment 01/05/23 19:34:51.105
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:51.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:51.126
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jan  5 19:34:51.132: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jan  5 19:34:51.145: INFO: Pod name sample-pod: Found 0 pods out of 1
Jan  5 19:34:56.148: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 19:34:56.148
Jan  5 19:34:56.149: INFO: Creating deployment "test-rolling-update-deployment"
Jan  5 19:34:56.155: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jan  5 19:34:56.163: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jan  5 19:34:58.170: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jan  5 19:34:58.180: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 19:34:58.195: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9859  b4ee702c-00d0-49a3-b62d-4c9d984efbc7 74584 1 2023-01-05 19:34:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-05 19:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ef368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 19:34:56 +0000 UTC,LastTransitionTime:2023-01-05 19:34:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-05 19:34:58 +0000 UTC,LastTransitionTime:2023-01-05 19:34:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 19:34:58.199: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-9859  8bd2056a-b8fa-4319-bf2b-f7e529e8edef 74576 1 2023-01-05 19:34:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b4ee702c-00d0-49a3-b62d-4c9d984efbc7 0xc0052084f7 0xc0052084f8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4ee702c-00d0-49a3-b62d-4c9d984efbc7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052085b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:34:58.199: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jan  5 19:34:58.199: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9859  91e45eda-a84d-47a8-841c-1952d9f8a8ea 74583 2 2023-01-05 19:34:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b4ee702c-00d0-49a3-b62d-4c9d984efbc7 0xc0052083c7 0xc0052083c8}] [] [{e2e.test Update apps/v1 2023-01-05 19:34:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4ee702c-00d0-49a3-b62d-4c9d984efbc7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005208488 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:34:58.202: INFO: Pod "test-rolling-update-deployment-7549d9f46d-qljgq" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-qljgq test-rolling-update-deployment-7549d9f46d- deployment-9859  e742fc5a-8186-40bf-a66a-7851370f0cba 74575 0 2023-01-05 19:34:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 8bd2056a-b8fa-4319-bf2b-f7e529e8edef 0xc005208a27 0xc005208a28}] [] [{kube-controller-manager Update v1 2023-01-05 19:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bd2056a-b8fa-4319-bf2b-f7e529e8edef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hbglw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hbglw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:34:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:34:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.221,StartTime:2023-01-05 19:34:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:34:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://09cdb71ceb6b54f5925009a032c71cba706b35d36703ca65c1548202ab5ca22d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  5 19:34:58.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-9859" for this suite. 01/05/23 19:34:58.209
------------------------------
• [SLOW TEST] [7.112 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:34:51.103
    Jan  5 19:34:51.103: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename deployment 01/05/23 19:34:51.105
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:51.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:51.126
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jan  5 19:34:51.132: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jan  5 19:34:51.145: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jan  5 19:34:56.148: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 19:34:56.148
    Jan  5 19:34:56.149: INFO: Creating deployment "test-rolling-update-deployment"
    Jan  5 19:34:56.155: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jan  5 19:34:56.163: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jan  5 19:34:58.170: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jan  5 19:34:58.180: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 19:34:58.195: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9859  b4ee702c-00d0-49a3-b62d-4c9d984efbc7 74584 1 2023-01-05 19:34:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-01-05 19:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0041ef368 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 19:34:56 +0000 UTC,LastTransitionTime:2023-01-05 19:34:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-7549d9f46d" has successfully progressed.,LastUpdateTime:2023-01-05 19:34:58 +0000 UTC,LastTransitionTime:2023-01-05 19:34:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 19:34:58.199: INFO: New ReplicaSet "test-rolling-update-deployment-7549d9f46d" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-7549d9f46d  deployment-9859  8bd2056a-b8fa-4319-bf2b-f7e529e8edef 74576 1 2023-01-05 19:34:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b4ee702c-00d0-49a3-b62d-4c9d984efbc7 0xc0052084f7 0xc0052084f8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4ee702c-00d0-49a3-b62d-4c9d984efbc7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 7549d9f46d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052085b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:34:58.199: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jan  5 19:34:58.199: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9859  91e45eda-a84d-47a8-841c-1952d9f8a8ea 74583 2 2023-01-05 19:34:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b4ee702c-00d0-49a3-b62d-4c9d984efbc7 0xc0052083c7 0xc0052083c8}] [] [{e2e.test Update apps/v1 2023-01-05 19:34:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b4ee702c-00d0-49a3-b62d-4c9d984efbc7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005208488 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:34:58.202: INFO: Pod "test-rolling-update-deployment-7549d9f46d-qljgq" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-7549d9f46d-qljgq test-rolling-update-deployment-7549d9f46d- deployment-9859  e742fc5a-8186-40bf-a66a-7851370f0cba 74575 0 2023-01-05 19:34:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:7549d9f46d] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-7549d9f46d 8bd2056a-b8fa-4319-bf2b-f7e529e8edef 0xc005208a27 0xc005208a28}] [] [{kube-controller-manager Update v1 2023-01-05 19:34:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8bd2056a-b8fa-4319-bf2b-f7e529e8edef\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:34:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hbglw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hbglw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:34:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:34:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:34:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.221,StartTime:2023-01-05 19:34:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:34:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://09cdb71ceb6b54f5925009a032c71cba706b35d36703ca65c1548202ab5ca22d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.221,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:34:58.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-9859" for this suite. 01/05/23 19:34:58.209
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:34:58.216
Jan  5 19:34:58.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename podtemplate 01/05/23 19:34:58.218
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:58.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:58.239
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:31
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 01/05/23 19:34:58.242
STEP: Replace a pod template 01/05/23 19:34:58.247
Jan  5 19:34:58.254: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/node/init/init.go:32
Jan  5 19:34:58.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] PodTemplates
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] PodTemplates
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] PodTemplates
  tear down framework | framework.go:193
STEP: Destroying namespace "podtemplate-8812" for this suite. 01/05/23 19:34:58.258
------------------------------
• [0.047 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:34:58.216
    Jan  5 19:34:58.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename podtemplate 01/05/23 19:34:58.218
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:58.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:58.239
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 01/05/23 19:34:58.242
    STEP: Replace a pod template 01/05/23 19:34:58.247
    Jan  5 19:34:58.254: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:34:58.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] PodTemplates
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] PodTemplates
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] PodTemplates
      tear down framework | framework.go:193
    STEP: Destroying namespace "podtemplate-8812" for this suite. 01/05/23 19:34:58.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:34:58.278
Jan  5 19:34:58.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:34:58.279
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:58.295
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:58.299
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153
Jan  5 19:34:58.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 19:34:59.859
Jan  5 19:34:59.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 --namespace=crd-publish-openapi-8369 create -f -'
Jan  5 19:35:00.949: INFO: stderr: ""
Jan  5 19:35:00.949: INFO: stdout: "e2e-test-crd-publish-openapi-8066-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  5 19:35:00.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 --namespace=crd-publish-openapi-8369 delete e2e-test-crd-publish-openapi-8066-crds test-cr'
Jan  5 19:35:01.032: INFO: stderr: ""
Jan  5 19:35:01.032: INFO: stdout: "e2e-test-crd-publish-openapi-8066-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jan  5 19:35:01.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 --namespace=crd-publish-openapi-8369 apply -f -'
Jan  5 19:35:01.333: INFO: stderr: ""
Jan  5 19:35:01.333: INFO: stdout: "e2e-test-crd-publish-openapi-8066-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jan  5 19:35:01.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 --namespace=crd-publish-openapi-8369 delete e2e-test-crd-publish-openapi-8066-crds test-cr'
Jan  5 19:35:01.415: INFO: stderr: ""
Jan  5 19:35:01.415: INFO: stdout: "e2e-test-crd-publish-openapi-8066-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 01/05/23 19:35:01.415
Jan  5 19:35:01.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 explain e2e-test-crd-publish-openapi-8066-crds'
Jan  5 19:35:01.676: INFO: stderr: ""
Jan  5 19:35:01.676: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8066-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:35:03.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-8369" for this suite. 01/05/23 19:35:03.859
------------------------------
• [SLOW TEST] [5.597 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:34:58.278
    Jan  5 19:34:58.278: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 19:34:58.279
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:34:58.295
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:34:58.299
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:153
    Jan  5 19:34:58.302: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 19:34:59.859
    Jan  5 19:34:59.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 --namespace=crd-publish-openapi-8369 create -f -'
    Jan  5 19:35:00.949: INFO: stderr: ""
    Jan  5 19:35:00.949: INFO: stdout: "e2e-test-crd-publish-openapi-8066-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  5 19:35:00.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 --namespace=crd-publish-openapi-8369 delete e2e-test-crd-publish-openapi-8066-crds test-cr'
    Jan  5 19:35:01.032: INFO: stderr: ""
    Jan  5 19:35:01.032: INFO: stdout: "e2e-test-crd-publish-openapi-8066-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jan  5 19:35:01.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 --namespace=crd-publish-openapi-8369 apply -f -'
    Jan  5 19:35:01.333: INFO: stderr: ""
    Jan  5 19:35:01.333: INFO: stdout: "e2e-test-crd-publish-openapi-8066-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jan  5 19:35:01.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 --namespace=crd-publish-openapi-8369 delete e2e-test-crd-publish-openapi-8066-crds test-cr'
    Jan  5 19:35:01.415: INFO: stderr: ""
    Jan  5 19:35:01.415: INFO: stdout: "e2e-test-crd-publish-openapi-8066-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 01/05/23 19:35:01.415
    Jan  5 19:35:01.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-8369 explain e2e-test-crd-publish-openapi-8066-crds'
    Jan  5 19:35:01.676: INFO: stderr: ""
    Jan  5 19:35:01.676: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8066-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:35:03.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-8369" for this suite. 01/05/23 19:35:03.859
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
[BeforeEach] [sig-storage] Projected combined
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:35:03.875
Jan  5 19:35:03.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:35:03.877
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:03.901
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:03.906
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:31
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44
STEP: Creating configMap with name configmap-projected-all-test-volume-b46215a2-eac9-4674-b974-9d0536e6f50d 01/05/23 19:35:03.909
STEP: Creating secret with name secret-projected-all-test-volume-2131b300-f1b7-47dc-905f-6f006bab4a71 01/05/23 19:35:03.915
STEP: Creating a pod to test Check all projections for projected volume plugin 01/05/23 19:35:03.918
Jan  5 19:35:03.928: INFO: Waiting up to 5m0s for pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf" in namespace "projected-1308" to be "Succeeded or Failed"
Jan  5 19:35:03.933: INFO: Pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163244ms
Jan  5 19:35:05.936: INFO: Pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007393121s
Jan  5 19:35:07.936: INFO: Pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007748102s
STEP: Saw pod success 01/05/23 19:35:07.937
Jan  5 19:35:07.937: INFO: Pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf" satisfied condition "Succeeded or Failed"
Jan  5 19:35:07.939: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf container projected-all-volume-test: <nil>
STEP: delete the pod 01/05/23 19:35:07.953
Jan  5 19:35:07.964: INFO: Waiting for pod projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf to disappear
Jan  5 19:35:07.967: INFO: Pod projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/node/init/init.go:32
Jan  5 19:35:07.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected combined
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected combined
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected combined
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1308" for this suite. 01/05/23 19:35:07.972
------------------------------
• [4.105 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:35:03.875
    Jan  5 19:35:03.875: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:35:03.877
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:03.901
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:03.906
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:31
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:44
    STEP: Creating configMap with name configmap-projected-all-test-volume-b46215a2-eac9-4674-b974-9d0536e6f50d 01/05/23 19:35:03.909
    STEP: Creating secret with name secret-projected-all-test-volume-2131b300-f1b7-47dc-905f-6f006bab4a71 01/05/23 19:35:03.915
    STEP: Creating a pod to test Check all projections for projected volume plugin 01/05/23 19:35:03.918
    Jan  5 19:35:03.928: INFO: Waiting up to 5m0s for pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf" in namespace "projected-1308" to be "Succeeded or Failed"
    Jan  5 19:35:03.933: INFO: Pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.163244ms
    Jan  5 19:35:05.936: INFO: Pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007393121s
    Jan  5 19:35:07.936: INFO: Pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007748102s
    STEP: Saw pod success 01/05/23 19:35:07.937
    Jan  5 19:35:07.937: INFO: Pod "projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf" satisfied condition "Succeeded or Failed"
    Jan  5 19:35:07.939: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf container projected-all-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:35:07.953
    Jan  5 19:35:07.964: INFO: Waiting for pod projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf to disappear
    Jan  5 19:35:07.967: INFO: Pod projected-volume-95960046-5d6a-4f0f-9a32-07d1287b72cf no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:35:07.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected combined
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected combined
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected combined
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1308" for this suite. 01/05/23 19:35:07.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:35:07.983
Jan  5 19:35:07.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:35:07.984
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:07.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:08
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:777
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:35:08.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6861" for this suite. 01/05/23 19:35:08.008
------------------------------
• [0.031 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:777

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:35:07.983
    Jan  5 19:35:07.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:35:07.984
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:07.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:08
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:777
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:35:08.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6861" for this suite. 01/05/23 19:35:08.008
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:35:08.019
Jan  5 19:35:08.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename daemonsets 01/05/23 19:35:08.021
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:08.035
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:08.041
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823
STEP: Creating simple DaemonSet "daemon-set" 01/05/23 19:35:08.059
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:35:08.065
Jan  5 19:35:08.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:35:08.070: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:35:09.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 19:35:09.079: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:35:10.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 19:35:10.078: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 01/05/23 19:35:10.08
STEP: DeleteCollection of the DaemonSets 01/05/23 19:35:10.083
STEP: Verify that ReplicaSets have been deleted 01/05/23 19:35:10.09
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
Jan  5 19:35:10.104: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74775"},"items":null}

Jan  5 19:35:10.108: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74775"},"items":[{"metadata":{"name":"daemon-set-2q862","generateName":"daemon-set-","namespace":"daemonsets-2161","uid":"2a4509b0-130f-47e1-ae29-57d7105ea55b","resourceVersion":"74765","creationTimestamp":"2023-01-05T19:35:08Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"439a30e4-1c08-4907-b50b-29d253ca1b38","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"439a30e4-1c08-4907-b50b-29d253ca1b38\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vgksg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vgksg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gke-gke-1-26-default-pool-05283374-16pz","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gke-gke-1-26-default-pool-05283374-16pz"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"}],"hostIP":"10.196.0.39","podIP":"10.16.2.118","podIPs":[{"ip":"10.16.2.118"}],"startTime":"2023-01-05T19:35:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T19:35:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://61319bc41b03ccff953ffdb79a69c84f405aa4061c2ba55a78dfe37c96109cef","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lfvch","generateName":"daemon-set-","namespace":"daemonsets-2161","uid":"4e39618e-f784-4509-94f3-f84db8f44d3a","resourceVersion":"74770","creationTimestamp":"2023-01-05T19:35:08Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"439a30e4-1c08-4907-b50b-29d253ca1b38","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"439a30e4-1c08-4907-b50b-29d253ca1b38\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.0.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xtnng","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xtnng","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gke-gke-1-26-default-pool-05283374-dbpc","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gke-gke-1-26-default-pool-05283374-dbpc"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"}],"hostIP":"10.196.0.37","podIP":"10.16.0.158","podIPs":[{"ip":"10.16.0.158"}],"startTime":"2023-01-05T19:35:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T19:35:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0ecf98cf7a59de17c2e36f7271cade620c071e9c16adb0ed86fe141a322cf621","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rnn2m","generateName":"daemon-set-","namespace":"daemonsets-2161","uid":"8deeed8e-a5ac-402e-8ee2-b8e37f2895ba","resourceVersion":"74761","creationTimestamp":"2023-01-05T19:35:08Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"439a30e4-1c08-4907-b50b-29d253ca1b38","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"439a30e4-1c08-4907-b50b-29d253ca1b38\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-24wzb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-24wzb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gke-gke-1-26-default-pool-05283374-qmj7","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gke-gke-1-26-default-pool-05283374-qmj7"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"}],"hostIP":"10.196.0.38","podIP":"10.16.1.222","podIPs":[{"ip":"10.16.1.222"}],"startTime":"2023-01-05T19:35:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T19:35:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f97bd6a24a4a191c2cd7f51e53651bab5bf7d65bb13a09d2248399fddd71bd08","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:35:10.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-2161" for this suite. 01/05/23 19:35:10.131
------------------------------
• [2.121 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:823

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:35:08.019
    Jan  5 19:35:08.019: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename daemonsets 01/05/23 19:35:08.021
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:08.035
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:08.041
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:823
    STEP: Creating simple DaemonSet "daemon-set" 01/05/23 19:35:08.059
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:35:08.065
    Jan  5 19:35:08.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:35:08.070: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:35:09.079: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 19:35:09.079: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:35:10.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 19:35:10.078: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 01/05/23 19:35:10.08
    STEP: DeleteCollection of the DaemonSets 01/05/23 19:35:10.083
    STEP: Verify that ReplicaSets have been deleted 01/05/23 19:35:10.09
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    Jan  5 19:35:10.104: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"74775"},"items":null}

    Jan  5 19:35:10.108: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"74775"},"items":[{"metadata":{"name":"daemon-set-2q862","generateName":"daemon-set-","namespace":"daemonsets-2161","uid":"2a4509b0-130f-47e1-ae29-57d7105ea55b","resourceVersion":"74765","creationTimestamp":"2023-01-05T19:35:08Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"439a30e4-1c08-4907-b50b-29d253ca1b38","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"439a30e4-1c08-4907-b50b-29d253ca1b38\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.118\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vgksg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vgksg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gke-gke-1-26-default-pool-05283374-16pz","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gke-gke-1-26-default-pool-05283374-16pz"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"}],"hostIP":"10.196.0.39","podIP":"10.16.2.118","podIPs":[{"ip":"10.16.2.118"}],"startTime":"2023-01-05T19:35:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T19:35:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://61319bc41b03ccff953ffdb79a69c84f405aa4061c2ba55a78dfe37c96109cef","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-lfvch","generateName":"daemon-set-","namespace":"daemonsets-2161","uid":"4e39618e-f784-4509-94f3-f84db8f44d3a","resourceVersion":"74770","creationTimestamp":"2023-01-05T19:35:08Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"439a30e4-1c08-4907-b50b-29d253ca1b38","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"439a30e4-1c08-4907-b50b-29d253ca1b38\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.0.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xtnng","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xtnng","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gke-gke-1-26-default-pool-05283374-dbpc","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gke-gke-1-26-default-pool-05283374-dbpc"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"}],"hostIP":"10.196.0.37","podIP":"10.16.0.158","podIPs":[{"ip":"10.16.0.158"}],"startTime":"2023-01-05T19:35:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T19:35:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0ecf98cf7a59de17c2e36f7271cade620c071e9c16adb0ed86fe141a322cf621","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-rnn2m","generateName":"daemon-set-","namespace":"daemonsets-2161","uid":"8deeed8e-a5ac-402e-8ee2-b8e37f2895ba","resourceVersion":"74761","creationTimestamp":"2023-01-05T19:35:08Z","labels":{"controller-revision-hash":"6cff669f8c","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"439a30e4-1c08-4907-b50b-29d253ca1b38","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"439a30e4-1c08-4907-b50b-29d253ca1b38\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-01-05T19:35:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-24wzb","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-24wzb","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"gke-gke-1-26-default-pool-05283374-qmj7","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["gke-gke-1-26-default-pool-05283374-qmj7"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-01-05T19:35:08Z"}],"hostIP":"10.196.0.38","podIP":"10.16.1.222","podIPs":[{"ip":"10.16.1.222"}],"startTime":"2023-01-05T19:35:08Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-01-05T19:35:09Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://f97bd6a24a4a191c2cd7f51e53651bab5bf7d65bb13a09d2248399fddd71bd08","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:35:10.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-2161" for this suite. 01/05/23 19:35:10.131
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:35:10.141
Jan  5 19:35:10.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename disruption 01/05/23 19:35:10.142
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:10.152
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:10.156
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:35:10.159
Jan  5 19:35:10.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename disruption-2 01/05/23 19:35:10.161
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:10.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:10.177
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:31
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:87
STEP: Waiting for the pdb to be processed 01/05/23 19:35:10.231
STEP: Waiting for the pdb to be processed 01/05/23 19:35:12.242
STEP: Waiting for the pdb to be processed 01/05/23 19:35:14.254
STEP: listing a collection of PDBs across all namespaces 01/05/23 19:35:16.26
STEP: listing a collection of PDBs in namespace disruption-1329 01/05/23 19:35:16.263
STEP: deleting a collection of PDBs 01/05/23 19:35:16.265
STEP: Waiting for the PDB collection to be deleted 01/05/23 19:35:16.274
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/node/init/init.go:32
Jan  5 19:35:16.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  5 19:35:16.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  dump namespaces | framework.go:196
[DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-2-1596" for this suite. 01/05/23 19:35:16.283
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1329" for this suite. 01/05/23 19:35:16.288
------------------------------
• [SLOW TEST] [6.152 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:78
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:35:10.141
    Jan  5 19:35:10.141: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename disruption 01/05/23 19:35:10.142
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:10.152
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:10.156
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:35:10.159
    Jan  5 19:35:10.160: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename disruption-2 01/05/23 19:35:10.161
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:10.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:10.177
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:31
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:87
    STEP: Waiting for the pdb to be processed 01/05/23 19:35:10.231
    STEP: Waiting for the pdb to be processed 01/05/23 19:35:12.242
    STEP: Waiting for the pdb to be processed 01/05/23 19:35:14.254
    STEP: listing a collection of PDBs across all namespaces 01/05/23 19:35:16.26
    STEP: listing a collection of PDBs in namespace disruption-1329 01/05/23 19:35:16.263
    STEP: deleting a collection of PDBs 01/05/23 19:35:16.265
    STEP: Waiting for the PDB collection to be deleted 01/05/23 19:35:16.274
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:35:16.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:35:16.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] Listing PodDisruptionBudgets for all namespaces
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-2-1596" for this suite. 01/05/23 19:35:16.283
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1329" for this suite. 01/05/23 19:35:16.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:35:16.299
Jan  5 19:35:16.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename var-expansion 01/05/23 19:35:16.3
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:16.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:16.318
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225
STEP: creating the pod with failed condition 01/05/23 19:35:16.321
Jan  5 19:35:16.332: INFO: Waiting up to 2m0s for pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" in namespace "var-expansion-2321" to be "running"
Jan  5 19:35:16.336: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071001ms
Jan  5 19:35:18.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007840664s
Jan  5 19:35:20.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008282228s
Jan  5 19:35:22.344: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012352968s
Jan  5 19:35:24.345: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013157366s
Jan  5 19:35:26.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007504082s
Jan  5 19:35:28.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007974207s
Jan  5 19:35:30.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008498201s
Jan  5 19:35:32.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008293418s
Jan  5 19:35:34.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008147681s
Jan  5 19:35:36.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008247749s
Jan  5 19:35:38.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007884304s
Jan  5 19:35:40.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007076249s
Jan  5 19:35:42.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008308099s
Jan  5 19:35:44.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008189478s
Jan  5 19:35:46.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007366378s
Jan  5 19:35:48.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008494388s
Jan  5 19:35:50.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007198596s
Jan  5 19:35:52.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007412871s
Jan  5 19:35:54.341: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00891156s
Jan  5 19:35:56.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007696525s
Jan  5 19:35:58.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008099728s
Jan  5 19:36:00.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008326529s
Jan  5 19:36:02.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007516488s
Jan  5 19:36:04.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007227257s
Jan  5 19:36:06.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007427269s
Jan  5 19:36:08.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008338102s
Jan  5 19:36:10.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00768003s
Jan  5 19:36:12.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007301744s
Jan  5 19:36:14.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008372804s
Jan  5 19:36:16.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008343925s
Jan  5 19:36:18.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007806394s
Jan  5 19:36:20.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008695086s
Jan  5 19:36:22.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00774089s
Jan  5 19:36:24.342: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010644343s
Jan  5 19:36:26.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007503883s
Jan  5 19:36:28.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007992597s
Jan  5 19:36:30.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008714549s
Jan  5 19:36:32.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008344403s
Jan  5 19:36:34.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007805484s
Jan  5 19:36:36.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007495112s
Jan  5 19:36:38.342: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010149878s
Jan  5 19:36:40.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008198426s
Jan  5 19:36:42.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007794995s
Jan  5 19:36:44.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007296344s
Jan  5 19:36:46.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008010041s
Jan  5 19:36:48.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007763159s
Jan  5 19:36:50.341: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00932618s
Jan  5 19:36:52.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008126051s
Jan  5 19:36:54.342: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010293634s
Jan  5 19:36:56.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007895421s
Jan  5 19:36:58.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00873096s
Jan  5 19:37:00.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007833024s
Jan  5 19:37:02.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008053843s
Jan  5 19:37:04.341: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009148354s
Jan  5 19:37:06.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007056342s
Jan  5 19:37:08.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007838834s
Jan  5 19:37:10.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008315151s
Jan  5 19:37:12.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007783742s
Jan  5 19:37:14.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007232013s
Jan  5 19:37:16.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00776516s
Jan  5 19:37:16.342: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01026602s
STEP: updating the pod 01/05/23 19:37:16.342
Jan  5 19:37:16.862: INFO: Successfully updated pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8"
STEP: waiting for pod running 01/05/23 19:37:16.864
Jan  5 19:37:16.864: INFO: Waiting up to 2m0s for pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" in namespace "var-expansion-2321" to be "running"
Jan  5 19:37:16.871: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.463625ms
Jan  5 19:37:18.875: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011632071s
Jan  5 19:37:18.875: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 19:37:18.875
Jan  5 19:37:18.876: INFO: Deleting pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" in namespace "var-expansion-2321"
Jan  5 19:37:18.881: INFO: Wait up to 5m0s for pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  5 19:37:50.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2321" for this suite. 01/05/23 19:37:50.893
------------------------------
• [SLOW TEST] [154.600 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:35:16.299
    Jan  5 19:35:16.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename var-expansion 01/05/23 19:35:16.3
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:35:16.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:35:16.318
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:225
    STEP: creating the pod with failed condition 01/05/23 19:35:16.321
    Jan  5 19:35:16.332: INFO: Waiting up to 2m0s for pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" in namespace "var-expansion-2321" to be "running"
    Jan  5 19:35:16.336: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.071001ms
    Jan  5 19:35:18.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007840664s
    Jan  5 19:35:20.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008282228s
    Jan  5 19:35:22.344: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.012352968s
    Jan  5 19:35:24.345: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.013157366s
    Jan  5 19:35:26.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007504082s
    Jan  5 19:35:28.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.007974207s
    Jan  5 19:35:30.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.008498201s
    Jan  5 19:35:32.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 16.008293418s
    Jan  5 19:35:34.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 18.008147681s
    Jan  5 19:35:36.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 20.008247749s
    Jan  5 19:35:38.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007884304s
    Jan  5 19:35:40.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007076249s
    Jan  5 19:35:42.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 26.008308099s
    Jan  5 19:35:44.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 28.008189478s
    Jan  5 19:35:46.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 30.007366378s
    Jan  5 19:35:48.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 32.008494388s
    Jan  5 19:35:50.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 34.007198596s
    Jan  5 19:35:52.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 36.007412871s
    Jan  5 19:35:54.341: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 38.00891156s
    Jan  5 19:35:56.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 40.007696525s
    Jan  5 19:35:58.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008099728s
    Jan  5 19:36:00.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008326529s
    Jan  5 19:36:02.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 46.007516488s
    Jan  5 19:36:04.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 48.007227257s
    Jan  5 19:36:06.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007427269s
    Jan  5 19:36:08.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 52.008338102s
    Jan  5 19:36:10.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 54.00768003s
    Jan  5 19:36:12.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 56.007301744s
    Jan  5 19:36:14.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 58.008372804s
    Jan  5 19:36:16.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.008343925s
    Jan  5 19:36:18.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.007806394s
    Jan  5 19:36:20.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008695086s
    Jan  5 19:36:22.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.00774089s
    Jan  5 19:36:24.342: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010644343s
    Jan  5 19:36:26.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007503883s
    Jan  5 19:36:28.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.007992597s
    Jan  5 19:36:30.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008714549s
    Jan  5 19:36:32.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008344403s
    Jan  5 19:36:34.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007805484s
    Jan  5 19:36:36.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.007495112s
    Jan  5 19:36:38.342: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010149878s
    Jan  5 19:36:40.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.008198426s
    Jan  5 19:36:42.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.007794995s
    Jan  5 19:36:44.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.007296344s
    Jan  5 19:36:46.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008010041s
    Jan  5 19:36:48.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007763159s
    Jan  5 19:36:50.341: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.00932618s
    Jan  5 19:36:52.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008126051s
    Jan  5 19:36:54.342: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010293634s
    Jan  5 19:36:56.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.007895421s
    Jan  5 19:36:58.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00873096s
    Jan  5 19:37:00.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007833024s
    Jan  5 19:37:02.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.008053843s
    Jan  5 19:37:04.341: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009148354s
    Jan  5 19:37:06.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.007056342s
    Jan  5 19:37:08.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.007838834s
    Jan  5 19:37:10.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.008315151s
    Jan  5 19:37:12.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.007783742s
    Jan  5 19:37:14.339: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007232013s
    Jan  5 19:37:16.340: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.00776516s
    Jan  5 19:37:16.342: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.01026602s
    STEP: updating the pod 01/05/23 19:37:16.342
    Jan  5 19:37:16.862: INFO: Successfully updated pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8"
    STEP: waiting for pod running 01/05/23 19:37:16.864
    Jan  5 19:37:16.864: INFO: Waiting up to 2m0s for pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" in namespace "var-expansion-2321" to be "running"
    Jan  5 19:37:16.871: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.463625ms
    Jan  5 19:37:18.875: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8": Phase="Running", Reason="", readiness=true. Elapsed: 2.011632071s
    Jan  5 19:37:18.875: INFO: Pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 19:37:18.875
    Jan  5 19:37:18.876: INFO: Deleting pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" in namespace "var-expansion-2321"
    Jan  5 19:37:18.881: INFO: Wait up to 5m0s for pod "var-expansion-a71802ab-47fe-4f1d-8338-e4ed1a53f1b8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:37:50.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2321" for this suite. 01/05/23 19:37:50.893
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:37:50.905
Jan  5 19:37:50.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename disruption 01/05/23 19:37:50.906
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:37:50.917
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:37:50.92
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108
STEP: creating the pdb 01/05/23 19:37:50.923
STEP: Waiting for the pdb to be processed 01/05/23 19:37:50.927
STEP: updating the pdb 01/05/23 19:37:52.932
STEP: Waiting for the pdb to be processed 01/05/23 19:37:52.939
STEP: patching the pdb 01/05/23 19:37:52.945
STEP: Waiting for the pdb to be processed 01/05/23 19:37:52.954
STEP: Waiting for the pdb to be deleted 01/05/23 19:37:54.965
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  5 19:37:54.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-7451" for this suite. 01/05/23 19:37:54.972
------------------------------
• [4.075 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:37:50.905
    Jan  5 19:37:50.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename disruption 01/05/23 19:37:50.906
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:37:50.917
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:37:50.92
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:108
    STEP: creating the pdb 01/05/23 19:37:50.923
    STEP: Waiting for the pdb to be processed 01/05/23 19:37:50.927
    STEP: updating the pdb 01/05/23 19:37:52.932
    STEP: Waiting for the pdb to be processed 01/05/23 19:37:52.939
    STEP: patching the pdb 01/05/23 19:37:52.945
    STEP: Waiting for the pdb to be processed 01/05/23 19:37:52.954
    STEP: Waiting for the pdb to be deleted 01/05/23 19:37:54.965
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:37:54.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-7451" for this suite. 01/05/23 19:37:54.972
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:37:54.98
Jan  5 19:37:54.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename statefulset 01/05/23 19:37:54.982
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:37:54.994
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:37:54.997
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6829 01/05/23 19:37:54.999
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:908
Jan  5 19:37:55.023: INFO: Found 0 stateful pods, waiting for 1
Jan  5 19:38:05.027: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 01/05/23 19:38:05.032
W0105 19:38:05.046146      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 19:38:05.052: INFO: Found 1 stateful pods, waiting for 2
Jan  5 19:38:15.084: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 19:38:15.085: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 01/05/23 19:38:15.102
STEP: Delete all of the StatefulSets 01/05/23 19:38:15.11
STEP: Verify that StatefulSets have been deleted 01/05/23 19:38:15.133
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  5 19:38:15.138: INFO: Deleting all statefulset in ns statefulset-6829
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:38:15.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6829" for this suite. 01/05/23 19:38:15.204
------------------------------
• [SLOW TEST] [20.238 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:908

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:37:54.98
    Jan  5 19:37:54.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename statefulset 01/05/23 19:37:54.982
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:37:54.994
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:37:54.997
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6829 01/05/23 19:37:54.999
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:908
    Jan  5 19:37:55.023: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 19:38:05.027: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 01/05/23 19:38:05.032
    W0105 19:38:05.046146      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 19:38:05.052: INFO: Found 1 stateful pods, waiting for 2
    Jan  5 19:38:15.084: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 19:38:15.085: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 01/05/23 19:38:15.102
    STEP: Delete all of the StatefulSets 01/05/23 19:38:15.11
    STEP: Verify that StatefulSets have been deleted 01/05/23 19:38:15.133
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  5 19:38:15.138: INFO: Deleting all statefulset in ns statefulset-6829
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:38:15.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6829" for this suite. 01/05/23 19:38:15.204
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:38:15.22
Jan  5 19:38:15.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-preemption 01/05/23 19:38:15.221
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:38:15.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:38:15.257
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan  5 19:38:15.290: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 19:39:15.337: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129
STEP: Create pods that use 4/5 of node resources. 01/05/23 19:39:15.34
Jan  5 19:39:15.375: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  5 19:39:15.384: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  5 19:39:15.416: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  5 19:39:15.427: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  5 19:39:15.456: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  5 19:39:15.469: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/05/23 19:39:15.469
Jan  5 19:39:15.469: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3411" to be "running"
Jan  5 19:39:15.474: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155211ms
Jan  5 19:39:17.478: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008974294s
Jan  5 19:39:19.477: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007536084s
Jan  5 19:39:21.478: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008134779s
Jan  5 19:39:23.479: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009625794s
Jan  5 19:39:25.477: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.007412946s
Jan  5 19:39:25.477: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  5 19:39:25.477: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
Jan  5 19:39:25.480: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.76255ms
Jan  5 19:39:25.480: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 19:39:25.480: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
Jan  5 19:39:25.482: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.407108ms
Jan  5 19:39:25.482: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 19:39:25.482: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
Jan  5 19:39:25.484: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.188598ms
Jan  5 19:39:25.484: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 19:39:25.484: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
Jan  5 19:39:25.487: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.280051ms
Jan  5 19:39:25.487: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 19:39:25.487: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
Jan  5 19:39:25.489: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.976303ms
Jan  5 19:39:25.489: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/05/23 19:39:25.489
Jan  5 19:39:25.495: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3411" to be "running"
Jan  5 19:39:25.501: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.623069ms
Jan  5 19:39:27.505: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009466763s
Jan  5 19:39:29.505: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009986981s
Jan  5 19:39:31.505: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009826464s
Jan  5 19:39:31.505: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:39:31.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-3411" for this suite. 01/05/23 19:39:31.576
------------------------------
• [SLOW TEST] [76.363 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:38:15.22
    Jan  5 19:38:15.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 19:38:15.221
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:38:15.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:38:15.257
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan  5 19:38:15.290: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 19:39:15.337: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:129
    STEP: Create pods that use 4/5 of node resources. 01/05/23 19:39:15.34
    Jan  5 19:39:15.375: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  5 19:39:15.384: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  5 19:39:15.416: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  5 19:39:15.427: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  5 19:39:15.456: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  5 19:39:15.469: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/05/23 19:39:15.469
    Jan  5 19:39:15.469: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-3411" to be "running"
    Jan  5 19:39:15.474: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.155211ms
    Jan  5 19:39:17.478: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008974294s
    Jan  5 19:39:19.477: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007536084s
    Jan  5 19:39:21.478: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008134779s
    Jan  5 19:39:23.479: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009625794s
    Jan  5 19:39:25.477: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 10.007412946s
    Jan  5 19:39:25.477: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  5 19:39:25.477: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
    Jan  5 19:39:25.480: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.76255ms
    Jan  5 19:39:25.480: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 19:39:25.480: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
    Jan  5 19:39:25.482: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.407108ms
    Jan  5 19:39:25.482: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 19:39:25.482: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
    Jan  5 19:39:25.484: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.188598ms
    Jan  5 19:39:25.484: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 19:39:25.484: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
    Jan  5 19:39:25.487: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.280051ms
    Jan  5 19:39:25.487: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 19:39:25.487: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-3411" to be "running"
    Jan  5 19:39:25.489: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 1.976303ms
    Jan  5 19:39:25.489: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 01/05/23 19:39:25.489
    Jan  5 19:39:25.495: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-3411" to be "running"
    Jan  5 19:39:25.501: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.623069ms
    Jan  5 19:39:27.505: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009466763s
    Jan  5 19:39:29.505: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009986981s
    Jan  5 19:39:31.505: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.009826464s
    Jan  5 19:39:31.505: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:39:31.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-3411" for this suite. 01/05/23 19:39:31.576
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:39:31.592
Jan  5 19:39:31.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:39:31.594
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:39:31.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:39:31.609
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124
STEP: Creating projection with configMap that has name projected-configmap-test-upd-56b2c8a9-57d6-45be-aa00-3103ef73f3f7 01/05/23 19:39:31.615
STEP: Creating the pod 01/05/23 19:39:31.62
Jan  5 19:39:31.629: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954" in namespace "projected-3139" to be "running and ready"
Jan  5 19:39:31.634: INFO: Pod "pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277031ms
Jan  5 19:39:31.634: INFO: The phase of Pod pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:39:33.638: INFO: Pod "pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954": Phase="Running", Reason="", readiness=true. Elapsed: 2.00931688s
Jan  5 19:39:33.638: INFO: The phase of Pod pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954 is Running (Ready = true)
Jan  5 19:39:33.638: INFO: Pod "pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-56b2c8a9-57d6-45be-aa00-3103ef73f3f7 01/05/23 19:39:33.656
STEP: waiting to observe update in volume 01/05/23 19:39:33.66
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:39:35.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3139" for this suite. 01/05/23 19:39:35.678
------------------------------
• [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:39:31.592
    Jan  5 19:39:31.592: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:39:31.594
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:39:31.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:39:31.609
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:124
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-56b2c8a9-57d6-45be-aa00-3103ef73f3f7 01/05/23 19:39:31.615
    STEP: Creating the pod 01/05/23 19:39:31.62
    Jan  5 19:39:31.629: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954" in namespace "projected-3139" to be "running and ready"
    Jan  5 19:39:31.634: INFO: Pod "pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954": Phase="Pending", Reason="", readiness=false. Elapsed: 5.277031ms
    Jan  5 19:39:31.634: INFO: The phase of Pod pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:39:33.638: INFO: Pod "pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954": Phase="Running", Reason="", readiness=true. Elapsed: 2.00931688s
    Jan  5 19:39:33.638: INFO: The phase of Pod pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954 is Running (Ready = true)
    Jan  5 19:39:33.638: INFO: Pod "pod-projected-configmaps-aa72d680-890c-40d0-a4de-de4c702a0954" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-56b2c8a9-57d6-45be-aa00-3103ef73f3f7 01/05/23 19:39:33.656
    STEP: waiting to observe update in volume 01/05/23 19:39:33.66
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:39:35.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3139" for this suite. 01/05/23 19:39:35.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:39:35.685
Jan  5 19:39:35.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename namespaces 01/05/23 19:39:35.686
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:39:35.698
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:39:35.701
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251
STEP: Creating a test namespace 01/05/23 19:39:35.704
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:39:35.715
STEP: Creating a service in the namespace 01/05/23 19:39:35.718
STEP: Deleting the namespace 01/05/23 19:39:35.737
STEP: Waiting for the namespace to be removed. 01/05/23 19:39:35.743
STEP: Recreating the namespace 01/05/23 19:39:41.747
STEP: Verifying there is no service in the namespace 01/05/23 19:39:41.762
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:39:41.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5274" for this suite. 01/05/23 19:39:41.768
STEP: Destroying namespace "nsdeletetest-2399" for this suite. 01/05/23 19:39:41.773
Jan  5 19:39:41.776: INFO: Namespace nsdeletetest-2399 was already deleted
STEP: Destroying namespace "nsdeletetest-6896" for this suite. 01/05/23 19:39:41.776
------------------------------
• [SLOW TEST] [6.096 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:39:35.685
    Jan  5 19:39:35.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename namespaces 01/05/23 19:39:35.686
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:39:35.698
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:39:35.701
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:251
    STEP: Creating a test namespace 01/05/23 19:39:35.704
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:39:35.715
    STEP: Creating a service in the namespace 01/05/23 19:39:35.718
    STEP: Deleting the namespace 01/05/23 19:39:35.737
    STEP: Waiting for the namespace to be removed. 01/05/23 19:39:35.743
    STEP: Recreating the namespace 01/05/23 19:39:41.747
    STEP: Verifying there is no service in the namespace 01/05/23 19:39:41.762
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:39:41.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5274" for this suite. 01/05/23 19:39:41.768
    STEP: Destroying namespace "nsdeletetest-2399" for this suite. 01/05/23 19:39:41.773
    Jan  5 19:39:41.776: INFO: Namespace nsdeletetest-2399 was already deleted
    STEP: Destroying namespace "nsdeletetest-6896" for this suite. 01/05/23 19:39:41.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:39:41.785
Jan  5 19:39:41.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename var-expansion 01/05/23 19:39:41.787
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:39:41.804
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:39:41.807
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297
STEP: creating the pod 01/05/23 19:39:41.81
STEP: waiting for pod running 01/05/23 19:39:41.82
Jan  5 19:39:41.820: INFO: Waiting up to 2m0s for pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" in namespace "var-expansion-2606" to be "running"
Jan  5 19:39:41.824: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11": Phase="Pending", Reason="", readiness=false. Elapsed: 3.797059ms
Jan  5 19:39:43.826: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11": Phase="Running", Reason="", readiness=true. Elapsed: 2.006330418s
Jan  5 19:39:43.826: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" satisfied condition "running"
STEP: creating a file in subpath 01/05/23 19:39:43.826
Jan  5 19:39:43.829: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2606 PodName:var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:39:43.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:39:43.830: INFO: ExecWithOptions: Clientset creation
Jan  5 19:39:43.830: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/var-expansion-2606/pods/var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 01/05/23 19:39:43.908
Jan  5 19:39:43.911: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2606 PodName:var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:39:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:39:43.912: INFO: ExecWithOptions: Clientset creation
Jan  5 19:39:43.912: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/var-expansion-2606/pods/var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 01/05/23 19:39:43.973
Jan  5 19:39:44.487: INFO: Successfully updated pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11"
STEP: waiting for annotated pod running 01/05/23 19:39:44.489
Jan  5 19:39:44.489: INFO: Waiting up to 2m0s for pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" in namespace "var-expansion-2606" to be "running"
Jan  5 19:39:44.492: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11": Phase="Running", Reason="", readiness=true. Elapsed: 2.637649ms
Jan  5 19:39:44.492: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" satisfied condition "running"
STEP: deleting the pod gracefully 01/05/23 19:39:44.492
Jan  5 19:39:44.492: INFO: Deleting pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" in namespace "var-expansion-2606"
Jan  5 19:39:44.499: INFO: Wait up to 5m0s for pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  5 19:40:18.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-2606" for this suite. 01/05/23 19:40:18.509
------------------------------
• [SLOW TEST] [36.729 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:297

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:39:41.785
    Jan  5 19:39:41.786: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename var-expansion 01/05/23 19:39:41.787
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:39:41.804
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:39:41.807
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:297
    STEP: creating the pod 01/05/23 19:39:41.81
    STEP: waiting for pod running 01/05/23 19:39:41.82
    Jan  5 19:39:41.820: INFO: Waiting up to 2m0s for pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" in namespace "var-expansion-2606" to be "running"
    Jan  5 19:39:41.824: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11": Phase="Pending", Reason="", readiness=false. Elapsed: 3.797059ms
    Jan  5 19:39:43.826: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11": Phase="Running", Reason="", readiness=true. Elapsed: 2.006330418s
    Jan  5 19:39:43.826: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" satisfied condition "running"
    STEP: creating a file in subpath 01/05/23 19:39:43.826
    Jan  5 19:39:43.829: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2606 PodName:var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:39:43.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:39:43.830: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:39:43.830: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/var-expansion-2606/pods/var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 01/05/23 19:39:43.908
    Jan  5 19:39:43.911: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2606 PodName:var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:39:43.911: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:39:43.912: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:39:43.912: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/var-expansion-2606/pods/var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 01/05/23 19:39:43.973
    Jan  5 19:39:44.487: INFO: Successfully updated pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11"
    STEP: waiting for annotated pod running 01/05/23 19:39:44.489
    Jan  5 19:39:44.489: INFO: Waiting up to 2m0s for pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" in namespace "var-expansion-2606" to be "running"
    Jan  5 19:39:44.492: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11": Phase="Running", Reason="", readiness=true. Elapsed: 2.637649ms
    Jan  5 19:39:44.492: INFO: Pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" satisfied condition "running"
    STEP: deleting the pod gracefully 01/05/23 19:39:44.492
    Jan  5 19:39:44.492: INFO: Deleting pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" in namespace "var-expansion-2606"
    Jan  5 19:39:44.499: INFO: Wait up to 5m0s for pod "var-expansion-eda771c4-f815-46db-ba04-5cc50b33bd11" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:40:18.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-2606" for this suite. 01/05/23 19:40:18.509
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:40:18.521
Jan  5 19:40:18.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:40:18.523
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:18.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:18.54
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:40:18.543
Jan  5 19:40:18.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27" in namespace "downward-api-8219" to be "Succeeded or Failed"
Jan  5 19:40:18.559: INFO: Pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.566821ms
Jan  5 19:40:20.563: INFO: Pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008255275s
Jan  5 19:40:22.563: INFO: Pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008546877s
STEP: Saw pod success 01/05/23 19:40:22.563
Jan  5 19:40:22.564: INFO: Pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27" satisfied condition "Succeeded or Failed"
Jan  5 19:40:22.566: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27 container client-container: <nil>
STEP: delete the pod 01/05/23 19:40:22.578
Jan  5 19:40:22.589: INFO: Waiting for pod downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27 to disappear
Jan  5 19:40:22.592: INFO: Pod downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:40:22.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8219" for this suite. 01/05/23 19:40:22.599
------------------------------
• [4.084 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:40:18.521
    Jan  5 19:40:18.522: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:40:18.523
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:18.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:18.54
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:53
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:40:18.543
    Jan  5 19:40:18.555: INFO: Waiting up to 5m0s for pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27" in namespace "downward-api-8219" to be "Succeeded or Failed"
    Jan  5 19:40:18.559: INFO: Pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.566821ms
    Jan  5 19:40:20.563: INFO: Pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008255275s
    Jan  5 19:40:22.563: INFO: Pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008546877s
    STEP: Saw pod success 01/05/23 19:40:22.563
    Jan  5 19:40:22.564: INFO: Pod "downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27" satisfied condition "Succeeded or Failed"
    Jan  5 19:40:22.566: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:40:22.578
    Jan  5 19:40:22.589: INFO: Waiting for pod downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27 to disappear
    Jan  5 19:40:22.592: INFO: Pod downwardapi-volume-42823ac3-a7a3-432b-8922-1688d8792b27 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:40:22.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8219" for this suite. 01/05/23 19:40:22.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:40:22.607
Jan  5 19:40:22.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:40:22.608
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:22.621
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:22.623
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:40:22.639
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:40:23.144
STEP: Deploying the webhook pod 01/05/23 19:40:23.149
STEP: Wait for the deployment to be ready 01/05/23 19:40:23.161
Jan  5 19:40:23.166: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 19:40:25.175
STEP: Verifying the service has paired with the endpoint 01/05/23 19:40:25.186
Jan  5 19:40:26.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341
Jan  5 19:40:26.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8366-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 19:40:26.7
STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 19:40:26.721
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:40:29.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8980" for this suite. 01/05/23 19:40:29.338
STEP: Destroying namespace "webhook-8980-markers" for this suite. 01/05/23 19:40:29.346
------------------------------
• [SLOW TEST] [6.755 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:341

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:40:22.607
    Jan  5 19:40:22.607: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:40:22.608
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:22.621
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:22.623
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:40:22.639
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:40:23.144
    STEP: Deploying the webhook pod 01/05/23 19:40:23.149
    STEP: Wait for the deployment to be ready 01/05/23 19:40:23.161
    Jan  5 19:40:23.166: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 19:40:25.175
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:40:25.186
    Jan  5 19:40:26.186: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:341
    Jan  5 19:40:26.189: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8366-crds.webhook.example.com via the AdmissionRegistration API 01/05/23 19:40:26.7
    STEP: Creating a custom resource that should be mutated by the webhook 01/05/23 19:40:26.721
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:40:29.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8980" for this suite. 01/05/23 19:40:29.338
    STEP: Destroying namespace "webhook-8980-markers" for this suite. 01/05/23 19:40:29.346
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:40:29.368
Jan  5 19:40:29.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:40:29.372
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:29.389
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:29.393
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:931
STEP: create deployment with httpd image 01/05/23 19:40:29.396
Jan  5 19:40:29.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-8886 create -f -'
Jan  5 19:40:30.877: INFO: stderr: ""
Jan  5 19:40:30.877: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 01/05/23 19:40:30.877
Jan  5 19:40:30.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-8886 diff -f -'
Jan  5 19:40:31.186: INFO: rc: 1
Jan  5 19:40:31.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-8886 delete -f -'
Jan  5 19:40:31.295: INFO: stderr: ""
Jan  5 19:40:31.295: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:40:31.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8886" for this suite. 01/05/23 19:40:31.299
------------------------------
• [1.952 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:925
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:931

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:40:29.368
    Jan  5 19:40:29.369: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:40:29.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:29.389
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:29.393
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:931
    STEP: create deployment with httpd image 01/05/23 19:40:29.396
    Jan  5 19:40:29.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-8886 create -f -'
    Jan  5 19:40:30.877: INFO: stderr: ""
    Jan  5 19:40:30.877: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 01/05/23 19:40:30.877
    Jan  5 19:40:30.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-8886 diff -f -'
    Jan  5 19:40:31.186: INFO: rc: 1
    Jan  5 19:40:31.186: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-8886 delete -f -'
    Jan  5 19:40:31.295: INFO: stderr: ""
    Jan  5 19:40:31.295: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:40:31.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8886" for this suite. 01/05/23 19:40:31.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:40:31.32
Jan  5 19:40:31.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename events 01/05/23 19:40:31.321
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:31.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:31.34
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 01/05/23 19:40:31.343
STEP: listing events in all namespaces 01/05/23 19:40:31.349
STEP: listing events in test namespace 01/05/23 19:40:31.353
STEP: listing events with field selection filtering on source 01/05/23 19:40:31.356
STEP: listing events with field selection filtering on reportingController 01/05/23 19:40:31.361
STEP: getting the test event 01/05/23 19:40:31.363
STEP: patching the test event 01/05/23 19:40:31.366
STEP: getting the test event 01/05/23 19:40:31.376
STEP: updating the test event 01/05/23 19:40:31.379
STEP: getting the test event 01/05/23 19:40:31.391
STEP: deleting the test event 01/05/23 19:40:31.395
STEP: listing events in all namespaces 01/05/23 19:40:31.401
STEP: listing events in test namespace 01/05/23 19:40:31.406
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan  5 19:40:31.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9767" for this suite. 01/05/23 19:40:31.414
------------------------------
• [0.098 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:40:31.32
    Jan  5 19:40:31.320: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename events 01/05/23 19:40:31.321
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:31.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:31.34
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 01/05/23 19:40:31.343
    STEP: listing events in all namespaces 01/05/23 19:40:31.349
    STEP: listing events in test namespace 01/05/23 19:40:31.353
    STEP: listing events with field selection filtering on source 01/05/23 19:40:31.356
    STEP: listing events with field selection filtering on reportingController 01/05/23 19:40:31.361
    STEP: getting the test event 01/05/23 19:40:31.363
    STEP: patching the test event 01/05/23 19:40:31.366
    STEP: getting the test event 01/05/23 19:40:31.376
    STEP: updating the test event 01/05/23 19:40:31.379
    STEP: getting the test event 01/05/23 19:40:31.391
    STEP: deleting the test event 01/05/23 19:40:31.395
    STEP: listing events in all namespaces 01/05/23 19:40:31.401
    STEP: listing events in test namespace 01/05/23 19:40:31.406
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:40:31.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9767" for this suite. 01/05/23 19:40:31.414
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:40:31.425
Jan  5 19:40:31.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:40:31.427
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:31.491
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:31.494
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1787
STEP: starting the proxy server 01/05/23 19:40:31.5
Jan  5 19:40:31.500: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-8814 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 01/05/23 19:40:31.61
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:40:31.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-8814" for this suite. 01/05/23 19:40:31.622
------------------------------
• [0.201 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1780
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:40:31.425
    Jan  5 19:40:31.426: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:40:31.427
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:31.491
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:31.494
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1787
    STEP: starting the proxy server 01/05/23 19:40:31.5
    Jan  5 19:40:31.500: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-8814 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 01/05/23 19:40:31.61
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:40:31.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-8814" for this suite. 01/05/23 19:40:31.622
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:40:31.627
Jan  5 19:40:31.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replicaset 01/05/23 19:40:31.63
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:31.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:31.644
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:31
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jan  5 19:40:31.652: INFO: Creating ReplicaSet my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b
Jan  5 19:40:31.659: INFO: Pod name my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b: Found 0 pods out of 1
Jan  5 19:40:36.662: INFO: Pod name my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b: Found 1 pods out of 1
Jan  5 19:40:36.662: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b" is running
Jan  5 19:40:36.662: INFO: Waiting up to 5m0s for pod "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl" in namespace "replicaset-203" to be "running"
Jan  5 19:40:36.665: INFO: Pod "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl": Phase="Running", Reason="", readiness=true. Elapsed: 3.176045ms
Jan  5 19:40:36.665: INFO: Pod "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl" satisfied condition "running"
Jan  5 19:40:36.665: INFO: Pod "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:40:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:40:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:40:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:40:31 +0000 UTC Reason: Message:}])
Jan  5 19:40:36.665: INFO: Trying to dial the pod
Jan  5 19:40:41.679: INFO: Controller my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b: Got expected result from replica 1 [my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl]: "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:40:41.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicaSet
  tear down framework | framework.go:193
STEP: Destroying namespace "replicaset-203" for this suite. 01/05/23 19:40:41.683
------------------------------
• [SLOW TEST] [10.063 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:40:31.627
    Jan  5 19:40:31.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replicaset 01/05/23 19:40:31.63
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:31.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:31.644
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:31
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jan  5 19:40:31.652: INFO: Creating ReplicaSet my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b
    Jan  5 19:40:31.659: INFO: Pod name my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b: Found 0 pods out of 1
    Jan  5 19:40:36.662: INFO: Pod name my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b: Found 1 pods out of 1
    Jan  5 19:40:36.662: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b" is running
    Jan  5 19:40:36.662: INFO: Waiting up to 5m0s for pod "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl" in namespace "replicaset-203" to be "running"
    Jan  5 19:40:36.665: INFO: Pod "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl": Phase="Running", Reason="", readiness=true. Elapsed: 3.176045ms
    Jan  5 19:40:36.665: INFO: Pod "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl" satisfied condition "running"
    Jan  5 19:40:36.665: INFO: Pod "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:40:31 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:40:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:40:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-01-05 19:40:31 +0000 UTC Reason: Message:}])
    Jan  5 19:40:36.665: INFO: Trying to dial the pod
    Jan  5 19:40:41.679: INFO: Controller my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b: Got expected result from replica 1 [my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl]: "my-hostname-basic-47b4a508-b38e-4c0a-8b2a-26abfa951b0b-sbzzl", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:40:41.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicaSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "replicaset-203" for this suite. 01/05/23 19:40:41.683
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:40:41.698
Jan  5 19:40:41.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:40:41.699
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:41.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:41.714
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1572
STEP: creating an pod 01/05/23 19:40:41.717
Jan  5 19:40:41.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jan  5 19:40:41.808: INFO: stderr: ""
Jan  5 19:40:41.808: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1592
STEP: Waiting for log generator to start. 01/05/23 19:40:41.809
Jan  5 19:40:41.809: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jan  5 19:40:41.809: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9599" to be "running and ready, or succeeded"
Jan  5 19:40:41.812: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16496ms
Jan  5 19:40:41.812: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'gke-gke-1-26-default-pool-05283374-16pz' to be 'Running' but was 'Pending'
Jan  5 19:40:43.815: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006405354s
Jan  5 19:40:43.815: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jan  5 19:40:43.815: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 01/05/23 19:40:43.815
Jan  5 19:40:43.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator'
Jan  5 19:40:43.909: INFO: stderr: ""
Jan  5 19:40:43.909: INFO: stdout: "I0105 19:40:42.734654       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/rthf 360\nI0105 19:40:42.935209       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/4tq 409\nI0105 19:40:43.135549       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/rvns 477\nI0105 19:40:43.334850       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/lcm 227\nI0105 19:40:43.535227       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/85dh 451\nI0105 19:40:43.735591       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/6685 253\n"
STEP: limiting log lines 01/05/23 19:40:43.909
Jan  5 19:40:43.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --tail=1'
Jan  5 19:40:44.000: INFO: stderr: ""
Jan  5 19:40:44.000: INFO: stdout: "I0105 19:40:43.934870       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vtt 495\n"
Jan  5 19:40:44.000: INFO: got output "I0105 19:40:43.934870       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vtt 495\n"
STEP: limiting log bytes 01/05/23 19:40:44
Jan  5 19:40:44.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --limit-bytes=1'
Jan  5 19:40:44.088: INFO: stderr: ""
Jan  5 19:40:44.088: INFO: stdout: "I"
Jan  5 19:40:44.088: INFO: got output "I"
STEP: exposing timestamps 01/05/23 19:40:44.088
Jan  5 19:40:44.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --tail=1 --timestamps'
Jan  5 19:40:44.181: INFO: stderr: ""
Jan  5 19:40:44.181: INFO: stdout: "2023-01-05T19:40:44.134961466Z I0105 19:40:44.134818       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9vs8 585\n"
Jan  5 19:40:44.181: INFO: got output "2023-01-05T19:40:44.134961466Z I0105 19:40:44.134818       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9vs8 585\n"
STEP: restricting to a time range 01/05/23 19:40:44.181
Jan  5 19:40:46.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --since=1s'
Jan  5 19:40:46.765: INFO: stderr: ""
Jan  5 19:40:46.765: INFO: stdout: "I0105 19:40:45.935206       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/t4fd 495\nI0105 19:40:46.134713       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/tjh 509\nI0105 19:40:46.335092       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qxn 326\nI0105 19:40:46.535525       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/nbfl 380\nI0105 19:40:46.734774       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/jqjd 293\n"
Jan  5 19:40:46.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --since=24h'
Jan  5 19:40:46.866: INFO: stderr: ""
Jan  5 19:40:46.866: INFO: stdout: "I0105 19:40:42.734654       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/rthf 360\nI0105 19:40:42.935209       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/4tq 409\nI0105 19:40:43.135549       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/rvns 477\nI0105 19:40:43.334850       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/lcm 227\nI0105 19:40:43.535227       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/85dh 451\nI0105 19:40:43.735591       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/6685 253\nI0105 19:40:43.934870       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vtt 495\nI0105 19:40:44.134818       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9vs8 585\nI0105 19:40:44.335183       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/p2cc 322\nI0105 19:40:44.535521       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/v84 267\nI0105 19:40:44.734761       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/qqwj 471\nI0105 19:40:44.935339       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/2sk 209\nI0105 19:40:45.135731       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/lc7 504\nI0105 19:40:45.335113       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/rbgp 455\nI0105 19:40:45.535534       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/xllv 238\nI0105 19:40:45.734832       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/jd7h 453\nI0105 19:40:45.935206       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/t4fd 495\nI0105 19:40:46.134713       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/tjh 509\nI0105 19:40:46.335092       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qxn 326\nI0105 19:40:46.535525       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/nbfl 380\nI0105 19:40:46.734774       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/jqjd 293\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1577
Jan  5 19:40:46.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 delete pod logs-generator'
Jan  5 19:40:48.112: INFO: stderr: ""
Jan  5 19:40:48.112: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:40:48.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9599" for this suite. 01/05/23 19:40:48.115
------------------------------
• [SLOW TEST] [6.423 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1569
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1592

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:40:41.698
    Jan  5 19:40:41.698: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:40:41.699
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:41.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:41.714
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1572
    STEP: creating an pod 01/05/23 19:40:41.717
    Jan  5 19:40:41.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jan  5 19:40:41.808: INFO: stderr: ""
    Jan  5 19:40:41.808: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1592
    STEP: Waiting for log generator to start. 01/05/23 19:40:41.809
    Jan  5 19:40:41.809: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jan  5 19:40:41.809: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-9599" to be "running and ready, or succeeded"
    Jan  5 19:40:41.812: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 3.16496ms
    Jan  5 19:40:41.812: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'gke-gke-1-26-default-pool-05283374-16pz' to be 'Running' but was 'Pending'
    Jan  5 19:40:43.815: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.006405354s
    Jan  5 19:40:43.815: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jan  5 19:40:43.815: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 01/05/23 19:40:43.815
    Jan  5 19:40:43.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator'
    Jan  5 19:40:43.909: INFO: stderr: ""
    Jan  5 19:40:43.909: INFO: stdout: "I0105 19:40:42.734654       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/rthf 360\nI0105 19:40:42.935209       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/4tq 409\nI0105 19:40:43.135549       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/rvns 477\nI0105 19:40:43.334850       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/lcm 227\nI0105 19:40:43.535227       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/85dh 451\nI0105 19:40:43.735591       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/6685 253\n"
    STEP: limiting log lines 01/05/23 19:40:43.909
    Jan  5 19:40:43.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --tail=1'
    Jan  5 19:40:44.000: INFO: stderr: ""
    Jan  5 19:40:44.000: INFO: stdout: "I0105 19:40:43.934870       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vtt 495\n"
    Jan  5 19:40:44.000: INFO: got output "I0105 19:40:43.934870       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vtt 495\n"
    STEP: limiting log bytes 01/05/23 19:40:44
    Jan  5 19:40:44.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --limit-bytes=1'
    Jan  5 19:40:44.088: INFO: stderr: ""
    Jan  5 19:40:44.088: INFO: stdout: "I"
    Jan  5 19:40:44.088: INFO: got output "I"
    STEP: exposing timestamps 01/05/23 19:40:44.088
    Jan  5 19:40:44.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --tail=1 --timestamps'
    Jan  5 19:40:44.181: INFO: stderr: ""
    Jan  5 19:40:44.181: INFO: stdout: "2023-01-05T19:40:44.134961466Z I0105 19:40:44.134818       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9vs8 585\n"
    Jan  5 19:40:44.181: INFO: got output "2023-01-05T19:40:44.134961466Z I0105 19:40:44.134818       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9vs8 585\n"
    STEP: restricting to a time range 01/05/23 19:40:44.181
    Jan  5 19:40:46.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --since=1s'
    Jan  5 19:40:46.765: INFO: stderr: ""
    Jan  5 19:40:46.765: INFO: stdout: "I0105 19:40:45.935206       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/t4fd 495\nI0105 19:40:46.134713       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/tjh 509\nI0105 19:40:46.335092       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qxn 326\nI0105 19:40:46.535525       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/nbfl 380\nI0105 19:40:46.734774       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/jqjd 293\n"
    Jan  5 19:40:46.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 logs logs-generator logs-generator --since=24h'
    Jan  5 19:40:46.866: INFO: stderr: ""
    Jan  5 19:40:46.866: INFO: stdout: "I0105 19:40:42.734654       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/rthf 360\nI0105 19:40:42.935209       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/4tq 409\nI0105 19:40:43.135549       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/rvns 477\nI0105 19:40:43.334850       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/lcm 227\nI0105 19:40:43.535227       1 logs_generator.go:76] 4 GET /api/v1/namespaces/ns/pods/85dh 451\nI0105 19:40:43.735591       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/6685 253\nI0105 19:40:43.934870       1 logs_generator.go:76] 6 GET /api/v1/namespaces/kube-system/pods/vtt 495\nI0105 19:40:44.134818       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/9vs8 585\nI0105 19:40:44.335183       1 logs_generator.go:76] 8 POST /api/v1/namespaces/kube-system/pods/p2cc 322\nI0105 19:40:44.535521       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/v84 267\nI0105 19:40:44.734761       1 logs_generator.go:76] 10 POST /api/v1/namespaces/kube-system/pods/qqwj 471\nI0105 19:40:44.935339       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/2sk 209\nI0105 19:40:45.135731       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/lc7 504\nI0105 19:40:45.335113       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/rbgp 455\nI0105 19:40:45.535534       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/xllv 238\nI0105 19:40:45.734832       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/jd7h 453\nI0105 19:40:45.935206       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/t4fd 495\nI0105 19:40:46.134713       1 logs_generator.go:76] 17 GET /api/v1/namespaces/ns/pods/tjh 509\nI0105 19:40:46.335092       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/qxn 326\nI0105 19:40:46.535525       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/nbfl 380\nI0105 19:40:46.734774       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/jqjd 293\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1577
    Jan  5 19:40:46.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9599 delete pod logs-generator'
    Jan  5 19:40:48.112: INFO: stderr: ""
    Jan  5 19:40:48.112: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:40:48.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9599" for this suite. 01/05/23 19:40:48.115
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:40:48.122
Jan  5 19:40:48.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename subpath 01/05/23 19:40:48.123
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:48.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:48.137
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 19:40:48.14
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-sjjr 01/05/23 19:40:48.149
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 19:40:48.149
Jan  5 19:40:48.162: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-sjjr" in namespace "subpath-2534" to be "Succeeded or Failed"
Jan  5 19:40:48.165: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921425ms
Jan  5 19:40:50.168: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005726434s
Jan  5 19:40:52.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 4.006815851s
Jan  5 19:40:54.170: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 6.007432426s
Jan  5 19:40:56.168: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 8.005658322s
Jan  5 19:40:58.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 10.00665353s
Jan  5 19:41:00.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 12.007103809s
Jan  5 19:41:02.168: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 14.005973516s
Jan  5 19:41:04.170: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 16.008043556s
Jan  5 19:41:06.168: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 18.005678979s
Jan  5 19:41:08.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 20.006361962s
Jan  5 19:41:10.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=false. Elapsed: 22.006766113s
Jan  5 19:41:12.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006276665s
STEP: Saw pod success 01/05/23 19:41:12.169
Jan  5 19:41:12.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr" satisfied condition "Succeeded or Failed"
Jan  5 19:41:12.172: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-subpath-test-downwardapi-sjjr container test-container-subpath-downwardapi-sjjr: <nil>
STEP: delete the pod 01/05/23 19:41:12.178
Jan  5 19:41:12.188: INFO: Waiting for pod pod-subpath-test-downwardapi-sjjr to disappear
Jan  5 19:41:12.191: INFO: Pod pod-subpath-test-downwardapi-sjjr no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-sjjr 01/05/23 19:41:12.191
Jan  5 19:41:12.191: INFO: Deleting pod "pod-subpath-test-downwardapi-sjjr" in namespace "subpath-2534"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  5 19:41:12.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2534" for this suite. 01/05/23 19:41:12.197
------------------------------
• [SLOW TEST] [24.080 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:40:48.122
    Jan  5 19:40:48.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename subpath 01/05/23 19:40:48.123
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:40:48.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:40:48.137
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 19:40:48.14
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-sjjr 01/05/23 19:40:48.149
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 19:40:48.149
    Jan  5 19:40:48.162: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-sjjr" in namespace "subpath-2534" to be "Succeeded or Failed"
    Jan  5 19:40:48.165: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.921425ms
    Jan  5 19:40:50.168: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 2.005726434s
    Jan  5 19:40:52.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 4.006815851s
    Jan  5 19:40:54.170: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 6.007432426s
    Jan  5 19:40:56.168: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 8.005658322s
    Jan  5 19:40:58.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 10.00665353s
    Jan  5 19:41:00.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 12.007103809s
    Jan  5 19:41:02.168: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 14.005973516s
    Jan  5 19:41:04.170: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 16.008043556s
    Jan  5 19:41:06.168: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 18.005678979s
    Jan  5 19:41:08.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=true. Elapsed: 20.006361962s
    Jan  5 19:41:10.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Running", Reason="", readiness=false. Elapsed: 22.006766113s
    Jan  5 19:41:12.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.006276665s
    STEP: Saw pod success 01/05/23 19:41:12.169
    Jan  5 19:41:12.169: INFO: Pod "pod-subpath-test-downwardapi-sjjr" satisfied condition "Succeeded or Failed"
    Jan  5 19:41:12.172: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-subpath-test-downwardapi-sjjr container test-container-subpath-downwardapi-sjjr: <nil>
    STEP: delete the pod 01/05/23 19:41:12.178
    Jan  5 19:41:12.188: INFO: Waiting for pod pod-subpath-test-downwardapi-sjjr to disappear
    Jan  5 19:41:12.191: INFO: Pod pod-subpath-test-downwardapi-sjjr no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-sjjr 01/05/23 19:41:12.191
    Jan  5 19:41:12.191: INFO: Deleting pod "pod-subpath-test-downwardapi-sjjr" in namespace "subpath-2534"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:41:12.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2534" for this suite. 01/05/23 19:41:12.197
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:41:12.203
Jan  5 19:41:12.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubelet-test 01/05/23 19:41:12.206
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:12.219
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:12.223
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:41:16.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-4040" for this suite. 01/05/23 19:41:16.246
------------------------------
• [4.049 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:41:12.203
    Jan  5 19:41:12.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 19:41:12.206
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:12.219
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:12.223
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:41:16.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-4040" for this suite. 01/05/23 19:41:16.246
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:41:16.253
Jan  5 19:41:16.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:41:16.255
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:16.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:16.27
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79
STEP: Creating secret with name secret-test-map-83a5c6b6-07d9-4203-a7f6-356540369567 01/05/23 19:41:16.272
STEP: Creating a pod to test consume secrets 01/05/23 19:41:16.277
Jan  5 19:41:16.291: INFO: Waiting up to 5m0s for pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7" in namespace "secrets-1075" to be "Succeeded or Failed"
Jan  5 19:41:16.295: INFO: Pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.651393ms
Jan  5 19:41:18.299: INFO: Pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007415615s
Jan  5 19:41:20.298: INFO: Pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006515751s
STEP: Saw pod success 01/05/23 19:41:20.298
Jan  5 19:41:20.298: INFO: Pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7" satisfied condition "Succeeded or Failed"
Jan  5 19:41:20.301: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 19:41:20.308
Jan  5 19:41:20.319: INFO: Waiting for pod pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7 to disappear
Jan  5 19:41:20.323: INFO: Pod pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:41:20.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-1075" for this suite. 01/05/23 19:41:20.327
------------------------------
• [4.079 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:79

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:41:16.253
    Jan  5 19:41:16.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:41:16.255
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:16.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:16.27
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:79
    STEP: Creating secret with name secret-test-map-83a5c6b6-07d9-4203-a7f6-356540369567 01/05/23 19:41:16.272
    STEP: Creating a pod to test consume secrets 01/05/23 19:41:16.277
    Jan  5 19:41:16.291: INFO: Waiting up to 5m0s for pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7" in namespace "secrets-1075" to be "Succeeded or Failed"
    Jan  5 19:41:16.295: INFO: Pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.651393ms
    Jan  5 19:41:18.299: INFO: Pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007415615s
    Jan  5 19:41:20.298: INFO: Pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006515751s
    STEP: Saw pod success 01/05/23 19:41:20.298
    Jan  5 19:41:20.298: INFO: Pod "pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7" satisfied condition "Succeeded or Failed"
    Jan  5 19:41:20.301: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:41:20.308
    Jan  5 19:41:20.319: INFO: Waiting for pod pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7 to disappear
    Jan  5 19:41:20.323: INFO: Pod pod-secrets-6a1594f4-fdfe-4e47-8420-51c14ff823f7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:41:20.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-1075" for this suite. 01/05/23 19:41:20.327
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:41:20.336
Jan  5 19:41:20.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:41:20.337
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:20.351
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:20.354
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536
Jan  5 19:41:20.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: creating the pod 01/05/23 19:41:20.358
STEP: submitting the pod to kubernetes 01/05/23 19:41:20.358
Jan  5 19:41:20.368: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684" in namespace "pods-2640" to be "running and ready"
Jan  5 19:41:20.371: INFO: Pod "pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684": Phase="Pending", Reason="", readiness=false. Elapsed: 3.713834ms
Jan  5 19:41:20.372: INFO: The phase of Pod pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:41:22.375: INFO: Pod "pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684": Phase="Running", Reason="", readiness=true. Elapsed: 2.007410081s
Jan  5 19:41:22.375: INFO: The phase of Pod pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684 is Running (Ready = true)
Jan  5 19:41:22.375: INFO: Pod "pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:41:22.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-2640" for this suite. 01/05/23 19:41:22.752
------------------------------
• [2.422 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:536

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:41:20.336
    Jan  5 19:41:20.336: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:41:20.337
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:20.351
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:20.354
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:536
    Jan  5 19:41:20.357: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: creating the pod 01/05/23 19:41:20.358
    STEP: submitting the pod to kubernetes 01/05/23 19:41:20.358
    Jan  5 19:41:20.368: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684" in namespace "pods-2640" to be "running and ready"
    Jan  5 19:41:20.371: INFO: Pod "pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684": Phase="Pending", Reason="", readiness=false. Elapsed: 3.713834ms
    Jan  5 19:41:20.372: INFO: The phase of Pod pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:41:22.375: INFO: Pod "pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684": Phase="Running", Reason="", readiness=true. Elapsed: 2.007410081s
    Jan  5 19:41:22.375: INFO: The phase of Pod pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684 is Running (Ready = true)
    Jan  5 19:41:22.375: INFO: Pod "pod-exec-websocket-1ea211a2-7413-4190-b067-0bebb6055684" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:41:22.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-2640" for this suite. 01/05/23 19:41:22.752
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:41:22.762
Jan  5 19:41:22.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename statefulset 01/05/23 19:41:22.763
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:22.777
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:22.782
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-6397 01/05/23 19:41:22.787
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:977
STEP: Creating statefulset ss in namespace statefulset-6397 01/05/23 19:41:22.796
Jan  5 19:41:22.811: INFO: Found 0 stateful pods, waiting for 1
Jan  5 19:41:32.815: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 01/05/23 19:41:32.822
STEP: Getting /status 01/05/23 19:41:32.831
Jan  5 19:41:32.835: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 01/05/23 19:41:32.835
Jan  5 19:41:32.845: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 01/05/23 19:41:32.845
Jan  5 19:41:32.848: INFO: Observed &StatefulSet event: ADDED
Jan  5 19:41:32.848: INFO: Found Statefulset ss in namespace statefulset-6397 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 19:41:32.848: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 01/05/23 19:41:32.848
Jan  5 19:41:32.848: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jan  5 19:41:32.856: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 01/05/23 19:41:32.856
Jan  5 19:41:32.858: INFO: Observed &StatefulSet event: ADDED
Jan  5 19:41:32.858: INFO: Observed Statefulset ss in namespace statefulset-6397 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jan  5 19:41:32.858: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  5 19:41:32.859: INFO: Deleting all statefulset in ns statefulset-6397
Jan  5 19:41:32.861: INFO: Scaling statefulset ss to 0
Jan  5 19:41:42.880: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 19:41:42.883: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  5 19:41:42.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-6397" for this suite. 01/05/23 19:41:42.907
------------------------------
• [SLOW TEST] [20.152 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:977

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:41:22.762
    Jan  5 19:41:22.762: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename statefulset 01/05/23 19:41:22.763
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:22.777
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:22.782
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-6397 01/05/23 19:41:22.787
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:977
    STEP: Creating statefulset ss in namespace statefulset-6397 01/05/23 19:41:22.796
    Jan  5 19:41:22.811: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 19:41:32.815: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 01/05/23 19:41:32.822
    STEP: Getting /status 01/05/23 19:41:32.831
    Jan  5 19:41:32.835: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 01/05/23 19:41:32.835
    Jan  5 19:41:32.845: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 01/05/23 19:41:32.845
    Jan  5 19:41:32.848: INFO: Observed &StatefulSet event: ADDED
    Jan  5 19:41:32.848: INFO: Found Statefulset ss in namespace statefulset-6397 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 19:41:32.848: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 01/05/23 19:41:32.848
    Jan  5 19:41:32.848: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jan  5 19:41:32.856: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 01/05/23 19:41:32.856
    Jan  5 19:41:32.858: INFO: Observed &StatefulSet event: ADDED
    Jan  5 19:41:32.858: INFO: Observed Statefulset ss in namespace statefulset-6397 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jan  5 19:41:32.858: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  5 19:41:32.859: INFO: Deleting all statefulset in ns statefulset-6397
    Jan  5 19:41:32.861: INFO: Scaling statefulset ss to 0
    Jan  5 19:41:42.880: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 19:41:42.883: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:41:42.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-6397" for this suite. 01/05/23 19:41:42.907
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:41:42.922
Jan  5 19:41:42.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:41:42.923
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:42.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:42.941
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191
STEP: creating service in namespace services-496 01/05/23 19:41:42.944
STEP: creating service affinity-clusterip in namespace services-496 01/05/23 19:41:42.944
STEP: creating replication controller affinity-clusterip in namespace services-496 01/05/23 19:41:42.957
I0105 19:41:42.963675      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-496, replica count: 3
I0105 19:41:46.015444      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 19:41:46.021: INFO: Creating new exec pod
Jan  5 19:41:46.027: INFO: Waiting up to 5m0s for pod "execpod-affinity5lsjn" in namespace "services-496" to be "running"
Jan  5 19:41:46.030: INFO: Pod "execpod-affinity5lsjn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.209662ms
Jan  5 19:41:48.033: INFO: Pod "execpod-affinity5lsjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005915084s
Jan  5 19:41:48.033: INFO: Pod "execpod-affinity5lsjn" satisfied condition "running"
Jan  5 19:41:49.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan  5 19:41:49.178: INFO: rc: 1
Jan  5 19:41:49.178: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-clusterip 80
nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:41:50.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan  5 19:41:50.315: INFO: rc: 1
Jan  5 19:41:50.315: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-clusterip 80
nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:41:51.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan  5 19:41:51.316: INFO: rc: 1
Jan  5 19:41:51.316: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-clusterip 80
nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:41:52.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan  5 19:41:52.330: INFO: rc: 1
Jan  5 19:41:52.330: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80:
Command stdout:

stderr:
+ nc -v -z -w 2 affinity-clusterip 80
nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:41:53.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
Jan  5 19:41:53.329: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jan  5 19:41:53.329: INFO: stdout: ""
Jan  5 19:41:53.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 10.20.6.221 80'
Jan  5 19:41:53.473: INFO: stderr: "+ nc -v -z -w 2 10.20.6.221 80\nConnection to 10.20.6.221 80 port [tcp/http] succeeded!\n"
Jan  5 19:41:53.473: INFO: stdout: ""
Jan  5 19:41:53.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.20.6.221:80/ ; done'
Jan  5 19:41:53.703: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n"
Jan  5 19:41:53.703: INFO: stdout: "\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4"
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
Jan  5 19:41:53.703: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-496, will wait for the garbage collector to delete the pods 01/05/23 19:41:53.716
Jan  5 19:41:53.777: INFO: Deleting ReplicationController affinity-clusterip took: 5.501574ms
Jan  5 19:41:53.877: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.681778ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:41:55.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-496" for this suite. 01/05/23 19:41:55.599
------------------------------
• [SLOW TEST] [12.682 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:41:42.922
    Jan  5 19:41:42.922: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:41:42.923
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:42.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:42.941
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2191
    STEP: creating service in namespace services-496 01/05/23 19:41:42.944
    STEP: creating service affinity-clusterip in namespace services-496 01/05/23 19:41:42.944
    STEP: creating replication controller affinity-clusterip in namespace services-496 01/05/23 19:41:42.957
    I0105 19:41:42.963675      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-496, replica count: 3
    I0105 19:41:46.015444      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 19:41:46.021: INFO: Creating new exec pod
    Jan  5 19:41:46.027: INFO: Waiting up to 5m0s for pod "execpod-affinity5lsjn" in namespace "services-496" to be "running"
    Jan  5 19:41:46.030: INFO: Pod "execpod-affinity5lsjn": Phase="Pending", Reason="", readiness=false. Elapsed: 3.209662ms
    Jan  5 19:41:48.033: INFO: Pod "execpod-affinity5lsjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.005915084s
    Jan  5 19:41:48.033: INFO: Pod "execpod-affinity5lsjn" satisfied condition "running"
    Jan  5 19:41:49.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan  5 19:41:49.178: INFO: rc: 1
    Jan  5 19:41:49.178: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-clusterip 80
    nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:41:50.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan  5 19:41:50.315: INFO: rc: 1
    Jan  5 19:41:50.315: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-clusterip 80
    nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:41:51.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan  5 19:41:51.316: INFO: rc: 1
    Jan  5 19:41:51.316: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-clusterip 80
    nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:41:52.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan  5 19:41:52.330: INFO: rc: 1
    Jan  5 19:41:52.330: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 affinity-clusterip 80
    nc: connect to affinity-clusterip port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:41:53.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 affinity-clusterip 80'
    Jan  5 19:41:53.329: INFO: stderr: "+ nc -v -z -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jan  5 19:41:53.329: INFO: stdout: ""
    Jan  5 19:41:53.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c nc -v -z -w 2 10.20.6.221 80'
    Jan  5 19:41:53.473: INFO: stderr: "+ nc -v -z -w 2 10.20.6.221 80\nConnection to 10.20.6.221 80 port [tcp/http] succeeded!\n"
    Jan  5 19:41:53.473: INFO: stdout: ""
    Jan  5 19:41:53.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-496 exec execpod-affinity5lsjn -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.20.6.221:80/ ; done'
    Jan  5 19:41:53.703: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.20.6.221:80/\n"
    Jan  5 19:41:53.703: INFO: stdout: "\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4\naffinity-clusterip-xvpv4"
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Received response from host: affinity-clusterip-xvpv4
    Jan  5 19:41:53.703: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-496, will wait for the garbage collector to delete the pods 01/05/23 19:41:53.716
    Jan  5 19:41:53.777: INFO: Deleting ReplicationController affinity-clusterip took: 5.501574ms
    Jan  5 19:41:53.877: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.681778ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:41:55.595: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-496" for this suite. 01/05/23 19:41:55.599
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:41:55.61
Jan  5 19:41:55.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:41:55.612
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:55.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:55.629
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302
STEP: creating service nodeport-test with type=NodePort in namespace services-1679 01/05/23 19:41:55.632
STEP: creating replication controller nodeport-test in namespace services-1679 01/05/23 19:41:55.644
I0105 19:41:55.652508      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1679, replica count: 2
I0105 19:41:58.703276      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 19:41:58.703: INFO: Creating new exec pod
Jan  5 19:41:58.710: INFO: Waiting up to 5m0s for pod "execpodg4hwd" in namespace "services-1679" to be "running"
Jan  5 19:41:58.714: INFO: Pod "execpodg4hwd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763655ms
Jan  5 19:42:00.717: INFO: Pod "execpodg4hwd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007041048s
Jan  5 19:42:00.717: INFO: Pod "execpodg4hwd" satisfied condition "running"
Jan  5 19:42:01.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan  5 19:42:03.874: INFO: rc: 1
Jan  5 19:42:03.874: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80:
Command stdout:

stderr:
+ nc -v -z -w 2 nodeport-test 80
nc: connect to nodeport-test port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 19:42:04.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
Jan  5 19:42:05.013: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jan  5 19:42:05.013: INFO: stdout: ""
Jan  5 19:42:05.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 10.20.4.106 80'
Jan  5 19:42:05.150: INFO: stderr: "+ nc -v -z -w 2 10.20.4.106 80\nConnection to 10.20.4.106 80 port [tcp/http] succeeded!\n"
Jan  5 19:42:05.150: INFO: stdout: ""
Jan  5 19:42:05.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 10.196.0.38 32018'
Jan  5 19:42:05.296: INFO: stderr: "+ nc -v -z -w 2 10.196.0.38 32018\nConnection to 10.196.0.38 32018 port [tcp/*] succeeded!\n"
Jan  5 19:42:05.296: INFO: stdout: ""
Jan  5 19:42:05.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 10.196.0.37 32018'
Jan  5 19:42:05.446: INFO: stderr: "+ nc -v -z -w 2 10.196.0.37 32018\nConnection to 10.196.0.37 32018 port [tcp/*] succeeded!\n"
Jan  5 19:42:05.446: INFO: stdout: ""
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:42:05.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1679" for this suite. 01/05/23 19:42:05.449
------------------------------
• [SLOW TEST] [9.848 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1302

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:41:55.61
    Jan  5 19:41:55.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:41:55.612
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:41:55.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:41:55.629
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1302
    STEP: creating service nodeport-test with type=NodePort in namespace services-1679 01/05/23 19:41:55.632
    STEP: creating replication controller nodeport-test in namespace services-1679 01/05/23 19:41:55.644
    I0105 19:41:55.652508      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-1679, replica count: 2
    I0105 19:41:58.703276      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 19:41:58.703: INFO: Creating new exec pod
    Jan  5 19:41:58.710: INFO: Waiting up to 5m0s for pod "execpodg4hwd" in namespace "services-1679" to be "running"
    Jan  5 19:41:58.714: INFO: Pod "execpodg4hwd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.763655ms
    Jan  5 19:42:00.717: INFO: Pod "execpodg4hwd": Phase="Running", Reason="", readiness=true. Elapsed: 2.007041048s
    Jan  5 19:42:00.717: INFO: Pod "execpodg4hwd" satisfied condition "running"
    Jan  5 19:42:01.722: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan  5 19:42:03.874: INFO: rc: 1
    Jan  5 19:42:03.874: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 nodeport-test 80
    nc: connect to nodeport-test port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 19:42:04.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 nodeport-test 80'
    Jan  5 19:42:05.013: INFO: stderr: "+ nc -v -z -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jan  5 19:42:05.013: INFO: stdout: ""
    Jan  5 19:42:05.013: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 10.20.4.106 80'
    Jan  5 19:42:05.150: INFO: stderr: "+ nc -v -z -w 2 10.20.4.106 80\nConnection to 10.20.4.106 80 port [tcp/http] succeeded!\n"
    Jan  5 19:42:05.150: INFO: stdout: ""
    Jan  5 19:42:05.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 10.196.0.38 32018'
    Jan  5 19:42:05.296: INFO: stderr: "+ nc -v -z -w 2 10.196.0.38 32018\nConnection to 10.196.0.38 32018 port [tcp/*] succeeded!\n"
    Jan  5 19:42:05.296: INFO: stdout: ""
    Jan  5 19:42:05.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-1679 exec execpodg4hwd -- /bin/sh -x -c nc -v -z -w 2 10.196.0.37 32018'
    Jan  5 19:42:05.446: INFO: stderr: "+ nc -v -z -w 2 10.196.0.37 32018\nConnection to 10.196.0.37 32018 port [tcp/*] succeeded!\n"
    Jan  5 19:42:05.446: INFO: stdout: ""
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:42:05.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1679" for this suite. 01/05/23 19:42:05.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:42:05.46
Jan  5 19:42:05.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:42:05.461
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:42:05.474
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:42:05.477
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:824
STEP: validating api versions 01/05/23 19:42:05.48
Jan  5 19:42:05.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9947 api-versions'
Jan  5 19:42:05.578: INFO: stderr: ""
Jan  5 19:42:05.578: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauto.gke.io/v1\nauto.gke.io/v1alpha1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncloud.google.com/v1\ncloud.google.com/v1beta1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhub.gke.io/v1\ninternal.autoscaling.gke.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nnetworking.gke.io/v1\nnetworking.gke.io/v1beta1\nnetworking.gke.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnodemanagement.gke.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:42:05.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9947" for this suite. 01/05/23 19:42:05.582
------------------------------
• [0.128 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:818
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:824

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:42:05.46
    Jan  5 19:42:05.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:42:05.461
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:42:05.474
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:42:05.477
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:824
    STEP: validating api versions 01/05/23 19:42:05.48
    Jan  5 19:42:05.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9947 api-versions'
    Jan  5 19:42:05.578: INFO: stderr: ""
    Jan  5 19:42:05.578: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauto.gke.io/v1\nauto.gke.io/v1alpha1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncloud.google.com/v1\ncloud.google.com/v1beta1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhub.gke.io/v1\ninternal.autoscaling.gke.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nnetworking.gke.io/v1\nnetworking.gke.io/v1beta1\nnetworking.gke.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnodemanagement.gke.io/v1alpha1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:42:05.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9947" for this suite. 01/05/23 19:42:05.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:42:05.589
Jan  5 19:42:05.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-preemption 01/05/23 19:42:05.59
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:42:05.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:42:05.608
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan  5 19:42:05.623: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 19:43:05.656: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222
STEP: Create pods that use 4/5 of node resources. 01/05/23 19:43:05.659
Jan  5 19:43:05.687: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jan  5 19:43:05.703: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jan  5 19:43:05.739: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jan  5 19:43:05.748: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jan  5 19:43:05.777: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jan  5 19:43:05.788: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 01/05/23 19:43:05.789
Jan  5 19:43:05.789: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7902" to be "running"
Jan  5 19:43:05.796: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.494845ms
Jan  5 19:43:07.801: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012449352s
Jan  5 19:43:07.802: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jan  5 19:43:07.802: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
Jan  5 19:43:07.805: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.946016ms
Jan  5 19:43:07.805: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 19:43:07.806: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
Jan  5 19:43:07.808: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765505ms
Jan  5 19:43:09.813: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007646988s
Jan  5 19:43:09.813: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 19:43:09.813: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
Jan  5 19:43:09.816: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.735412ms
Jan  5 19:43:09.816: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 19:43:09.816: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
Jan  5 19:43:09.819: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.557167ms
Jan  5 19:43:09.819: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jan  5 19:43:09.819: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
Jan  5 19:43:09.822: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.857817ms
Jan  5 19:43:09.822: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 01/05/23 19:43:09.822
Jan  5 19:43:09.839: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jan  5 19:43:09.846: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.067748ms
Jan  5 19:43:11.850: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010534734s
Jan  5 19:43:13.850: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010379607s
Jan  5 19:43:13.850: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:43:13.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-7902" for this suite. 01/05/23 19:43:13.967
------------------------------
• [SLOW TEST] [68.386 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:222

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:42:05.589
    Jan  5 19:42:05.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 19:42:05.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:42:05.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:42:05.608
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan  5 19:42:05.623: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 19:43:05.656: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:222
    STEP: Create pods that use 4/5 of node resources. 01/05/23 19:43:05.659
    Jan  5 19:43:05.687: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jan  5 19:43:05.703: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jan  5 19:43:05.739: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jan  5 19:43:05.748: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jan  5 19:43:05.777: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jan  5 19:43:05.788: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 01/05/23 19:43:05.789
    Jan  5 19:43:05.789: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7902" to be "running"
    Jan  5 19:43:05.796: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.494845ms
    Jan  5 19:43:07.801: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.012449352s
    Jan  5 19:43:07.802: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jan  5 19:43:07.802: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
    Jan  5 19:43:07.805: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.946016ms
    Jan  5 19:43:07.805: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 19:43:07.806: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
    Jan  5 19:43:07.808: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765505ms
    Jan  5 19:43:09.813: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.007646988s
    Jan  5 19:43:09.813: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 19:43:09.813: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
    Jan  5 19:43:09.816: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.735412ms
    Jan  5 19:43:09.816: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 19:43:09.816: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
    Jan  5 19:43:09.819: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.557167ms
    Jan  5 19:43:09.819: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jan  5 19:43:09.819: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7902" to be "running"
    Jan  5 19:43:09.822: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.857817ms
    Jan  5 19:43:09.822: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 01/05/23 19:43:09.822
    Jan  5 19:43:09.839: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jan  5 19:43:09.846: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.067748ms
    Jan  5 19:43:11.850: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010534734s
    Jan  5 19:43:13.850: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.010379607s
    Jan  5 19:43:13.850: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:43:13.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-7902" for this suite. 01/05/23 19:43:13.967
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:43:13.979
Jan  5 19:43:13.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename gc 01/05/23 19:43:13.98
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:43:13.995
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:43:13.999
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 01/05/23 19:43:14.009
STEP: delete the rc 01/05/23 19:43:19.019
STEP: wait for the rc to be deleted 01/05/23 19:43:19.026
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/05/23 19:43:24.035
STEP: Gathering metrics 01/05/23 19:43:54.045
W0105 19:43:54.055031      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 19:43:54.055: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jan  5 19:43:54.056: INFO: Deleting pod "simpletest.rc-2d5sm" in namespace "gc-490"
Jan  5 19:43:54.067: INFO: Deleting pod "simpletest.rc-2rpgs" in namespace "gc-490"
Jan  5 19:43:54.096: INFO: Deleting pod "simpletest.rc-2wkn5" in namespace "gc-490"
Jan  5 19:43:54.111: INFO: Deleting pod "simpletest.rc-69pmq" in namespace "gc-490"
Jan  5 19:43:54.123: INFO: Deleting pod "simpletest.rc-6dnqc" in namespace "gc-490"
Jan  5 19:43:54.134: INFO: Deleting pod "simpletest.rc-6ftnm" in namespace "gc-490"
Jan  5 19:43:54.146: INFO: Deleting pod "simpletest.rc-6k2hc" in namespace "gc-490"
Jan  5 19:43:54.159: INFO: Deleting pod "simpletest.rc-6kgvf" in namespace "gc-490"
Jan  5 19:43:54.176: INFO: Deleting pod "simpletest.rc-6mbbp" in namespace "gc-490"
Jan  5 19:43:54.187: INFO: Deleting pod "simpletest.rc-7t6cq" in namespace "gc-490"
Jan  5 19:43:54.201: INFO: Deleting pod "simpletest.rc-89hsd" in namespace "gc-490"
Jan  5 19:43:54.218: INFO: Deleting pod "simpletest.rc-8dkzj" in namespace "gc-490"
Jan  5 19:43:54.255: INFO: Deleting pod "simpletest.rc-8mtcr" in namespace "gc-490"
Jan  5 19:43:54.267: INFO: Deleting pod "simpletest.rc-99xft" in namespace "gc-490"
Jan  5 19:43:54.280: INFO: Deleting pod "simpletest.rc-9pcc2" in namespace "gc-490"
Jan  5 19:43:54.319: INFO: Deleting pod "simpletest.rc-9rhlq" in namespace "gc-490"
Jan  5 19:43:54.362: INFO: Deleting pod "simpletest.rc-bb9fd" in namespace "gc-490"
Jan  5 19:43:54.411: INFO: Deleting pod "simpletest.rc-bhpbl" in namespace "gc-490"
Jan  5 19:43:54.442: INFO: Deleting pod "simpletest.rc-bj6mk" in namespace "gc-490"
Jan  5 19:43:54.465: INFO: Deleting pod "simpletest.rc-bqlw7" in namespace "gc-490"
Jan  5 19:43:54.482: INFO: Deleting pod "simpletest.rc-bt5n8" in namespace "gc-490"
Jan  5 19:43:54.498: INFO: Deleting pod "simpletest.rc-btww7" in namespace "gc-490"
Jan  5 19:43:54.517: INFO: Deleting pod "simpletest.rc-bv4pp" in namespace "gc-490"
Jan  5 19:43:54.542: INFO: Deleting pod "simpletest.rc-bw5m7" in namespace "gc-490"
Jan  5 19:43:54.560: INFO: Deleting pod "simpletest.rc-c6g2z" in namespace "gc-490"
Jan  5 19:43:54.578: INFO: Deleting pod "simpletest.rc-cq8qq" in namespace "gc-490"
Jan  5 19:43:54.594: INFO: Deleting pod "simpletest.rc-cqdk9" in namespace "gc-490"
Jan  5 19:43:54.612: INFO: Deleting pod "simpletest.rc-d6nzd" in namespace "gc-490"
Jan  5 19:43:54.632: INFO: Deleting pod "simpletest.rc-d995z" in namespace "gc-490"
Jan  5 19:43:54.644: INFO: Deleting pod "simpletest.rc-dg4wj" in namespace "gc-490"
Jan  5 19:43:54.669: INFO: Deleting pod "simpletest.rc-dj995" in namespace "gc-490"
Jan  5 19:43:54.689: INFO: Deleting pod "simpletest.rc-dn4lg" in namespace "gc-490"
Jan  5 19:43:54.703: INFO: Deleting pod "simpletest.rc-dzgsm" in namespace "gc-490"
Jan  5 19:43:54.715: INFO: Deleting pod "simpletest.rc-f9vz4" in namespace "gc-490"
Jan  5 19:43:54.730: INFO: Deleting pod "simpletest.rc-fr4np" in namespace "gc-490"
Jan  5 19:43:54.742: INFO: Deleting pod "simpletest.rc-fv8jh" in namespace "gc-490"
Jan  5 19:43:54.758: INFO: Deleting pod "simpletest.rc-fzfg6" in namespace "gc-490"
Jan  5 19:43:54.773: INFO: Deleting pod "simpletest.rc-g2wfq" in namespace "gc-490"
Jan  5 19:43:54.785: INFO: Deleting pod "simpletest.rc-g5qqp" in namespace "gc-490"
Jan  5 19:43:54.796: INFO: Deleting pod "simpletest.rc-g82p6" in namespace "gc-490"
Jan  5 19:43:54.806: INFO: Deleting pod "simpletest.rc-ghw9x" in namespace "gc-490"
Jan  5 19:43:54.819: INFO: Deleting pod "simpletest.rc-gsfbx" in namespace "gc-490"
Jan  5 19:43:54.836: INFO: Deleting pod "simpletest.rc-gtmk7" in namespace "gc-490"
Jan  5 19:43:54.858: INFO: Deleting pod "simpletest.rc-h24tn" in namespace "gc-490"
Jan  5 19:43:54.876: INFO: Deleting pod "simpletest.rc-hcmwm" in namespace "gc-490"
Jan  5 19:43:54.889: INFO: Deleting pod "simpletest.rc-hj882" in namespace "gc-490"
Jan  5 19:43:54.902: INFO: Deleting pod "simpletest.rc-hkn49" in namespace "gc-490"
Jan  5 19:43:54.917: INFO: Deleting pod "simpletest.rc-hmdwc" in namespace "gc-490"
Jan  5 19:43:54.932: INFO: Deleting pod "simpletest.rc-j4smt" in namespace "gc-490"
Jan  5 19:43:54.945: INFO: Deleting pod "simpletest.rc-jhckz" in namespace "gc-490"
Jan  5 19:43:54.959: INFO: Deleting pod "simpletest.rc-kl52l" in namespace "gc-490"
Jan  5 19:43:54.971: INFO: Deleting pod "simpletest.rc-knj9w" in namespace "gc-490"
Jan  5 19:43:54.989: INFO: Deleting pod "simpletest.rc-lccx4" in namespace "gc-490"
Jan  5 19:43:54.999: INFO: Deleting pod "simpletest.rc-lg8v4" in namespace "gc-490"
Jan  5 19:43:55.012: INFO: Deleting pod "simpletest.rc-lnm2l" in namespace "gc-490"
Jan  5 19:43:55.027: INFO: Deleting pod "simpletest.rc-m496w" in namespace "gc-490"
Jan  5 19:43:55.041: INFO: Deleting pod "simpletest.rc-m6gck" in namespace "gc-490"
Jan  5 19:43:55.055: INFO: Deleting pod "simpletest.rc-m7gg4" in namespace "gc-490"
Jan  5 19:43:55.071: INFO: Deleting pod "simpletest.rc-mgzr7" in namespace "gc-490"
Jan  5 19:43:55.086: INFO: Deleting pod "simpletest.rc-nmlsg" in namespace "gc-490"
Jan  5 19:43:55.100: INFO: Deleting pod "simpletest.rc-nr75g" in namespace "gc-490"
Jan  5 19:43:55.121: INFO: Deleting pod "simpletest.rc-nscbm" in namespace "gc-490"
Jan  5 19:43:55.137: INFO: Deleting pod "simpletest.rc-p4m94" in namespace "gc-490"
Jan  5 19:43:55.153: INFO: Deleting pod "simpletest.rc-pj4w5" in namespace "gc-490"
Jan  5 19:43:55.166: INFO: Deleting pod "simpletest.rc-prvdx" in namespace "gc-490"
Jan  5 19:43:55.177: INFO: Deleting pod "simpletest.rc-qr5qg" in namespace "gc-490"
Jan  5 19:43:55.192: INFO: Deleting pod "simpletest.rc-qz89n" in namespace "gc-490"
Jan  5 19:43:55.205: INFO: Deleting pod "simpletest.rc-r8zl5" in namespace "gc-490"
Jan  5 19:43:55.219: INFO: Deleting pod "simpletest.rc-rghtv" in namespace "gc-490"
Jan  5 19:43:55.235: INFO: Deleting pod "simpletest.rc-rzvxx" in namespace "gc-490"
Jan  5 19:43:55.256: INFO: Deleting pod "simpletest.rc-s6k8r" in namespace "gc-490"
Jan  5 19:43:55.273: INFO: Deleting pod "simpletest.rc-s9zpp" in namespace "gc-490"
Jan  5 19:43:55.286: INFO: Deleting pod "simpletest.rc-sd65r" in namespace "gc-490"
Jan  5 19:43:55.305: INFO: Deleting pod "simpletest.rc-shjxw" in namespace "gc-490"
Jan  5 19:43:55.350: INFO: Deleting pod "simpletest.rc-snvrv" in namespace "gc-490"
Jan  5 19:43:55.404: INFO: Deleting pod "simpletest.rc-sqxcw" in namespace "gc-490"
Jan  5 19:43:55.448: INFO: Deleting pod "simpletest.rc-svhqd" in namespace "gc-490"
Jan  5 19:43:55.499: INFO: Deleting pod "simpletest.rc-t2pc2" in namespace "gc-490"
Jan  5 19:43:55.547: INFO: Deleting pod "simpletest.rc-t58nk" in namespace "gc-490"
Jan  5 19:43:55.598: INFO: Deleting pod "simpletest.rc-t5tdj" in namespace "gc-490"
Jan  5 19:43:55.647: INFO: Deleting pod "simpletest.rc-tfzdh" in namespace "gc-490"
Jan  5 19:43:55.697: INFO: Deleting pod "simpletest.rc-tlkl6" in namespace "gc-490"
Jan  5 19:43:55.746: INFO: Deleting pod "simpletest.rc-tpgr6" in namespace "gc-490"
Jan  5 19:43:55.796: INFO: Deleting pod "simpletest.rc-tvmvq" in namespace "gc-490"
Jan  5 19:43:55.847: INFO: Deleting pod "simpletest.rc-v5fcx" in namespace "gc-490"
Jan  5 19:43:55.901: INFO: Deleting pod "simpletest.rc-v9zvn" in namespace "gc-490"
Jan  5 19:43:55.949: INFO: Deleting pod "simpletest.rc-vvwjt" in namespace "gc-490"
Jan  5 19:43:55.998: INFO: Deleting pod "simpletest.rc-vzrqq" in namespace "gc-490"
Jan  5 19:43:56.048: INFO: Deleting pod "simpletest.rc-wh629" in namespace "gc-490"
Jan  5 19:43:56.102: INFO: Deleting pod "simpletest.rc-wjp45" in namespace "gc-490"
Jan  5 19:43:56.148: INFO: Deleting pod "simpletest.rc-wl47h" in namespace "gc-490"
Jan  5 19:43:56.198: INFO: Deleting pod "simpletest.rc-wrftp" in namespace "gc-490"
Jan  5 19:43:56.246: INFO: Deleting pod "simpletest.rc-x7t9c" in namespace "gc-490"
Jan  5 19:43:56.299: INFO: Deleting pod "simpletest.rc-x9f9s" in namespace "gc-490"
Jan  5 19:43:56.350: INFO: Deleting pod "simpletest.rc-xq5hx" in namespace "gc-490"
Jan  5 19:43:56.400: INFO: Deleting pod "simpletest.rc-xwdwx" in namespace "gc-490"
Jan  5 19:43:56.454: INFO: Deleting pod "simpletest.rc-xwhnr" in namespace "gc-490"
Jan  5 19:43:56.500: INFO: Deleting pod "simpletest.rc-zb56g" in namespace "gc-490"
Jan  5 19:43:56.549: INFO: Deleting pod "simpletest.rc-zdg2s" in namespace "gc-490"
Jan  5 19:43:56.595: INFO: Deleting pod "simpletest.rc-zjcq2" in namespace "gc-490"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  5 19:43:56.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-490" for this suite. 01/05/23 19:43:56.69
------------------------------
• [SLOW TEST] [42.762 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:43:13.979
    Jan  5 19:43:13.979: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename gc 01/05/23 19:43:13.98
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:43:13.995
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:43:13.999
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 01/05/23 19:43:14.009
    STEP: delete the rc 01/05/23 19:43:19.019
    STEP: wait for the rc to be deleted 01/05/23 19:43:19.026
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 01/05/23 19:43:24.035
    STEP: Gathering metrics 01/05/23 19:43:54.045
    W0105 19:43:54.055031      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 19:43:54.055: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jan  5 19:43:54.056: INFO: Deleting pod "simpletest.rc-2d5sm" in namespace "gc-490"
    Jan  5 19:43:54.067: INFO: Deleting pod "simpletest.rc-2rpgs" in namespace "gc-490"
    Jan  5 19:43:54.096: INFO: Deleting pod "simpletest.rc-2wkn5" in namespace "gc-490"
    Jan  5 19:43:54.111: INFO: Deleting pod "simpletest.rc-69pmq" in namespace "gc-490"
    Jan  5 19:43:54.123: INFO: Deleting pod "simpletest.rc-6dnqc" in namespace "gc-490"
    Jan  5 19:43:54.134: INFO: Deleting pod "simpletest.rc-6ftnm" in namespace "gc-490"
    Jan  5 19:43:54.146: INFO: Deleting pod "simpletest.rc-6k2hc" in namespace "gc-490"
    Jan  5 19:43:54.159: INFO: Deleting pod "simpletest.rc-6kgvf" in namespace "gc-490"
    Jan  5 19:43:54.176: INFO: Deleting pod "simpletest.rc-6mbbp" in namespace "gc-490"
    Jan  5 19:43:54.187: INFO: Deleting pod "simpletest.rc-7t6cq" in namespace "gc-490"
    Jan  5 19:43:54.201: INFO: Deleting pod "simpletest.rc-89hsd" in namespace "gc-490"
    Jan  5 19:43:54.218: INFO: Deleting pod "simpletest.rc-8dkzj" in namespace "gc-490"
    Jan  5 19:43:54.255: INFO: Deleting pod "simpletest.rc-8mtcr" in namespace "gc-490"
    Jan  5 19:43:54.267: INFO: Deleting pod "simpletest.rc-99xft" in namespace "gc-490"
    Jan  5 19:43:54.280: INFO: Deleting pod "simpletest.rc-9pcc2" in namespace "gc-490"
    Jan  5 19:43:54.319: INFO: Deleting pod "simpletest.rc-9rhlq" in namespace "gc-490"
    Jan  5 19:43:54.362: INFO: Deleting pod "simpletest.rc-bb9fd" in namespace "gc-490"
    Jan  5 19:43:54.411: INFO: Deleting pod "simpletest.rc-bhpbl" in namespace "gc-490"
    Jan  5 19:43:54.442: INFO: Deleting pod "simpletest.rc-bj6mk" in namespace "gc-490"
    Jan  5 19:43:54.465: INFO: Deleting pod "simpletest.rc-bqlw7" in namespace "gc-490"
    Jan  5 19:43:54.482: INFO: Deleting pod "simpletest.rc-bt5n8" in namespace "gc-490"
    Jan  5 19:43:54.498: INFO: Deleting pod "simpletest.rc-btww7" in namespace "gc-490"
    Jan  5 19:43:54.517: INFO: Deleting pod "simpletest.rc-bv4pp" in namespace "gc-490"
    Jan  5 19:43:54.542: INFO: Deleting pod "simpletest.rc-bw5m7" in namespace "gc-490"
    Jan  5 19:43:54.560: INFO: Deleting pod "simpletest.rc-c6g2z" in namespace "gc-490"
    Jan  5 19:43:54.578: INFO: Deleting pod "simpletest.rc-cq8qq" in namespace "gc-490"
    Jan  5 19:43:54.594: INFO: Deleting pod "simpletest.rc-cqdk9" in namespace "gc-490"
    Jan  5 19:43:54.612: INFO: Deleting pod "simpletest.rc-d6nzd" in namespace "gc-490"
    Jan  5 19:43:54.632: INFO: Deleting pod "simpletest.rc-d995z" in namespace "gc-490"
    Jan  5 19:43:54.644: INFO: Deleting pod "simpletest.rc-dg4wj" in namespace "gc-490"
    Jan  5 19:43:54.669: INFO: Deleting pod "simpletest.rc-dj995" in namespace "gc-490"
    Jan  5 19:43:54.689: INFO: Deleting pod "simpletest.rc-dn4lg" in namespace "gc-490"
    Jan  5 19:43:54.703: INFO: Deleting pod "simpletest.rc-dzgsm" in namespace "gc-490"
    Jan  5 19:43:54.715: INFO: Deleting pod "simpletest.rc-f9vz4" in namespace "gc-490"
    Jan  5 19:43:54.730: INFO: Deleting pod "simpletest.rc-fr4np" in namespace "gc-490"
    Jan  5 19:43:54.742: INFO: Deleting pod "simpletest.rc-fv8jh" in namespace "gc-490"
    Jan  5 19:43:54.758: INFO: Deleting pod "simpletest.rc-fzfg6" in namespace "gc-490"
    Jan  5 19:43:54.773: INFO: Deleting pod "simpletest.rc-g2wfq" in namespace "gc-490"
    Jan  5 19:43:54.785: INFO: Deleting pod "simpletest.rc-g5qqp" in namespace "gc-490"
    Jan  5 19:43:54.796: INFO: Deleting pod "simpletest.rc-g82p6" in namespace "gc-490"
    Jan  5 19:43:54.806: INFO: Deleting pod "simpletest.rc-ghw9x" in namespace "gc-490"
    Jan  5 19:43:54.819: INFO: Deleting pod "simpletest.rc-gsfbx" in namespace "gc-490"
    Jan  5 19:43:54.836: INFO: Deleting pod "simpletest.rc-gtmk7" in namespace "gc-490"
    Jan  5 19:43:54.858: INFO: Deleting pod "simpletest.rc-h24tn" in namespace "gc-490"
    Jan  5 19:43:54.876: INFO: Deleting pod "simpletest.rc-hcmwm" in namespace "gc-490"
    Jan  5 19:43:54.889: INFO: Deleting pod "simpletest.rc-hj882" in namespace "gc-490"
    Jan  5 19:43:54.902: INFO: Deleting pod "simpletest.rc-hkn49" in namespace "gc-490"
    Jan  5 19:43:54.917: INFO: Deleting pod "simpletest.rc-hmdwc" in namespace "gc-490"
    Jan  5 19:43:54.932: INFO: Deleting pod "simpletest.rc-j4smt" in namespace "gc-490"
    Jan  5 19:43:54.945: INFO: Deleting pod "simpletest.rc-jhckz" in namespace "gc-490"
    Jan  5 19:43:54.959: INFO: Deleting pod "simpletest.rc-kl52l" in namespace "gc-490"
    Jan  5 19:43:54.971: INFO: Deleting pod "simpletest.rc-knj9w" in namespace "gc-490"
    Jan  5 19:43:54.989: INFO: Deleting pod "simpletest.rc-lccx4" in namespace "gc-490"
    Jan  5 19:43:54.999: INFO: Deleting pod "simpletest.rc-lg8v4" in namespace "gc-490"
    Jan  5 19:43:55.012: INFO: Deleting pod "simpletest.rc-lnm2l" in namespace "gc-490"
    Jan  5 19:43:55.027: INFO: Deleting pod "simpletest.rc-m496w" in namespace "gc-490"
    Jan  5 19:43:55.041: INFO: Deleting pod "simpletest.rc-m6gck" in namespace "gc-490"
    Jan  5 19:43:55.055: INFO: Deleting pod "simpletest.rc-m7gg4" in namespace "gc-490"
    Jan  5 19:43:55.071: INFO: Deleting pod "simpletest.rc-mgzr7" in namespace "gc-490"
    Jan  5 19:43:55.086: INFO: Deleting pod "simpletest.rc-nmlsg" in namespace "gc-490"
    Jan  5 19:43:55.100: INFO: Deleting pod "simpletest.rc-nr75g" in namespace "gc-490"
    Jan  5 19:43:55.121: INFO: Deleting pod "simpletest.rc-nscbm" in namespace "gc-490"
    Jan  5 19:43:55.137: INFO: Deleting pod "simpletest.rc-p4m94" in namespace "gc-490"
    Jan  5 19:43:55.153: INFO: Deleting pod "simpletest.rc-pj4w5" in namespace "gc-490"
    Jan  5 19:43:55.166: INFO: Deleting pod "simpletest.rc-prvdx" in namespace "gc-490"
    Jan  5 19:43:55.177: INFO: Deleting pod "simpletest.rc-qr5qg" in namespace "gc-490"
    Jan  5 19:43:55.192: INFO: Deleting pod "simpletest.rc-qz89n" in namespace "gc-490"
    Jan  5 19:43:55.205: INFO: Deleting pod "simpletest.rc-r8zl5" in namespace "gc-490"
    Jan  5 19:43:55.219: INFO: Deleting pod "simpletest.rc-rghtv" in namespace "gc-490"
    Jan  5 19:43:55.235: INFO: Deleting pod "simpletest.rc-rzvxx" in namespace "gc-490"
    Jan  5 19:43:55.256: INFO: Deleting pod "simpletest.rc-s6k8r" in namespace "gc-490"
    Jan  5 19:43:55.273: INFO: Deleting pod "simpletest.rc-s9zpp" in namespace "gc-490"
    Jan  5 19:43:55.286: INFO: Deleting pod "simpletest.rc-sd65r" in namespace "gc-490"
    Jan  5 19:43:55.305: INFO: Deleting pod "simpletest.rc-shjxw" in namespace "gc-490"
    Jan  5 19:43:55.350: INFO: Deleting pod "simpletest.rc-snvrv" in namespace "gc-490"
    Jan  5 19:43:55.404: INFO: Deleting pod "simpletest.rc-sqxcw" in namespace "gc-490"
    Jan  5 19:43:55.448: INFO: Deleting pod "simpletest.rc-svhqd" in namespace "gc-490"
    Jan  5 19:43:55.499: INFO: Deleting pod "simpletest.rc-t2pc2" in namespace "gc-490"
    Jan  5 19:43:55.547: INFO: Deleting pod "simpletest.rc-t58nk" in namespace "gc-490"
    Jan  5 19:43:55.598: INFO: Deleting pod "simpletest.rc-t5tdj" in namespace "gc-490"
    Jan  5 19:43:55.647: INFO: Deleting pod "simpletest.rc-tfzdh" in namespace "gc-490"
    Jan  5 19:43:55.697: INFO: Deleting pod "simpletest.rc-tlkl6" in namespace "gc-490"
    Jan  5 19:43:55.746: INFO: Deleting pod "simpletest.rc-tpgr6" in namespace "gc-490"
    Jan  5 19:43:55.796: INFO: Deleting pod "simpletest.rc-tvmvq" in namespace "gc-490"
    Jan  5 19:43:55.847: INFO: Deleting pod "simpletest.rc-v5fcx" in namespace "gc-490"
    Jan  5 19:43:55.901: INFO: Deleting pod "simpletest.rc-v9zvn" in namespace "gc-490"
    Jan  5 19:43:55.949: INFO: Deleting pod "simpletest.rc-vvwjt" in namespace "gc-490"
    Jan  5 19:43:55.998: INFO: Deleting pod "simpletest.rc-vzrqq" in namespace "gc-490"
    Jan  5 19:43:56.048: INFO: Deleting pod "simpletest.rc-wh629" in namespace "gc-490"
    Jan  5 19:43:56.102: INFO: Deleting pod "simpletest.rc-wjp45" in namespace "gc-490"
    Jan  5 19:43:56.148: INFO: Deleting pod "simpletest.rc-wl47h" in namespace "gc-490"
    Jan  5 19:43:56.198: INFO: Deleting pod "simpletest.rc-wrftp" in namespace "gc-490"
    Jan  5 19:43:56.246: INFO: Deleting pod "simpletest.rc-x7t9c" in namespace "gc-490"
    Jan  5 19:43:56.299: INFO: Deleting pod "simpletest.rc-x9f9s" in namespace "gc-490"
    Jan  5 19:43:56.350: INFO: Deleting pod "simpletest.rc-xq5hx" in namespace "gc-490"
    Jan  5 19:43:56.400: INFO: Deleting pod "simpletest.rc-xwdwx" in namespace "gc-490"
    Jan  5 19:43:56.454: INFO: Deleting pod "simpletest.rc-xwhnr" in namespace "gc-490"
    Jan  5 19:43:56.500: INFO: Deleting pod "simpletest.rc-zb56g" in namespace "gc-490"
    Jan  5 19:43:56.549: INFO: Deleting pod "simpletest.rc-zdg2s" in namespace "gc-490"
    Jan  5 19:43:56.595: INFO: Deleting pod "simpletest.rc-zjcq2" in namespace "gc-490"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:43:56.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-490" for this suite. 01/05/23 19:43:56.69
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:43:56.743
Jan  5 19:43:56.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:43:56.745
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:43:56.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:43:56.78
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:43:56.783
Jan  5 19:43:56.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e" in namespace "downward-api-6412" to be "Succeeded or Failed"
Jan  5 19:43:56.802: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.095052ms
Jan  5 19:43:58.812: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015473084s
Jan  5 19:44:00.811: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013521428s
Jan  5 19:44:02.806: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008875409s
Jan  5 19:44:04.806: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008840615s
Jan  5 19:44:06.807: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009515359s
Jan  5 19:44:08.806: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.009394538s
STEP: Saw pod success 01/05/23 19:44:08.807
Jan  5 19:44:08.807: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e" satisfied condition "Succeeded or Failed"
Jan  5 19:44:08.810: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e container client-container: <nil>
STEP: delete the pod 01/05/23 19:44:08.823
Jan  5 19:44:08.835: INFO: Waiting for pod downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e to disappear
Jan  5 19:44:08.839: INFO: Pod downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:44:08.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-6412" for this suite. 01/05/23 19:44:08.843
------------------------------
• [SLOW TEST] [12.106 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:43:56.743
    Jan  5 19:43:56.743: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:43:56.745
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:43:56.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:43:56.78
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:193
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:43:56.783
    Jan  5 19:43:56.797: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e" in namespace "downward-api-6412" to be "Succeeded or Failed"
    Jan  5 19:43:56.802: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.095052ms
    Jan  5 19:43:58.812: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015473084s
    Jan  5 19:44:00.811: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.013521428s
    Jan  5 19:44:02.806: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008875409s
    Jan  5 19:44:04.806: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008840615s
    Jan  5 19:44:06.807: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009515359s
    Jan  5 19:44:08.806: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.009394538s
    STEP: Saw pod success 01/05/23 19:44:08.807
    Jan  5 19:44:08.807: INFO: Pod "downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e" satisfied condition "Succeeded or Failed"
    Jan  5 19:44:08.810: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e container client-container: <nil>
    STEP: delete the pod 01/05/23 19:44:08.823
    Jan  5 19:44:08.835: INFO: Waiting for pod downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e to disappear
    Jan  5 19:44:08.839: INFO: Pod downwardapi-volume-b402b082-5489-4bea-8537-27acd098478e no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:44:08.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-6412" for this suite. 01/05/23 19:44:08.843
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:44:08.851
Jan  5 19:44:08.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:44:08.853
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:08.871
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:08.874
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jan  5 19:44:08.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:44:09.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "custom-resource-definition-6438" for this suite. 01/05/23 19:44:09.905
------------------------------
• [1.061 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:44:08.851
    Jan  5 19:44:08.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename custom-resource-definition 01/05/23 19:44:08.853
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:08.871
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:08.874
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jan  5 19:44:08.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:44:09.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "custom-resource-definition-6438" for this suite. 01/05/23 19:44:09.905
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:44:09.916
Jan  5 19:44:09.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 19:44:09.918
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:09.933
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:09.937
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6828 01/05/23 19:44:09.94
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 19:44:09.957
STEP: creating service externalsvc in namespace services-6828 01/05/23 19:44:09.958
STEP: creating replication controller externalsvc in namespace services-6828 01/05/23 19:44:09.978
I0105 19:44:09.987883      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6828, replica count: 2
I0105 19:44:13.040233      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 01/05/23 19:44:13.043
Jan  5 19:44:13.054: INFO: Creating new exec pod
Jan  5 19:44:13.062: INFO: Waiting up to 5m0s for pod "execpodflvnt" in namespace "services-6828" to be "running"
Jan  5 19:44:13.065: INFO: Pod "execpodflvnt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.95076ms
Jan  5 19:44:15.118: INFO: Pod "execpodflvnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.055061948s
Jan  5 19:44:15.118: INFO: Pod "execpodflvnt" satisfied condition "running"
Jan  5 19:44:15.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-6828 exec execpodflvnt -- /bin/sh -x -c nslookup clusterip-service.services-6828.svc.cluster.local'
Jan  5 19:44:15.345: INFO: stderr: "+ nslookup clusterip-service.services-6828.svc.cluster.local\n"
Jan  5 19:44:15.345: INFO: stdout: "Server:\t\t10.20.0.10\nAddress:\t10.20.0.10#53\n\nclusterip-service.services-6828.svc.cluster.local\tcanonical name = externalsvc.services-6828.svc.cluster.local.\nName:\texternalsvc.services-6828.svc.cluster.local\nAddress: 10.20.8.204\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-6828, will wait for the garbage collector to delete the pods 01/05/23 19:44:15.345
Jan  5 19:44:15.406: INFO: Deleting ReplicationController externalsvc took: 6.44676ms
Jan  5 19:44:15.506: INFO: Terminating ReplicationController externalsvc pods took: 100.711968ms
Jan  5 19:44:17.520: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 19:44:17.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-6828" for this suite. 01/05/23 19:44:17.536
------------------------------
• [SLOW TEST] [7.627 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1515

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:44:09.916
    Jan  5 19:44:09.916: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 19:44:09.918
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:09.933
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:09.937
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1515
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-6828 01/05/23 19:44:09.94
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 19:44:09.957
    STEP: creating service externalsvc in namespace services-6828 01/05/23 19:44:09.958
    STEP: creating replication controller externalsvc in namespace services-6828 01/05/23 19:44:09.978
    I0105 19:44:09.987883      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-6828, replica count: 2
    I0105 19:44:13.040233      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 01/05/23 19:44:13.043
    Jan  5 19:44:13.054: INFO: Creating new exec pod
    Jan  5 19:44:13.062: INFO: Waiting up to 5m0s for pod "execpodflvnt" in namespace "services-6828" to be "running"
    Jan  5 19:44:13.065: INFO: Pod "execpodflvnt": Phase="Pending", Reason="", readiness=false. Elapsed: 2.95076ms
    Jan  5 19:44:15.118: INFO: Pod "execpodflvnt": Phase="Running", Reason="", readiness=true. Elapsed: 2.055061948s
    Jan  5 19:44:15.118: INFO: Pod "execpodflvnt" satisfied condition "running"
    Jan  5 19:44:15.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-6828 exec execpodflvnt -- /bin/sh -x -c nslookup clusterip-service.services-6828.svc.cluster.local'
    Jan  5 19:44:15.345: INFO: stderr: "+ nslookup clusterip-service.services-6828.svc.cluster.local\n"
    Jan  5 19:44:15.345: INFO: stdout: "Server:\t\t10.20.0.10\nAddress:\t10.20.0.10#53\n\nclusterip-service.services-6828.svc.cluster.local\tcanonical name = externalsvc.services-6828.svc.cluster.local.\nName:\texternalsvc.services-6828.svc.cluster.local\nAddress: 10.20.8.204\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-6828, will wait for the garbage collector to delete the pods 01/05/23 19:44:15.345
    Jan  5 19:44:15.406: INFO: Deleting ReplicationController externalsvc took: 6.44676ms
    Jan  5 19:44:15.506: INFO: Terminating ReplicationController externalsvc pods took: 100.711968ms
    Jan  5 19:44:17.520: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:44:17.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-6828" for this suite. 01/05/23 19:44:17.536
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:44:17.549
Jan  5 19:44:17.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:44:17.551
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:17.564
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:17.567
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:44:17.571
Jan  5 19:44:17.582: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619" in namespace "projected-5485" to be "Succeeded or Failed"
Jan  5 19:44:17.588: INFO: Pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619": Phase="Pending", Reason="", readiness=false. Elapsed: 5.050358ms
Jan  5 19:44:19.592: INFO: Pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009299789s
Jan  5 19:44:21.593: INFO: Pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01000928s
STEP: Saw pod success 01/05/23 19:44:21.593
Jan  5 19:44:21.593: INFO: Pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619" satisfied condition "Succeeded or Failed"
Jan  5 19:44:21.596: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619 container client-container: <nil>
STEP: delete the pod 01/05/23 19:44:21.613
Jan  5 19:44:21.625: INFO: Waiting for pod downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619 to disappear
Jan  5 19:44:21.628: INFO: Pod downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:44:21.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5485" for this suite. 01/05/23 19:44:21.632
------------------------------
• [4.089 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:44:17.549
    Jan  5 19:44:17.549: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:44:17.551
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:17.564
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:17.567
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:249
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:44:17.571
    Jan  5 19:44:17.582: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619" in namespace "projected-5485" to be "Succeeded or Failed"
    Jan  5 19:44:17.588: INFO: Pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619": Phase="Pending", Reason="", readiness=false. Elapsed: 5.050358ms
    Jan  5 19:44:19.592: INFO: Pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009299789s
    Jan  5 19:44:21.593: INFO: Pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01000928s
    STEP: Saw pod success 01/05/23 19:44:21.593
    Jan  5 19:44:21.593: INFO: Pod "downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619" satisfied condition "Succeeded or Failed"
    Jan  5 19:44:21.596: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:44:21.613
    Jan  5 19:44:21.625: INFO: Waiting for pod downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619 to disappear
    Jan  5 19:44:21.628: INFO: Pod downwardapi-volume-efdc7bad-8aaa-41c2-9a6c-92f9af730619 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:44:21.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5485" for this suite. 01/05/23 19:44:21.632
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:44:21.64
Jan  5 19:44:21.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 19:44:21.641
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:21.653
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:21.656
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217
STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 19:44:21.659
Jan  5 19:44:21.669: INFO: Waiting up to 5m0s for pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f" in namespace "emptydir-6649" to be "Succeeded or Failed"
Jan  5 19:44:21.674: INFO: Pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.506023ms
Jan  5 19:44:23.677: INFO: Pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007704031s
Jan  5 19:44:25.677: INFO: Pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008164146s
STEP: Saw pod success 01/05/23 19:44:25.677
Jan  5 19:44:25.678: INFO: Pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f" satisfied condition "Succeeded or Failed"
Jan  5 19:44:25.680: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f container test-container: <nil>
STEP: delete the pod 01/05/23 19:44:25.688
Jan  5 19:44:25.701: INFO: Waiting for pod pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f to disappear
Jan  5 19:44:25.704: INFO: Pod pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 19:44:25.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-6649" for this suite. 01/05/23 19:44:25.711
------------------------------
• [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:217

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:44:21.64
    Jan  5 19:44:21.640: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 19:44:21.641
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:21.653
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:21.656
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:217
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 19:44:21.659
    Jan  5 19:44:21.669: INFO: Waiting up to 5m0s for pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f" in namespace "emptydir-6649" to be "Succeeded or Failed"
    Jan  5 19:44:21.674: INFO: Pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.506023ms
    Jan  5 19:44:23.677: INFO: Pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007704031s
    Jan  5 19:44:25.677: INFO: Pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008164146s
    STEP: Saw pod success 01/05/23 19:44:25.677
    Jan  5 19:44:25.678: INFO: Pod "pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f" satisfied condition "Succeeded or Failed"
    Jan  5 19:44:25.680: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f container test-container: <nil>
    STEP: delete the pod 01/05/23 19:44:25.688
    Jan  5 19:44:25.701: INFO: Waiting for pod pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f to disappear
    Jan  5 19:44:25.704: INFO: Pod pod-4caa1175-5de0-4498-9398-fc0c2c3a1b9f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:44:25.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-6649" for this suite. 01/05/23 19:44:25.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:44:25.719
Jan  5 19:44:25.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:44:25.72
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:25.733
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:25.736
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:44:25.739
Jan  5 19:44:25.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749" in namespace "projected-2736" to be "Succeeded or Failed"
Jan  5 19:44:25.754: INFO: Pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047605ms
Jan  5 19:44:27.759: INFO: Pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008984313s
Jan  5 19:44:29.758: INFO: Pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007306658s
STEP: Saw pod success 01/05/23 19:44:29.758
Jan  5 19:44:29.758: INFO: Pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749" satisfied condition "Succeeded or Failed"
Jan  5 19:44:29.761: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749 container client-container: <nil>
STEP: delete the pod 01/05/23 19:44:29.767
Jan  5 19:44:29.782: INFO: Waiting for pod downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749 to disappear
Jan  5 19:44:29.786: INFO: Pod downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:44:29.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2736" for this suite. 01/05/23 19:44:29.79
------------------------------
• [4.078 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:44:25.719
    Jan  5 19:44:25.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:44:25.72
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:25.733
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:25.736
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:261
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:44:25.739
    Jan  5 19:44:25.750: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749" in namespace "projected-2736" to be "Succeeded or Failed"
    Jan  5 19:44:25.754: INFO: Pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749": Phase="Pending", Reason="", readiness=false. Elapsed: 3.047605ms
    Jan  5 19:44:27.759: INFO: Pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008984313s
    Jan  5 19:44:29.758: INFO: Pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007306658s
    STEP: Saw pod success 01/05/23 19:44:29.758
    Jan  5 19:44:29.758: INFO: Pod "downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749" satisfied condition "Succeeded or Failed"
    Jan  5 19:44:29.761: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:44:29.767
    Jan  5 19:44:29.782: INFO: Waiting for pod downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749 to disappear
    Jan  5 19:44:29.786: INFO: Pod downwardapi-volume-4ccc3f3a-3b22-448a-a163-32e1dc99c749 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:44:29.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2736" for this suite. 01/05/23 19:44:29.79
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:44:29.797
Jan  5 19:44:29.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename security-context-test 01/05/23 19:44:29.8
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:29.819
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:29.822
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:528
Jan  5 19:44:29.842: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc" in namespace "security-context-test-8" to be "Succeeded or Failed"
Jan  5 19:44:29.846: INFO: Pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807966ms
Jan  5 19:44:31.850: INFO: Pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007644093s
Jan  5 19:44:33.852: INFO: Pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00901525s
Jan  5 19:44:33.852: INFO: Pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc" satisfied condition "Succeeded or Failed"
Jan  5 19:44:33.860: INFO: Got logs for pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  5 19:44:33.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-8" for this suite. 01/05/23 19:44:33.866
------------------------------
• [4.077 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:491
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:44:29.797
    Jan  5 19:44:29.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename security-context-test 01/05/23 19:44:29.8
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:29.819
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:29.822
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:528
    Jan  5 19:44:29.842: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc" in namespace "security-context-test-8" to be "Succeeded or Failed"
    Jan  5 19:44:29.846: INFO: Pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.807966ms
    Jan  5 19:44:31.850: INFO: Pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007644093s
    Jan  5 19:44:33.852: INFO: Pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00901525s
    Jan  5 19:44:33.852: INFO: Pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc" satisfied condition "Succeeded or Failed"
    Jan  5 19:44:33.860: INFO: Got logs for pod "busybox-privileged-false-2c397d2d-96b2-4a31-a8f0-2508d02083dc": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:44:33.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-8" for this suite. 01/05/23 19:44:33.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:44:33.878
Jan  5 19:44:33.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename proxy 01/05/23 19:44:33.88
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:33.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:33.894
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 01/05/23 19:44:33.908
STEP: creating replication controller proxy-service-48dct in namespace proxy-1896 01/05/23 19:44:33.909
I0105 19:44:33.915888      23 runners.go:193] Created replication controller with name: proxy-service-48dct, namespace: proxy-1896, replica count: 1
I0105 19:44:34.967481      23 runners.go:193] proxy-service-48dct Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0105 19:44:35.967802      23 runners.go:193] proxy-service-48dct Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 19:44:35.971: INFO: setup took 2.072965699s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/05/23 19:44:35.971
Jan  5 19:44:35.985: INFO: (0) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.756054ms)
Jan  5 19:44:35.991: INFO: (0) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 18.703289ms)
Jan  5 19:44:35.991: INFO: (0) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 18.934831ms)
Jan  5 19:44:35.991: INFO: (0) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 18.115793ms)
Jan  5 19:44:35.994: INFO: (0) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 21.498648ms)
Jan  5 19:44:35.994: INFO: (0) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 20.623376ms)
Jan  5 19:44:35.994: INFO: (0) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 21.059871ms)
Jan  5 19:44:35.994: INFO: (0) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 21.071695ms)
Jan  5 19:44:35.996: INFO: (0) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 22.912371ms)
Jan  5 19:44:35.996: INFO: (0) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 23.716746ms)
Jan  5 19:44:36.000: INFO: (0) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 26.340406ms)
Jan  5 19:44:36.000: INFO: (0) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 28.010644ms)
Jan  5 19:44:36.000: INFO: (0) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 27.545143ms)
Jan  5 19:44:36.001: INFO: (0) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 28.359021ms)
Jan  5 19:44:36.001: INFO: (0) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 28.940726ms)
Jan  5 19:44:36.001: INFO: (0) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 28.897737ms)
Jan  5 19:44:36.010: INFO: (1) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 7.8022ms)
Jan  5 19:44:36.010: INFO: (1) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 7.937138ms)
Jan  5 19:44:36.011: INFO: (1) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 9.370717ms)
Jan  5 19:44:36.012: INFO: (1) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 8.637093ms)
Jan  5 19:44:36.012: INFO: (1) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 9.165696ms)
Jan  5 19:44:36.012: INFO: (1) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 9.567374ms)
Jan  5 19:44:36.014: INFO: (1) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 12.49955ms)
Jan  5 19:44:36.014: INFO: (1) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 11.25814ms)
Jan  5 19:44:36.014: INFO: (1) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 11.4907ms)
Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 13.389809ms)
Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.119285ms)
Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.797952ms)
Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.447557ms)
Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 12.342208ms)
Jan  5 19:44:36.017: INFO: (1) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 15.196308ms)
Jan  5 19:44:36.017: INFO: (1) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 14.770467ms)
Jan  5 19:44:36.022: INFO: (2) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 4.579674ms)
Jan  5 19:44:36.022: INFO: (2) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 4.469945ms)
Jan  5 19:44:36.028: INFO: (2) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 10.079365ms)
Jan  5 19:44:36.028: INFO: (2) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 10.348012ms)
Jan  5 19:44:36.028: INFO: (2) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 10.037024ms)
Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 10.64863ms)
Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 10.6966ms)
Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 10.625055ms)
Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 10.870152ms)
Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 10.974033ms)
Jan  5 19:44:36.030: INFO: (2) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.109425ms)
Jan  5 19:44:36.030: INFO: (2) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.759814ms)
Jan  5 19:44:36.031: INFO: (2) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 12.650416ms)
Jan  5 19:44:36.031: INFO: (2) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 13.303877ms)
Jan  5 19:44:36.032: INFO: (2) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 13.614943ms)
Jan  5 19:44:36.034: INFO: (2) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 16.088259ms)
Jan  5 19:44:36.040: INFO: (3) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 5.553493ms)
Jan  5 19:44:36.042: INFO: (3) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 7.214114ms)
Jan  5 19:44:36.043: INFO: (3) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 8.553186ms)
Jan  5 19:44:36.045: INFO: (3) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 10.273872ms)
Jan  5 19:44:36.045: INFO: (3) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 10.564561ms)
Jan  5 19:44:36.046: INFO: (3) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 11.49657ms)
Jan  5 19:44:36.046: INFO: (3) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 11.8334ms)
Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 12.002542ms)
Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 12.063912ms)
Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.152128ms)
Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 12.283505ms)
Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.112094ms)
Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 12.349397ms)
Jan  5 19:44:36.048: INFO: (3) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 13.641463ms)
Jan  5 19:44:36.048: INFO: (3) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 13.560641ms)
Jan  5 19:44:36.049: INFO: (3) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 14.640653ms)
Jan  5 19:44:36.054: INFO: (4) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 4.59224ms)
Jan  5 19:44:36.058: INFO: (4) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 8.419091ms)
Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 11.068175ms)
Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 10.707896ms)
Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 10.780476ms)
Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 10.700777ms)
Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 10.687195ms)
Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 11.388397ms)
Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 11.699347ms)
Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 11.68912ms)
Jan  5 19:44:36.062: INFO: (4) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 11.556273ms)
Jan  5 19:44:36.063: INFO: (4) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 13.501683ms)
Jan  5 19:44:36.063: INFO: (4) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 13.549311ms)
Jan  5 19:44:36.067: INFO: (4) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.287775ms)
Jan  5 19:44:36.070: INFO: (4) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 19.967325ms)
Jan  5 19:44:36.071: INFO: (4) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 21.392926ms)
Jan  5 19:44:36.080: INFO: (5) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 8.348494ms)
Jan  5 19:44:36.083: INFO: (5) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 11.02496ms)
Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 14.348851ms)
Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 14.535586ms)
Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 14.744215ms)
Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 14.782499ms)
Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 15.195315ms)
Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 15.113803ms)
Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 15.223564ms)
Jan  5 19:44:36.088: INFO: (5) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 16.034154ms)
Jan  5 19:44:36.088: INFO: (5) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 15.609877ms)
Jan  5 19:44:36.088: INFO: (5) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 15.912713ms)
Jan  5 19:44:36.088: INFO: (5) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 16.060569ms)
Jan  5 19:44:36.090: INFO: (5) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 18.029611ms)
Jan  5 19:44:36.091: INFO: (5) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 19.075357ms)
Jan  5 19:44:36.091: INFO: (5) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 18.694615ms)
Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 16.147106ms)
Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 16.53872ms)
Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 15.76393ms)
Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.215003ms)
Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.491704ms)
Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 16.6823ms)
Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 16.448231ms)
Jan  5 19:44:36.109: INFO: (6) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 17.298848ms)
Jan  5 19:44:36.109: INFO: (6) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 17.122009ms)
Jan  5 19:44:36.109: INFO: (6) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 17.685501ms)
Jan  5 19:44:36.111: INFO: (6) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 19.222064ms)
Jan  5 19:44:36.111: INFO: (6) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 19.82715ms)
Jan  5 19:44:36.112: INFO: (6) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 20.679031ms)
Jan  5 19:44:36.112: INFO: (6) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 20.246204ms)
Jan  5 19:44:36.112: INFO: (6) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 20.76435ms)
Jan  5 19:44:36.113: INFO: (6) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 21.409015ms)
Jan  5 19:44:36.122: INFO: (7) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 8.4344ms)
Jan  5 19:44:36.122: INFO: (7) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 8.489907ms)
Jan  5 19:44:36.143: INFO: (7) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 29.378486ms)
Jan  5 19:44:36.144: INFO: (7) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 30.552878ms)
Jan  5 19:44:36.144: INFO: (7) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 29.966428ms)
Jan  5 19:44:36.144: INFO: (7) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 30.521993ms)
Jan  5 19:44:36.145: INFO: (7) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 31.282454ms)
Jan  5 19:44:36.145: INFO: (7) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 31.165377ms)
Jan  5 19:44:36.146: INFO: (7) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 31.823881ms)
Jan  5 19:44:36.146: INFO: (7) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 31.585749ms)
Jan  5 19:44:36.146: INFO: (7) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 32.353271ms)
Jan  5 19:44:36.146: INFO: (7) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 32.221503ms)
Jan  5 19:44:36.148: INFO: (7) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 34.169732ms)
Jan  5 19:44:36.148: INFO: (7) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 34.719879ms)
Jan  5 19:44:36.149: INFO: (7) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 35.236395ms)
Jan  5 19:44:36.149: INFO: (7) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 35.908813ms)
Jan  5 19:44:36.213: INFO: (8) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 63.795202ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 63.799696ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 63.17906ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 63.841711ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 63.453264ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 64.405783ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 63.417779ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 64.170419ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 64.387344ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 63.985536ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 63.367378ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 63.463792ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 64.153985ms)
Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 64.086742ms)
Jan  5 19:44:36.219: INFO: (8) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 68.689557ms)
Jan  5 19:44:36.220: INFO: (8) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 70.199883ms)
Jan  5 19:44:36.242: INFO: (9) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 15.346376ms)
Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 18.723885ms)
Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 19.301816ms)
Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 18.159618ms)
Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 18.51281ms)
Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 18.581631ms)
Jan  5 19:44:36.246: INFO: (9) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 19.993712ms)
Jan  5 19:44:36.247: INFO: (9) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 21.09279ms)
Jan  5 19:44:36.247: INFO: (9) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 20.823834ms)
Jan  5 19:44:36.247: INFO: (9) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 20.405158ms)
Jan  5 19:44:36.251: INFO: (9) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 25.122659ms)
Jan  5 19:44:36.253: INFO: (9) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 26.852298ms)
Jan  5 19:44:36.253: INFO: (9) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 26.496038ms)
Jan  5 19:44:36.253: INFO: (9) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 26.442124ms)
Jan  5 19:44:36.260: INFO: (9) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 34.416355ms)
Jan  5 19:44:36.261: INFO: (9) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 35.229079ms)
Jan  5 19:44:36.285: INFO: (10) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 23.292339ms)
Jan  5 19:44:36.286: INFO: (10) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 24.559391ms)
Jan  5 19:44:36.287: INFO: (10) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 24.894886ms)
Jan  5 19:44:36.289: INFO: (10) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 27.714594ms)
Jan  5 19:44:36.290: INFO: (10) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 28.672721ms)
Jan  5 19:44:36.291: INFO: (10) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 27.972534ms)
Jan  5 19:44:36.291: INFO: (10) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 28.699854ms)
Jan  5 19:44:36.292: INFO: (10) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 30.166776ms)
Jan  5 19:44:36.292: INFO: (10) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 30.006739ms)
Jan  5 19:44:36.293: INFO: (10) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 31.357055ms)
Jan  5 19:44:36.293: INFO: (10) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 31.171033ms)
Jan  5 19:44:36.294: INFO: (10) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 31.832454ms)
Jan  5 19:44:36.302: INFO: (10) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 39.484957ms)
Jan  5 19:44:36.313: INFO: (10) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 51.252722ms)
Jan  5 19:44:36.313: INFO: (10) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 51.144295ms)
Jan  5 19:44:36.313: INFO: (10) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 51.352414ms)
Jan  5 19:44:36.346: INFO: (11) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 30.714323ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 31.827448ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 31.335572ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 30.87991ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 32.030731ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 31.38319ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 31.158012ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 31.622962ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 32.03166ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 31.131139ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 31.652319ms)
Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 32.124554ms)
Jan  5 19:44:36.349: INFO: (11) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 33.641699ms)
Jan  5 19:44:36.350: INFO: (11) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 33.939395ms)
Jan  5 19:44:36.350: INFO: (11) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 34.755319ms)
Jan  5 19:44:36.350: INFO: (11) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 34.509508ms)
Jan  5 19:44:36.364: INFO: (12) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 13.058066ms)
Jan  5 19:44:36.366: INFO: (12) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 15.509789ms)
Jan  5 19:44:36.366: INFO: (12) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 14.885616ms)
Jan  5 19:44:36.367: INFO: (12) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 16.91937ms)
Jan  5 19:44:36.368: INFO: (12) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 17.264908ms)
Jan  5 19:44:36.368: INFO: (12) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 17.447928ms)
Jan  5 19:44:36.369: INFO: (12) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 18.462728ms)
Jan  5 19:44:36.369: INFO: (12) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 18.075187ms)
Jan  5 19:44:36.369: INFO: (12) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 19.158646ms)
Jan  5 19:44:36.369: INFO: (12) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 18.679447ms)
Jan  5 19:44:36.371: INFO: (12) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 20.672648ms)
Jan  5 19:44:36.372: INFO: (12) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 22.228513ms)
Jan  5 19:44:36.373: INFO: (12) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 22.789383ms)
Jan  5 19:44:36.373: INFO: (12) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 22.416512ms)
Jan  5 19:44:36.373: INFO: (12) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 23.365736ms)
Jan  5 19:44:36.374: INFO: (12) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 23.48911ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 19.083637ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 18.333489ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 17.995273ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 18.552006ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 18.515955ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 17.906124ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 17.822906ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 18.259339ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 18.751398ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 19.119558ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 18.288718ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 18.099865ms)
Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 18.302493ms)
Jan  5 19:44:36.394: INFO: (13) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 18.65002ms)
Jan  5 19:44:36.394: INFO: (13) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 18.708299ms)
Jan  5 19:44:36.394: INFO: (13) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 18.27335ms)
Jan  5 19:44:36.402: INFO: (14) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 8.128444ms)
Jan  5 19:44:36.402: INFO: (14) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 8.06444ms)
Jan  5 19:44:36.402: INFO: (14) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 8.017979ms)
Jan  5 19:44:36.402: INFO: (14) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 8.710473ms)
Jan  5 19:44:36.403: INFO: (14) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 8.482164ms)
Jan  5 19:44:36.403: INFO: (14) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 8.56632ms)
Jan  5 19:44:36.403: INFO: (14) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 8.677428ms)
Jan  5 19:44:36.409: INFO: (14) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 6.869275ms)
Jan  5 19:44:36.410: INFO: (14) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 7.575916ms)
Jan  5 19:44:36.410: INFO: (14) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 8.386803ms)
Jan  5 19:44:36.411: INFO: (14) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 8.055094ms)
Jan  5 19:44:36.411: INFO: (14) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 16.287601ms)
Jan  5 19:44:36.411: INFO: (14) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 8.795303ms)
Jan  5 19:44:36.418: INFO: (14) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 15.057497ms)
Jan  5 19:44:36.418: INFO: (14) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 15.775406ms)
Jan  5 19:44:36.418: INFO: (14) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 23.195183ms)
Jan  5 19:44:36.434: INFO: (15) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 14.861337ms)
Jan  5 19:44:36.435: INFO: (15) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 16.603347ms)
Jan  5 19:44:36.435: INFO: (15) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 16.830419ms)
Jan  5 19:44:36.435: INFO: (15) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 17.158412ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 16.123097ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 17.680272ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 17.622919ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 16.495521ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 16.812853ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 16.71986ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 17.569497ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 16.717119ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 16.872192ms)
Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.139175ms)
Jan  5 19:44:36.437: INFO: (15) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 17.950926ms)
Jan  5 19:44:36.437: INFO: (15) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 16.709195ms)
Jan  5 19:44:36.447: INFO: (16) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 8.49957ms)
Jan  5 19:44:36.447: INFO: (16) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 8.597387ms)
Jan  5 19:44:36.447: INFO: (16) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 8.748848ms)
Jan  5 19:44:36.449: INFO: (16) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 9.525176ms)
Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 12.277039ms)
Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 12.484937ms)
Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.444878ms)
Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 11.860728ms)
Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.412611ms)
Jan  5 19:44:36.459: INFO: (16) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 20.030461ms)
Jan  5 19:44:36.459: INFO: (16) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 20.254987ms)
Jan  5 19:44:36.460: INFO: (16) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 20.618623ms)
Jan  5 19:44:36.461: INFO: (16) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 21.367424ms)
Jan  5 19:44:36.461: INFO: (16) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 21.885994ms)
Jan  5 19:44:36.462: INFO: (16) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 22.826816ms)
Jan  5 19:44:36.462: INFO: (16) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 22.898426ms)
Jan  5 19:44:36.469: INFO: (17) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 6.019619ms)
Jan  5 19:44:36.470: INFO: (17) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 7.49279ms)
Jan  5 19:44:36.473: INFO: (17) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 9.696095ms)
Jan  5 19:44:36.474: INFO: (17) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 10.616543ms)
Jan  5 19:44:36.474: INFO: (17) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 10.251827ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.309829ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.670618ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 12.326984ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 13.532984ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 12.430918ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 12.851748ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 13.133059ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 12.946413ms)
Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 12.909227ms)
Jan  5 19:44:36.478: INFO: (17) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 15.48257ms)
Jan  5 19:44:36.481: INFO: (17) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 17.469031ms)
Jan  5 19:44:36.493: INFO: (18) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 11.367295ms)
Jan  5 19:44:36.494: INFO: (18) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 12.722014ms)
Jan  5 19:44:36.494: INFO: (18) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 12.502761ms)
Jan  5 19:44:36.495: INFO: (18) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 12.896218ms)
Jan  5 19:44:36.495: INFO: (18) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 12.855088ms)
Jan  5 19:44:36.495: INFO: (18) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 13.06089ms)
Jan  5 19:44:36.496: INFO: (18) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 14.389374ms)
Jan  5 19:44:36.497: INFO: (18) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 14.9726ms)
Jan  5 19:44:36.497: INFO: (18) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 15.200857ms)
Jan  5 19:44:36.498: INFO: (18) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 15.96728ms)
Jan  5 19:44:36.498: INFO: (18) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 16.16668ms)
Jan  5 19:44:36.498: INFO: (18) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 16.941965ms)
Jan  5 19:44:36.499: INFO: (18) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 17.656998ms)
Jan  5 19:44:36.499: INFO: (18) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 17.359319ms)
Jan  5 19:44:36.500: INFO: (18) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 18.320241ms)
Jan  5 19:44:36.501: INFO: (18) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 19.636971ms)
Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 12.955214ms)
Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.776208ms)
Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 11.976354ms)
Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 12.729249ms)
Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 11.980006ms)
Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 12.630549ms)
Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.604079ms)
Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 12.772871ms)
Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 12.558558ms)
Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 12.507462ms)
Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 12.279246ms)
Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.367678ms)
Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 13.023348ms)
Jan  5 19:44:36.519: INFO: (19) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 17.310272ms)
Jan  5 19:44:36.520: INFO: (19) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.19818ms)
Jan  5 19:44:36.520: INFO: (19) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 18.939818ms)
STEP: deleting ReplicationController proxy-service-48dct in namespace proxy-1896, will wait for the garbage collector to delete the pods 01/05/23 19:44:36.521
Jan  5 19:44:36.581: INFO: Deleting ReplicationController proxy-service-48dct took: 5.457777ms
Jan  5 19:44:36.681: INFO: Terminating ReplicationController proxy-service-48dct pods took: 100.258269ms
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  5 19:44:39.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-1896" for this suite. 01/05/23 19:44:39.588
------------------------------
• [SLOW TEST] [5.716 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:44:33.878
    Jan  5 19:44:33.878: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename proxy 01/05/23 19:44:33.88
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:33.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:33.894
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 01/05/23 19:44:33.908
    STEP: creating replication controller proxy-service-48dct in namespace proxy-1896 01/05/23 19:44:33.909
    I0105 19:44:33.915888      23 runners.go:193] Created replication controller with name: proxy-service-48dct, namespace: proxy-1896, replica count: 1
    I0105 19:44:34.967481      23 runners.go:193] proxy-service-48dct Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0105 19:44:35.967802      23 runners.go:193] proxy-service-48dct Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 19:44:35.971: INFO: setup took 2.072965699s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 01/05/23 19:44:35.971
    Jan  5 19:44:35.985: INFO: (0) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.756054ms)
    Jan  5 19:44:35.991: INFO: (0) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 18.703289ms)
    Jan  5 19:44:35.991: INFO: (0) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 18.934831ms)
    Jan  5 19:44:35.991: INFO: (0) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 18.115793ms)
    Jan  5 19:44:35.994: INFO: (0) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 21.498648ms)
    Jan  5 19:44:35.994: INFO: (0) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 20.623376ms)
    Jan  5 19:44:35.994: INFO: (0) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 21.059871ms)
    Jan  5 19:44:35.994: INFO: (0) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 21.071695ms)
    Jan  5 19:44:35.996: INFO: (0) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 22.912371ms)
    Jan  5 19:44:35.996: INFO: (0) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 23.716746ms)
    Jan  5 19:44:36.000: INFO: (0) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 26.340406ms)
    Jan  5 19:44:36.000: INFO: (0) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 28.010644ms)
    Jan  5 19:44:36.000: INFO: (0) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 27.545143ms)
    Jan  5 19:44:36.001: INFO: (0) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 28.359021ms)
    Jan  5 19:44:36.001: INFO: (0) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 28.940726ms)
    Jan  5 19:44:36.001: INFO: (0) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 28.897737ms)
    Jan  5 19:44:36.010: INFO: (1) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 7.8022ms)
    Jan  5 19:44:36.010: INFO: (1) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 7.937138ms)
    Jan  5 19:44:36.011: INFO: (1) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 9.370717ms)
    Jan  5 19:44:36.012: INFO: (1) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 8.637093ms)
    Jan  5 19:44:36.012: INFO: (1) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 9.165696ms)
    Jan  5 19:44:36.012: INFO: (1) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 9.567374ms)
    Jan  5 19:44:36.014: INFO: (1) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 12.49955ms)
    Jan  5 19:44:36.014: INFO: (1) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 11.25814ms)
    Jan  5 19:44:36.014: INFO: (1) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 11.4907ms)
    Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 13.389809ms)
    Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.119285ms)
    Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.797952ms)
    Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.447557ms)
    Jan  5 19:44:36.015: INFO: (1) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 12.342208ms)
    Jan  5 19:44:36.017: INFO: (1) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 15.196308ms)
    Jan  5 19:44:36.017: INFO: (1) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 14.770467ms)
    Jan  5 19:44:36.022: INFO: (2) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 4.579674ms)
    Jan  5 19:44:36.022: INFO: (2) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 4.469945ms)
    Jan  5 19:44:36.028: INFO: (2) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 10.079365ms)
    Jan  5 19:44:36.028: INFO: (2) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 10.348012ms)
    Jan  5 19:44:36.028: INFO: (2) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 10.037024ms)
    Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 10.64863ms)
    Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 10.6966ms)
    Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 10.625055ms)
    Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 10.870152ms)
    Jan  5 19:44:36.029: INFO: (2) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 10.974033ms)
    Jan  5 19:44:36.030: INFO: (2) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.109425ms)
    Jan  5 19:44:36.030: INFO: (2) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.759814ms)
    Jan  5 19:44:36.031: INFO: (2) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 12.650416ms)
    Jan  5 19:44:36.031: INFO: (2) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 13.303877ms)
    Jan  5 19:44:36.032: INFO: (2) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 13.614943ms)
    Jan  5 19:44:36.034: INFO: (2) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 16.088259ms)
    Jan  5 19:44:36.040: INFO: (3) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 5.553493ms)
    Jan  5 19:44:36.042: INFO: (3) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 7.214114ms)
    Jan  5 19:44:36.043: INFO: (3) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 8.553186ms)
    Jan  5 19:44:36.045: INFO: (3) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 10.273872ms)
    Jan  5 19:44:36.045: INFO: (3) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 10.564561ms)
    Jan  5 19:44:36.046: INFO: (3) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 11.49657ms)
    Jan  5 19:44:36.046: INFO: (3) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 11.8334ms)
    Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 12.002542ms)
    Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 12.063912ms)
    Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.152128ms)
    Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 12.283505ms)
    Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.112094ms)
    Jan  5 19:44:36.047: INFO: (3) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 12.349397ms)
    Jan  5 19:44:36.048: INFO: (3) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 13.641463ms)
    Jan  5 19:44:36.048: INFO: (3) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 13.560641ms)
    Jan  5 19:44:36.049: INFO: (3) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 14.640653ms)
    Jan  5 19:44:36.054: INFO: (4) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 4.59224ms)
    Jan  5 19:44:36.058: INFO: (4) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 8.419091ms)
    Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 11.068175ms)
    Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 10.707896ms)
    Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 10.780476ms)
    Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 10.700777ms)
    Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 10.687195ms)
    Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 11.388397ms)
    Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 11.699347ms)
    Jan  5 19:44:36.061: INFO: (4) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 11.68912ms)
    Jan  5 19:44:36.062: INFO: (4) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 11.556273ms)
    Jan  5 19:44:36.063: INFO: (4) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 13.501683ms)
    Jan  5 19:44:36.063: INFO: (4) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 13.549311ms)
    Jan  5 19:44:36.067: INFO: (4) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.287775ms)
    Jan  5 19:44:36.070: INFO: (4) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 19.967325ms)
    Jan  5 19:44:36.071: INFO: (4) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 21.392926ms)
    Jan  5 19:44:36.080: INFO: (5) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 8.348494ms)
    Jan  5 19:44:36.083: INFO: (5) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 11.02496ms)
    Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 14.348851ms)
    Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 14.535586ms)
    Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 14.744215ms)
    Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 14.782499ms)
    Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 15.195315ms)
    Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 15.113803ms)
    Jan  5 19:44:36.087: INFO: (5) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 15.223564ms)
    Jan  5 19:44:36.088: INFO: (5) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 16.034154ms)
    Jan  5 19:44:36.088: INFO: (5) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 15.609877ms)
    Jan  5 19:44:36.088: INFO: (5) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 15.912713ms)
    Jan  5 19:44:36.088: INFO: (5) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 16.060569ms)
    Jan  5 19:44:36.090: INFO: (5) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 18.029611ms)
    Jan  5 19:44:36.091: INFO: (5) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 19.075357ms)
    Jan  5 19:44:36.091: INFO: (5) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 18.694615ms)
    Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 16.147106ms)
    Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 16.53872ms)
    Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 15.76393ms)
    Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.215003ms)
    Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.491704ms)
    Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 16.6823ms)
    Jan  5 19:44:36.108: INFO: (6) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 16.448231ms)
    Jan  5 19:44:36.109: INFO: (6) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 17.298848ms)
    Jan  5 19:44:36.109: INFO: (6) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 17.122009ms)
    Jan  5 19:44:36.109: INFO: (6) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 17.685501ms)
    Jan  5 19:44:36.111: INFO: (6) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 19.222064ms)
    Jan  5 19:44:36.111: INFO: (6) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 19.82715ms)
    Jan  5 19:44:36.112: INFO: (6) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 20.679031ms)
    Jan  5 19:44:36.112: INFO: (6) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 20.246204ms)
    Jan  5 19:44:36.112: INFO: (6) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 20.76435ms)
    Jan  5 19:44:36.113: INFO: (6) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 21.409015ms)
    Jan  5 19:44:36.122: INFO: (7) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 8.4344ms)
    Jan  5 19:44:36.122: INFO: (7) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 8.489907ms)
    Jan  5 19:44:36.143: INFO: (7) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 29.378486ms)
    Jan  5 19:44:36.144: INFO: (7) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 30.552878ms)
    Jan  5 19:44:36.144: INFO: (7) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 29.966428ms)
    Jan  5 19:44:36.144: INFO: (7) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 30.521993ms)
    Jan  5 19:44:36.145: INFO: (7) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 31.282454ms)
    Jan  5 19:44:36.145: INFO: (7) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 31.165377ms)
    Jan  5 19:44:36.146: INFO: (7) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 31.823881ms)
    Jan  5 19:44:36.146: INFO: (7) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 31.585749ms)
    Jan  5 19:44:36.146: INFO: (7) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 32.353271ms)
    Jan  5 19:44:36.146: INFO: (7) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 32.221503ms)
    Jan  5 19:44:36.148: INFO: (7) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 34.169732ms)
    Jan  5 19:44:36.148: INFO: (7) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 34.719879ms)
    Jan  5 19:44:36.149: INFO: (7) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 35.236395ms)
    Jan  5 19:44:36.149: INFO: (7) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 35.908813ms)
    Jan  5 19:44:36.213: INFO: (8) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 63.795202ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 63.799696ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 63.17906ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 63.841711ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 63.453264ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 64.405783ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 63.417779ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 64.170419ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 64.387344ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 63.985536ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 63.367378ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 63.463792ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 64.153985ms)
    Jan  5 19:44:36.214: INFO: (8) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 64.086742ms)
    Jan  5 19:44:36.219: INFO: (8) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 68.689557ms)
    Jan  5 19:44:36.220: INFO: (8) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 70.199883ms)
    Jan  5 19:44:36.242: INFO: (9) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 15.346376ms)
    Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 18.723885ms)
    Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 19.301816ms)
    Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 18.159618ms)
    Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 18.51281ms)
    Jan  5 19:44:36.245: INFO: (9) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 18.581631ms)
    Jan  5 19:44:36.246: INFO: (9) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 19.993712ms)
    Jan  5 19:44:36.247: INFO: (9) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 21.09279ms)
    Jan  5 19:44:36.247: INFO: (9) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 20.823834ms)
    Jan  5 19:44:36.247: INFO: (9) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 20.405158ms)
    Jan  5 19:44:36.251: INFO: (9) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 25.122659ms)
    Jan  5 19:44:36.253: INFO: (9) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 26.852298ms)
    Jan  5 19:44:36.253: INFO: (9) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 26.496038ms)
    Jan  5 19:44:36.253: INFO: (9) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 26.442124ms)
    Jan  5 19:44:36.260: INFO: (9) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 34.416355ms)
    Jan  5 19:44:36.261: INFO: (9) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 35.229079ms)
    Jan  5 19:44:36.285: INFO: (10) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 23.292339ms)
    Jan  5 19:44:36.286: INFO: (10) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 24.559391ms)
    Jan  5 19:44:36.287: INFO: (10) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 24.894886ms)
    Jan  5 19:44:36.289: INFO: (10) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 27.714594ms)
    Jan  5 19:44:36.290: INFO: (10) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 28.672721ms)
    Jan  5 19:44:36.291: INFO: (10) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 27.972534ms)
    Jan  5 19:44:36.291: INFO: (10) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 28.699854ms)
    Jan  5 19:44:36.292: INFO: (10) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 30.166776ms)
    Jan  5 19:44:36.292: INFO: (10) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 30.006739ms)
    Jan  5 19:44:36.293: INFO: (10) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 31.357055ms)
    Jan  5 19:44:36.293: INFO: (10) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 31.171033ms)
    Jan  5 19:44:36.294: INFO: (10) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 31.832454ms)
    Jan  5 19:44:36.302: INFO: (10) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 39.484957ms)
    Jan  5 19:44:36.313: INFO: (10) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 51.252722ms)
    Jan  5 19:44:36.313: INFO: (10) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 51.144295ms)
    Jan  5 19:44:36.313: INFO: (10) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 51.352414ms)
    Jan  5 19:44:36.346: INFO: (11) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 30.714323ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 31.827448ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 31.335572ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 30.87991ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 32.030731ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 31.38319ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 31.158012ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 31.622962ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 32.03166ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 31.131139ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 31.652319ms)
    Jan  5 19:44:36.347: INFO: (11) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 32.124554ms)
    Jan  5 19:44:36.349: INFO: (11) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 33.641699ms)
    Jan  5 19:44:36.350: INFO: (11) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 33.939395ms)
    Jan  5 19:44:36.350: INFO: (11) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 34.755319ms)
    Jan  5 19:44:36.350: INFO: (11) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 34.509508ms)
    Jan  5 19:44:36.364: INFO: (12) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 13.058066ms)
    Jan  5 19:44:36.366: INFO: (12) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 15.509789ms)
    Jan  5 19:44:36.366: INFO: (12) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 14.885616ms)
    Jan  5 19:44:36.367: INFO: (12) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 16.91937ms)
    Jan  5 19:44:36.368: INFO: (12) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 17.264908ms)
    Jan  5 19:44:36.368: INFO: (12) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 17.447928ms)
    Jan  5 19:44:36.369: INFO: (12) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 18.462728ms)
    Jan  5 19:44:36.369: INFO: (12) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 18.075187ms)
    Jan  5 19:44:36.369: INFO: (12) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 19.158646ms)
    Jan  5 19:44:36.369: INFO: (12) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 18.679447ms)
    Jan  5 19:44:36.371: INFO: (12) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 20.672648ms)
    Jan  5 19:44:36.372: INFO: (12) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 22.228513ms)
    Jan  5 19:44:36.373: INFO: (12) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 22.789383ms)
    Jan  5 19:44:36.373: INFO: (12) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 22.416512ms)
    Jan  5 19:44:36.373: INFO: (12) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 23.365736ms)
    Jan  5 19:44:36.374: INFO: (12) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 23.48911ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 19.083637ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 18.333489ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 17.995273ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 18.552006ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 18.515955ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 17.906124ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 17.822906ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 18.259339ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 18.751398ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 19.119558ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 18.288718ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 18.099865ms)
    Jan  5 19:44:36.393: INFO: (13) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 18.302493ms)
    Jan  5 19:44:36.394: INFO: (13) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 18.65002ms)
    Jan  5 19:44:36.394: INFO: (13) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 18.708299ms)
    Jan  5 19:44:36.394: INFO: (13) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 18.27335ms)
    Jan  5 19:44:36.402: INFO: (14) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 8.128444ms)
    Jan  5 19:44:36.402: INFO: (14) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 8.06444ms)
    Jan  5 19:44:36.402: INFO: (14) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 8.017979ms)
    Jan  5 19:44:36.402: INFO: (14) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 8.710473ms)
    Jan  5 19:44:36.403: INFO: (14) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 8.482164ms)
    Jan  5 19:44:36.403: INFO: (14) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 8.56632ms)
    Jan  5 19:44:36.403: INFO: (14) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 8.677428ms)
    Jan  5 19:44:36.409: INFO: (14) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 6.869275ms)
    Jan  5 19:44:36.410: INFO: (14) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 7.575916ms)
    Jan  5 19:44:36.410: INFO: (14) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 8.386803ms)
    Jan  5 19:44:36.411: INFO: (14) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 8.055094ms)
    Jan  5 19:44:36.411: INFO: (14) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 16.287601ms)
    Jan  5 19:44:36.411: INFO: (14) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 8.795303ms)
    Jan  5 19:44:36.418: INFO: (14) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 15.057497ms)
    Jan  5 19:44:36.418: INFO: (14) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 15.775406ms)
    Jan  5 19:44:36.418: INFO: (14) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 23.195183ms)
    Jan  5 19:44:36.434: INFO: (15) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 14.861337ms)
    Jan  5 19:44:36.435: INFO: (15) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 16.603347ms)
    Jan  5 19:44:36.435: INFO: (15) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 16.830419ms)
    Jan  5 19:44:36.435: INFO: (15) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 17.158412ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 16.123097ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 17.680272ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 17.622919ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 16.495521ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 16.812853ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 16.71986ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 17.569497ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 16.717119ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 16.872192ms)
    Jan  5 19:44:36.436: INFO: (15) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.139175ms)
    Jan  5 19:44:36.437: INFO: (15) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 17.950926ms)
    Jan  5 19:44:36.437: INFO: (15) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 16.709195ms)
    Jan  5 19:44:36.447: INFO: (16) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 8.49957ms)
    Jan  5 19:44:36.447: INFO: (16) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 8.597387ms)
    Jan  5 19:44:36.447: INFO: (16) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 8.748848ms)
    Jan  5 19:44:36.449: INFO: (16) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 9.525176ms)
    Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 12.277039ms)
    Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 12.484937ms)
    Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.444878ms)
    Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 11.860728ms)
    Jan  5 19:44:36.451: INFO: (16) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.412611ms)
    Jan  5 19:44:36.459: INFO: (16) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 20.030461ms)
    Jan  5 19:44:36.459: INFO: (16) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 20.254987ms)
    Jan  5 19:44:36.460: INFO: (16) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 20.618623ms)
    Jan  5 19:44:36.461: INFO: (16) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 21.367424ms)
    Jan  5 19:44:36.461: INFO: (16) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 21.885994ms)
    Jan  5 19:44:36.462: INFO: (16) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 22.826816ms)
    Jan  5 19:44:36.462: INFO: (16) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 22.898426ms)
    Jan  5 19:44:36.469: INFO: (17) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 6.019619ms)
    Jan  5 19:44:36.470: INFO: (17) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 7.49279ms)
    Jan  5 19:44:36.473: INFO: (17) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 9.696095ms)
    Jan  5 19:44:36.474: INFO: (17) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 10.616543ms)
    Jan  5 19:44:36.474: INFO: (17) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 10.251827ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.309829ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.670618ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 12.326984ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 13.532984ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 12.430918ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 12.851748ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 13.133059ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 12.946413ms)
    Jan  5 19:44:36.476: INFO: (17) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 12.909227ms)
    Jan  5 19:44:36.478: INFO: (17) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 15.48257ms)
    Jan  5 19:44:36.481: INFO: (17) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 17.469031ms)
    Jan  5 19:44:36.493: INFO: (18) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 11.367295ms)
    Jan  5 19:44:36.494: INFO: (18) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 12.722014ms)
    Jan  5 19:44:36.494: INFO: (18) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 12.502761ms)
    Jan  5 19:44:36.495: INFO: (18) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 12.896218ms)
    Jan  5 19:44:36.495: INFO: (18) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 12.855088ms)
    Jan  5 19:44:36.495: INFO: (18) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 13.06089ms)
    Jan  5 19:44:36.496: INFO: (18) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 14.389374ms)
    Jan  5 19:44:36.497: INFO: (18) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 14.9726ms)
    Jan  5 19:44:36.497: INFO: (18) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 15.200857ms)
    Jan  5 19:44:36.498: INFO: (18) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 15.96728ms)
    Jan  5 19:44:36.498: INFO: (18) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 16.16668ms)
    Jan  5 19:44:36.498: INFO: (18) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 16.941965ms)
    Jan  5 19:44:36.499: INFO: (18) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 17.656998ms)
    Jan  5 19:44:36.499: INFO: (18) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 17.359319ms)
    Jan  5 19:44:36.500: INFO: (18) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 18.320241ms)
    Jan  5 19:44:36.501: INFO: (18) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 19.636971ms)
    Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:460/proxy/: tls baz (200; 12.955214ms)
    Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname1/proxy/: tls baz (200; 12.776208ms)
    Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:160/proxy/: foo (200; 11.976354ms)
    Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:443/proxy/tlsrewritem... (200; 12.729249ms)
    Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname2/proxy/: bar (200; 11.980006ms)
    Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:1080/proxy/rewriteme">test<... (200; 12.630549ms)
    Jan  5 19:44:36.514: INFO: (19) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:1080/proxy/rewriteme">... (200; 12.604079ms)
    Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/services/proxy-service-48dct:portname1/proxy/: foo (200; 12.772871ms)
    Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/pods/http:proxy-service-48dct-nltnv:162/proxy/: bar (200; 12.558558ms)
    Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/pods/https:proxy-service-48dct-nltnv:462/proxy/: tls qux (200; 12.507462ms)
    Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:160/proxy/: foo (200; 12.279246ms)
    Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/services/https:proxy-service-48dct:tlsportname2/proxy/: tls qux (200; 12.367678ms)
    Jan  5 19:44:36.515: INFO: (19) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/: <a href="/api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv/proxy/rewriteme">test</a> (200; 13.023348ms)
    Jan  5 19:44:36.519: INFO: (19) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname2/proxy/: bar (200; 17.310272ms)
    Jan  5 19:44:36.520: INFO: (19) /api/v1/namespaces/proxy-1896/pods/proxy-service-48dct-nltnv:162/proxy/: bar (200; 17.19818ms)
    Jan  5 19:44:36.520: INFO: (19) /api/v1/namespaces/proxy-1896/services/http:proxy-service-48dct:portname1/proxy/: foo (200; 18.939818ms)
    STEP: deleting ReplicationController proxy-service-48dct in namespace proxy-1896, will wait for the garbage collector to delete the pods 01/05/23 19:44:36.521
    Jan  5 19:44:36.581: INFO: Deleting ReplicationController proxy-service-48dct took: 5.457777ms
    Jan  5 19:44:36.681: INFO: Terminating ReplicationController proxy-service-48dct pods took: 100.258269ms
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:44:39.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-1896" for this suite. 01/05/23 19:44:39.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:44:39.62
Jan  5 19:44:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-pred 01/05/23 19:44:39.628
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:39.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:39.649
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  5 19:44:39.654: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 19:44:39.663: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 19:44:39.671: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-16pz before test
Jan  5 19:44:39.680: INFO: fluentbit-gke-5jd2q from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.680: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 19:44:39.680: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 19:44:39.680: INFO: gke-metrics-agent-vk8fs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.681: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 19:44:39.681: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 19:44:39.681: INFO: konnectivity-agent-57b5db4478-nsnb9 from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.681: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 19:44:39.681: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-16pz from kube-system started at 2023-01-05 17:43:36 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.681: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 19:44:39.681: INFO: metrics-server-v0.5.2-7b9768bdd5-w2lc6 from kube-system started at 2023-01-05 17:57:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.681: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 19:44:39.681: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan  5 19:44:39.682: INFO: pdcsi-node-2nkb4 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.682: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 19:44:39.682: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 19:44:39.682: INFO: sonobuoy from sonobuoy started at 2023-01-05 19:01:43 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.682: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 19:44:39.682: INFO: sonobuoy-e2e-job-7d2488470a1c477d from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.682: INFO: 	Container e2e ready: true, restart count 0
Jan  5 19:44:39.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 19:44:39.682: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.683: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 19:44:39.683: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 19:44:39.683: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-dbpc before test
Jan  5 19:44:39.689: INFO: event-exporter-gke-6dc4db644f-dh2bd from kube-system started at 2023-01-05 17:46:55 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.690: INFO: 	Container event-exporter ready: true, restart count 0
Jan  5 19:44:39.690: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Jan  5 19:44:39.690: INFO: fluentbit-gke-xzr85 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.690: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 19:44:39.690: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 19:44:39.690: INFO: gke-metrics-agent-hc4fg from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.690: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 19:44:39.690: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 19:44:39.690: INFO: konnectivity-agent-57b5db4478-7sktm from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.691: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 19:44:39.691: INFO: konnectivity-agent-autoscaler-797b8755f5-bbvjz from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.691: INFO: 	Container autoscaler ready: true, restart count 0
Jan  5 19:44:39.691: INFO: kube-dns-549f79cb95-5mb7d from kube-system started at 2023-01-05 17:46:55 +0000 UTC (4 container statuses recorded)
Jan  5 19:44:39.691: INFO: 	Container dnsmasq ready: true, restart count 0
Jan  5 19:44:39.691: INFO: 	Container kubedns ready: true, restart count 0
Jan  5 19:44:39.691: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jan  5 19:44:39.691: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 19:44:39.691: INFO: kube-dns-autoscaler-758c4689b9-pl85w from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.692: INFO: 	Container autoscaler ready: true, restart count 0
Jan  5 19:44:39.692: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.692: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 19:44:39.692: INFO: l7-default-backend-7bcfd7bc79-lqt6s from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.692: INFO: 	Container default-http-backend ready: true, restart count 0
Jan  5 19:44:39.692: INFO: pdcsi-node-dq4p5 from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.692: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 19:44:39.692: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 19:44:39.692: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.693: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 19:44:39.693: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 19:44:39.693: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-qmj7 before test
Jan  5 19:44:39.699: INFO: fluentbit-gke-n7chw from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.699: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 19:44:39.699: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 19:44:39.699: INFO: gke-metrics-agent-w95hs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.699: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 19:44:39.699: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 19:44:39.699: INFO: konnectivity-agent-57b5db4478-b22kt from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.700: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 19:44:39.700: INFO: kube-dns-549f79cb95-7zg82 from kube-system started at 2023-01-05 17:47:07 +0000 UTC (4 container statuses recorded)
Jan  5 19:44:39.700: INFO: 	Container dnsmasq ready: true, restart count 0
Jan  5 19:44:39.700: INFO: 	Container kubedns ready: true, restart count 0
Jan  5 19:44:39.700: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jan  5 19:44:39.700: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 19:44:39.700: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
Jan  5 19:44:39.700: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 19:44:39.700: INFO: pdcsi-node-vdldr from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.701: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 19:44:39.701: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 19:44:39.701: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:44:39.701: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 19:44:39.701: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 19:44:39.701
Jan  5 19:44:39.711: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2328" to be "running"
Jan  5 19:44:39.715: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968162ms
Jan  5 19:44:41.720: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008161937s
Jan  5 19:44:41.720: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 19:44:41.722
STEP: Trying to apply a random label on the found node. 01/05/23 19:44:41.737
STEP: verifying the node has the label kubernetes.io/e2e-036d3d88-2b82-4903-98db-3ea0aee5d5a9 95 01/05/23 19:44:41.756
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/05/23 19:44:41.759
Jan  5 19:44:41.774: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2328" to be "not pending"
Jan  5 19:44:41.792: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.643968ms
Jan  5 19:44:43.796: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017779691s
Jan  5 19:44:43.796: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.196.0.39 on the node which pod4 resides and expect not scheduled 01/05/23 19:44:43.796
Jan  5 19:44:43.804: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2328" to be "not pending"
Jan  5 19:44:43.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666402ms
Jan  5 19:44:45.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008087824s
Jan  5 19:44:47.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010072321s
Jan  5 19:44:49.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007552773s
Jan  5 19:44:51.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007880802s
Jan  5 19:44:53.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007603914s
Jan  5 19:44:55.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008846501s
Jan  5 19:44:57.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007322651s
Jan  5 19:44:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006847979s
Jan  5 19:45:01.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006922347s
Jan  5 19:45:03.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00696771s
Jan  5 19:45:05.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007697502s
Jan  5 19:45:07.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007765866s
Jan  5 19:45:09.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00753322s
Jan  5 19:45:11.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007634636s
Jan  5 19:45:13.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009482337s
Jan  5 19:45:15.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007681643s
Jan  5 19:45:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006857769s
Jan  5 19:45:19.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006520662s
Jan  5 19:45:21.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007588086s
Jan  5 19:45:23.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009019532s
Jan  5 19:45:25.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008005981s
Jan  5 19:45:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01327626s
Jan  5 19:45:29.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006822967s
Jan  5 19:45:31.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008179849s
Jan  5 19:45:33.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007848493s
Jan  5 19:45:35.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006784581s
Jan  5 19:45:37.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009042331s
Jan  5 19:45:39.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00842444s
Jan  5 19:45:41.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007078246s
Jan  5 19:45:43.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007395982s
Jan  5 19:45:45.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008791021s
Jan  5 19:45:47.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006972583s
Jan  5 19:45:49.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007036583s
Jan  5 19:45:51.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007034924s
Jan  5 19:45:53.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007169965s
Jan  5 19:45:55.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008140869s
Jan  5 19:45:57.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008325581s
Jan  5 19:45:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007147629s
Jan  5 19:46:01.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007391786s
Jan  5 19:46:03.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009286283s
Jan  5 19:46:05.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008165523s
Jan  5 19:46:07.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007188456s
Jan  5 19:46:09.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006602008s
Jan  5 19:46:11.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006866123s
Jan  5 19:46:13.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008475481s
Jan  5 19:46:15.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007892811s
Jan  5 19:46:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006985027s
Jan  5 19:46:19.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008140119s
Jan  5 19:46:21.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00673201s
Jan  5 19:46:23.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008639658s
Jan  5 19:46:25.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.0070159s
Jan  5 19:46:27.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007579307s
Jan  5 19:46:29.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007696299s
Jan  5 19:46:31.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008214924s
Jan  5 19:46:33.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008531596s
Jan  5 19:46:35.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008228555s
Jan  5 19:46:37.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007068757s
Jan  5 19:46:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.013791494s
Jan  5 19:46:41.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007576453s
Jan  5 19:46:43.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008277384s
Jan  5 19:46:45.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008034591s
Jan  5 19:46:47.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.007137301s
Jan  5 19:46:49.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.00668072s
Jan  5 19:46:51.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007173054s
Jan  5 19:46:53.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00788894s
Jan  5 19:46:55.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006926033s
Jan  5 19:46:57.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008992341s
Jan  5 19:46:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007376827s
Jan  5 19:47:01.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007315878s
Jan  5 19:47:03.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007024254s
Jan  5 19:47:05.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007079688s
Jan  5 19:47:07.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007994619s
Jan  5 19:47:09.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006749515s
Jan  5 19:47:11.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.006858451s
Jan  5 19:47:13.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007262568s
Jan  5 19:47:15.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007684782s
Jan  5 19:47:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007464697s
Jan  5 19:47:19.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006835266s
Jan  5 19:47:21.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006799382s
Jan  5 19:47:23.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.00787986s
Jan  5 19:47:25.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008445859s
Jan  5 19:47:27.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007352583s
Jan  5 19:47:29.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007217798s
Jan  5 19:47:31.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007558655s
Jan  5 19:47:33.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008847302s
Jan  5 19:47:35.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007711311s
Jan  5 19:47:37.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008024154s
Jan  5 19:47:39.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007075055s
Jan  5 19:47:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.012809139s
Jan  5 19:47:43.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008069172s
Jan  5 19:47:45.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.00698944s
Jan  5 19:47:47.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.007673682s
Jan  5 19:47:49.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007706476s
Jan  5 19:47:51.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007360634s
Jan  5 19:47:53.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008954359s
Jan  5 19:47:55.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007250031s
Jan  5 19:47:57.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007193574s
Jan  5 19:47:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007134794s
Jan  5 19:48:01.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007399185s
Jan  5 19:48:03.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007875999s
Jan  5 19:48:05.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006665718s
Jan  5 19:48:07.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007398929s
Jan  5 19:48:09.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009393599s
Jan  5 19:48:11.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007177805s
Jan  5 19:48:13.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007945718s
Jan  5 19:48:15.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.007405488s
Jan  5 19:48:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.006871958s
Jan  5 19:48:19.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007227183s
Jan  5 19:48:21.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006897638s
Jan  5 19:48:23.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008262504s
Jan  5 19:48:25.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008344568s
Jan  5 19:48:27.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.010563103s
Jan  5 19:48:29.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.006763001s
Jan  5 19:48:31.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007923241s
Jan  5 19:48:33.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007259739s
Jan  5 19:48:35.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007110422s
Jan  5 19:48:37.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.006812436s
Jan  5 19:48:39.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006844743s
Jan  5 19:48:41.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007634754s
Jan  5 19:48:43.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009388353s
Jan  5 19:48:45.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008412012s
Jan  5 19:48:47.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.007768826s
Jan  5 19:48:49.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007830079s
Jan  5 19:48:51.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007102178s
Jan  5 19:48:53.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008080162s
Jan  5 19:48:55.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.007549619s
Jan  5 19:48:57.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006723561s
Jan  5 19:48:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007165504s
Jan  5 19:49:01.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.0076485s
Jan  5 19:49:03.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00758657s
Jan  5 19:49:05.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00785714s
Jan  5 19:49:07.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.00946728s
Jan  5 19:49:09.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006777956s
Jan  5 19:49:11.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007333965s
Jan  5 19:49:13.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008242661s
Jan  5 19:49:15.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006975545s
Jan  5 19:49:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007487945s
Jan  5 19:49:19.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007753934s
Jan  5 19:49:21.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006950593s
Jan  5 19:49:23.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008839213s
Jan  5 19:49:25.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008477378s
Jan  5 19:49:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.012952209s
Jan  5 19:49:29.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.007164237s
Jan  5 19:49:31.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007185319s
Jan  5 19:49:33.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008023417s
Jan  5 19:49:35.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007899504s
Jan  5 19:49:37.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007242243s
Jan  5 19:49:39.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008518786s
Jan  5 19:49:41.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008013497s
Jan  5 19:49:43.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007581229s
Jan  5 19:49:43.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010276655s
STEP: removing the label kubernetes.io/e2e-036d3d88-2b82-4903-98db-3ea0aee5d5a9 off the node gke-gke-1-26-default-pool-05283374-16pz 01/05/23 19:49:43.814
STEP: verifying the node doesn't have the label kubernetes.io/e2e-036d3d88-2b82-4903-98db-3ea0aee5d5a9 01/05/23 19:49:43.828
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:49:43.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2328" for this suite. 01/05/23 19:49:43.84
------------------------------
• [SLOW TEST] [304.227 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:704

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:44:39.62
    Jan  5 19:44:39.620: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-pred 01/05/23 19:44:39.628
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:44:39.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:44:39.649
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  5 19:44:39.654: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 19:44:39.663: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 19:44:39.671: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-16pz before test
    Jan  5 19:44:39.680: INFO: fluentbit-gke-5jd2q from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.680: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 19:44:39.680: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 19:44:39.680: INFO: gke-metrics-agent-vk8fs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.681: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 19:44:39.681: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 19:44:39.681: INFO: konnectivity-agent-57b5db4478-nsnb9 from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.681: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 19:44:39.681: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-16pz from kube-system started at 2023-01-05 17:43:36 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.681: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 19:44:39.681: INFO: metrics-server-v0.5.2-7b9768bdd5-w2lc6 from kube-system started at 2023-01-05 17:57:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.681: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 19:44:39.681: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan  5 19:44:39.682: INFO: pdcsi-node-2nkb4 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.682: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 19:44:39.682: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 19:44:39.682: INFO: sonobuoy from sonobuoy started at 2023-01-05 19:01:43 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.682: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 19:44:39.682: INFO: sonobuoy-e2e-job-7d2488470a1c477d from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.682: INFO: 	Container e2e ready: true, restart count 0
    Jan  5 19:44:39.682: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 19:44:39.682: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.683: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 19:44:39.683: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 19:44:39.683: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-dbpc before test
    Jan  5 19:44:39.689: INFO: event-exporter-gke-6dc4db644f-dh2bd from kube-system started at 2023-01-05 17:46:55 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.690: INFO: 	Container event-exporter ready: true, restart count 0
    Jan  5 19:44:39.690: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
    Jan  5 19:44:39.690: INFO: fluentbit-gke-xzr85 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.690: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 19:44:39.690: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 19:44:39.690: INFO: gke-metrics-agent-hc4fg from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.690: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 19:44:39.690: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 19:44:39.690: INFO: konnectivity-agent-57b5db4478-7sktm from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.691: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 19:44:39.691: INFO: konnectivity-agent-autoscaler-797b8755f5-bbvjz from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.691: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  5 19:44:39.691: INFO: kube-dns-549f79cb95-5mb7d from kube-system started at 2023-01-05 17:46:55 +0000 UTC (4 container statuses recorded)
    Jan  5 19:44:39.691: INFO: 	Container dnsmasq ready: true, restart count 0
    Jan  5 19:44:39.691: INFO: 	Container kubedns ready: true, restart count 0
    Jan  5 19:44:39.691: INFO: 	Container prometheus-to-sd ready: true, restart count 0
    Jan  5 19:44:39.691: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 19:44:39.691: INFO: kube-dns-autoscaler-758c4689b9-pl85w from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.692: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  5 19:44:39.692: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.692: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 19:44:39.692: INFO: l7-default-backend-7bcfd7bc79-lqt6s from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.692: INFO: 	Container default-http-backend ready: true, restart count 0
    Jan  5 19:44:39.692: INFO: pdcsi-node-dq4p5 from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.692: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 19:44:39.692: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 19:44:39.692: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.693: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 19:44:39.693: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 19:44:39.693: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-qmj7 before test
    Jan  5 19:44:39.699: INFO: fluentbit-gke-n7chw from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.699: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 19:44:39.699: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 19:44:39.699: INFO: gke-metrics-agent-w95hs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.699: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 19:44:39.699: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 19:44:39.699: INFO: konnectivity-agent-57b5db4478-b22kt from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.700: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 19:44:39.700: INFO: kube-dns-549f79cb95-7zg82 from kube-system started at 2023-01-05 17:47:07 +0000 UTC (4 container statuses recorded)
    Jan  5 19:44:39.700: INFO: 	Container dnsmasq ready: true, restart count 0
    Jan  5 19:44:39.700: INFO: 	Container kubedns ready: true, restart count 0
    Jan  5 19:44:39.700: INFO: 	Container prometheus-to-sd ready: true, restart count 0
    Jan  5 19:44:39.700: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 19:44:39.700: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
    Jan  5 19:44:39.700: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 19:44:39.700: INFO: pdcsi-node-vdldr from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.701: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 19:44:39.701: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 19:44:39.701: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:44:39.701: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 19:44:39.701: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:704
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 19:44:39.701
    Jan  5 19:44:39.711: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-2328" to be "running"
    Jan  5 19:44:39.715: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.968162ms
    Jan  5 19:44:41.720: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008161937s
    Jan  5 19:44:41.720: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 19:44:41.722
    STEP: Trying to apply a random label on the found node. 01/05/23 19:44:41.737
    STEP: verifying the node has the label kubernetes.io/e2e-036d3d88-2b82-4903-98db-3ea0aee5d5a9 95 01/05/23 19:44:41.756
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 01/05/23 19:44:41.759
    Jan  5 19:44:41.774: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-2328" to be "not pending"
    Jan  5 19:44:41.792: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 13.643968ms
    Jan  5 19:44:43.796: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.017779691s
    Jan  5 19:44:43.796: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.196.0.39 on the node which pod4 resides and expect not scheduled 01/05/23 19:44:43.796
    Jan  5 19:44:43.804: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-2328" to be "not pending"
    Jan  5 19:44:43.808: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.666402ms
    Jan  5 19:44:45.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008087824s
    Jan  5 19:44:47.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.010072321s
    Jan  5 19:44:49.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.007552773s
    Jan  5 19:44:51.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007880802s
    Jan  5 19:44:53.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.007603914s
    Jan  5 19:44:55.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.008846501s
    Jan  5 19:44:57.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.007322651s
    Jan  5 19:44:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.006847979s
    Jan  5 19:45:01.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.006922347s
    Jan  5 19:45:03.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.00696771s
    Jan  5 19:45:05.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.007697502s
    Jan  5 19:45:07.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.007765866s
    Jan  5 19:45:09.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.00753322s
    Jan  5 19:45:11.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.007634636s
    Jan  5 19:45:13.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.009482337s
    Jan  5 19:45:15.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.007681643s
    Jan  5 19:45:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.006857769s
    Jan  5 19:45:19.810: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.006520662s
    Jan  5 19:45:21.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.007588086s
    Jan  5 19:45:23.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009019532s
    Jan  5 19:45:25.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.008005981s
    Jan  5 19:45:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.01327626s
    Jan  5 19:45:29.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.006822967s
    Jan  5 19:45:31.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.008179849s
    Jan  5 19:45:33.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.007848493s
    Jan  5 19:45:35.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.006784581s
    Jan  5 19:45:37.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.009042331s
    Jan  5 19:45:39.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.00842444s
    Jan  5 19:45:41.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.007078246s
    Jan  5 19:45:43.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.007395982s
    Jan  5 19:45:45.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.008791021s
    Jan  5 19:45:47.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.006972583s
    Jan  5 19:45:49.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.007036583s
    Jan  5 19:45:51.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.007034924s
    Jan  5 19:45:53.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.007169965s
    Jan  5 19:45:55.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.008140869s
    Jan  5 19:45:57.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.008325581s
    Jan  5 19:45:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.007147629s
    Jan  5 19:46:01.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.007391786s
    Jan  5 19:46:03.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009286283s
    Jan  5 19:46:05.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.008165523s
    Jan  5 19:46:07.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.007188456s
    Jan  5 19:46:09.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.006602008s
    Jan  5 19:46:11.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.006866123s
    Jan  5 19:46:13.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.008475481s
    Jan  5 19:46:15.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.007892811s
    Jan  5 19:46:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.006985027s
    Jan  5 19:46:19.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.008140119s
    Jan  5 19:46:21.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.00673201s
    Jan  5 19:46:23.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.008639658s
    Jan  5 19:46:25.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.0070159s
    Jan  5 19:46:27.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.007579307s
    Jan  5 19:46:29.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.007696299s
    Jan  5 19:46:31.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.008214924s
    Jan  5 19:46:33.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.008531596s
    Jan  5 19:46:35.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.008228555s
    Jan  5 19:46:37.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.007068757s
    Jan  5 19:46:39.818: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.013791494s
    Jan  5 19:46:41.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.007576453s
    Jan  5 19:46:43.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.008277384s
    Jan  5 19:46:45.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.008034591s
    Jan  5 19:46:47.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.007137301s
    Jan  5 19:46:49.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.00668072s
    Jan  5 19:46:51.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.007173054s
    Jan  5 19:46:53.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.00788894s
    Jan  5 19:46:55.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.006926033s
    Jan  5 19:46:57.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.008992341s
    Jan  5 19:46:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.007376827s
    Jan  5 19:47:01.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.007315878s
    Jan  5 19:47:03.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.007024254s
    Jan  5 19:47:05.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.007079688s
    Jan  5 19:47:07.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.007994619s
    Jan  5 19:47:09.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.006749515s
    Jan  5 19:47:11.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.006858451s
    Jan  5 19:47:13.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.007262568s
    Jan  5 19:47:15.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.007684782s
    Jan  5 19:47:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.007464697s
    Jan  5 19:47:19.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.006835266s
    Jan  5 19:47:21.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.006799382s
    Jan  5 19:47:23.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.00787986s
    Jan  5 19:47:25.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.008445859s
    Jan  5 19:47:27.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.007352583s
    Jan  5 19:47:29.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.007217798s
    Jan  5 19:47:31.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.007558655s
    Jan  5 19:47:33.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.008847302s
    Jan  5 19:47:35.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.007711311s
    Jan  5 19:47:37.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.008024154s
    Jan  5 19:47:39.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.007075055s
    Jan  5 19:47:41.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.012809139s
    Jan  5 19:47:43.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.008069172s
    Jan  5 19:47:45.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.00698944s
    Jan  5 19:47:47.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.007673682s
    Jan  5 19:47:49.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.007706476s
    Jan  5 19:47:51.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.007360634s
    Jan  5 19:47:53.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.008954359s
    Jan  5 19:47:55.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.007250031s
    Jan  5 19:47:57.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.007193574s
    Jan  5 19:47:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.007134794s
    Jan  5 19:48:01.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.007399185s
    Jan  5 19:48:03.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.007875999s
    Jan  5 19:48:05.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.006665718s
    Jan  5 19:48:07.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.007398929s
    Jan  5 19:48:09.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.009393599s
    Jan  5 19:48:11.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.007177805s
    Jan  5 19:48:13.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.007945718s
    Jan  5 19:48:15.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.007405488s
    Jan  5 19:48:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.006871958s
    Jan  5 19:48:19.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.007227183s
    Jan  5 19:48:21.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.006897638s
    Jan  5 19:48:23.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.008262504s
    Jan  5 19:48:25.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008344568s
    Jan  5 19:48:27.815: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.010563103s
    Jan  5 19:48:29.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.006763001s
    Jan  5 19:48:31.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.007923241s
    Jan  5 19:48:33.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.007259739s
    Jan  5 19:48:35.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.007110422s
    Jan  5 19:48:37.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.006812436s
    Jan  5 19:48:39.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.006844743s
    Jan  5 19:48:41.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.007634754s
    Jan  5 19:48:43.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.009388353s
    Jan  5 19:48:45.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008412012s
    Jan  5 19:48:47.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.007768826s
    Jan  5 19:48:49.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.007830079s
    Jan  5 19:48:51.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.007102178s
    Jan  5 19:48:53.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.008080162s
    Jan  5 19:48:55.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.007549619s
    Jan  5 19:48:57.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.006723561s
    Jan  5 19:48:59.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.007165504s
    Jan  5 19:49:01.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.0076485s
    Jan  5 19:49:03.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.00758657s
    Jan  5 19:49:05.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.00785714s
    Jan  5 19:49:07.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.00946728s
    Jan  5 19:49:09.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.006777956s
    Jan  5 19:49:11.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.007333965s
    Jan  5 19:49:13.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.008242661s
    Jan  5 19:49:15.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.006975545s
    Jan  5 19:49:17.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.007487945s
    Jan  5 19:49:19.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.007753934s
    Jan  5 19:49:21.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.006950593s
    Jan  5 19:49:23.813: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.008839213s
    Jan  5 19:49:25.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.008477378s
    Jan  5 19:49:27.817: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.012952209s
    Jan  5 19:49:29.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.007164237s
    Jan  5 19:49:31.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.007185319s
    Jan  5 19:49:33.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.008023417s
    Jan  5 19:49:35.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.007899504s
    Jan  5 19:49:37.811: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.007242243s
    Jan  5 19:49:39.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.008518786s
    Jan  5 19:49:41.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.008013497s
    Jan  5 19:49:43.812: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.007581229s
    Jan  5 19:49:43.814: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010276655s
    STEP: removing the label kubernetes.io/e2e-036d3d88-2b82-4903-98db-3ea0aee5d5a9 off the node gke-gke-1-26-default-pool-05283374-16pz 01/05/23 19:49:43.814
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-036d3d88-2b82-4903-98db-3ea0aee5d5a9 01/05/23 19:49:43.828
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:49:43.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2328" for this suite. 01/05/23 19:49:43.84
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:49:43.846
Jan  5 19:49:43.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename endpointslice 01/05/23 19:49:43.848
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:43.873
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:43.876
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  5 19:49:47.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-74" for this suite. 01/05/23 19:49:47.943
------------------------------
• [4.103 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:102

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:49:43.846
    Jan  5 19:49:43.846: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename endpointslice 01/05/23 19:49:43.848
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:43.873
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:43.876
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:102
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:49:47.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-74" for this suite. 01/05/23 19:49:47.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:49:47.949
Jan  5 19:49:47.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename csistoragecapacity 01/05/23 19:49:47.951
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:47.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:47.969
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:31
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 01/05/23 19:49:47.971
STEP: getting /apis/storage.k8s.io 01/05/23 19:49:47.975
STEP: getting /apis/storage.k8s.io/v1 01/05/23 19:49:47.977
STEP: creating 01/05/23 19:49:47.978
STEP: watching 01/05/23 19:49:47.993
Jan  5 19:49:47.993: INFO: starting watch
STEP: getting 01/05/23 19:49:47.998
STEP: listing in namespace 01/05/23 19:49:48
STEP: listing across namespaces 01/05/23 19:49:48.002
STEP: patching 01/05/23 19:49:48.004
STEP: updating 01/05/23 19:49:48.009
Jan  5 19:49:48.016: INFO: waiting for watch events with expected annotations in namespace
Jan  5 19:49:48.016: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 01/05/23 19:49:48.016
STEP: deleting a collection 01/05/23 19:49:48.026
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/node/init/init.go:32
Jan  5 19:49:48.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
  tear down framework | framework.go:193
STEP: Destroying namespace "csistoragecapacity-6864" for this suite. 01/05/23 19:49:48.039
------------------------------
• [0.095 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:49:47.949
    Jan  5 19:49:47.950: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename csistoragecapacity 01/05/23 19:49:47.951
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:47.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:47.969
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:31
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 01/05/23 19:49:47.971
    STEP: getting /apis/storage.k8s.io 01/05/23 19:49:47.975
    STEP: getting /apis/storage.k8s.io/v1 01/05/23 19:49:47.977
    STEP: creating 01/05/23 19:49:47.978
    STEP: watching 01/05/23 19:49:47.993
    Jan  5 19:49:47.993: INFO: starting watch
    STEP: getting 01/05/23 19:49:47.998
    STEP: listing in namespace 01/05/23 19:49:48
    STEP: listing across namespaces 01/05/23 19:49:48.002
    STEP: patching 01/05/23 19:49:48.004
    STEP: updating 01/05/23 19:49:48.009
    Jan  5 19:49:48.016: INFO: waiting for watch events with expected annotations in namespace
    Jan  5 19:49:48.016: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 01/05/23 19:49:48.016
    STEP: deleting a collection 01/05/23 19:49:48.026
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:49:48.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIStorageCapacity
      tear down framework | framework.go:193
    STEP: Destroying namespace "csistoragecapacity-6864" for this suite. 01/05/23 19:49:48.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:49:48.048
Jan  5 19:49:48.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:49:48.05
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:48.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:48.065
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90
STEP: Creating a pod to test downward api env vars 01/05/23 19:49:48.068
Jan  5 19:49:48.080: INFO: Waiting up to 5m0s for pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e" in namespace "downward-api-8407" to be "Succeeded or Failed"
Jan  5 19:49:48.086: INFO: Pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.305941ms
Jan  5 19:49:50.092: INFO: Pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011118854s
Jan  5 19:49:52.090: INFO: Pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009433716s
STEP: Saw pod success 01/05/23 19:49:52.09
Jan  5 19:49:52.091: INFO: Pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e" satisfied condition "Succeeded or Failed"
Jan  5 19:49:52.093: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e container dapi-container: <nil>
STEP: delete the pod 01/05/23 19:49:52.111
Jan  5 19:49:52.122: INFO: Waiting for pod downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e to disappear
Jan  5 19:49:52.125: INFO: Pod downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  5 19:49:52.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8407" for this suite. 01/05/23 19:49:52.129
------------------------------
• [4.085 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:49:48.048
    Jan  5 19:49:48.049: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:49:48.05
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:48.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:48.065
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:90
    STEP: Creating a pod to test downward api env vars 01/05/23 19:49:48.068
    Jan  5 19:49:48.080: INFO: Waiting up to 5m0s for pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e" in namespace "downward-api-8407" to be "Succeeded or Failed"
    Jan  5 19:49:48.086: INFO: Pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.305941ms
    Jan  5 19:49:50.092: INFO: Pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011118854s
    Jan  5 19:49:52.090: INFO: Pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009433716s
    STEP: Saw pod success 01/05/23 19:49:52.09
    Jan  5 19:49:52.091: INFO: Pod "downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e" satisfied condition "Succeeded or Failed"
    Jan  5 19:49:52.093: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e container dapi-container: <nil>
    STEP: delete the pod 01/05/23 19:49:52.111
    Jan  5 19:49:52.122: INFO: Waiting for pod downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e to disappear
    Jan  5 19:49:52.125: INFO: Pod downward-api-3a6e8985-3c61-40e6-b8ce-8b3041422e3e no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:49:52.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8407" for this suite. 01/05/23 19:49:52.129
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:49:52.145
Jan  5 19:49:52.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:49:52.146
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:52.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:52.163
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:49:52.177
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:49:53.114
STEP: Deploying the webhook pod 01/05/23 19:49:53.12
STEP: Wait for the deployment to be ready 01/05/23 19:49:53.132
Jan  5 19:49:53.142: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 19:49:55.15
STEP: Verifying the service has paired with the endpoint 01/05/23 19:49:55.161
Jan  5 19:49:56.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508
STEP: Creating a mutating webhook configuration 01/05/23 19:49:56.164
STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/05/23 19:49:56.185
STEP: Creating a configMap that should not be mutated 01/05/23 19:49:56.192
STEP: Patching a mutating webhook configuration's rules to include the create operation 01/05/23 19:49:56.201
STEP: Creating a configMap that should be mutated 01/05/23 19:49:56.208
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:49:56.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3060" for this suite. 01/05/23 19:49:56.272
STEP: Destroying namespace "webhook-3060-markers" for this suite. 01/05/23 19:49:56.279
------------------------------
• [4.138 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:508

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:49:52.145
    Jan  5 19:49:52.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:49:52.146
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:52.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:52.163
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:49:52.177
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:49:53.114
    STEP: Deploying the webhook pod 01/05/23 19:49:53.12
    STEP: Wait for the deployment to be ready 01/05/23 19:49:53.132
    Jan  5 19:49:53.142: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 19:49:55.15
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:49:55.161
    Jan  5 19:49:56.161: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:508
    STEP: Creating a mutating webhook configuration 01/05/23 19:49:56.164
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 01/05/23 19:49:56.185
    STEP: Creating a configMap that should not be mutated 01/05/23 19:49:56.192
    STEP: Patching a mutating webhook configuration's rules to include the create operation 01/05/23 19:49:56.201
    STEP: Creating a configMap that should be mutated 01/05/23 19:49:56.208
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:49:56.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3060" for this suite. 01/05/23 19:49:56.272
    STEP: Destroying namespace "webhook-3060-markers" for this suite. 01/05/23 19:49:56.279
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:49:56.284
Jan  5 19:49:56.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename events 01/05/23 19:49:56.285
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:56.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:56.301
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 01/05/23 19:49:56.306
STEP: get a list of Events with a label in the current namespace 01/05/23 19:49:56.32
STEP: delete a list of events 01/05/23 19:49:56.323
Jan  5 19:49:56.323: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 01/05/23 19:49:56.335
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/node/init/init.go:32
Jan  5 19:49:56.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-instrumentation] Events API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-instrumentation] Events API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-instrumentation] Events API
  tear down framework | framework.go:193
STEP: Destroying namespace "events-9851" for this suite. 01/05/23 19:49:56.342
------------------------------
• [0.063 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:49:56.284
    Jan  5 19:49:56.284: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename events 01/05/23 19:49:56.285
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:56.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:56.301
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 01/05/23 19:49:56.306
    STEP: get a list of Events with a label in the current namespace 01/05/23 19:49:56.32
    STEP: delete a list of events 01/05/23 19:49:56.323
    Jan  5 19:49:56.323: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 01/05/23 19:49:56.335
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:49:56.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-instrumentation] Events API
      tear down framework | framework.go:193
    STEP: Destroying namespace "events-9851" for this suite. 01/05/23 19:49:56.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:49:56.352
Jan  5 19:49:56.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename gc 01/05/23 19:49:56.353
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:56.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:56.369
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 01/05/23 19:49:56.372
STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 19:49:56.379
STEP: delete the deployment 01/05/23 19:49:56.889
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/05/23 19:49:56.894
STEP: Gathering metrics 01/05/23 19:49:57.412
W0105 19:49:57.425665      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 19:49:57.426: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  5 19:49:57.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-7102" for this suite. 01/05/23 19:49:57.432
------------------------------
• [1.085 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:49:56.352
    Jan  5 19:49:56.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename gc 01/05/23 19:49:56.353
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:56.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:56.369
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 01/05/23 19:49:56.372
    STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 19:49:56.379
    STEP: delete the deployment 01/05/23 19:49:56.889
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 01/05/23 19:49:56.894
    STEP: Gathering metrics 01/05/23 19:49:57.412
    W0105 19:49:57.425665      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 19:49:57.426: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:49:57.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-7102" for this suite. 01/05/23 19:49:57.432
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:49:57.438
Jan  5 19:49:57.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-probe 01/05/23 19:49:57.442
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:57.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:57.461
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215
STEP: Creating pod test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22 in namespace container-probe-1070 01/05/23 19:49:57.466
Jan  5 19:49:57.475: INFO: Waiting up to 5m0s for pod "test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22" in namespace "container-probe-1070" to be "not pending"
Jan  5 19:49:57.478: INFO: Pod "test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.523472ms
Jan  5 19:49:59.481: INFO: Pod "test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22": Phase="Running", Reason="", readiness=true. Elapsed: 2.006112718s
Jan  5 19:49:59.481: INFO: Pod "test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22" satisfied condition "not pending"
Jan  5 19:49:59.481: INFO: Started pod test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22 in namespace container-probe-1070
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 19:49:59.481
Jan  5 19:49:59.484: INFO: Initial restart count of pod test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22 is 0
STEP: deleting the pod 01/05/23 19:53:59.964
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  5 19:53:59.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-1070" for this suite. 01/05/23 19:53:59.988
------------------------------
• [SLOW TEST] [242.560 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:49:57.438
    Jan  5 19:49:57.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-probe 01/05/23 19:49:57.442
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:49:57.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:49:57.461
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:215
    STEP: Creating pod test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22 in namespace container-probe-1070 01/05/23 19:49:57.466
    Jan  5 19:49:57.475: INFO: Waiting up to 5m0s for pod "test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22" in namespace "container-probe-1070" to be "not pending"
    Jan  5 19:49:57.478: INFO: Pod "test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22": Phase="Pending", Reason="", readiness=false. Elapsed: 2.523472ms
    Jan  5 19:49:59.481: INFO: Pod "test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22": Phase="Running", Reason="", readiness=true. Elapsed: 2.006112718s
    Jan  5 19:49:59.481: INFO: Pod "test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22" satisfied condition "not pending"
    Jan  5 19:49:59.481: INFO: Started pod test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22 in namespace container-probe-1070
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 19:49:59.481
    Jan  5 19:49:59.484: INFO: Initial restart count of pod test-webserver-3d2b2541-f9b2-470d-ac3c-095220ba9b22 is 0
    STEP: deleting the pod 01/05/23 19:53:59.964
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:53:59.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-1070" for this suite. 01/05/23 19:53:59.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:00.005
Jan  5 19:54:00.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename proxy 01/05/23 19:54:00.006
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:00.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:00.028
[BeforeEach] version v1
  test/e2e/framework/metrics/init/init.go:31
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jan  5 19:54:00.031: INFO: Creating pod...
Jan  5 19:54:00.140: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2324" to be "running"
Jan  5 19:54:00.149: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 9.639691ms
Jan  5 19:54:02.153: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013257106s
Jan  5 19:54:02.153: INFO: Pod "agnhost" satisfied condition "running"
Jan  5 19:54:02.153: INFO: Creating service...
Jan  5 19:54:02.166: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/DELETE
Jan  5 19:54:02.178: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 19:54:02.179: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/GET
Jan  5 19:54:02.183: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  5 19:54:02.183: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/HEAD
Jan  5 19:54:02.186: INFO: http.Client request:HEAD | StatusCode:200
Jan  5 19:54:02.186: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/OPTIONS
Jan  5 19:54:02.189: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 19:54:02.189: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/PATCH
Jan  5 19:54:02.193: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 19:54:02.193: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/POST
Jan  5 19:54:02.200: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 19:54:02.200: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/PUT
Jan  5 19:54:02.204: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jan  5 19:54:02.204: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/DELETE
Jan  5 19:54:02.211: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jan  5 19:54:02.211: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/GET
Jan  5 19:54:02.216: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jan  5 19:54:02.216: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/HEAD
Jan  5 19:54:02.222: INFO: http.Client request:HEAD | StatusCode:200
Jan  5 19:54:02.222: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/OPTIONS
Jan  5 19:54:02.231: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jan  5 19:54:02.231: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/PATCH
Jan  5 19:54:02.237: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jan  5 19:54:02.237: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/POST
Jan  5 19:54:02.242: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jan  5 19:54:02.242: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/PUT
Jan  5 19:54:02.247: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:02.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] version v1
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] version v1
  dump namespaces | framework.go:196
[DeferCleanup (Each)] version v1
  tear down framework | framework.go:193
STEP: Destroying namespace "proxy-2324" for this suite. 01/05/23 19:54:02.252
------------------------------
• [2.253 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:00.005
    Jan  5 19:54:00.005: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename proxy 01/05/23 19:54:00.006
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:00.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:00.028
    [BeforeEach] version v1
      test/e2e/framework/metrics/init/init.go:31
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jan  5 19:54:00.031: INFO: Creating pod...
    Jan  5 19:54:00.140: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-2324" to be "running"
    Jan  5 19:54:00.149: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 9.639691ms
    Jan  5 19:54:02.153: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.013257106s
    Jan  5 19:54:02.153: INFO: Pod "agnhost" satisfied condition "running"
    Jan  5 19:54:02.153: INFO: Creating service...
    Jan  5 19:54:02.166: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/DELETE
    Jan  5 19:54:02.178: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 19:54:02.179: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/GET
    Jan  5 19:54:02.183: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  5 19:54:02.183: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/HEAD
    Jan  5 19:54:02.186: INFO: http.Client request:HEAD | StatusCode:200
    Jan  5 19:54:02.186: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/OPTIONS
    Jan  5 19:54:02.189: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 19:54:02.189: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/PATCH
    Jan  5 19:54:02.193: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 19:54:02.193: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/POST
    Jan  5 19:54:02.200: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 19:54:02.200: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/pods/agnhost/proxy/some/path/with/PUT
    Jan  5 19:54:02.204: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jan  5 19:54:02.204: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/DELETE
    Jan  5 19:54:02.211: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jan  5 19:54:02.211: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/GET
    Jan  5 19:54:02.216: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jan  5 19:54:02.216: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/HEAD
    Jan  5 19:54:02.222: INFO: http.Client request:HEAD | StatusCode:200
    Jan  5 19:54:02.222: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/OPTIONS
    Jan  5 19:54:02.231: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jan  5 19:54:02.231: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/PATCH
    Jan  5 19:54:02.237: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jan  5 19:54:02.237: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/POST
    Jan  5 19:54:02.242: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jan  5 19:54:02.242: INFO: Starting http.Client for https://10.20.0.1:443/api/v1/namespaces/proxy-2324/services/test-service/proxy/some/path/with/PUT
    Jan  5 19:54:02.247: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:02.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] version v1
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] version v1
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] version v1
      tear down framework | framework.go:193
    STEP: Destroying namespace "proxy-2324" for this suite. 01/05/23 19:54:02.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:02.263
Jan  5 19:54:02.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename namespaces 01/05/23 19:54:02.266
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:02.277
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:02.281
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394
STEP: Creating namespace "e2e-ns-tzzpp" 01/05/23 19:54:02.284
Jan  5 19:54:02.296: INFO: Namespace "e2e-ns-tzzpp-2225" has []v1.FinalizerName{"kubernetes"}
STEP: Adding e2e finalizer to namespace "e2e-ns-tzzpp-2225" 01/05/23 19:54:02.296
Jan  5 19:54:02.304: INFO: Namespace "e2e-ns-tzzpp-2225" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
STEP: Removing e2e finalizer from namespace "e2e-ns-tzzpp-2225" 01/05/23 19:54:02.304
Jan  5 19:54:02.312: INFO: Namespace "e2e-ns-tzzpp-2225" has []v1.FinalizerName{"kubernetes"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:02.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-6856" for this suite. 01/05/23 19:54:02.315
STEP: Destroying namespace "e2e-ns-tzzpp-2225" for this suite. 01/05/23 19:54:02.322
------------------------------
• [0.065 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply a finalizer to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:02.263
    Jan  5 19:54:02.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename namespaces 01/05/23 19:54:02.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:02.277
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:02.281
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply a finalizer to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:394
    STEP: Creating namespace "e2e-ns-tzzpp" 01/05/23 19:54:02.284
    Jan  5 19:54:02.296: INFO: Namespace "e2e-ns-tzzpp-2225" has []v1.FinalizerName{"kubernetes"}
    STEP: Adding e2e finalizer to namespace "e2e-ns-tzzpp-2225" 01/05/23 19:54:02.296
    Jan  5 19:54:02.304: INFO: Namespace "e2e-ns-tzzpp-2225" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
    STEP: Removing e2e finalizer from namespace "e2e-ns-tzzpp-2225" 01/05/23 19:54:02.304
    Jan  5 19:54:02.312: INFO: Namespace "e2e-ns-tzzpp-2225" has []v1.FinalizerName{"kubernetes"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:02.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-6856" for this suite. 01/05/23 19:54:02.315
    STEP: Destroying namespace "e2e-ns-tzzpp-2225" for this suite. 01/05/23 19:54:02.322
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:02.327
Jan  5 19:54:02.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename gc 01/05/23 19:54:02.328
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:02.337
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:02.34
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jan  5 19:54:02.387: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8d9f40bf-f5e4-45e3-936c-77116644ad64", Controller:(*bool)(0xc0063e1456), BlockOwnerDeletion:(*bool)(0xc0063e1457)}}
Jan  5 19:54:02.401: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3bb7d23a-b1f8-49a3-894d-31d16c75fc17", Controller:(*bool)(0xc00634c2ae), BlockOwnerDeletion:(*bool)(0xc00634c2af)}}
Jan  5 19:54:02.414: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f9f7e0f9-bb55-46dc-9c3d-d70ce0717a28", Controller:(*bool)(0xc00634c52e), BlockOwnerDeletion:(*bool)(0xc00634c52f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:07.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1166" for this suite. 01/05/23 19:54:07.432
------------------------------
• [SLOW TEST] [5.110 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:02.327
    Jan  5 19:54:02.327: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename gc 01/05/23 19:54:02.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:02.337
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:02.34
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jan  5 19:54:02.387: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"8d9f40bf-f5e4-45e3-936c-77116644ad64", Controller:(*bool)(0xc0063e1456), BlockOwnerDeletion:(*bool)(0xc0063e1457)}}
    Jan  5 19:54:02.401: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"3bb7d23a-b1f8-49a3-894d-31d16c75fc17", Controller:(*bool)(0xc00634c2ae), BlockOwnerDeletion:(*bool)(0xc00634c2af)}}
    Jan  5 19:54:02.414: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f9f7e0f9-bb55-46dc-9c3d-d70ce0717a28", Controller:(*bool)(0xc00634c52e), BlockOwnerDeletion:(*bool)(0xc00634c52f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:07.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1166" for this suite. 01/05/23 19:54:07.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:07.438
Jan  5 19:54:07.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename runtimeclass 01/05/23 19:54:07.441
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:07.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:07.462
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jan  5 19:54:07.487: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7441 to be scheduled
Jan  5 19:54:07.492: INFO: 1 pods are not scheduled: [runtimeclass-7441/test-runtimeclass-runtimeclass-7441-preconfigured-handler-dsjr8(cb36a39b-73aa-442d-b63d-3acd3a3d52c8)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:09.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-7441" for this suite. 01/05/23 19:54:09.516
------------------------------
• [2.092 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:07.438
    Jan  5 19:54:07.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 19:54:07.441
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:07.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:07.462
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jan  5 19:54:07.487: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7441 to be scheduled
    Jan  5 19:54:07.492: INFO: 1 pods are not scheduled: [runtimeclass-7441/test-runtimeclass-runtimeclass-7441-preconfigured-handler-dsjr8(cb36a39b-73aa-442d-b63d-3acd3a3d52c8)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:09.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-7441" for this suite. 01/05/23 19:54:09.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:09.532
Jan  5 19:54:09.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-probe 01/05/23 19:54:09.534
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:09.549
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:09.554
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72
Jan  5 19:54:09.568: INFO: Waiting up to 5m0s for pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2" in namespace "container-probe-2544" to be "running and ready"
Jan  5 19:54:09.573: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488711ms
Jan  5 19:54:09.573: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:54:11.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008677784s
Jan  5 19:54:11.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:13.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 4.009517703s
Jan  5 19:54:13.578: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:15.576: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 6.00824372s
Jan  5 19:54:15.576: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:17.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 8.010054879s
Jan  5 19:54:17.578: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:19.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 10.009457252s
Jan  5 19:54:19.578: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:21.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 12.009439094s
Jan  5 19:54:21.578: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:23.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 14.009186907s
Jan  5 19:54:23.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:25.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 16.010390854s
Jan  5 19:54:25.579: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:27.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 18.008607493s
Jan  5 19:54:27.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:29.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 20.008549139s
Jan  5 19:54:29.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
Jan  5 19:54:31.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=true. Elapsed: 22.008465551s
Jan  5 19:54:31.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = true)
Jan  5 19:54:31.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2" satisfied condition "running and ready"
Jan  5 19:54:31.579: INFO: Container started at 2023-01-05 19:54:10 +0000 UTC, pod became ready at 2023-01-05 19:54:29 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:31.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-2544" for this suite. 01/05/23 19:54:31.582
------------------------------
• [SLOW TEST] [22.054 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:09.532
    Jan  5 19:54:09.532: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-probe 01/05/23 19:54:09.534
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:09.549
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:09.554
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:72
    Jan  5 19:54:09.568: INFO: Waiting up to 5m0s for pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2" in namespace "container-probe-2544" to be "running and ready"
    Jan  5 19:54:09.573: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.488711ms
    Jan  5 19:54:09.573: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:54:11.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 2.008677784s
    Jan  5 19:54:11.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:13.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 4.009517703s
    Jan  5 19:54:13.578: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:15.576: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 6.00824372s
    Jan  5 19:54:15.576: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:17.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 8.010054879s
    Jan  5 19:54:17.578: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:19.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 10.009457252s
    Jan  5 19:54:19.578: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:21.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 12.009439094s
    Jan  5 19:54:21.578: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:23.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 14.009186907s
    Jan  5 19:54:23.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:25.578: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 16.010390854s
    Jan  5 19:54:25.579: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:27.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 18.008607493s
    Jan  5 19:54:27.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:29.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=false. Elapsed: 20.008549139s
    Jan  5 19:54:29.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = false)
    Jan  5 19:54:31.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2": Phase="Running", Reason="", readiness=true. Elapsed: 22.008465551s
    Jan  5 19:54:31.577: INFO: The phase of Pod test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2 is Running (Ready = true)
    Jan  5 19:54:31.577: INFO: Pod "test-webserver-1bb00975-96d5-4802-be7e-4031df8436a2" satisfied condition "running and ready"
    Jan  5 19:54:31.579: INFO: Container started at 2023-01-05 19:54:10 +0000 UTC, pod became ready at 2023-01-05 19:54:29 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:31.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-2544" for this suite. 01/05/23 19:54:31.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:31.587
Jan  5 19:54:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:54:31.59
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:31.603
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:31.605
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1250
STEP: validating cluster-info 01/05/23 19:54:31.608
Jan  5 19:54:31.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-2368 cluster-info'
Jan  5 19:54:31.690: INFO: stderr: ""
Jan  5 19:54:31.690: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.20.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:31.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-2368" for this suite. 01/05/23 19:54:31.696
------------------------------
• [0.114 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1244
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:31.587
    Jan  5 19:54:31.587: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:54:31.59
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:31.603
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:31.605
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1250
    STEP: validating cluster-info 01/05/23 19:54:31.608
    Jan  5 19:54:31.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-2368 cluster-info'
    Jan  5 19:54:31.690: INFO: stderr: ""
    Jan  5 19:54:31.690: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.20.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:31.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-2368" for this suite. 01/05/23 19:54:31.696
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:31.702
Jan  5 19:54:31.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replication-controller 01/05/23 19:54:31.705
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:31.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:31.72
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101
STEP: Given a ReplicationController is created 01/05/23 19:54:31.724
STEP: When the matched label of one of its pods change 01/05/23 19:54:31.728
Jan  5 19:54:31.731: INFO: Pod name pod-release: Found 0 pods out of 1
Jan  5 19:54:36.737: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 01/05/23 19:54:36.759
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:37.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-7129" for this suite. 01/05/23 19:54:37.772
------------------------------
• [SLOW TEST] [6.078 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:31.702
    Jan  5 19:54:31.702: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replication-controller 01/05/23 19:54:31.705
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:31.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:31.72
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:101
    STEP: Given a ReplicationController is created 01/05/23 19:54:31.724
    STEP: When the matched label of one of its pods change 01/05/23 19:54:31.728
    Jan  5 19:54:31.731: INFO: Pod name pod-release: Found 0 pods out of 1
    Jan  5 19:54:36.737: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 01/05/23 19:54:36.759
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:37.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-7129" for this suite. 01/05/23 19:54:37.772
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:37.781
Jan  5 19:54:37.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:54:37.784
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:37.803
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:37.809
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44
STEP: Creating a pod to test downward api env vars 01/05/23 19:54:37.813
Jan  5 19:54:37.824: INFO: Waiting up to 5m0s for pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310" in namespace "downward-api-8588" to be "Succeeded or Failed"
Jan  5 19:54:37.828: INFO: Pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058904ms
Jan  5 19:54:39.831: INFO: Pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007521734s
Jan  5 19:54:41.831: INFO: Pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007403769s
STEP: Saw pod success 01/05/23 19:54:41.831
Jan  5 19:54:41.831: INFO: Pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310" satisfied condition "Succeeded or Failed"
Jan  5 19:54:41.834: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310 container dapi-container: <nil>
STEP: delete the pod 01/05/23 19:54:41.851
Jan  5 19:54:41.864: INFO: Waiting for pod downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310 to disappear
Jan  5 19:54:41.867: INFO: Pod downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:41.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-8588" for this suite. 01/05/23 19:54:41.872
------------------------------
• [4.096 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:37.781
    Jan  5 19:54:37.781: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:54:37.784
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:37.803
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:37.809
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:44
    STEP: Creating a pod to test downward api env vars 01/05/23 19:54:37.813
    Jan  5 19:54:37.824: INFO: Waiting up to 5m0s for pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310" in namespace "downward-api-8588" to be "Succeeded or Failed"
    Jan  5 19:54:37.828: INFO: Pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058904ms
    Jan  5 19:54:39.831: INFO: Pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007521734s
    Jan  5 19:54:41.831: INFO: Pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007403769s
    STEP: Saw pod success 01/05/23 19:54:41.831
    Jan  5 19:54:41.831: INFO: Pod "downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310" satisfied condition "Succeeded or Failed"
    Jan  5 19:54:41.834: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 19:54:41.851
    Jan  5 19:54:41.864: INFO: Waiting for pod downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310 to disappear
    Jan  5 19:54:41.867: INFO: Pod downward-api-511c163b-2ba8-43c5-a6a4-19b8c6540310 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:41.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-8588" for this suite. 01/05/23 19:54:41.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:41.886
Jan  5 19:54:41.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:54:41.888
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:41.9
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:41.903
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47
STEP: Creating secret with name secret-test-7fce5775-dd08-4a15-88cc-c1f8ff617cbf 01/05/23 19:54:41.906
STEP: Creating a pod to test consume secrets 01/05/23 19:54:41.911
Jan  5 19:54:41.931: INFO: Waiting up to 5m0s for pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7" in namespace "secrets-7900" to be "Succeeded or Failed"
Jan  5 19:54:41.937: INFO: Pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.338335ms
Jan  5 19:54:43.941: INFO: Pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009812084s
Jan  5 19:54:45.942: INFO: Pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010727108s
STEP: Saw pod success 01/05/23 19:54:45.942
Jan  5 19:54:45.943: INFO: Pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7" satisfied condition "Succeeded or Failed"
Jan  5 19:54:45.945: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 19:54:45.951
Jan  5 19:54:45.965: INFO: Waiting for pod pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7 to disappear
Jan  5 19:54:45.968: INFO: Pod pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:45.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7900" for this suite. 01/05/23 19:54:45.975
------------------------------
• [4.093 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:41.886
    Jan  5 19:54:41.887: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:54:41.888
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:41.9
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:41.903
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:47
    STEP: Creating secret with name secret-test-7fce5775-dd08-4a15-88cc-c1f8ff617cbf 01/05/23 19:54:41.906
    STEP: Creating a pod to test consume secrets 01/05/23 19:54:41.911
    Jan  5 19:54:41.931: INFO: Waiting up to 5m0s for pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7" in namespace "secrets-7900" to be "Succeeded or Failed"
    Jan  5 19:54:41.937: INFO: Pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.338335ms
    Jan  5 19:54:43.941: INFO: Pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009812084s
    Jan  5 19:54:45.942: INFO: Pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010727108s
    STEP: Saw pod success 01/05/23 19:54:45.942
    Jan  5 19:54:45.943: INFO: Pod "pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7" satisfied condition "Succeeded or Failed"
    Jan  5 19:54:45.945: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:54:45.951
    Jan  5 19:54:45.965: INFO: Waiting for pod pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7 to disappear
    Jan  5 19:54:45.968: INFO: Pod pod-secrets-b379656b-07a4-4415-9af4-aa08ddfb91f7 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:45.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7900" for this suite. 01/05/23 19:54:45.975
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:45.983
Jan  5 19:54:45.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:54:45.985
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:45.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:46.002
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:54:46.005
Jan  5 19:54:46.015: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610" in namespace "projected-1658" to be "Succeeded or Failed"
Jan  5 19:54:46.019: INFO: Pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610": Phase="Pending", Reason="", readiness=false. Elapsed: 3.567586ms
Jan  5 19:54:48.023: INFO: Pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007312508s
Jan  5 19:54:50.024: INFO: Pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008731517s
STEP: Saw pod success 01/05/23 19:54:50.024
Jan  5 19:54:50.025: INFO: Pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610" satisfied condition "Succeeded or Failed"
Jan  5 19:54:50.028: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610 container client-container: <nil>
STEP: delete the pod 01/05/23 19:54:50.033
Jan  5 19:54:50.046: INFO: Waiting for pod downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610 to disappear
Jan  5 19:54:50.049: INFO: Pod downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:50.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-1658" for this suite. 01/05/23 19:54:50.052
------------------------------
• [4.074 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:45.983
    Jan  5 19:54:45.984: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:54:45.985
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:45.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:46.002
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:53
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:54:46.005
    Jan  5 19:54:46.015: INFO: Waiting up to 5m0s for pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610" in namespace "projected-1658" to be "Succeeded or Failed"
    Jan  5 19:54:46.019: INFO: Pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610": Phase="Pending", Reason="", readiness=false. Elapsed: 3.567586ms
    Jan  5 19:54:48.023: INFO: Pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007312508s
    Jan  5 19:54:50.024: INFO: Pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008731517s
    STEP: Saw pod success 01/05/23 19:54:50.024
    Jan  5 19:54:50.025: INFO: Pod "downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610" satisfied condition "Succeeded or Failed"
    Jan  5 19:54:50.028: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:54:50.033
    Jan  5 19:54:50.046: INFO: Waiting for pod downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610 to disappear
    Jan  5 19:54:50.049: INFO: Pod downwardapi-volume-79e54271-b258-4d67-a4d1-62486cf34610 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:50.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-1658" for this suite. 01/05/23 19:54:50.052
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:50.061
Jan  5 19:54:50.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:54:50.063
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:50.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:50.081
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78
STEP: Creating projection with secret that has name projected-secret-test-map-9460e346-23ac-46d8-b94f-451eea5da6a4 01/05/23 19:54:50.083
STEP: Creating a pod to test consume secrets 01/05/23 19:54:50.087
Jan  5 19:54:50.097: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3" in namespace "projected-7971" to be "Succeeded or Failed"
Jan  5 19:54:50.102: INFO: Pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.729325ms
Jan  5 19:54:52.115: INFO: Pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012866643s
Jan  5 19:54:54.106: INFO: Pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008198429s
STEP: Saw pod success 01/05/23 19:54:54.106
Jan  5 19:54:54.106: INFO: Pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3" satisfied condition "Succeeded or Failed"
Jan  5 19:54:54.110: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3 container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 19:54:54.116
Jan  5 19:54:54.129: INFO: Waiting for pod pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3 to disappear
Jan  5 19:54:54.132: INFO: Pod pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:54.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-7971" for this suite. 01/05/23 19:54:54.136
------------------------------
• [4.081 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:50.061
    Jan  5 19:54:50.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:54:50.063
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:50.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:50.081
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:78
    STEP: Creating projection with secret that has name projected-secret-test-map-9460e346-23ac-46d8-b94f-451eea5da6a4 01/05/23 19:54:50.083
    STEP: Creating a pod to test consume secrets 01/05/23 19:54:50.087
    Jan  5 19:54:50.097: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3" in namespace "projected-7971" to be "Succeeded or Failed"
    Jan  5 19:54:50.102: INFO: Pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.729325ms
    Jan  5 19:54:52.115: INFO: Pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012866643s
    Jan  5 19:54:54.106: INFO: Pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008198429s
    STEP: Saw pod success 01/05/23 19:54:54.106
    Jan  5 19:54:54.106: INFO: Pod "pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3" satisfied condition "Succeeded or Failed"
    Jan  5 19:54:54.110: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3 container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:54:54.116
    Jan  5 19:54:54.129: INFO: Waiting for pod pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3 to disappear
    Jan  5 19:54:54.132: INFO: Pod pod-projected-secrets-7be800e3-e0c8-4ee5-ad2e-e098789a0da3 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:54.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-7971" for this suite. 01/05/23 19:54:54.136
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:54.15
Jan  5 19:54:54.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 19:54:54.151
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:54.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:54.165
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57
STEP: Creating configMap with name configmap-test-volume-1309e9d9-4d5f-4016-89af-a8ee422ca06d 01/05/23 19:54:54.168
STEP: Creating a pod to test consume configMaps 01/05/23 19:54:54.173
Jan  5 19:54:54.180: INFO: Waiting up to 5m0s for pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2" in namespace "configmap-5860" to be "Succeeded or Failed"
Jan  5 19:54:54.184: INFO: Pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43645ms
Jan  5 19:54:56.188: INFO: Pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2": Phase="Running", Reason="", readiness=false. Elapsed: 2.007237121s
Jan  5 19:54:58.187: INFO: Pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006754918s
STEP: Saw pod success 01/05/23 19:54:58.187
Jan  5 19:54:58.187: INFO: Pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2" satisfied condition "Succeeded or Failed"
Jan  5 19:54:58.190: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:54:58.195
Jan  5 19:54:58.211: INFO: Waiting for pod pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2 to disappear
Jan  5 19:54:58.213: INFO: Pod pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:54:58.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-5860" for this suite. 01/05/23 19:54:58.217
------------------------------
• [4.072 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:54.15
    Jan  5 19:54:54.150: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 19:54:54.151
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:54.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:54.165
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:57
    STEP: Creating configMap with name configmap-test-volume-1309e9d9-4d5f-4016-89af-a8ee422ca06d 01/05/23 19:54:54.168
    STEP: Creating a pod to test consume configMaps 01/05/23 19:54:54.173
    Jan  5 19:54:54.180: INFO: Waiting up to 5m0s for pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2" in namespace "configmap-5860" to be "Succeeded or Failed"
    Jan  5 19:54:54.184: INFO: Pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.43645ms
    Jan  5 19:54:56.188: INFO: Pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2": Phase="Running", Reason="", readiness=false. Elapsed: 2.007237121s
    Jan  5 19:54:58.187: INFO: Pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006754918s
    STEP: Saw pod success 01/05/23 19:54:58.187
    Jan  5 19:54:58.187: INFO: Pod "pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2" satisfied condition "Succeeded or Failed"
    Jan  5 19:54:58.190: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:54:58.195
    Jan  5 19:54:58.211: INFO: Waiting for pod pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2 to disappear
    Jan  5 19:54:58.213: INFO: Pod pod-configmaps-56e0c92e-99b9-4181-b91d-f6fe772c51e2 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:54:58.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-5860" for this suite. 01/05/23 19:54:58.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:54:58.223
Jan  5 19:54:58.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename var-expansion 01/05/23 19:54:58.225
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:58.238
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:58.241
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92
STEP: Creating a pod to test substitution in container's args 01/05/23 19:54:58.244
Jan  5 19:54:58.253: INFO: Waiting up to 5m0s for pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84" in namespace "var-expansion-7201" to be "Succeeded or Failed"
Jan  5 19:54:58.257: INFO: Pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.82562ms
Jan  5 19:55:00.261: INFO: Pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008160126s
Jan  5 19:55:02.261: INFO: Pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007882022s
STEP: Saw pod success 01/05/23 19:55:02.261
Jan  5 19:55:02.261: INFO: Pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84" satisfied condition "Succeeded or Failed"
Jan  5 19:55:02.264: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84 container dapi-container: <nil>
STEP: delete the pod 01/05/23 19:55:02.27
Jan  5 19:55:02.279: INFO: Waiting for pod var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84 to disappear
Jan  5 19:55:02.284: INFO: Pod var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:02.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-7201" for this suite. 01/05/23 19:55:02.291
------------------------------
• [4.074 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:54:58.223
    Jan  5 19:54:58.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename var-expansion 01/05/23 19:54:58.225
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:54:58.238
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:54:58.241
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:92
    STEP: Creating a pod to test substitution in container's args 01/05/23 19:54:58.244
    Jan  5 19:54:58.253: INFO: Waiting up to 5m0s for pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84" in namespace "var-expansion-7201" to be "Succeeded or Failed"
    Jan  5 19:54:58.257: INFO: Pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.82562ms
    Jan  5 19:55:00.261: INFO: Pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008160126s
    Jan  5 19:55:02.261: INFO: Pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007882022s
    STEP: Saw pod success 01/05/23 19:55:02.261
    Jan  5 19:55:02.261: INFO: Pod "var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84" satisfied condition "Succeeded or Failed"
    Jan  5 19:55:02.264: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 19:55:02.27
    Jan  5 19:55:02.279: INFO: Waiting for pod var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84 to disappear
    Jan  5 19:55:02.284: INFO: Pod var-expansion-e8d8340e-379c-47d1-82d4-e8481076ad84 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:02.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-7201" for this suite. 01/05/23 19:55:02.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
[BeforeEach] [sig-node] Variable Expansion
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:02.31
Jan  5 19:55:02.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename var-expansion 01/05/23 19:55:02.311
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:02.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:02.328
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:31
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152
Jan  5 19:55:02.339: INFO: Waiting up to 2m0s for pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8" in namespace "var-expansion-155" to be "container 0 failed with reason CreateContainerConfigError"
Jan  5 19:55:02.341: INFO: Pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072102ms
Jan  5 19:55:04.344: INFO: Pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005444919s
Jan  5 19:55:04.344: INFO: Pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jan  5 19:55:04.344: INFO: Deleting pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8" in namespace "var-expansion-155"
Jan  5 19:55:04.351: INFO: Wait up to 5m0s for pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:06.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Variable Expansion
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Variable Expansion
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Variable Expansion
  tear down framework | framework.go:193
STEP: Destroying namespace "var-expansion-155" for this suite. 01/05/23 19:55:06.362
------------------------------
• [4.057 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:02.31
    Jan  5 19:55:02.310: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename var-expansion 01/05/23 19:55:02.311
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:02.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:02.328
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:152
    Jan  5 19:55:02.339: INFO: Waiting up to 2m0s for pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8" in namespace "var-expansion-155" to be "container 0 failed with reason CreateContainerConfigError"
    Jan  5 19:55:02.341: INFO: Pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072102ms
    Jan  5 19:55:04.344: INFO: Pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005444919s
    Jan  5 19:55:04.344: INFO: Pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jan  5 19:55:04.344: INFO: Deleting pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8" in namespace "var-expansion-155"
    Jan  5 19:55:04.351: INFO: Wait up to 5m0s for pod "var-expansion-6f59ffa9-a2c5-4910-959f-2de8eac124c8" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:06.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Variable Expansion
      tear down framework | framework.go:193
    STEP: Destroying namespace "var-expansion-155" for this suite. 01/05/23 19:55:06.362
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:06.37
Jan  5 19:55:06.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename disruption 01/05/23 19:55:06.372
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:06.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:06.385
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141
STEP: Waiting for the pdb to be processed 01/05/23 19:55:06.393
STEP: Waiting for all pods to be running 01/05/23 19:55:08.436
Jan  5 19:55:08.445: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:10.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-1894" for this suite. 01/05/23 19:55:10.455
------------------------------
• [4.091 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:141

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:06.37
    Jan  5 19:55:06.371: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename disruption 01/05/23 19:55:06.372
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:06.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:06.385
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:141
    STEP: Waiting for the pdb to be processed 01/05/23 19:55:06.393
    STEP: Waiting for all pods to be running 01/05/23 19:55:08.436
    Jan  5 19:55:08.445: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:10.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-1894" for this suite. 01/05/23 19:55:10.455
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:10.461
Jan  5 19:55:10.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename dns 01/05/23 19:55:10.463
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:10.477
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:10.48
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 01/05/23 19:55:10.483
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6233 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6233;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6233 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6233;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6233.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6233.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6233.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6233.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6233.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6233.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6233.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6233.svc;check="$$(dig +notcp +noall +answer +search 74.15.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.15.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.15.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.15.74_tcp@PTR;sleep 1; done
 01/05/23 19:55:10.502
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6233 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6233;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6233 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6233;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6233.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6233.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6233.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6233.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6233.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6233.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6233.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6233.svc;check="$$(dig +notcp +noall +answer +search 74.15.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.15.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.15.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.15.74_tcp@PTR;sleep 1; done
 01/05/23 19:55:10.503
STEP: creating a pod to probe DNS 01/05/23 19:55:10.503
STEP: submitting the pod to kubernetes 01/05/23 19:55:10.503
Jan  5 19:55:10.516: INFO: Waiting up to 15m0s for pod "dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8" in namespace "dns-6233" to be "running"
Jan  5 19:55:10.519: INFO: Pod "dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728069ms
Jan  5 19:55:12.524: INFO: Pod "dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006992521s
Jan  5 19:55:12.524: INFO: Pod "dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:55:12.524
STEP: looking for the results for each expected name from probers 01/05/23 19:55:12.526
Jan  5 19:55:12.534: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.538: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.544: INFO: Unable to read wheezy_udp@dns-test-service.dns-6233 from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.549: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6233 from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.553: INFO: Unable to read wheezy_udp@dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.558: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.562: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.568: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.594: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.597: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.602: INFO: Unable to read jessie_udp@dns-test-service.dns-6233 from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.607: INFO: Unable to read jessie_tcp@dns-test-service.dns-6233 from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.610: INFO: Unable to read jessie_udp@dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.615: INFO: Unable to read jessie_tcp@dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.618: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.621: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
Jan  5 19:55:12.639: INFO: Lookups using dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6233 wheezy_tcp@dns-test-service.dns-6233 wheezy_udp@dns-test-service.dns-6233.svc wheezy_tcp@dns-test-service.dns-6233.svc wheezy_udp@_http._tcp.dns-test-service.dns-6233.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6233.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6233 jessie_tcp@dns-test-service.dns-6233 jessie_udp@dns-test-service.dns-6233.svc jessie_tcp@dns-test-service.dns-6233.svc jessie_udp@_http._tcp.dns-test-service.dns-6233.svc jessie_tcp@_http._tcp.dns-test-service.dns-6233.svc]

Jan  5 19:55:17.737: INFO: DNS probes using dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8 succeeded

STEP: deleting the pod 01/05/23 19:55:17.737
STEP: deleting the test service 01/05/23 19:55:17.761
STEP: deleting the test headless service 01/05/23 19:55:17.799
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:17.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-6233" for this suite. 01/05/23 19:55:17.816
------------------------------
• [SLOW TEST] [7.363 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:10.461
    Jan  5 19:55:10.462: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename dns 01/05/23 19:55:10.463
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:10.477
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:10.48
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 01/05/23 19:55:10.483
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6233 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6233;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6233 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6233;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6233.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6233.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6233.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6233.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6233.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6233.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6233.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6233.svc;check="$$(dig +notcp +noall +answer +search 74.15.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.15.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.15.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.15.74_tcp@PTR;sleep 1; done
     01/05/23 19:55:10.502
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6233 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6233;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6233 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6233;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6233.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6233.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6233.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6233.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6233.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6233.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6233.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6233.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6233.svc;check="$$(dig +notcp +noall +answer +search 74.15.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.15.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.15.20.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.20.15.74_tcp@PTR;sleep 1; done
     01/05/23 19:55:10.503
    STEP: creating a pod to probe DNS 01/05/23 19:55:10.503
    STEP: submitting the pod to kubernetes 01/05/23 19:55:10.503
    Jan  5 19:55:10.516: INFO: Waiting up to 15m0s for pod "dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8" in namespace "dns-6233" to be "running"
    Jan  5 19:55:10.519: INFO: Pod "dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.728069ms
    Jan  5 19:55:12.524: INFO: Pod "dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8": Phase="Running", Reason="", readiness=true. Elapsed: 2.006992521s
    Jan  5 19:55:12.524: INFO: Pod "dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:55:12.524
    STEP: looking for the results for each expected name from probers 01/05/23 19:55:12.526
    Jan  5 19:55:12.534: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.538: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.544: INFO: Unable to read wheezy_udp@dns-test-service.dns-6233 from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.549: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6233 from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.553: INFO: Unable to read wheezy_udp@dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.558: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.562: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.568: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.594: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.597: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.602: INFO: Unable to read jessie_udp@dns-test-service.dns-6233 from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.607: INFO: Unable to read jessie_tcp@dns-test-service.dns-6233 from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.610: INFO: Unable to read jessie_udp@dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.615: INFO: Unable to read jessie_tcp@dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.618: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.621: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6233.svc from pod dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8: the server could not find the requested resource (get pods dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8)
    Jan  5 19:55:12.639: INFO: Lookups using dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6233 wheezy_tcp@dns-test-service.dns-6233 wheezy_udp@dns-test-service.dns-6233.svc wheezy_tcp@dns-test-service.dns-6233.svc wheezy_udp@_http._tcp.dns-test-service.dns-6233.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6233.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6233 jessie_tcp@dns-test-service.dns-6233 jessie_udp@dns-test-service.dns-6233.svc jessie_tcp@dns-test-service.dns-6233.svc jessie_udp@_http._tcp.dns-test-service.dns-6233.svc jessie_tcp@_http._tcp.dns-test-service.dns-6233.svc]

    Jan  5 19:55:17.737: INFO: DNS probes using dns-6233/dns-test-cc84a784-655c-458b-b4d1-1eb1375c7ab8 succeeded

    STEP: deleting the pod 01/05/23 19:55:17.737
    STEP: deleting the test service 01/05/23 19:55:17.761
    STEP: deleting the test headless service 01/05/23 19:55:17.799
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:17.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-6233" for this suite. 01/05/23 19:55:17.816
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:17.84
Jan  5 19:55:17.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:55:17.844
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:17.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:17.869
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375
STEP: Creating configMap with name projected-configmap-test-volume-6d68f015-dfe9-4e4f-a9dd-efeb4947b497 01/05/23 19:55:17.876
STEP: Creating a pod to test consume configMaps 01/05/23 19:55:17.881
Jan  5 19:55:17.891: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea" in namespace "projected-619" to be "Succeeded or Failed"
Jan  5 19:55:17.896: INFO: Pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363384ms
Jan  5 19:55:19.900: INFO: Pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008274812s
Jan  5 19:55:21.899: INFO: Pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007755992s
STEP: Saw pod success 01/05/23 19:55:21.899
Jan  5 19:55:21.900: INFO: Pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea" satisfied condition "Succeeded or Failed"
Jan  5 19:55:21.902: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea container projected-configmap-volume-test: <nil>
STEP: delete the pod 01/05/23 19:55:21.909
Jan  5 19:55:21.918: INFO: Waiting for pod pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea to disappear
Jan  5 19:55:21.923: INFO: Pod pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:21.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-619" for this suite. 01/05/23 19:55:21.927
------------------------------
• [4.092 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:375

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:17.84
    Jan  5 19:55:17.841: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:55:17.844
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:17.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:17.869
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:375
    STEP: Creating configMap with name projected-configmap-test-volume-6d68f015-dfe9-4e4f-a9dd-efeb4947b497 01/05/23 19:55:17.876
    STEP: Creating a pod to test consume configMaps 01/05/23 19:55:17.881
    Jan  5 19:55:17.891: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea" in namespace "projected-619" to be "Succeeded or Failed"
    Jan  5 19:55:17.896: INFO: Pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 4.363384ms
    Jan  5 19:55:19.900: INFO: Pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008274812s
    Jan  5 19:55:21.899: INFO: Pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007755992s
    STEP: Saw pod success 01/05/23 19:55:21.899
    Jan  5 19:55:21.900: INFO: Pod "pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea" satisfied condition "Succeeded or Failed"
    Jan  5 19:55:21.902: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea container projected-configmap-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:55:21.909
    Jan  5 19:55:21.918: INFO: Waiting for pod pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea to disappear
    Jan  5 19:55:21.923: INFO: Pod pod-projected-configmaps-0864cfb2-e9fd-4344-aede-f05fde9fa7ea no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:21.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-619" for this suite. 01/05/23 19:55:21.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:21.938
Jan  5 19:55:21.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename ephemeral-containers-test 01/05/23 19:55:21.939
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:21.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:21.956
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 01/05/23 19:55:21.959
Jan  5 19:55:21.969: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-803" to be "running and ready"
Jan  5 19:55:21.971: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.215855ms
Jan  5 19:55:21.971: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:55:23.975: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0063154s
Jan  5 19:55:23.975: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jan  5 19:55:23.976: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 01/05/23 19:55:23.979
Jan  5 19:55:23.992: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-803" to be "container debugger running"
Jan  5 19:55:23.995: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.289416ms
Jan  5 19:55:25.998: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006260009s
Jan  5 19:55:27.999: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006343067s
Jan  5 19:55:27.999: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 01/05/23 19:55:27.999
Jan  5 19:55:27.999: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-803 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:55:27.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:55:28.000: INFO: ExecWithOptions: Clientset creation
Jan  5 19:55:28.001: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/ephemeral-containers-test-803/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jan  5 19:55:28.066: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:28.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "ephemeral-containers-test-803" for this suite. 01/05/23 19:55:28.076
------------------------------
• [SLOW TEST] [6.142 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:21.938
    Jan  5 19:55:21.938: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename ephemeral-containers-test 01/05/23 19:55:21.939
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:21.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:21.956
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 01/05/23 19:55:21.959
    Jan  5 19:55:21.969: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-803" to be "running and ready"
    Jan  5 19:55:21.971: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.215855ms
    Jan  5 19:55:21.971: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:55:23.975: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0063154s
    Jan  5 19:55:23.975: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jan  5 19:55:23.976: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 01/05/23 19:55:23.979
    Jan  5 19:55:23.992: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-803" to be "container debugger running"
    Jan  5 19:55:23.995: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.289416ms
    Jan  5 19:55:25.998: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006260009s
    Jan  5 19:55:27.999: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.006343067s
    Jan  5 19:55:27.999: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 01/05/23 19:55:27.999
    Jan  5 19:55:27.999: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-803 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:55:27.999: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:55:28.000: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:55:28.001: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/ephemeral-containers-test-803/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jan  5 19:55:28.066: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:28.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Ephemeral Containers [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "ephemeral-containers-test-803" for this suite. 01/05/23 19:55:28.076
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:28.083
Jan  5 19:55:28.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:55:28.085
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:28.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:28.098
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57
STEP: Creating configMap with name projected-configmap-test-volume-f7a36f51-7180-4119-a8ae-aa469c49b69a 01/05/23 19:55:28.101
STEP: Creating a pod to test consume configMaps 01/05/23 19:55:28.105
Jan  5 19:55:28.113: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9" in namespace "projected-6458" to be "Succeeded or Failed"
Jan  5 19:55:28.120: INFO: Pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.921061ms
Jan  5 19:55:30.124: INFO: Pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011025243s
Jan  5 19:55:32.123: INFO: Pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010320774s
STEP: Saw pod success 01/05/23 19:55:32.124
Jan  5 19:55:32.124: INFO: Pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9" satisfied condition "Succeeded or Failed"
Jan  5 19:55:32.126: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:55:32.142
Jan  5 19:55:32.154: INFO: Waiting for pod pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9 to disappear
Jan  5 19:55:32.157: INFO: Pod pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:32.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6458" for this suite. 01/05/23 19:55:32.161
------------------------------
• [4.083 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:28.083
    Jan  5 19:55:28.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:55:28.085
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:28.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:28.098
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:57
    STEP: Creating configMap with name projected-configmap-test-volume-f7a36f51-7180-4119-a8ae-aa469c49b69a 01/05/23 19:55:28.101
    STEP: Creating a pod to test consume configMaps 01/05/23 19:55:28.105
    Jan  5 19:55:28.113: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9" in namespace "projected-6458" to be "Succeeded or Failed"
    Jan  5 19:55:28.120: INFO: Pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.921061ms
    Jan  5 19:55:30.124: INFO: Pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011025243s
    Jan  5 19:55:32.123: INFO: Pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010320774s
    STEP: Saw pod success 01/05/23 19:55:32.124
    Jan  5 19:55:32.124: INFO: Pod "pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9" satisfied condition "Succeeded or Failed"
    Jan  5 19:55:32.126: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:55:32.142
    Jan  5 19:55:32.154: INFO: Waiting for pod pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9 to disappear
    Jan  5 19:55:32.157: INFO: Pod pod-projected-configmaps-f36fa63e-18ce-4184-9447-853bc12491b9 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:32.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6458" for this suite. 01/05/23 19:55:32.161
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:32.172
Jan  5 19:55:32.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:55:32.174
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:32.187
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:32.19
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:55:32.193
Jan  5 19:55:32.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645" in namespace "downward-api-9352" to be "Succeeded or Failed"
Jan  5 19:55:32.206: INFO: Pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645": Phase="Pending", Reason="", readiness=false. Elapsed: 4.702088ms
Jan  5 19:55:34.211: INFO: Pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008915854s
Jan  5 19:55:36.210: INFO: Pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008521636s
STEP: Saw pod success 01/05/23 19:55:36.21
Jan  5 19:55:36.211: INFO: Pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645" satisfied condition "Succeeded or Failed"
Jan  5 19:55:36.213: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645 container client-container: <nil>
STEP: delete the pod 01/05/23 19:55:36.22
Jan  5 19:55:36.228: INFO: Waiting for pod downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645 to disappear
Jan  5 19:55:36.231: INFO: Pod downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:36.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9352" for this suite. 01/05/23 19:55:36.235
------------------------------
• [4.067 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:261

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:32.172
    Jan  5 19:55:32.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:55:32.174
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:32.187
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:32.19
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:261
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:55:32.193
    Jan  5 19:55:32.202: INFO: Waiting up to 5m0s for pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645" in namespace "downward-api-9352" to be "Succeeded or Failed"
    Jan  5 19:55:32.206: INFO: Pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645": Phase="Pending", Reason="", readiness=false. Elapsed: 4.702088ms
    Jan  5 19:55:34.211: INFO: Pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008915854s
    Jan  5 19:55:36.210: INFO: Pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008521636s
    STEP: Saw pod success 01/05/23 19:55:36.21
    Jan  5 19:55:36.211: INFO: Pod "downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645" satisfied condition "Succeeded or Failed"
    Jan  5 19:55:36.213: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:55:36.22
    Jan  5 19:55:36.228: INFO: Waiting for pod downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645 to disappear
    Jan  5 19:55:36.231: INFO: Pod downwardapi-volume-267b82f1-3819-4e6e-9ed0-ed5db2069645 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:36.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9352" for this suite. 01/05/23 19:55:36.235
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:36.249
Jan  5 19:55:36.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:55:36.25
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:36.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:36.265
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154
STEP: creating a secret 01/05/23 19:55:36.268
STEP: listing secrets in all namespaces to ensure that there are more than zero 01/05/23 19:55:36.272
STEP: patching the secret 01/05/23 19:55:36.274
STEP: deleting the secret using a LabelSelector 01/05/23 19:55:36.28
STEP: listing secrets in all namespaces, searching for label name and value in patch 01/05/23 19:55:36.286
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:36.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-8603" for this suite. 01/05/23 19:55:36.292
------------------------------
• [0.048 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:36.249
    Jan  5 19:55:36.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:55:36.25
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:36.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:36.265
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:154
    STEP: creating a secret 01/05/23 19:55:36.268
    STEP: listing secrets in all namespaces to ensure that there are more than zero 01/05/23 19:55:36.272
    STEP: patching the secret 01/05/23 19:55:36.274
    STEP: deleting the secret using a LabelSelector 01/05/23 19:55:36.28
    STEP: listing secrets in all namespaces, searching for label name and value in patch 01/05/23 19:55:36.286
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:36.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-8603" for this suite. 01/05/23 19:55:36.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:36.297
Jan  5 19:55:36.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:55:36.299
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:36.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:36.314
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742
Jan  5 19:55:36.319: INFO: Got root ca configmap in namespace "svcaccounts-64"
Jan  5 19:55:36.323: INFO: Deleted root ca configmap in namespace "svcaccounts-64"
STEP: waiting for a new root ca configmap created 01/05/23 19:55:36.824
Jan  5 19:55:36.827: INFO: Recreated root ca configmap in namespace "svcaccounts-64"
Jan  5 19:55:36.832: INFO: Updated root ca configmap in namespace "svcaccounts-64"
STEP: waiting for the root ca configmap reconciled 01/05/23 19:55:37.332
Jan  5 19:55:37.337: INFO: Reconciled root ca configmap in namespace "svcaccounts-64"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:37.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-64" for this suite. 01/05/23 19:55:37.342
------------------------------
• [1.051 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:742

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:36.297
    Jan  5 19:55:36.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 19:55:36.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:36.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:36.314
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:742
    Jan  5 19:55:36.319: INFO: Got root ca configmap in namespace "svcaccounts-64"
    Jan  5 19:55:36.323: INFO: Deleted root ca configmap in namespace "svcaccounts-64"
    STEP: waiting for a new root ca configmap created 01/05/23 19:55:36.824
    Jan  5 19:55:36.827: INFO: Recreated root ca configmap in namespace "svcaccounts-64"
    Jan  5 19:55:36.832: INFO: Updated root ca configmap in namespace "svcaccounts-64"
    STEP: waiting for the root ca configmap reconciled 01/05/23 19:55:37.332
    Jan  5 19:55:37.337: INFO: Reconciled root ca configmap in namespace "svcaccounts-64"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:37.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-64" for this suite. 01/05/23 19:55:37.342
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:37.349
Jan  5 19:55:37.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:55:37.351
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:37.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:37.367
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47
STEP: Creating configMap with name projected-configmap-test-volume-8d1a5143-4234-4674-a06b-7423b02da950 01/05/23 19:55:37.371
STEP: Creating a pod to test consume configMaps 01/05/23 19:55:37.376
Jan  5 19:55:37.391: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1" in namespace "projected-4048" to be "Succeeded or Failed"
Jan  5 19:55:37.395: INFO: Pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478998ms
Jan  5 19:55:39.399: INFO: Pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007737435s
Jan  5 19:55:41.398: INFO: Pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007000527s
STEP: Saw pod success 01/05/23 19:55:41.398
Jan  5 19:55:41.398: INFO: Pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1" satisfied condition "Succeeded or Failed"
Jan  5 19:55:41.401: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:55:41.411
Jan  5 19:55:41.423: INFO: Waiting for pod pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1 to disappear
Jan  5 19:55:41.428: INFO: Pod pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:41.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4048" for this suite. 01/05/23 19:55:41.432
------------------------------
• [4.089 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:37.349
    Jan  5 19:55:37.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:55:37.351
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:37.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:37.367
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:47
    STEP: Creating configMap with name projected-configmap-test-volume-8d1a5143-4234-4674-a06b-7423b02da950 01/05/23 19:55:37.371
    STEP: Creating a pod to test consume configMaps 01/05/23 19:55:37.376
    Jan  5 19:55:37.391: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1" in namespace "projected-4048" to be "Succeeded or Failed"
    Jan  5 19:55:37.395: INFO: Pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.478998ms
    Jan  5 19:55:39.399: INFO: Pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007737435s
    Jan  5 19:55:41.398: INFO: Pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007000527s
    STEP: Saw pod success 01/05/23 19:55:41.398
    Jan  5 19:55:41.398: INFO: Pod "pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1" satisfied condition "Succeeded or Failed"
    Jan  5 19:55:41.401: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:55:41.411
    Jan  5 19:55:41.423: INFO: Waiting for pod pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1 to disappear
    Jan  5 19:55:41.428: INFO: Pod pod-projected-configmaps-4feb9211-0247-40b8-a725-845bbb29a2c1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:41.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4048" for this suite. 01/05/23 19:55:41.432
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:41.441
Jan  5 19:55:41.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-runtime 01/05/23 19:55:41.443
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:41.457
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:41.459
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:216
STEP: create the container 01/05/23 19:55:41.464
STEP: wait for the container to reach Failed 01/05/23 19:55:41.476
STEP: get the container status 01/05/23 19:55:45.494
STEP: the container should be terminated 01/05/23 19:55:45.496
STEP: the termination message should be set 01/05/23 19:55:45.496
Jan  5 19:55:45.496: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/05/23 19:55:45.496
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:45.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1194" for this suite. 01/05/23 19:55:45.513
------------------------------
• [4.078 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:41.441
    Jan  5 19:55:41.442: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-runtime 01/05/23 19:55:41.443
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:41.457
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:41.459
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:216
    STEP: create the container 01/05/23 19:55:41.464
    STEP: wait for the container to reach Failed 01/05/23 19:55:41.476
    STEP: get the container status 01/05/23 19:55:45.494
    STEP: the container should be terminated 01/05/23 19:55:45.496
    STEP: the termination message should be set 01/05/23 19:55:45.496
    Jan  5 19:55:45.496: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/05/23 19:55:45.496
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:45.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1194" for this suite. 01/05/23 19:55:45.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:45.523
Jan  5 19:55:45.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:55:45.524
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:45.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:45.541
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:55:45.557
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:55:45.865
STEP: Deploying the webhook pod 01/05/23 19:55:45.87
STEP: Wait for the deployment to be ready 01/05/23 19:55:45.879
Jan  5 19:55:45.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 19:55:47.899
STEP: Verifying the service has paired with the endpoint 01/05/23 19:55:47.91
Jan  5 19:55:48.911: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413
STEP: Creating a validating webhook configuration 01/05/23 19:55:48.914
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:55:48.938
STEP: Updating a validating webhook configuration's rules to not include the create operation 01/05/23 19:55:48.95
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:55:48.959
STEP: Patching a validating webhook configuration's rules to include the create operation 01/05/23 19:55:48.967
STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:55:48.976
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:48.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-9567" for this suite. 01/05/23 19:55:49.043
STEP: Destroying namespace "webhook-9567-markers" for this suite. 01/05/23 19:55:49.052
------------------------------
• [3.537 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:45.523
    Jan  5 19:55:45.523: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:55:45.524
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:45.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:45.541
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:55:45.557
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:55:45.865
    STEP: Deploying the webhook pod 01/05/23 19:55:45.87
    STEP: Wait for the deployment to be ready 01/05/23 19:55:45.879
    Jan  5 19:55:45.886: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 19:55:47.899
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:55:47.91
    Jan  5 19:55:48.911: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:413
    STEP: Creating a validating webhook configuration 01/05/23 19:55:48.914
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:55:48.938
    STEP: Updating a validating webhook configuration's rules to not include the create operation 01/05/23 19:55:48.95
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:55:48.959
    STEP: Patching a validating webhook configuration's rules to include the create operation 01/05/23 19:55:48.967
    STEP: Creating a configMap that does not comply to the validation webhook rules 01/05/23 19:55:48.976
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:48.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-9567" for this suite. 01/05/23 19:55:49.043
    STEP: Destroying namespace "webhook-9567-markers" for this suite. 01/05/23 19:55:49.052
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:49.064
Jan  5 19:55:49.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename security-context-test 01/05/23 19:55:49.065
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:49.076
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:49.079
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:347
Jan  5 19:55:49.091: INFO: Waiting up to 5m0s for pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8" in namespace "security-context-test-4921" to be "Succeeded or Failed"
Jan  5 19:55:49.094: INFO: Pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719099ms
Jan  5 19:55:51.098: INFO: Pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007401402s
Jan  5 19:55:53.098: INFO: Pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007906071s
Jan  5 19:55:53.098: INFO: Pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:53.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-4921" for this suite. 01/05/23 19:55:53.102
------------------------------
• [4.044 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:309
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:49.064
    Jan  5 19:55:49.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename security-context-test 01/05/23 19:55:49.065
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:49.076
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:49.079
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:347
    Jan  5 19:55:49.091: INFO: Waiting up to 5m0s for pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8" in namespace "security-context-test-4921" to be "Succeeded or Failed"
    Jan  5 19:55:49.094: INFO: Pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719099ms
    Jan  5 19:55:51.098: INFO: Pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007401402s
    Jan  5 19:55:53.098: INFO: Pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007906071s
    Jan  5 19:55:53.098: INFO: Pod "busybox-user-65534-963b2a4c-9712-4798-ac26-050a06ad90b8" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:53.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-4921" for this suite. 01/05/23 19:55:53.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:53.109
Jan  5 19:55:53.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 19:55:53.111
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:53.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:53.127
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1415
STEP: creating Agnhost RC 01/05/23 19:55:53.131
Jan  5 19:55:53.131: INFO: namespace kubectl-6160
Jan  5 19:55:53.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6160 create -f -'
Jan  5 19:55:53.375: INFO: stderr: ""
Jan  5 19:55:53.375: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 19:55:53.375
Jan  5 19:55:54.385: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 19:55:54.385: INFO: Found 0 / 1
Jan  5 19:55:55.380: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 19:55:55.380: INFO: Found 1 / 1
Jan  5 19:55:55.380: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  5 19:55:55.382: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 19:55:55.382: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 19:55:55.382: INFO: wait on agnhost-primary startup in kubectl-6160 
Jan  5 19:55:55.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6160 logs agnhost-primary-x4wj2 agnhost-primary'
Jan  5 19:55:55.463: INFO: stderr: ""
Jan  5 19:55:55.463: INFO: stdout: "Paused\n"
STEP: exposing RC 01/05/23 19:55:55.463
Jan  5 19:55:55.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6160 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jan  5 19:55:55.552: INFO: stderr: ""
Jan  5 19:55:55.552: INFO: stdout: "service/rm2 exposed\n"
Jan  5 19:55:55.557: INFO: Service rm2 in namespace kubectl-6160 found.
STEP: exposing service 01/05/23 19:55:57.57
Jan  5 19:55:57.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6160 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jan  5 19:55:57.718: INFO: stderr: ""
Jan  5 19:55:57.718: INFO: stdout: "service/rm3 exposed\n"
Jan  5 19:55:57.723: INFO: Service rm3 in namespace kubectl-6160 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 19:55:59.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-6160" for this suite. 01/05/23 19:55:59.733
------------------------------
• [SLOW TEST] [6.628 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1409
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1415

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:53.109
    Jan  5 19:55:53.110: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 19:55:53.111
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:53.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:53.127
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1415
    STEP: creating Agnhost RC 01/05/23 19:55:53.131
    Jan  5 19:55:53.131: INFO: namespace kubectl-6160
    Jan  5 19:55:53.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6160 create -f -'
    Jan  5 19:55:53.375: INFO: stderr: ""
    Jan  5 19:55:53.375: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 19:55:53.375
    Jan  5 19:55:54.385: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 19:55:54.385: INFO: Found 0 / 1
    Jan  5 19:55:55.380: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 19:55:55.380: INFO: Found 1 / 1
    Jan  5 19:55:55.380: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  5 19:55:55.382: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 19:55:55.382: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 19:55:55.382: INFO: wait on agnhost-primary startup in kubectl-6160 
    Jan  5 19:55:55.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6160 logs agnhost-primary-x4wj2 agnhost-primary'
    Jan  5 19:55:55.463: INFO: stderr: ""
    Jan  5 19:55:55.463: INFO: stdout: "Paused\n"
    STEP: exposing RC 01/05/23 19:55:55.463
    Jan  5 19:55:55.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6160 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jan  5 19:55:55.552: INFO: stderr: ""
    Jan  5 19:55:55.552: INFO: stdout: "service/rm2 exposed\n"
    Jan  5 19:55:55.557: INFO: Service rm2 in namespace kubectl-6160 found.
    STEP: exposing service 01/05/23 19:55:57.57
    Jan  5 19:55:57.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-6160 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jan  5 19:55:57.718: INFO: stderr: ""
    Jan  5 19:55:57.718: INFO: stdout: "service/rm3 exposed\n"
    Jan  5 19:55:57.723: INFO: Service rm3 in namespace kubectl-6160 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:55:59.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-6160" for this suite. 01/05/23 19:55:59.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:481
[BeforeEach] [sig-apps] Job
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:55:59.738
Jan  5 19:55:59.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename job 01/05/23 19:55:59.739
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:59.751
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:59.754
[BeforeEach] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:31
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:481
STEP: Creating a job 01/05/23 19:55:59.756
STEP: Ensuring active pods == parallelism 01/05/23 19:55:59.762
STEP: delete a job 01/05/23 19:56:01.766
STEP: deleting Job.batch foo in namespace job-9505, will wait for the garbage collector to delete the pods 01/05/23 19:56:01.766
Jan  5 19:56:01.825: INFO: Deleting Job.batch foo took: 6.347666ms
Jan  5 19:56:01.926: INFO: Terminating Job.batch foo pods took: 100.899937ms
STEP: Ensuring job was deleted 01/05/23 19:56:34.527
[AfterEach] [sig-apps] Job
  test/e2e/framework/node/init/init.go:32
Jan  5 19:56:34.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Job
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Job
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Job
  tear down framework | framework.go:193
STEP: Destroying namespace "job-9505" for this suite. 01/05/23 19:56:34.534
------------------------------
• [SLOW TEST] [34.800 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:55:59.738
    Jan  5 19:55:59.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename job 01/05/23 19:55:59.739
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:55:59.751
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:55:59.754
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:481
    STEP: Creating a job 01/05/23 19:55:59.756
    STEP: Ensuring active pods == parallelism 01/05/23 19:55:59.762
    STEP: delete a job 01/05/23 19:56:01.766
    STEP: deleting Job.batch foo in namespace job-9505, will wait for the garbage collector to delete the pods 01/05/23 19:56:01.766
    Jan  5 19:56:01.825: INFO: Deleting Job.batch foo took: 6.347666ms
    Jan  5 19:56:01.926: INFO: Terminating Job.batch foo pods took: 100.899937ms
    STEP: Ensuring job was deleted 01/05/23 19:56:34.527
    [AfterEach] [sig-apps] Job
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:56:34.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Job
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Job
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Job
      tear down framework | framework.go:193
    STEP: Destroying namespace "job-9505" for this suite. 01/05/23 19:56:34.534
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:56:34.54
Jan  5 19:56:34.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 19:56:34.542
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:34.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:34.557
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 19:56:34.57
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:56:34.911
STEP: Deploying the webhook pod 01/05/23 19:56:34.917
STEP: Wait for the deployment to be ready 01/05/23 19:56:34.926
Jan  5 19:56:34.930: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 19:56:36.94
STEP: Verifying the service has paired with the endpoint 01/05/23 19:56:36.95
Jan  5 19:56:37.950: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117
STEP: fetching the /apis discovery document 01/05/23 19:56:37.953
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/05/23 19:56:37.954
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 19:56:37.954
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/05/23 19:56:37.954
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/05/23 19:56:37.955
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 19:56:37.955
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 19:56:37.957
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:56:37.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-3773" for this suite. 01/05/23 19:56:38.005
STEP: Destroying namespace "webhook-3773-markers" for this suite. 01/05/23 19:56:38.018
------------------------------
• [3.486 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:56:34.54
    Jan  5 19:56:34.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 19:56:34.542
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:34.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:34.557
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 19:56:34.57
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 19:56:34.911
    STEP: Deploying the webhook pod 01/05/23 19:56:34.917
    STEP: Wait for the deployment to be ready 01/05/23 19:56:34.926
    Jan  5 19:56:34.930: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 19:56:36.94
    STEP: Verifying the service has paired with the endpoint 01/05/23 19:56:36.95
    Jan  5 19:56:37.950: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:117
    STEP: fetching the /apis discovery document 01/05/23 19:56:37.953
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 01/05/23 19:56:37.954
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 01/05/23 19:56:37.954
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 01/05/23 19:56:37.954
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 01/05/23 19:56:37.955
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 19:56:37.955
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 01/05/23 19:56:37.957
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:56:37.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-3773" for this suite. 01/05/23 19:56:38.005
    STEP: Destroying namespace "webhook-3773-markers" for this suite. 01/05/23 19:56:38.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:56:38.028
Jan  5 19:56:38.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename dns 01/05/23 19:56:38.03
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:38.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:38.046
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/05/23 19:56:38.049
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 01/05/23 19:56:38.05
STEP: creating a pod to probe DNS 01/05/23 19:56:38.05
STEP: submitting the pod to kubernetes 01/05/23 19:56:38.05
Jan  5 19:56:38.062: INFO: Waiting up to 15m0s for pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5" in namespace "dns-5210" to be "running"
Jan  5 19:56:38.066: INFO: Pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.939266ms
Jan  5 19:56:40.070: INFO: Pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008470207s
Jan  5 19:56:42.070: INFO: Pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5": Phase="Running", Reason="", readiness=true. Elapsed: 4.008794871s
Jan  5 19:56:42.070: INFO: Pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:56:42.07
STEP: looking for the results for each expected name from probers 01/05/23 19:56:42.073
Jan  5 19:56:42.097: INFO: DNS probes using dns-5210/dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5 succeeded

STEP: deleting the pod 01/05/23 19:56:42.097
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  5 19:56:42.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-5210" for this suite. 01/05/23 19:56:42.116
------------------------------
• [4.092 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:56:38.028
    Jan  5 19:56:38.028: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename dns 01/05/23 19:56:38.03
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:38.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:38.046
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/05/23 19:56:38.049
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     01/05/23 19:56:38.05
    STEP: creating a pod to probe DNS 01/05/23 19:56:38.05
    STEP: submitting the pod to kubernetes 01/05/23 19:56:38.05
    Jan  5 19:56:38.062: INFO: Waiting up to 15m0s for pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5" in namespace "dns-5210" to be "running"
    Jan  5 19:56:38.066: INFO: Pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.939266ms
    Jan  5 19:56:40.070: INFO: Pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008470207s
    Jan  5 19:56:42.070: INFO: Pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5": Phase="Running", Reason="", readiness=true. Elapsed: 4.008794871s
    Jan  5 19:56:42.070: INFO: Pod "dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:56:42.07
    STEP: looking for the results for each expected name from probers 01/05/23 19:56:42.073
    Jan  5 19:56:42.097: INFO: DNS probes using dns-5210/dns-test-959ea287-4e68-42b5-ac5d-8568e6150ac5 succeeded

    STEP: deleting the pod 01/05/23 19:56:42.097
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:56:42.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-5210" for this suite. 01/05/23 19:56:42.116
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:56:42.125
Jan  5 19:56:42.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename dns 01/05/23 19:56:42.127
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:42.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:42.142
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/05/23 19:56:42.144
Jan  5 19:56:42.154: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1761  910ea493-7533-4f66-b16b-721365b94527 88863 0 2023-01-05 19:56:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-05 19:56:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzxr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzxr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 19:56:42.154: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1761" to be "running and ready"
Jan  5 19:56:42.158: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.754081ms
Jan  5 19:56:42.158: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:56:44.161: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007463658s
Jan  5 19:56:44.161: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jan  5 19:56:44.161: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 01/05/23 19:56:44.162
Jan  5 19:56:44.162: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1761 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:56:44.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:56:44.163: INFO: ExecWithOptions: Clientset creation
Jan  5 19:56:44.163: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/dns-1761/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 01/05/23 19:56:44.241
Jan  5 19:56:44.242: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1761 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 19:56:44.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 19:56:44.243: INFO: ExecWithOptions: Clientset creation
Jan  5 19:56:44.243: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/dns-1761/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 19:56:44.320: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  5 19:56:44.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-1761" for this suite. 01/05/23 19:56:44.336
------------------------------
• [2.216 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:56:42.125
    Jan  5 19:56:42.125: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename dns 01/05/23 19:56:42.127
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:42.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:42.142
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 01/05/23 19:56:42.144
    Jan  5 19:56:42.154: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1761  910ea493-7533-4f66-b16b-721365b94527 88863 0 2023-01-05 19:56:42 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-01-05 19:56:42 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lzxr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lzxr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 19:56:42.154: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-1761" to be "running and ready"
    Jan  5 19:56:42.158: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 3.754081ms
    Jan  5 19:56:42.158: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:56:44.161: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007463658s
    Jan  5 19:56:44.161: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jan  5 19:56:44.161: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 01/05/23 19:56:44.162
    Jan  5 19:56:44.162: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1761 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:56:44.162: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:56:44.163: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:56:44.163: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/dns-1761/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 01/05/23 19:56:44.241
    Jan  5 19:56:44.242: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1761 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 19:56:44.242: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 19:56:44.243: INFO: ExecWithOptions: Clientset creation
    Jan  5 19:56:44.243: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/dns-1761/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 19:56:44.320: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:56:44.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-1761" for this suite. 01/05/23 19:56:44.336
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:56:44.347
Jan  5 19:56:44.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename daemonsets 01/05/23 19:56:44.349
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:44.359
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:44.363
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194
Jan  5 19:56:44.381: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 01/05/23 19:56:44.386
Jan  5 19:56:44.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:56:44.389: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 01/05/23 19:56:44.389
Jan  5 19:56:44.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:56:44.410: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
Jan  5 19:56:45.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:56:45.414: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
Jan  5 19:56:46.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 19:56:46.414: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 01/05/23 19:56:46.417
Jan  5 19:56:46.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 19:56:46.437: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Jan  5 19:56:47.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:56:47.441: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/05/23 19:56:47.441
Jan  5 19:56:47.450: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:56:47.450: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
Jan  5 19:56:48.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:56:48.454: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
Jan  5 19:56:49.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:56:49.454: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
Jan  5 19:56:50.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jan  5 19:56:50.454: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:56:50.459
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7552, will wait for the garbage collector to delete the pods 01/05/23 19:56:50.459
Jan  5 19:56:50.517: INFO: Deleting DaemonSet.extensions daemon-set took: 5.241684ms
Jan  5 19:56:50.617: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.249034ms
Jan  5 19:56:52.921: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 19:56:52.921: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 19:56:52.923: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89018"},"items":null}

Jan  5 19:56:52.926: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89018"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:56:52.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-7552" for this suite. 01/05/23 19:56:53.019
------------------------------
• [SLOW TEST] [8.689 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:56:44.347
    Jan  5 19:56:44.347: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename daemonsets 01/05/23 19:56:44.349
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:44.359
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:44.363
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:194
    Jan  5 19:56:44.381: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 01/05/23 19:56:44.386
    Jan  5 19:56:44.389: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:56:44.389: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 01/05/23 19:56:44.389
    Jan  5 19:56:44.410: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:56:44.410: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
    Jan  5 19:56:45.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:56:45.414: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
    Jan  5 19:56:46.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 19:56:46.414: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 01/05/23 19:56:46.417
    Jan  5 19:56:46.437: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 19:56:46.437: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Jan  5 19:56:47.441: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:56:47.441: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 01/05/23 19:56:47.441
    Jan  5 19:56:47.450: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:56:47.450: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
    Jan  5 19:56:48.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:56:48.454: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
    Jan  5 19:56:49.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:56:49.454: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
    Jan  5 19:56:50.454: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jan  5 19:56:50.454: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 19:56:50.459
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7552, will wait for the garbage collector to delete the pods 01/05/23 19:56:50.459
    Jan  5 19:56:50.517: INFO: Deleting DaemonSet.extensions daemon-set took: 5.241684ms
    Jan  5 19:56:50.617: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.249034ms
    Jan  5 19:56:52.921: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 19:56:52.921: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 19:56:52.923: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"89018"},"items":null}

    Jan  5 19:56:52.926: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"89018"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:56:52.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-7552" for this suite. 01/05/23 19:56:53.019
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:56:53.039
Jan  5 19:56:53.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename conformance-tests 01/05/23 19:56:53.04
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:53.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:53.069
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:31
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 01/05/23 19:56:53.072
Jan  5 19:56:53.072: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/node/init/init.go:32
Jan  5 19:56:53.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-architecture] Conformance Tests
  tear down framework | framework.go:193
STEP: Destroying namespace "conformance-tests-4128" for this suite. 01/05/23 19:56:53.082
------------------------------
• [0.052 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:56:53.039
    Jan  5 19:56:53.039: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename conformance-tests 01/05/23 19:56:53.04
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:53.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:53.069
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:31
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 01/05/23 19:56:53.072
    Jan  5 19:56:53.072: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:56:53.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-architecture] Conformance Tests
      tear down framework | framework.go:193
    STEP: Destroying namespace "conformance-tests-4128" for this suite. 01/05/23 19:56:53.082
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:56:53.094
Jan  5 19:56:53.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename server-version 01/05/23 19:56:53.096
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:53.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:53.112
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:31
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 01/05/23 19:56:53.115
STEP: Confirm major version 01/05/23 19:56:53.116
Jan  5 19:56:53.117: INFO: Major version: 1
STEP: Confirm minor version 01/05/23 19:56:53.117
Jan  5 19:56:53.117: INFO: cleanMinorVersion: 26
Jan  5 19:56:53.117: INFO: Minor version: 26
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/node/init/init.go:32
Jan  5 19:56:53.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] server version
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] server version
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] server version
  tear down framework | framework.go:193
STEP: Destroying namespace "server-version-59" for this suite. 01/05/23 19:56:53.123
------------------------------
• [0.036 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:56:53.094
    Jan  5 19:56:53.095: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename server-version 01/05/23 19:56:53.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:53.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:53.112
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:31
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 01/05/23 19:56:53.115
    STEP: Confirm major version 01/05/23 19:56:53.116
    Jan  5 19:56:53.117: INFO: Major version: 1
    STEP: Confirm minor version 01/05/23 19:56:53.117
    Jan  5 19:56:53.117: INFO: cleanMinorVersion: 26
    Jan  5 19:56:53.117: INFO: Minor version: 26
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:56:53.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] server version
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] server version
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] server version
      tear down framework | framework.go:193
    STEP: Destroying namespace "server-version-59" for this suite. 01/05/23 19:56:53.123
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:56:53.134
Jan  5 19:56:53.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 19:56:53.136
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:53.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:53.15
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100
STEP: Counting existing ResourceQuota 01/05/23 19:56:53.152
STEP: Creating a ResourceQuota 01/05/23 19:56:58.159
STEP: Ensuring resource quota status is calculated 01/05/23 19:56:58.166
STEP: Creating a Service 01/05/23 19:57:00.172
STEP: Creating a NodePort Service 01/05/23 19:57:00.212
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/05/23 19:57:00.249
STEP: Ensuring resource quota status captures service creation 01/05/23 19:57:00.269
STEP: Deleting Services 01/05/23 19:57:02.274
STEP: Ensuring resource quota status released usage 01/05/23 19:57:02.298
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 19:57:04.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4800" for this suite. 01/05/23 19:57:04.306
------------------------------
• [SLOW TEST] [11.177 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:56:53.134
    Jan  5 19:56:53.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 19:56:53.136
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:56:53.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:56:53.15
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:100
    STEP: Counting existing ResourceQuota 01/05/23 19:56:53.152
    STEP: Creating a ResourceQuota 01/05/23 19:56:58.159
    STEP: Ensuring resource quota status is calculated 01/05/23 19:56:58.166
    STEP: Creating a Service 01/05/23 19:57:00.172
    STEP: Creating a NodePort Service 01/05/23 19:57:00.212
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 01/05/23 19:57:00.249
    STEP: Ensuring resource quota status captures service creation 01/05/23 19:57:00.269
    STEP: Deleting Services 01/05/23 19:57:02.274
    STEP: Ensuring resource quota status released usage 01/05/23 19:57:02.298
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:57:04.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4800" for this suite. 01/05/23 19:57:04.306
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:57:04.313
Jan  5 19:57:04.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 19:57:04.315
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:04.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:04.332
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444
Jan  5 19:57:04.345: INFO: Waiting up to 5m0s for pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a" in namespace "pods-8080" to be "running and ready"
Jan  5 19:57:04.348: INFO: Pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302105ms
Jan  5 19:57:04.348: INFO: The phase of Pod server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:57:06.351: INFO: Pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006488684s
Jan  5 19:57:06.351: INFO: The phase of Pod server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:57:08.353: INFO: Pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a": Phase="Running", Reason="", readiness=true. Elapsed: 4.008298234s
Jan  5 19:57:08.353: INFO: The phase of Pod server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a is Running (Ready = true)
Jan  5 19:57:08.353: INFO: Pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a" satisfied condition "running and ready"
Jan  5 19:57:08.377: INFO: Waiting up to 5m0s for pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc" in namespace "pods-8080" to be "Succeeded or Failed"
Jan  5 19:57:08.382: INFO: Pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318155ms
Jan  5 19:57:10.386: INFO: Pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00806486s
Jan  5 19:57:12.386: INFO: Pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008016387s
STEP: Saw pod success 01/05/23 19:57:12.386
Jan  5 19:57:12.387: INFO: Pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc" satisfied condition "Succeeded or Failed"
Jan  5 19:57:12.390: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc container env3cont: <nil>
STEP: delete the pod 01/05/23 19:57:12.399
Jan  5 19:57:12.411: INFO: Waiting for pod client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc to disappear
Jan  5 19:57:12.415: INFO: Pod client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 19:57:12.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-8080" for this suite. 01/05/23 19:57:12.419
------------------------------
• [SLOW TEST] [8.112 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:444

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:57:04.313
    Jan  5 19:57:04.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 19:57:04.315
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:04.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:04.332
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:444
    Jan  5 19:57:04.345: INFO: Waiting up to 5m0s for pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a" in namespace "pods-8080" to be "running and ready"
    Jan  5 19:57:04.348: INFO: Pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.302105ms
    Jan  5 19:57:04.348: INFO: The phase of Pod server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:57:06.351: INFO: Pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006488684s
    Jan  5 19:57:06.351: INFO: The phase of Pod server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:57:08.353: INFO: Pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a": Phase="Running", Reason="", readiness=true. Elapsed: 4.008298234s
    Jan  5 19:57:08.353: INFO: The phase of Pod server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a is Running (Ready = true)
    Jan  5 19:57:08.353: INFO: Pod "server-envvars-9bc00e1e-06a4-4ce5-bbd9-bbd32f2ebe1a" satisfied condition "running and ready"
    Jan  5 19:57:08.377: INFO: Waiting up to 5m0s for pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc" in namespace "pods-8080" to be "Succeeded or Failed"
    Jan  5 19:57:08.382: INFO: Pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.318155ms
    Jan  5 19:57:10.386: INFO: Pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00806486s
    Jan  5 19:57:12.386: INFO: Pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008016387s
    STEP: Saw pod success 01/05/23 19:57:12.386
    Jan  5 19:57:12.387: INFO: Pod "client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc" satisfied condition "Succeeded or Failed"
    Jan  5 19:57:12.390: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc container env3cont: <nil>
    STEP: delete the pod 01/05/23 19:57:12.399
    Jan  5 19:57:12.411: INFO: Waiting for pod client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc to disappear
    Jan  5 19:57:12.415: INFO: Pod client-envvars-f772f611-2bba-4eb6-bd04-e8310b0b56dc no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:57:12.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-8080" for this suite. 01/05/23 19:57:12.419
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:57:12.427
Jan  5 19:57:12.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename namespaces 01/05/23 19:57:12.429
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:12.44
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:12.443
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243
STEP: Creating a test namespace 01/05/23 19:57:12.447
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:12.459
STEP: Creating a pod in the namespace 01/05/23 19:57:12.462
STEP: Waiting for the pod to have running status 01/05/23 19:57:12.472
Jan  5 19:57:12.472: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9945" to be "running"
Jan  5 19:57:12.475: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.204896ms
Jan  5 19:57:14.479: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00695261s
Jan  5 19:57:14.479: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 01/05/23 19:57:14.479
STEP: Waiting for the namespace to be removed. 01/05/23 19:57:14.484
STEP: Recreating the namespace 01/05/23 19:57:25.487
STEP: Verifying there are no pods in the namespace 01/05/23 19:57:25.499
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:57:25.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-9458" for this suite. 01/05/23 19:57:25.505
STEP: Destroying namespace "nsdeletetest-9945" for this suite. 01/05/23 19:57:25.51
Jan  5 19:57:25.512: INFO: Namespace nsdeletetest-9945 was already deleted
STEP: Destroying namespace "nsdeletetest-6138" for this suite. 01/05/23 19:57:25.512
------------------------------
• [SLOW TEST] [13.089 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:243

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:57:12.427
    Jan  5 19:57:12.427: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename namespaces 01/05/23 19:57:12.429
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:12.44
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:12.443
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:243
    STEP: Creating a test namespace 01/05/23 19:57:12.447
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:12.459
    STEP: Creating a pod in the namespace 01/05/23 19:57:12.462
    STEP: Waiting for the pod to have running status 01/05/23 19:57:12.472
    Jan  5 19:57:12.472: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-9945" to be "running"
    Jan  5 19:57:12.475: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.204896ms
    Jan  5 19:57:14.479: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.00695261s
    Jan  5 19:57:14.479: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 01/05/23 19:57:14.479
    STEP: Waiting for the namespace to be removed. 01/05/23 19:57:14.484
    STEP: Recreating the namespace 01/05/23 19:57:25.487
    STEP: Verifying there are no pods in the namespace 01/05/23 19:57:25.499
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:57:25.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-9458" for this suite. 01/05/23 19:57:25.505
    STEP: Destroying namespace "nsdeletetest-9945" for this suite. 01/05/23 19:57:25.51
    Jan  5 19:57:25.512: INFO: Namespace nsdeletetest-9945 was already deleted
    STEP: Destroying namespace "nsdeletetest-6138" for this suite. 01/05/23 19:57:25.512
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:57:25.517
Jan  5 19:57:25.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename dns 01/05/23 19:57:25.518
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:25.528
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:25.531
[BeforeEach] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:31
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 01/05/23 19:57:25.535
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9657.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9657.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 01/05/23 19:57:25.541
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9657.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9657.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 01/05/23 19:57:25.541
STEP: creating a pod to probe DNS 01/05/23 19:57:25.541
STEP: submitting the pod to kubernetes 01/05/23 19:57:25.542
Jan  5 19:57:25.555: INFO: Waiting up to 15m0s for pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c" in namespace "dns-9657" to be "running"
Jan  5 19:57:25.560: INFO: Pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.554726ms
Jan  5 19:57:27.565: INFO: Pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01040716s
Jan  5 19:57:29.565: INFO: Pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c": Phase="Running", Reason="", readiness=true. Elapsed: 4.009781295s
Jan  5 19:57:29.565: INFO: Pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c" satisfied condition "running"
STEP: retrieving the pod 01/05/23 19:57:29.565
STEP: looking for the results for each expected name from probers 01/05/23 19:57:29.568
Jan  5 19:57:29.589: INFO: DNS probes using dns-9657/dns-test-59ee0d3d-c244-4473-bd25-9965391b080c succeeded

STEP: deleting the pod 01/05/23 19:57:29.589
STEP: deleting the test headless service 01/05/23 19:57:29.617
[AfterEach] [sig-network] DNS
  test/e2e/framework/node/init/init.go:32
Jan  5 19:57:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] DNS
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] DNS
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] DNS
  tear down framework | framework.go:193
STEP: Destroying namespace "dns-9657" for this suite. 01/05/23 19:57:29.641
------------------------------
• [4.131 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:57:25.517
    Jan  5 19:57:25.517: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename dns 01/05/23 19:57:25.518
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:25.528
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:25.531
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 01/05/23 19:57:25.535
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9657.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9657.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     01/05/23 19:57:25.541
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9657.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9657.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     01/05/23 19:57:25.541
    STEP: creating a pod to probe DNS 01/05/23 19:57:25.541
    STEP: submitting the pod to kubernetes 01/05/23 19:57:25.542
    Jan  5 19:57:25.555: INFO: Waiting up to 15m0s for pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c" in namespace "dns-9657" to be "running"
    Jan  5 19:57:25.560: INFO: Pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.554726ms
    Jan  5 19:57:27.565: INFO: Pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01040716s
    Jan  5 19:57:29.565: INFO: Pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c": Phase="Running", Reason="", readiness=true. Elapsed: 4.009781295s
    Jan  5 19:57:29.565: INFO: Pod "dns-test-59ee0d3d-c244-4473-bd25-9965391b080c" satisfied condition "running"
    STEP: retrieving the pod 01/05/23 19:57:29.565
    STEP: looking for the results for each expected name from probers 01/05/23 19:57:29.568
    Jan  5 19:57:29.589: INFO: DNS probes using dns-9657/dns-test-59ee0d3d-c244-4473-bd25-9965391b080c succeeded

    STEP: deleting the pod 01/05/23 19:57:29.589
    STEP: deleting the test headless service 01/05/23 19:57:29.617
    [AfterEach] [sig-network] DNS
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:57:29.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] DNS
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] DNS
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] DNS
      tear down framework | framework.go:193
    STEP: Destroying namespace "dns-9657" for this suite. 01/05/23 19:57:29.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:57:29.658
Jan  5 19:57:29.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-pred 01/05/23 19:57:29.663
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:29.675
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:29.679
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  5 19:57:29.681: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 19:57:29.687: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 19:57:29.690: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-16pz before test
Jan  5 19:57:29.698: INFO: fluentbit-gke-5jd2q from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.698: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 19:57:29.698: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 19:57:29.698: INFO: gke-metrics-agent-vk8fs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.698: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 19:57:29.698: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 19:57:29.698: INFO: konnectivity-agent-57b5db4478-nsnb9 from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.698: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 19:57:29.698: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-16pz from kube-system started at 2023-01-05 17:43:36 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.698: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 19:57:29.698: INFO: metrics-server-v0.5.2-7b9768bdd5-w2lc6 from kube-system started at 2023-01-05 17:57:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.698: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 19:57:29.698: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan  5 19:57:29.698: INFO: pdcsi-node-2nkb4 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.698: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 19:57:29.698: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 19:57:29.698: INFO: sonobuoy from sonobuoy started at 2023-01-05 19:01:43 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.698: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 19:57:29.698: INFO: sonobuoy-e2e-job-7d2488470a1c477d from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.699: INFO: 	Container e2e ready: true, restart count 0
Jan  5 19:57:29.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 19:57:29.699: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 19:57:29.699: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 19:57:29.699: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-dbpc before test
Jan  5 19:57:29.706: INFO: event-exporter-gke-6dc4db644f-dh2bd from kube-system started at 2023-01-05 17:46:55 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.706: INFO: 	Container event-exporter ready: true, restart count 0
Jan  5 19:57:29.706: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Jan  5 19:57:29.706: INFO: fluentbit-gke-xzr85 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.706: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 19:57:29.706: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 19:57:29.706: INFO: gke-metrics-agent-hc4fg from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.706: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 19:57:29.707: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 19:57:29.707: INFO: konnectivity-agent-57b5db4478-7sktm from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.707: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 19:57:29.707: INFO: konnectivity-agent-autoscaler-797b8755f5-bbvjz from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.707: INFO: 	Container autoscaler ready: true, restart count 0
Jan  5 19:57:29.707: INFO: kube-dns-549f79cb95-5mb7d from kube-system started at 2023-01-05 17:46:55 +0000 UTC (4 container statuses recorded)
Jan  5 19:57:29.707: INFO: 	Container dnsmasq ready: true, restart count 0
Jan  5 19:57:29.707: INFO: 	Container kubedns ready: true, restart count 0
Jan  5 19:57:29.708: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jan  5 19:57:29.708: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 19:57:29.708: INFO: kube-dns-autoscaler-758c4689b9-pl85w from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.708: INFO: 	Container autoscaler ready: true, restart count 0
Jan  5 19:57:29.708: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.708: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 19:57:29.708: INFO: l7-default-backend-7bcfd7bc79-lqt6s from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.708: INFO: 	Container default-http-backend ready: true, restart count 0
Jan  5 19:57:29.709: INFO: pdcsi-node-dq4p5 from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.709: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 19:57:29.709: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 19:57:29.709: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 19:57:29.709: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 19:57:29.709: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-qmj7 before test
Jan  5 19:57:29.715: INFO: fluentbit-gke-n7chw from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.715: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 19:57:29.715: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 19:57:29.715: INFO: gke-metrics-agent-w95hs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.716: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 19:57:29.716: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 19:57:29.716: INFO: konnectivity-agent-57b5db4478-b22kt from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.716: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 19:57:29.716: INFO: kube-dns-549f79cb95-7zg82 from kube-system started at 2023-01-05 17:47:07 +0000 UTC (4 container statuses recorded)
Jan  5 19:57:29.716: INFO: 	Container dnsmasq ready: true, restart count 0
Jan  5 19:57:29.716: INFO: 	Container kubedns ready: true, restart count 0
Jan  5 19:57:29.716: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jan  5 19:57:29.717: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 19:57:29.717: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
Jan  5 19:57:29.717: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 19:57:29.717: INFO: pdcsi-node-vdldr from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.717: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 19:57:29.717: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 19:57:29.717: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 19:57:29.718: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 19:57:29.718: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331
STEP: verifying the node has the label node gke-gke-1-26-default-pool-05283374-16pz 01/05/23 19:57:29.743
STEP: verifying the node has the label node gke-gke-1-26-default-pool-05283374-dbpc 01/05/23 19:57:29.76
STEP: verifying the node has the label node gke-gke-1-26-default-pool-05283374-qmj7 01/05/23 19:57:29.782
Jan  5 19:57:29.798: INFO: Pod event-exporter-gke-6dc4db644f-dh2bd requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod fluentbit-gke-5jd2q requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.799: INFO: Pod fluentbit-gke-n7chw requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-qmj7
Jan  5 19:57:29.799: INFO: Pod fluentbit-gke-xzr85 requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod gke-metrics-agent-hc4fg requesting resource cpu=6m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod gke-metrics-agent-vk8fs requesting resource cpu=6m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.799: INFO: Pod gke-metrics-agent-w95hs requesting resource cpu=6m on Node gke-gke-1-26-default-pool-05283374-qmj7
Jan  5 19:57:29.799: INFO: Pod konnectivity-agent-57b5db4478-7sktm requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod konnectivity-agent-57b5db4478-b22kt requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-qmj7
Jan  5 19:57:29.799: INFO: Pod konnectivity-agent-57b5db4478-nsnb9 requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.799: INFO: Pod konnectivity-agent-autoscaler-797b8755f5-bbvjz requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod kube-dns-549f79cb95-5mb7d requesting resource cpu=260m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod kube-dns-549f79cb95-7zg82 requesting resource cpu=260m on Node gke-gke-1-26-default-pool-05283374-qmj7
Jan  5 19:57:29.799: INFO: Pod kube-dns-autoscaler-758c4689b9-pl85w requesting resource cpu=20m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod kube-proxy-gke-gke-1-26-default-pool-05283374-16pz requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.799: INFO: Pod kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-qmj7
Jan  5 19:57:29.799: INFO: Pod l7-default-backend-7bcfd7bc79-lqt6s requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod metrics-server-v0.5.2-7b9768bdd5-w2lc6 requesting resource cpu=48m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.799: INFO: Pod pdcsi-node-2nkb4 requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.799: INFO: Pod pdcsi-node-dq4p5 requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod pdcsi-node-vdldr requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-qmj7
Jan  5 19:57:29.799: INFO: Pod sonobuoy requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.799: INFO: Pod sonobuoy-e2e-job-7d2488470a1c477d requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.799: INFO: Pod sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.799: INFO: Pod sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-qmj7
Jan  5 19:57:29.799: INFO: Pod sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-16pz
STEP: Starting Pods to consume most of the cluster CPU. 01/05/23 19:57:29.799
Jan  5 19:57:29.799: INFO: Creating a pod which consumes cpu=466m on Node gke-gke-1-26-default-pool-05283374-16pz
Jan  5 19:57:29.812: INFO: Creating a pod which consumes cpu=289m on Node gke-gke-1-26-default-pool-05283374-dbpc
Jan  5 19:57:29.823: INFO: Creating a pod which consumes cpu=317m on Node gke-gke-1-26-default-pool-05283374-qmj7
Jan  5 19:57:29.837: INFO: Waiting up to 5m0s for pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3" in namespace "sched-pred-2749" to be "running"
Jan  5 19:57:29.845: INFO: Pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.81874ms
Jan  5 19:57:31.849: INFO: Pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012032835s
Jan  5 19:57:33.851: INFO: Pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3": Phase="Running", Reason="", readiness=true. Elapsed: 4.0138497s
Jan  5 19:57:33.851: INFO: Pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3" satisfied condition "running"
Jan  5 19:57:33.851: INFO: Waiting up to 5m0s for pod "filler-pod-cc73c171-f395-495e-96a9-772675dd5efe" in namespace "sched-pred-2749" to be "running"
Jan  5 19:57:33.854: INFO: Pod "filler-pod-cc73c171-f395-495e-96a9-772675dd5efe": Phase="Running", Reason="", readiness=true. Elapsed: 2.748808ms
Jan  5 19:57:33.854: INFO: Pod "filler-pod-cc73c171-f395-495e-96a9-772675dd5efe" satisfied condition "running"
Jan  5 19:57:33.854: INFO: Waiting up to 5m0s for pod "filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c" in namespace "sched-pred-2749" to be "running"
Jan  5 19:57:33.857: INFO: Pod "filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.695655ms
Jan  5 19:57:33.857: INFO: Pod "filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 01/05/23 19:57:33.857
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3.173781eef004af90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2749/filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3 to gke-gke-1-26-default-pool-05283374-16pz] 01/05/23 19:57:33.861
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3.173781ef3b54d42f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/05/23 19:57:33.861
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3.173781ef3c4ffd97], Reason = [Created], Message = [Created container filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3] 01/05/23 19:57:33.862
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3.173781ef675a4fa3], Reason = [Started], Message = [Started container filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3] 01/05/23 19:57:33.862
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c.173781eef1c95f2d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2749/filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c to gke-gke-1-26-default-pool-05283374-qmj7] 01/05/23 19:57:33.862
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c.173781ef47e193d2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/05/23 19:57:33.862
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c.173781ef49713585], Reason = [Created], Message = [Created container filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c] 01/05/23 19:57:33.862
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c.173781ef8e29b70a], Reason = [Started], Message = [Started container filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c] 01/05/23 19:57:33.863
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc73c171-f395-495e-96a9-772675dd5efe.173781eef0cdbc2e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2749/filler-pod-cc73c171-f395-495e-96a9-772675dd5efe to gke-gke-1-26-default-pool-05283374-dbpc] 01/05/23 19:57:33.863
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc73c171-f395-495e-96a9-772675dd5efe.173781ef51f5c7ad], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/05/23 19:57:33.863
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc73c171-f395-495e-96a9-772675dd5efe.173781ef5349c629], Reason = [Created], Message = [Created container filler-pod-cc73c171-f395-495e-96a9-772675dd5efe] 01/05/23 19:57:33.863
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc73c171-f395-495e-96a9-772675dd5efe.173781ef9af45ba8], Reason = [Started], Message = [Started container filler-pod-cc73c171-f395-495e-96a9-772675dd5efe] 01/05/23 19:57:33.864
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.173781efe19899ef], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 01/05/23 19:57:33.881
STEP: removing the label node off the node gke-gke-1-26-default-pool-05283374-16pz 01/05/23 19:57:34.88
STEP: verifying the node doesn't have the label node 01/05/23 19:57:34.895
STEP: removing the label node off the node gke-gke-1-26-default-pool-05283374-dbpc 01/05/23 19:57:34.903
STEP: verifying the node doesn't have the label node 01/05/23 19:57:34.923
STEP: removing the label node off the node gke-gke-1-26-default-pool-05283374-qmj7 01/05/23 19:57:34.928
STEP: verifying the node doesn't have the label node 01/05/23 19:57:34.944
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:57:34.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-2749" for this suite. 01/05/23 19:57:34.954
------------------------------
• [SLOW TEST] [5.310 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:331

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:57:29.658
    Jan  5 19:57:29.659: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-pred 01/05/23 19:57:29.663
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:29.675
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:29.679
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  5 19:57:29.681: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 19:57:29.687: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 19:57:29.690: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-16pz before test
    Jan  5 19:57:29.698: INFO: fluentbit-gke-5jd2q from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.698: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: gke-metrics-agent-vk8fs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.698: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: konnectivity-agent-57b5db4478-nsnb9 from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.698: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-16pz from kube-system started at 2023-01-05 17:43:36 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.698: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: metrics-server-v0.5.2-7b9768bdd5-w2lc6 from kube-system started at 2023-01-05 17:57:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.698: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: pdcsi-node-2nkb4 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.698: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: sonobuoy from sonobuoy started at 2023-01-05 19:01:43 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.698: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 19:57:29.698: INFO: sonobuoy-e2e-job-7d2488470a1c477d from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.699: INFO: 	Container e2e ready: true, restart count 0
    Jan  5 19:57:29.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 19:57:29.699: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.699: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 19:57:29.699: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 19:57:29.699: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-dbpc before test
    Jan  5 19:57:29.706: INFO: event-exporter-gke-6dc4db644f-dh2bd from kube-system started at 2023-01-05 17:46:55 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.706: INFO: 	Container event-exporter ready: true, restart count 0
    Jan  5 19:57:29.706: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
    Jan  5 19:57:29.706: INFO: fluentbit-gke-xzr85 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.706: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 19:57:29.706: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 19:57:29.706: INFO: gke-metrics-agent-hc4fg from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.706: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 19:57:29.707: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 19:57:29.707: INFO: konnectivity-agent-57b5db4478-7sktm from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.707: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 19:57:29.707: INFO: konnectivity-agent-autoscaler-797b8755f5-bbvjz from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.707: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  5 19:57:29.707: INFO: kube-dns-549f79cb95-5mb7d from kube-system started at 2023-01-05 17:46:55 +0000 UTC (4 container statuses recorded)
    Jan  5 19:57:29.707: INFO: 	Container dnsmasq ready: true, restart count 0
    Jan  5 19:57:29.707: INFO: 	Container kubedns ready: true, restart count 0
    Jan  5 19:57:29.708: INFO: 	Container prometheus-to-sd ready: true, restart count 0
    Jan  5 19:57:29.708: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 19:57:29.708: INFO: kube-dns-autoscaler-758c4689b9-pl85w from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.708: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  5 19:57:29.708: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.708: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 19:57:29.708: INFO: l7-default-backend-7bcfd7bc79-lqt6s from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.708: INFO: 	Container default-http-backend ready: true, restart count 0
    Jan  5 19:57:29.709: INFO: pdcsi-node-dq4p5 from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.709: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 19:57:29.709: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 19:57:29.709: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.709: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 19:57:29.709: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 19:57:29.709: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-qmj7 before test
    Jan  5 19:57:29.715: INFO: fluentbit-gke-n7chw from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.715: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 19:57:29.715: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 19:57:29.715: INFO: gke-metrics-agent-w95hs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.716: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 19:57:29.716: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 19:57:29.716: INFO: konnectivity-agent-57b5db4478-b22kt from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.716: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 19:57:29.716: INFO: kube-dns-549f79cb95-7zg82 from kube-system started at 2023-01-05 17:47:07 +0000 UTC (4 container statuses recorded)
    Jan  5 19:57:29.716: INFO: 	Container dnsmasq ready: true, restart count 0
    Jan  5 19:57:29.716: INFO: 	Container kubedns ready: true, restart count 0
    Jan  5 19:57:29.716: INFO: 	Container prometheus-to-sd ready: true, restart count 0
    Jan  5 19:57:29.717: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 19:57:29.717: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
    Jan  5 19:57:29.717: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 19:57:29.717: INFO: pdcsi-node-vdldr from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.717: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 19:57:29.717: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 19:57:29.717: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 19:57:29.718: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 19:57:29.718: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:331
    STEP: verifying the node has the label node gke-gke-1-26-default-pool-05283374-16pz 01/05/23 19:57:29.743
    STEP: verifying the node has the label node gke-gke-1-26-default-pool-05283374-dbpc 01/05/23 19:57:29.76
    STEP: verifying the node has the label node gke-gke-1-26-default-pool-05283374-qmj7 01/05/23 19:57:29.782
    Jan  5 19:57:29.798: INFO: Pod event-exporter-gke-6dc4db644f-dh2bd requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod fluentbit-gke-5jd2q requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.799: INFO: Pod fluentbit-gke-n7chw requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-qmj7
    Jan  5 19:57:29.799: INFO: Pod fluentbit-gke-xzr85 requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod gke-metrics-agent-hc4fg requesting resource cpu=6m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod gke-metrics-agent-vk8fs requesting resource cpu=6m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.799: INFO: Pod gke-metrics-agent-w95hs requesting resource cpu=6m on Node gke-gke-1-26-default-pool-05283374-qmj7
    Jan  5 19:57:29.799: INFO: Pod konnectivity-agent-57b5db4478-7sktm requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod konnectivity-agent-57b5db4478-b22kt requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-qmj7
    Jan  5 19:57:29.799: INFO: Pod konnectivity-agent-57b5db4478-nsnb9 requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.799: INFO: Pod konnectivity-agent-autoscaler-797b8755f5-bbvjz requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod kube-dns-549f79cb95-5mb7d requesting resource cpu=260m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod kube-dns-549f79cb95-7zg82 requesting resource cpu=260m on Node gke-gke-1-26-default-pool-05283374-qmj7
    Jan  5 19:57:29.799: INFO: Pod kube-dns-autoscaler-758c4689b9-pl85w requesting resource cpu=20m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod kube-proxy-gke-gke-1-26-default-pool-05283374-16pz requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.799: INFO: Pod kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 requesting resource cpu=100m on Node gke-gke-1-26-default-pool-05283374-qmj7
    Jan  5 19:57:29.799: INFO: Pod l7-default-backend-7bcfd7bc79-lqt6s requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod metrics-server-v0.5.2-7b9768bdd5-w2lc6 requesting resource cpu=48m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.799: INFO: Pod pdcsi-node-2nkb4 requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.799: INFO: Pod pdcsi-node-dq4p5 requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod pdcsi-node-vdldr requesting resource cpu=10m on Node gke-gke-1-26-default-pool-05283374-qmj7
    Jan  5 19:57:29.799: INFO: Pod sonobuoy requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.799: INFO: Pod sonobuoy-e2e-job-7d2488470a1c477d requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.799: INFO: Pod sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.799: INFO: Pod sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-qmj7
    Jan  5 19:57:29.799: INFO: Pod sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x requesting resource cpu=0m on Node gke-gke-1-26-default-pool-05283374-16pz
    STEP: Starting Pods to consume most of the cluster CPU. 01/05/23 19:57:29.799
    Jan  5 19:57:29.799: INFO: Creating a pod which consumes cpu=466m on Node gke-gke-1-26-default-pool-05283374-16pz
    Jan  5 19:57:29.812: INFO: Creating a pod which consumes cpu=289m on Node gke-gke-1-26-default-pool-05283374-dbpc
    Jan  5 19:57:29.823: INFO: Creating a pod which consumes cpu=317m on Node gke-gke-1-26-default-pool-05283374-qmj7
    Jan  5 19:57:29.837: INFO: Waiting up to 5m0s for pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3" in namespace "sched-pred-2749" to be "running"
    Jan  5 19:57:29.845: INFO: Pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.81874ms
    Jan  5 19:57:31.849: INFO: Pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012032835s
    Jan  5 19:57:33.851: INFO: Pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3": Phase="Running", Reason="", readiness=true. Elapsed: 4.0138497s
    Jan  5 19:57:33.851: INFO: Pod "filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3" satisfied condition "running"
    Jan  5 19:57:33.851: INFO: Waiting up to 5m0s for pod "filler-pod-cc73c171-f395-495e-96a9-772675dd5efe" in namespace "sched-pred-2749" to be "running"
    Jan  5 19:57:33.854: INFO: Pod "filler-pod-cc73c171-f395-495e-96a9-772675dd5efe": Phase="Running", Reason="", readiness=true. Elapsed: 2.748808ms
    Jan  5 19:57:33.854: INFO: Pod "filler-pod-cc73c171-f395-495e-96a9-772675dd5efe" satisfied condition "running"
    Jan  5 19:57:33.854: INFO: Waiting up to 5m0s for pod "filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c" in namespace "sched-pred-2749" to be "running"
    Jan  5 19:57:33.857: INFO: Pod "filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c": Phase="Running", Reason="", readiness=true. Elapsed: 2.695655ms
    Jan  5 19:57:33.857: INFO: Pod "filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 01/05/23 19:57:33.857
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3.173781eef004af90], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2749/filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3 to gke-gke-1-26-default-pool-05283374-16pz] 01/05/23 19:57:33.861
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3.173781ef3b54d42f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/05/23 19:57:33.861
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3.173781ef3c4ffd97], Reason = [Created], Message = [Created container filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3] 01/05/23 19:57:33.862
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3.173781ef675a4fa3], Reason = [Started], Message = [Started container filler-pod-403277c0-8943-4f71-8b38-e304d0acd3c3] 01/05/23 19:57:33.862
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c.173781eef1c95f2d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2749/filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c to gke-gke-1-26-default-pool-05283374-qmj7] 01/05/23 19:57:33.862
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c.173781ef47e193d2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/05/23 19:57:33.862
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c.173781ef49713585], Reason = [Created], Message = [Created container filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c] 01/05/23 19:57:33.862
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c.173781ef8e29b70a], Reason = [Started], Message = [Started container filler-pod-5ad3dce9-a4d4-480f-92f3-5711df0e1e2c] 01/05/23 19:57:33.863
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cc73c171-f395-495e-96a9-772675dd5efe.173781eef0cdbc2e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2749/filler-pod-cc73c171-f395-495e-96a9-772675dd5efe to gke-gke-1-26-default-pool-05283374-dbpc] 01/05/23 19:57:33.863
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cc73c171-f395-495e-96a9-772675dd5efe.173781ef51f5c7ad], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] 01/05/23 19:57:33.863
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cc73c171-f395-495e-96a9-772675dd5efe.173781ef5349c629], Reason = [Created], Message = [Created container filler-pod-cc73c171-f395-495e-96a9-772675dd5efe] 01/05/23 19:57:33.863
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-cc73c171-f395-495e-96a9-772675dd5efe.173781ef9af45ba8], Reason = [Started], Message = [Started container filler-pod-cc73c171-f395-495e-96a9-772675dd5efe] 01/05/23 19:57:33.864
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.173781efe19899ef], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] 01/05/23 19:57:33.881
    STEP: removing the label node off the node gke-gke-1-26-default-pool-05283374-16pz 01/05/23 19:57:34.88
    STEP: verifying the node doesn't have the label node 01/05/23 19:57:34.895
    STEP: removing the label node off the node gke-gke-1-26-default-pool-05283374-dbpc 01/05/23 19:57:34.903
    STEP: verifying the node doesn't have the label node 01/05/23 19:57:34.923
    STEP: removing the label node off the node gke-gke-1-26-default-pool-05283374-qmj7 01/05/23 19:57:34.928
    STEP: verifying the node doesn't have the label node 01/05/23 19:57:34.944
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:57:34.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-2749" for this suite. 01/05/23 19:57:34.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:57:34.975
Jan  5 19:57:34.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename taint-multiple-pods 01/05/23 19:57:34.977
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:34.993
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:34.998
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:383
Jan  5 19:57:35.001: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 19:58:35.032: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455
Jan  5 19:58:35.034: INFO: Starting informer...
STEP: Starting pods... 01/05/23 19:58:35.034
Jan  5 19:58:35.253: INFO: Pod1 is running on gke-gke-1-26-default-pool-05283374-16pz. Tainting Node
Jan  5 19:58:35.469: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2707" to be "running"
Jan  5 19:58:35.472: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665775ms
Jan  5 19:58:37.475: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005833678s
Jan  5 19:58:37.475: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jan  5 19:58:37.475: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2707" to be "running"
Jan  5 19:58:37.477: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.260792ms
Jan  5 19:58:37.477: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jan  5 19:58:37.477: INFO: Pod2 is running on gke-gke-1-26-default-pool-05283374-16pz. Tainting Node
STEP: Trying to apply a taint on the Node 01/05/23 19:58:37.477
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 19:58:37.494
STEP: Waiting for Pod1 and Pod2 to be deleted 01/05/23 19:58:37.5
Jan  5 19:58:43.146: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jan  5 19:59:02.897: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 19:59:02.916
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:02.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-multiple-pods-2707" for this suite. 01/05/23 19:59:02.926
------------------------------
• [SLOW TEST] [87.956 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:455

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:57:34.975
    Jan  5 19:57:34.976: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename taint-multiple-pods 01/05/23 19:57:34.977
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:57:34.993
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:57:34.998
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:383
    Jan  5 19:57:35.001: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 19:58:35.032: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:455
    Jan  5 19:58:35.034: INFO: Starting informer...
    STEP: Starting pods... 01/05/23 19:58:35.034
    Jan  5 19:58:35.253: INFO: Pod1 is running on gke-gke-1-26-default-pool-05283374-16pz. Tainting Node
    Jan  5 19:58:35.469: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-2707" to be "running"
    Jan  5 19:58:35.472: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.665775ms
    Jan  5 19:58:37.475: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.005833678s
    Jan  5 19:58:37.475: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jan  5 19:58:37.475: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-2707" to be "running"
    Jan  5 19:58:37.477: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 2.260792ms
    Jan  5 19:58:37.477: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jan  5 19:58:37.477: INFO: Pod2 is running on gke-gke-1-26-default-pool-05283374-16pz. Tainting Node
    STEP: Trying to apply a taint on the Node 01/05/23 19:58:37.477
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 19:58:37.494
    STEP: Waiting for Pod1 and Pod2 to be deleted 01/05/23 19:58:37.5
    Jan  5 19:58:43.146: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jan  5 19:59:02.897: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 19:59:02.916
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:02.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-multiple-pods-2707" for this suite. 01/05/23 19:59:02.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:02.935
Jan  5 19:59:02.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:59:02.937
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:02.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:02.951
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:59:02.954
Jan  5 19:59:02.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6" in namespace "downward-api-1329" to be "Succeeded or Failed"
Jan  5 19:59:02.972: INFO: Pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514549ms
Jan  5 19:59:04.975: INFO: Pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006160886s
Jan  5 19:59:06.975: INFO: Pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005800344s
STEP: Saw pod success 01/05/23 19:59:06.975
Jan  5 19:59:06.975: INFO: Pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6" satisfied condition "Succeeded or Failed"
Jan  5 19:59:06.978: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6 container client-container: <nil>
STEP: delete the pod 01/05/23 19:59:06.995
Jan  5 19:59:07.005: INFO: Waiting for pod downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6 to disappear
Jan  5 19:59:07.009: INFO: Pod downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-1329" for this suite. 01/05/23 19:59:07.014
------------------------------
• [4.085 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:84

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:02.935
    Jan  5 19:59:02.935: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:59:02.937
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:02.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:02.951
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:84
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:59:02.954
    Jan  5 19:59:02.969: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6" in namespace "downward-api-1329" to be "Succeeded or Failed"
    Jan  5 19:59:02.972: INFO: Pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.514549ms
    Jan  5 19:59:04.975: INFO: Pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006160886s
    Jan  5 19:59:06.975: INFO: Pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005800344s
    STEP: Saw pod success 01/05/23 19:59:06.975
    Jan  5 19:59:06.975: INFO: Pod "downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6" satisfied condition "Succeeded or Failed"
    Jan  5 19:59:06.978: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:59:06.995
    Jan  5 19:59:07.005: INFO: Waiting for pod downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6 to disappear
    Jan  5 19:59:07.009: INFO: Pod downwardapi-volume-4d5df4ba-3b1a-4ebd-adbb-19cc3712dec6 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:07.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-1329" for this suite. 01/05/23 19:59:07.014
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
[BeforeEach] [sig-storage] Downward API volume
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:07.026
Jan  5 19:59:07.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 19:59:07.027
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:07.044
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:07.048
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:44
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249
STEP: Creating a pod to test downward API volume plugin 01/05/23 19:59:07.051
Jan  5 19:59:07.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323" in namespace "downward-api-3596" to be "Succeeded or Failed"
Jan  5 19:59:07.064: INFO: Pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27741ms
Jan  5 19:59:09.067: INFO: Pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006528665s
Jan  5 19:59:11.077: INFO: Pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016543928s
STEP: Saw pod success 01/05/23 19:59:11.078
Jan  5 19:59:11.078: INFO: Pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323" satisfied condition "Succeeded or Failed"
Jan  5 19:59:11.080: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323 container client-container: <nil>
STEP: delete the pod 01/05/23 19:59:11.086
Jan  5 19:59:11.097: INFO: Waiting for pod downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323 to disappear
Jan  5 19:59:11.102: INFO: Pod downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:11.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Downward API volume
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Downward API volume
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Downward API volume
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-3596" for this suite. 01/05/23 19:59:11.11
------------------------------
• [4.093 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:249

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:07.026
    Jan  5 19:59:07.026: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 19:59:07.027
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:07.044
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:07.048
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:44
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:249
    STEP: Creating a pod to test downward API volume plugin 01/05/23 19:59:07.051
    Jan  5 19:59:07.061: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323" in namespace "downward-api-3596" to be "Succeeded or Failed"
    Jan  5 19:59:07.064: INFO: Pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323": Phase="Pending", Reason="", readiness=false. Elapsed: 3.27741ms
    Jan  5 19:59:09.067: INFO: Pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006528665s
    Jan  5 19:59:11.077: INFO: Pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016543928s
    STEP: Saw pod success 01/05/23 19:59:11.078
    Jan  5 19:59:11.078: INFO: Pod "downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323" satisfied condition "Succeeded or Failed"
    Jan  5 19:59:11.080: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323 container client-container: <nil>
    STEP: delete the pod 01/05/23 19:59:11.086
    Jan  5 19:59:11.097: INFO: Waiting for pod downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323 to disappear
    Jan  5 19:59:11.102: INFO: Pod downwardapi-volume-e02c4418-8128-49df-a7a4-f5c696075323 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:11.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Downward API volume
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-3596" for this suite. 01/05/23 19:59:11.11
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:11.121
Jan  5 19:59:11.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 19:59:11.122
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:11.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:11.153
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240
STEP: Creating configMap with name cm-test-opt-del-473041e5-aa19-4a33-ad29-256e5aefab6e 01/05/23 19:59:11.161
STEP: Creating configMap with name cm-test-opt-upd-980edbab-8ae3-49c6-bbbe-59fc797e2657 01/05/23 19:59:11.167
STEP: Creating the pod 01/05/23 19:59:11.178
Jan  5 19:59:11.200: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420" in namespace "configmap-8299" to be "running and ready"
Jan  5 19:59:11.205: INFO: Pod "pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420": Phase="Pending", Reason="", readiness=false. Elapsed: 4.98925ms
Jan  5 19:59:11.205: INFO: The phase of Pod pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 19:59:13.210: INFO: Pod "pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420": Phase="Running", Reason="", readiness=true. Elapsed: 2.010179048s
Jan  5 19:59:13.210: INFO: The phase of Pod pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420 is Running (Ready = true)
Jan  5 19:59:13.210: INFO: Pod "pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-473041e5-aa19-4a33-ad29-256e5aefab6e 01/05/23 19:59:13.228
STEP: Updating configmap cm-test-opt-upd-980edbab-8ae3-49c6-bbbe-59fc797e2657 01/05/23 19:59:13.233
STEP: Creating configMap with name cm-test-opt-create-36114534-2a42-45b3-930d-e367af6eec9c 01/05/23 19:59:13.242
STEP: waiting to observe update in volume 01/05/23 19:59:13.249
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:15.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-8299" for this suite. 01/05/23 19:59:15.317
------------------------------
• [4.204 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:240

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:11.121
    Jan  5 19:59:11.121: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 19:59:11.122
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:11.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:11.153
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:240
    STEP: Creating configMap with name cm-test-opt-del-473041e5-aa19-4a33-ad29-256e5aefab6e 01/05/23 19:59:11.161
    STEP: Creating configMap with name cm-test-opt-upd-980edbab-8ae3-49c6-bbbe-59fc797e2657 01/05/23 19:59:11.167
    STEP: Creating the pod 01/05/23 19:59:11.178
    Jan  5 19:59:11.200: INFO: Waiting up to 5m0s for pod "pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420" in namespace "configmap-8299" to be "running and ready"
    Jan  5 19:59:11.205: INFO: Pod "pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420": Phase="Pending", Reason="", readiness=false. Elapsed: 4.98925ms
    Jan  5 19:59:11.205: INFO: The phase of Pod pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 19:59:13.210: INFO: Pod "pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420": Phase="Running", Reason="", readiness=true. Elapsed: 2.010179048s
    Jan  5 19:59:13.210: INFO: The phase of Pod pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420 is Running (Ready = true)
    Jan  5 19:59:13.210: INFO: Pod "pod-configmaps-eb55744b-9c41-45df-8af5-d68404a4e420" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-473041e5-aa19-4a33-ad29-256e5aefab6e 01/05/23 19:59:13.228
    STEP: Updating configmap cm-test-opt-upd-980edbab-8ae3-49c6-bbbe-59fc797e2657 01/05/23 19:59:13.233
    STEP: Creating configMap with name cm-test-opt-create-36114534-2a42-45b3-930d-e367af6eec9c 01/05/23 19:59:13.242
    STEP: waiting to observe update in volume 01/05/23 19:59:13.249
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:15.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-8299" for this suite. 01/05/23 19:59:15.317
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:15.329
Jan  5 19:59:15.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename init-container 01/05/23 19:59:15.331
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:15.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:15.358
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:165
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177
STEP: creating the pod 01/05/23 19:59:15.361
Jan  5 19:59:15.361: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:21.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "init-container-5571" for this suite. 01/05/23 19:59:21.258
------------------------------
• [SLOW TEST] [5.936 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:177

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:15.329
    Jan  5 19:59:15.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename init-container 01/05/23 19:59:15.331
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:15.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:15.358
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:165
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:177
    STEP: creating the pod 01/05/23 19:59:15.361
    Jan  5 19:59:15.361: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:21.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] InitContainer [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "init-container-5571" for this suite. 01/05/23 19:59:21.258
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:21.269
Jan  5 19:59:21.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename deployment 01/05/23 19:59:21.27
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:21.288
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:21.291
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jan  5 19:59:21.309: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jan  5 19:59:26.313: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 19:59:26.313
Jan  5 19:59:26.313: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jan  5 19:59:28.316: INFO: Creating deployment "test-rollover-deployment"
Jan  5 19:59:28.324: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jan  5 19:59:30.330: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jan  5 19:59:30.337: INFO: Ensure that both replica sets have 1 created replica
Jan  5 19:59:30.341: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jan  5 19:59:30.357: INFO: Updating deployment test-rollover-deployment
Jan  5 19:59:30.357: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jan  5 19:59:32.366: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jan  5 19:59:32.371: INFO: Make sure deployment "test-rollover-deployment" is complete
Jan  5 19:59:32.376: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 19:59:32.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 19:59:34.382: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 19:59:34.382: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 19:59:36.382: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 19:59:36.382: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 19:59:38.384: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 19:59:38.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 19:59:40.383: INFO: all replica sets need to contain the pod-template-hash label
Jan  5 19:59:40.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 19:59:42.382: INFO: 
Jan  5 19:59:42.382: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 19:59:42.388: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-8729  b84af4c2-b214-4d6f-92c0-fa5265f573b5 90972 2 2023-01-05 19:59:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d61b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 19:59:28 +0000 UTC,LastTransitionTime:2023-01-05 19:59:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-05 19:59:42 +0000 UTC,LastTransitionTime:2023-01-05 19:59:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 19:59:42.391: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8729  a6b9a453-f51c-4ebd-a285-ccd19ff4202c 90963 2 2023-01-05 19:59:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b84af4c2-b214-4d6f-92c0-fa5265f573b5 0xc004c23bd7 0xc004c23bd8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b84af4c2-b214-4d6f-92c0-fa5265f573b5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c23c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:59:42.391: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jan  5 19:59:42.391: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8729  d9283243-e9e7-49c3-8832-673858c4de69 90971 2 2023-01-05 19:59:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b84af4c2-b214-4d6f-92c0-fa5265f573b5 0xc004c23a97 0xc004c23a98}] [] [{e2e.test Update apps/v1 2023-01-05 19:59:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b84af4c2-b214-4d6f-92c0-fa5265f573b5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c23b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:59:42.392: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8729  5cb0c60a-3f06-4a44-bce7-3fe2480c3c52 90851 2 2023-01-05 19:59:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b84af4c2-b214-4d6f-92c0-fa5265f573b5 0xc004c23d07 0xc004c23d08}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b84af4c2-b214-4d6f-92c0-fa5265f573b5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c23db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jan  5 19:59:42.394: INFO: Pod "test-rollover-deployment-6c6df9974f-tn2lz" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-tn2lz test-rollover-deployment-6c6df9974f- deployment-8729  5ed2c128-2cd3-4a2b-a62f-08705052ae12 90873 0 2023-01-05 19:59:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f a6b9a453-f51c-4ebd-a285-ccd19ff4202c 0xc005282327 0xc005282328}] [] [{kube-controller-manager Update v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6b9a453-f51c-4ebd-a285-ccd19ff4202c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:59:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fffrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fffrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:59:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:59:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:59:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:59:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.222,StartTime:2023-01-05 19:59:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:59:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://959c14e7d5b6bf40d653549a88ba57491fca36942db59076f2788cd14dcd403b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:42.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-8729" for this suite. 01/05/23 19:59:42.398
------------------------------
• [SLOW TEST] [21.134 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:21.269
    Jan  5 19:59:21.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename deployment 01/05/23 19:59:21.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:21.288
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:21.291
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jan  5 19:59:21.309: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jan  5 19:59:26.313: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 19:59:26.313
    Jan  5 19:59:26.313: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jan  5 19:59:28.316: INFO: Creating deployment "test-rollover-deployment"
    Jan  5 19:59:28.324: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jan  5 19:59:30.330: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jan  5 19:59:30.337: INFO: Ensure that both replica sets have 1 created replica
    Jan  5 19:59:30.341: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jan  5 19:59:30.357: INFO: Updating deployment test-rollover-deployment
    Jan  5 19:59:30.357: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jan  5 19:59:32.366: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jan  5 19:59:32.371: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jan  5 19:59:32.376: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 19:59:32.376: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 19:59:34.382: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 19:59:34.382: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 19:59:36.382: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 19:59:36.382: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 19:59:38.384: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 19:59:38.384: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 19:59:40.383: INFO: all replica sets need to contain the pod-template-hash label
    Jan  5 19:59:40.383: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 19, 59, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 19, 59, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6c6df9974f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 19:59:42.382: INFO: 
    Jan  5 19:59:42.382: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 19:59:42.388: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-8729  b84af4c2-b214-4d6f-92c0-fa5265f573b5 90972 2 2023-01-05 19:59:28 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000d61b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 19:59:28 +0000 UTC,LastTransitionTime:2023-01-05 19:59:28 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6c6df9974f" has successfully progressed.,LastUpdateTime:2023-01-05 19:59:42 +0000 UTC,LastTransitionTime:2023-01-05 19:59:28 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 19:59:42.391: INFO: New ReplicaSet "test-rollover-deployment-6c6df9974f" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6c6df9974f  deployment-8729  a6b9a453-f51c-4ebd-a285-ccd19ff4202c 90963 2 2023-01-05 19:59:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b84af4c2-b214-4d6f-92c0-fa5265f573b5 0xc004c23bd7 0xc004c23bd8}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b84af4c2-b214-4d6f-92c0-fa5265f573b5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6c6df9974f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c23c88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:59:42.391: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jan  5 19:59:42.391: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-8729  d9283243-e9e7-49c3-8832-673858c4de69 90971 2 2023-01-05 19:59:21 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b84af4c2-b214-4d6f-92c0-fa5265f573b5 0xc004c23a97 0xc004c23a98}] [] [{e2e.test Update apps/v1 2023-01-05 19:59:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b84af4c2-b214-4d6f-92c0-fa5265f573b5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004c23b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:59:42.392: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-768dcbc65b  deployment-8729  5cb0c60a-3f06-4a44-bce7-3fe2480c3c52 90851 2 2023-01-05 19:59:28 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b84af4c2-b214-4d6f-92c0-fa5265f573b5 0xc004c23d07 0xc004c23d08}] [] [{kube-controller-manager Update apps/v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b84af4c2-b214-4d6f-92c0-fa5265f573b5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 768dcbc65b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:768dcbc65b] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004c23db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 19:59:42.394: INFO: Pod "test-rollover-deployment-6c6df9974f-tn2lz" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6c6df9974f-tn2lz test-rollover-deployment-6c6df9974f- deployment-8729  5ed2c128-2cd3-4a2b-a62f-08705052ae12 90873 0 2023-01-05 19:59:30 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6c6df9974f] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6c6df9974f a6b9a453-f51c-4ebd-a285-ccd19ff4202c 0xc005282327 0xc005282328}] [] [{kube-controller-manager Update v1 2023-01-05 19:59:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a6b9a453-f51c-4ebd-a285-ccd19ff4202c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 19:59:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fffrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fffrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:59:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:59:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:59:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 19:59:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.222,StartTime:2023-01-05 19:59:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 19:59:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://959c14e7d5b6bf40d653549a88ba57491fca36942db59076f2788cd14dcd403b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.222,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:42.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-8729" for this suite. 01/05/23 19:59:42.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:42.414
Jan  5 19:59:42.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 19:59:42.416
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:42.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:42.43
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68
STEP: Creating secret with name secret-test-912cdf68-52a2-4244-853d-60d36b60b45f 01/05/23 19:59:42.434
STEP: Creating a pod to test consume secrets 01/05/23 19:59:42.438
Jan  5 19:59:42.448: INFO: Waiting up to 5m0s for pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e" in namespace "secrets-5814" to be "Succeeded or Failed"
Jan  5 19:59:42.450: INFO: Pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.593402ms
Jan  5 19:59:44.455: INFO: Pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007101436s
Jan  5 19:59:46.454: INFO: Pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006502881s
STEP: Saw pod success 01/05/23 19:59:46.454
Jan  5 19:59:46.454: INFO: Pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e" satisfied condition "Succeeded or Failed"
Jan  5 19:59:46.457: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 19:59:46.467
Jan  5 19:59:46.480: INFO: Waiting for pod pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e to disappear
Jan  5 19:59:46.483: INFO: Pod pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:46.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5814" for this suite. 01/05/23 19:59:46.488
------------------------------
• [4.080 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:42.414
    Jan  5 19:59:42.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 19:59:42.416
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:42.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:42.43
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:68
    STEP: Creating secret with name secret-test-912cdf68-52a2-4244-853d-60d36b60b45f 01/05/23 19:59:42.434
    STEP: Creating a pod to test consume secrets 01/05/23 19:59:42.438
    Jan  5 19:59:42.448: INFO: Waiting up to 5m0s for pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e" in namespace "secrets-5814" to be "Succeeded or Failed"
    Jan  5 19:59:42.450: INFO: Pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.593402ms
    Jan  5 19:59:44.455: INFO: Pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007101436s
    Jan  5 19:59:46.454: INFO: Pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006502881s
    STEP: Saw pod success 01/05/23 19:59:46.454
    Jan  5 19:59:46.454: INFO: Pod "pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e" satisfied condition "Succeeded or Failed"
    Jan  5 19:59:46.457: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 19:59:46.467
    Jan  5 19:59:46.480: INFO: Waiting for pod pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e to disappear
    Jan  5 19:59:46.483: INFO: Pod pod-secrets-85919bc8-128f-486a-aa5a-aa3971884c8e no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:46.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5814" for this suite. 01/05/23 19:59:46.488
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:46.496
Jan  5 19:59:46.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename discovery 01/05/23 19:59:46.497
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:46.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:46.516
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 01/05/23 19:59:46.52
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jan  5 19:59:46.823: INFO: Checking APIGroup: apiregistration.k8s.io
Jan  5 19:59:46.825: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jan  5 19:59:46.825: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jan  5 19:59:46.825: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jan  5 19:59:46.825: INFO: Checking APIGroup: apps
Jan  5 19:59:46.826: INFO: PreferredVersion.GroupVersion: apps/v1
Jan  5 19:59:46.826: INFO: Versions found [{apps/v1 v1}]
Jan  5 19:59:46.826: INFO: apps/v1 matches apps/v1
Jan  5 19:59:46.826: INFO: Checking APIGroup: events.k8s.io
Jan  5 19:59:46.827: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jan  5 19:59:46.827: INFO: Versions found [{events.k8s.io/v1 v1}]
Jan  5 19:59:46.827: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jan  5 19:59:46.827: INFO: Checking APIGroup: authentication.k8s.io
Jan  5 19:59:46.827: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jan  5 19:59:46.827: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jan  5 19:59:46.827: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jan  5 19:59:46.827: INFO: Checking APIGroup: authorization.k8s.io
Jan  5 19:59:46.828: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jan  5 19:59:46.828: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jan  5 19:59:46.828: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jan  5 19:59:46.828: INFO: Checking APIGroup: autoscaling
Jan  5 19:59:46.829: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jan  5 19:59:46.829: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
Jan  5 19:59:46.829: INFO: autoscaling/v2 matches autoscaling/v2
Jan  5 19:59:46.829: INFO: Checking APIGroup: batch
Jan  5 19:59:46.830: INFO: PreferredVersion.GroupVersion: batch/v1
Jan  5 19:59:46.830: INFO: Versions found [{batch/v1 v1}]
Jan  5 19:59:46.830: INFO: batch/v1 matches batch/v1
Jan  5 19:59:46.830: INFO: Checking APIGroup: certificates.k8s.io
Jan  5 19:59:46.831: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jan  5 19:59:46.831: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jan  5 19:59:46.831: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jan  5 19:59:46.831: INFO: Checking APIGroup: networking.k8s.io
Jan  5 19:59:46.832: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jan  5 19:59:46.832: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jan  5 19:59:46.832: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jan  5 19:59:46.832: INFO: Checking APIGroup: policy
Jan  5 19:59:46.832: INFO: PreferredVersion.GroupVersion: policy/v1
Jan  5 19:59:46.832: INFO: Versions found [{policy/v1 v1}]
Jan  5 19:59:46.832: INFO: policy/v1 matches policy/v1
Jan  5 19:59:46.832: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jan  5 19:59:46.833: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jan  5 19:59:46.833: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jan  5 19:59:46.833: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jan  5 19:59:46.833: INFO: Checking APIGroup: storage.k8s.io
Jan  5 19:59:46.834: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jan  5 19:59:46.835: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jan  5 19:59:46.835: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jan  5 19:59:46.835: INFO: Checking APIGroup: admissionregistration.k8s.io
Jan  5 19:59:46.835: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jan  5 19:59:46.835: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jan  5 19:59:46.835: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jan  5 19:59:46.835: INFO: Checking APIGroup: apiextensions.k8s.io
Jan  5 19:59:46.836: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jan  5 19:59:46.836: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jan  5 19:59:46.836: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jan  5 19:59:46.836: INFO: Checking APIGroup: scheduling.k8s.io
Jan  5 19:59:46.837: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jan  5 19:59:46.837: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jan  5 19:59:46.837: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jan  5 19:59:46.837: INFO: Checking APIGroup: coordination.k8s.io
Jan  5 19:59:46.838: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jan  5 19:59:46.838: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jan  5 19:59:46.838: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jan  5 19:59:46.838: INFO: Checking APIGroup: node.k8s.io
Jan  5 19:59:46.839: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jan  5 19:59:46.839: INFO: Versions found [{node.k8s.io/v1 v1}]
Jan  5 19:59:46.839: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jan  5 19:59:46.839: INFO: Checking APIGroup: discovery.k8s.io
Jan  5 19:59:46.842: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jan  5 19:59:46.842: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jan  5 19:59:46.842: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jan  5 19:59:46.842: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jan  5 19:59:46.843: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
Jan  5 19:59:46.843: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
Jan  5 19:59:46.843: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
Jan  5 19:59:46.843: INFO: Checking APIGroup: auto.gke.io
Jan  5 19:59:46.844: INFO: PreferredVersion.GroupVersion: auto.gke.io/v1
Jan  5 19:59:46.844: INFO: Versions found [{auto.gke.io/v1 v1} {auto.gke.io/v1alpha1 v1alpha1}]
Jan  5 19:59:46.844: INFO: auto.gke.io/v1 matches auto.gke.io/v1
Jan  5 19:59:46.844: INFO: Checking APIGroup: cloud.google.com
Jan  5 19:59:46.845: INFO: PreferredVersion.GroupVersion: cloud.google.com/v1
Jan  5 19:59:46.845: INFO: Versions found [{cloud.google.com/v1 v1} {cloud.google.com/v1beta1 v1beta1}]
Jan  5 19:59:46.845: INFO: cloud.google.com/v1 matches cloud.google.com/v1
Jan  5 19:59:46.845: INFO: Checking APIGroup: hub.gke.io
Jan  5 19:59:46.846: INFO: PreferredVersion.GroupVersion: hub.gke.io/v1
Jan  5 19:59:46.846: INFO: Versions found [{hub.gke.io/v1 v1}]
Jan  5 19:59:46.846: INFO: hub.gke.io/v1 matches hub.gke.io/v1
Jan  5 19:59:46.846: INFO: Checking APIGroup: networking.gke.io
Jan  5 19:59:46.847: INFO: PreferredVersion.GroupVersion: networking.gke.io/v1
Jan  5 19:59:46.847: INFO: Versions found [{networking.gke.io/v1 v1} {networking.gke.io/v1beta2 v1beta2} {networking.gke.io/v1beta1 v1beta1}]
Jan  5 19:59:46.847: INFO: networking.gke.io/v1 matches networking.gke.io/v1
Jan  5 19:59:46.847: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jan  5 19:59:46.848: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jan  5 19:59:46.848: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
Jan  5 19:59:46.848: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jan  5 19:59:46.848: INFO: Checking APIGroup: internal.autoscaling.gke.io
Jan  5 19:59:46.849: INFO: PreferredVersion.GroupVersion: internal.autoscaling.gke.io/v1alpha1
Jan  5 19:59:46.849: INFO: Versions found [{internal.autoscaling.gke.io/v1alpha1 v1alpha1}]
Jan  5 19:59:46.849: INFO: internal.autoscaling.gke.io/v1alpha1 matches internal.autoscaling.gke.io/v1alpha1
Jan  5 19:59:46.849: INFO: Checking APIGroup: migration.k8s.io
Jan  5 19:59:46.850: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jan  5 19:59:46.850: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jan  5 19:59:46.850: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jan  5 19:59:46.850: INFO: Checking APIGroup: nodemanagement.gke.io
Jan  5 19:59:46.851: INFO: PreferredVersion.GroupVersion: nodemanagement.gke.io/v1alpha1
Jan  5 19:59:46.851: INFO: Versions found [{nodemanagement.gke.io/v1alpha1 v1alpha1}]
Jan  5 19:59:46.851: INFO: nodemanagement.gke.io/v1alpha1 matches nodemanagement.gke.io/v1alpha1
Jan  5 19:59:46.851: INFO: Checking APIGroup: metrics.k8s.io
Jan  5 19:59:46.852: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jan  5 19:59:46.852: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jan  5 19:59:46.852: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:46.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Discovery
  tear down framework | framework.go:193
STEP: Destroying namespace "discovery-8541" for this suite. 01/05/23 19:59:46.855
------------------------------
• [0.366 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:46.496
    Jan  5 19:59:46.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename discovery 01/05/23 19:59:46.497
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:46.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:46.516
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 01/05/23 19:59:46.52
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jan  5 19:59:46.823: INFO: Checking APIGroup: apiregistration.k8s.io
    Jan  5 19:59:46.825: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jan  5 19:59:46.825: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jan  5 19:59:46.825: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jan  5 19:59:46.825: INFO: Checking APIGroup: apps
    Jan  5 19:59:46.826: INFO: PreferredVersion.GroupVersion: apps/v1
    Jan  5 19:59:46.826: INFO: Versions found [{apps/v1 v1}]
    Jan  5 19:59:46.826: INFO: apps/v1 matches apps/v1
    Jan  5 19:59:46.826: INFO: Checking APIGroup: events.k8s.io
    Jan  5 19:59:46.827: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jan  5 19:59:46.827: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jan  5 19:59:46.827: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jan  5 19:59:46.827: INFO: Checking APIGroup: authentication.k8s.io
    Jan  5 19:59:46.827: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jan  5 19:59:46.827: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jan  5 19:59:46.827: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jan  5 19:59:46.827: INFO: Checking APIGroup: authorization.k8s.io
    Jan  5 19:59:46.828: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jan  5 19:59:46.828: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jan  5 19:59:46.828: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jan  5 19:59:46.828: INFO: Checking APIGroup: autoscaling
    Jan  5 19:59:46.829: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jan  5 19:59:46.829: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
    Jan  5 19:59:46.829: INFO: autoscaling/v2 matches autoscaling/v2
    Jan  5 19:59:46.829: INFO: Checking APIGroup: batch
    Jan  5 19:59:46.830: INFO: PreferredVersion.GroupVersion: batch/v1
    Jan  5 19:59:46.830: INFO: Versions found [{batch/v1 v1}]
    Jan  5 19:59:46.830: INFO: batch/v1 matches batch/v1
    Jan  5 19:59:46.830: INFO: Checking APIGroup: certificates.k8s.io
    Jan  5 19:59:46.831: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jan  5 19:59:46.831: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jan  5 19:59:46.831: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jan  5 19:59:46.831: INFO: Checking APIGroup: networking.k8s.io
    Jan  5 19:59:46.832: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jan  5 19:59:46.832: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jan  5 19:59:46.832: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jan  5 19:59:46.832: INFO: Checking APIGroup: policy
    Jan  5 19:59:46.832: INFO: PreferredVersion.GroupVersion: policy/v1
    Jan  5 19:59:46.832: INFO: Versions found [{policy/v1 v1}]
    Jan  5 19:59:46.832: INFO: policy/v1 matches policy/v1
    Jan  5 19:59:46.832: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jan  5 19:59:46.833: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jan  5 19:59:46.833: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jan  5 19:59:46.833: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jan  5 19:59:46.833: INFO: Checking APIGroup: storage.k8s.io
    Jan  5 19:59:46.834: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jan  5 19:59:46.835: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jan  5 19:59:46.835: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jan  5 19:59:46.835: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jan  5 19:59:46.835: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jan  5 19:59:46.835: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jan  5 19:59:46.835: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jan  5 19:59:46.835: INFO: Checking APIGroup: apiextensions.k8s.io
    Jan  5 19:59:46.836: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jan  5 19:59:46.836: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jan  5 19:59:46.836: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jan  5 19:59:46.836: INFO: Checking APIGroup: scheduling.k8s.io
    Jan  5 19:59:46.837: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jan  5 19:59:46.837: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jan  5 19:59:46.837: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jan  5 19:59:46.837: INFO: Checking APIGroup: coordination.k8s.io
    Jan  5 19:59:46.838: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jan  5 19:59:46.838: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jan  5 19:59:46.838: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jan  5 19:59:46.838: INFO: Checking APIGroup: node.k8s.io
    Jan  5 19:59:46.839: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jan  5 19:59:46.839: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jan  5 19:59:46.839: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jan  5 19:59:46.839: INFO: Checking APIGroup: discovery.k8s.io
    Jan  5 19:59:46.842: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jan  5 19:59:46.842: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jan  5 19:59:46.842: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jan  5 19:59:46.842: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jan  5 19:59:46.843: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
    Jan  5 19:59:46.843: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
    Jan  5 19:59:46.843: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
    Jan  5 19:59:46.843: INFO: Checking APIGroup: auto.gke.io
    Jan  5 19:59:46.844: INFO: PreferredVersion.GroupVersion: auto.gke.io/v1
    Jan  5 19:59:46.844: INFO: Versions found [{auto.gke.io/v1 v1} {auto.gke.io/v1alpha1 v1alpha1}]
    Jan  5 19:59:46.844: INFO: auto.gke.io/v1 matches auto.gke.io/v1
    Jan  5 19:59:46.844: INFO: Checking APIGroup: cloud.google.com
    Jan  5 19:59:46.845: INFO: PreferredVersion.GroupVersion: cloud.google.com/v1
    Jan  5 19:59:46.845: INFO: Versions found [{cloud.google.com/v1 v1} {cloud.google.com/v1beta1 v1beta1}]
    Jan  5 19:59:46.845: INFO: cloud.google.com/v1 matches cloud.google.com/v1
    Jan  5 19:59:46.845: INFO: Checking APIGroup: hub.gke.io
    Jan  5 19:59:46.846: INFO: PreferredVersion.GroupVersion: hub.gke.io/v1
    Jan  5 19:59:46.846: INFO: Versions found [{hub.gke.io/v1 v1}]
    Jan  5 19:59:46.846: INFO: hub.gke.io/v1 matches hub.gke.io/v1
    Jan  5 19:59:46.846: INFO: Checking APIGroup: networking.gke.io
    Jan  5 19:59:46.847: INFO: PreferredVersion.GroupVersion: networking.gke.io/v1
    Jan  5 19:59:46.847: INFO: Versions found [{networking.gke.io/v1 v1} {networking.gke.io/v1beta2 v1beta2} {networking.gke.io/v1beta1 v1beta1}]
    Jan  5 19:59:46.847: INFO: networking.gke.io/v1 matches networking.gke.io/v1
    Jan  5 19:59:46.847: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jan  5 19:59:46.848: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jan  5 19:59:46.848: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
    Jan  5 19:59:46.848: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jan  5 19:59:46.848: INFO: Checking APIGroup: internal.autoscaling.gke.io
    Jan  5 19:59:46.849: INFO: PreferredVersion.GroupVersion: internal.autoscaling.gke.io/v1alpha1
    Jan  5 19:59:46.849: INFO: Versions found [{internal.autoscaling.gke.io/v1alpha1 v1alpha1}]
    Jan  5 19:59:46.849: INFO: internal.autoscaling.gke.io/v1alpha1 matches internal.autoscaling.gke.io/v1alpha1
    Jan  5 19:59:46.849: INFO: Checking APIGroup: migration.k8s.io
    Jan  5 19:59:46.850: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    Jan  5 19:59:46.850: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    Jan  5 19:59:46.850: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    Jan  5 19:59:46.850: INFO: Checking APIGroup: nodemanagement.gke.io
    Jan  5 19:59:46.851: INFO: PreferredVersion.GroupVersion: nodemanagement.gke.io/v1alpha1
    Jan  5 19:59:46.851: INFO: Versions found [{nodemanagement.gke.io/v1alpha1 v1alpha1}]
    Jan  5 19:59:46.851: INFO: nodemanagement.gke.io/v1alpha1 matches nodemanagement.gke.io/v1alpha1
    Jan  5 19:59:46.851: INFO: Checking APIGroup: metrics.k8s.io
    Jan  5 19:59:46.852: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jan  5 19:59:46.852: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jan  5 19:59:46.852: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:46.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Discovery
      tear down framework | framework.go:193
    STEP: Destroying namespace "discovery-8541" for this suite. 01/05/23 19:59:46.855
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:46.863
Jan  5 19:59:46.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 19:59:46.865
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:46.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:46.88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89
STEP: Creating configMap with name projected-configmap-test-volume-map-5d857b4d-3aec-4c7e-9f5f-f071ee7239a1 01/05/23 19:59:46.883
STEP: Creating a pod to test consume configMaps 01/05/23 19:59:46.888
Jan  5 19:59:46.897: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484" in namespace "projected-2262" to be "Succeeded or Failed"
Jan  5 19:59:46.900: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506486ms
Jan  5 19:59:48.903: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005789159s
Jan  5 19:59:50.904: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006661048s
Jan  5 19:59:52.903: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005975567s
STEP: Saw pod success 01/05/23 19:59:52.903
Jan  5 19:59:52.904: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484" satisfied condition "Succeeded or Failed"
Jan  5 19:59:52.906: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 19:59:52.913
Jan  5 19:59:52.928: INFO: Waiting for pod pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484 to disappear
Jan  5 19:59:52.932: INFO: Pod pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:52.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2262" for this suite. 01/05/23 19:59:52.936
------------------------------
• [SLOW TEST] [6.080 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:46.863
    Jan  5 19:59:46.863: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 19:59:46.865
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:46.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:46.88
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:89
    STEP: Creating configMap with name projected-configmap-test-volume-map-5d857b4d-3aec-4c7e-9f5f-f071ee7239a1 01/05/23 19:59:46.883
    STEP: Creating a pod to test consume configMaps 01/05/23 19:59:46.888
    Jan  5 19:59:46.897: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484" in namespace "projected-2262" to be "Succeeded or Failed"
    Jan  5 19:59:46.900: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.506486ms
    Jan  5 19:59:48.903: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005789159s
    Jan  5 19:59:50.904: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484": Phase="Pending", Reason="", readiness=false. Elapsed: 4.006661048s
    Jan  5 19:59:52.903: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.005975567s
    STEP: Saw pod success 01/05/23 19:59:52.903
    Jan  5 19:59:52.904: INFO: Pod "pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484" satisfied condition "Succeeded or Failed"
    Jan  5 19:59:52.906: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 19:59:52.913
    Jan  5 19:59:52.928: INFO: Waiting for pod pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484 to disappear
    Jan  5 19:59:52.932: INFO: Pod pod-projected-configmaps-6f45edea-2d5a-4be9-b3ee-aada12bca484 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:52.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2262" for this suite. 01/05/23 19:59:52.936
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:52.945
Jan  5 19:59:52.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename lease-test 01/05/23 19:59:52.947
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:52.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:52.972
[BeforeEach] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:31
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:53.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Lease
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Lease
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Lease
  tear down framework | framework.go:193
STEP: Destroying namespace "lease-test-9154" for this suite. 01/05/23 19:59:53.038
------------------------------
• [0.107 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:52.945
    Jan  5 19:59:52.945: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename lease-test 01/05/23 19:59:52.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:52.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:52.972
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:31
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:53.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Lease
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Lease
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Lease
      tear down framework | framework.go:193
    STEP: Destroying namespace "lease-test-9154" for this suite. 01/05/23 19:59:53.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:53.056
Jan  5 19:59:53.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename controllerrevisions 01/05/23 19:59:53.058
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:53.078
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:53.083
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-jdxgx-daemon-set" 01/05/23 19:59:53.118
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:59:53.125
Jan  5 19:59:53.136: INFO: Number of nodes with available pods controlled by daemonset e2e-jdxgx-daemon-set: 0
Jan  5 19:59:53.136: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:59:54.143: INFO: Number of nodes with available pods controlled by daemonset e2e-jdxgx-daemon-set: 0
Jan  5 19:59:54.144: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 19:59:55.143: INFO: Number of nodes with available pods controlled by daemonset e2e-jdxgx-daemon-set: 3
Jan  5 19:59:55.144: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-jdxgx-daemon-set
STEP: Confirm DaemonSet "e2e-jdxgx-daemon-set" successfully created with "daemonset-name=e2e-jdxgx-daemon-set" label 01/05/23 19:59:55.148
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-jdxgx-daemon-set" 01/05/23 19:59:55.154
Jan  5 19:59:55.156: INFO: Located ControllerRevision: "e2e-jdxgx-daemon-set-58b87fd88b"
STEP: Patching ControllerRevision "e2e-jdxgx-daemon-set-58b87fd88b" 01/05/23 19:59:55.158
Jan  5 19:59:55.166: INFO: e2e-jdxgx-daemon-set-58b87fd88b has been patched
STEP: Create a new ControllerRevision 01/05/23 19:59:55.166
Jan  5 19:59:55.171: INFO: Created ControllerRevision: e2e-jdxgx-daemon-set-54f4c57bbd
STEP: Confirm that there are two ControllerRevisions 01/05/23 19:59:55.171
Jan  5 19:59:55.171: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 19:59:55.173: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-jdxgx-daemon-set-58b87fd88b" 01/05/23 19:59:55.173
STEP: Confirm that there is only one ControllerRevision 01/05/23 19:59:55.178
Jan  5 19:59:55.178: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 19:59:55.181: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-jdxgx-daemon-set-54f4c57bbd" 01/05/23 19:59:55.183
Jan  5 19:59:55.192: INFO: e2e-jdxgx-daemon-set-54f4c57bbd has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 01/05/23 19:59:55.192
W0105 19:59:55.200016      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 01/05/23 19:59:55.2
Jan  5 19:59:55.200: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 19:59:56.203: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 19:59:56.206: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-jdxgx-daemon-set-54f4c57bbd=updated" 01/05/23 19:59:56.206
STEP: Confirm that there is only one ControllerRevision 01/05/23 19:59:56.214
Jan  5 19:59:56.214: INFO: Requesting list of ControllerRevisions to confirm quantity
Jan  5 19:59:56.217: INFO: Found 1 ControllerRevisions
Jan  5 19:59:56.219: INFO: ControllerRevision "e2e-jdxgx-daemon-set-6bb5d944d9" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-jdxgx-daemon-set" 01/05/23 19:59:56.221
STEP: deleting DaemonSet.extensions e2e-jdxgx-daemon-set in namespace controllerrevisions-8599, will wait for the garbage collector to delete the pods 01/05/23 19:59:56.221
Jan  5 19:59:56.288: INFO: Deleting DaemonSet.extensions e2e-jdxgx-daemon-set took: 13.444ms
Jan  5 19:59:56.389: INFO: Terminating DaemonSet.extensions e2e-jdxgx-daemon-set pods took: 100.617839ms
Jan  5 19:59:58.193: INFO: Number of nodes with available pods controlled by daemonset e2e-jdxgx-daemon-set: 0
Jan  5 19:59:58.193: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-jdxgx-daemon-set
Jan  5 19:59:58.195: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"91223"},"items":null}

Jan  5 19:59:58.197: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"91223"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 19:59:58.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "controllerrevisions-8599" for this suite. 01/05/23 19:59:58.211
------------------------------
• [SLOW TEST] [5.161 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:53.056
    Jan  5 19:59:53.056: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename controllerrevisions 01/05/23 19:59:53.058
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:53.078
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:53.083
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-jdxgx-daemon-set" 01/05/23 19:59:53.118
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 19:59:53.125
    Jan  5 19:59:53.136: INFO: Number of nodes with available pods controlled by daemonset e2e-jdxgx-daemon-set: 0
    Jan  5 19:59:53.136: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:59:54.143: INFO: Number of nodes with available pods controlled by daemonset e2e-jdxgx-daemon-set: 0
    Jan  5 19:59:54.144: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 19:59:55.143: INFO: Number of nodes with available pods controlled by daemonset e2e-jdxgx-daemon-set: 3
    Jan  5 19:59:55.144: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-jdxgx-daemon-set
    STEP: Confirm DaemonSet "e2e-jdxgx-daemon-set" successfully created with "daemonset-name=e2e-jdxgx-daemon-set" label 01/05/23 19:59:55.148
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-jdxgx-daemon-set" 01/05/23 19:59:55.154
    Jan  5 19:59:55.156: INFO: Located ControllerRevision: "e2e-jdxgx-daemon-set-58b87fd88b"
    STEP: Patching ControllerRevision "e2e-jdxgx-daemon-set-58b87fd88b" 01/05/23 19:59:55.158
    Jan  5 19:59:55.166: INFO: e2e-jdxgx-daemon-set-58b87fd88b has been patched
    STEP: Create a new ControllerRevision 01/05/23 19:59:55.166
    Jan  5 19:59:55.171: INFO: Created ControllerRevision: e2e-jdxgx-daemon-set-54f4c57bbd
    STEP: Confirm that there are two ControllerRevisions 01/05/23 19:59:55.171
    Jan  5 19:59:55.171: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 19:59:55.173: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-jdxgx-daemon-set-58b87fd88b" 01/05/23 19:59:55.173
    STEP: Confirm that there is only one ControllerRevision 01/05/23 19:59:55.178
    Jan  5 19:59:55.178: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 19:59:55.181: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-jdxgx-daemon-set-54f4c57bbd" 01/05/23 19:59:55.183
    Jan  5 19:59:55.192: INFO: e2e-jdxgx-daemon-set-54f4c57bbd has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 01/05/23 19:59:55.192
    W0105 19:59:55.200016      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 01/05/23 19:59:55.2
    Jan  5 19:59:55.200: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 19:59:56.203: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 19:59:56.206: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-jdxgx-daemon-set-54f4c57bbd=updated" 01/05/23 19:59:56.206
    STEP: Confirm that there is only one ControllerRevision 01/05/23 19:59:56.214
    Jan  5 19:59:56.214: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jan  5 19:59:56.217: INFO: Found 1 ControllerRevisions
    Jan  5 19:59:56.219: INFO: ControllerRevision "e2e-jdxgx-daemon-set-6bb5d944d9" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-jdxgx-daemon-set" 01/05/23 19:59:56.221
    STEP: deleting DaemonSet.extensions e2e-jdxgx-daemon-set in namespace controllerrevisions-8599, will wait for the garbage collector to delete the pods 01/05/23 19:59:56.221
    Jan  5 19:59:56.288: INFO: Deleting DaemonSet.extensions e2e-jdxgx-daemon-set took: 13.444ms
    Jan  5 19:59:56.389: INFO: Terminating DaemonSet.extensions e2e-jdxgx-daemon-set pods took: 100.617839ms
    Jan  5 19:59:58.193: INFO: Number of nodes with available pods controlled by daemonset e2e-jdxgx-daemon-set: 0
    Jan  5 19:59:58.193: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-jdxgx-daemon-set
    Jan  5 19:59:58.195: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"91223"},"items":null}

    Jan  5 19:59:58.197: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"91223"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 19:59:58.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ControllerRevision [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "controllerrevisions-8599" for this suite. 01/05/23 19:59:58.211
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 19:59:58.221
Jan  5 19:59:58.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replication-controller 01/05/23 19:59:58.223
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:58.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:58.239
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83
Jan  5 19:59:58.243: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/05/23 19:59:59.254
STEP: Checking rc "condition-test" has the desired failure condition set 01/05/23 19:59:59.261
STEP: Scaling down rc "condition-test" to satisfy pod quota 01/05/23 20:00:00.27
Jan  5 20:00:00.291: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 01/05/23 20:00:00.291
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  5 20:00:00.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-5117" for this suite. 01/05/23 20:00:00.31
------------------------------
• [2.101 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 19:59:58.221
    Jan  5 19:59:58.222: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replication-controller 01/05/23 19:59:58.223
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 19:59:58.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 19:59:58.239
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:83
    Jan  5 19:59:58.243: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 01/05/23 19:59:59.254
    STEP: Checking rc "condition-test" has the desired failure condition set 01/05/23 19:59:59.261
    STEP: Scaling down rc "condition-test" to satisfy pod quota 01/05/23 20:00:00.27
    Jan  5 20:00:00.291: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 01/05/23 20:00:00.291
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:00:00.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-5117" for this suite. 01/05/23 20:00:00.31
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:00:00.324
Jan  5 20:00:00.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename runtimeclass 01/05/23 20:00:00.327
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:00.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:00.362
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:31
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/node/init/init.go:32
Jan  5 20:00:00.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] RuntimeClass
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] RuntimeClass
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] RuntimeClass
  tear down framework | framework.go:193
STEP: Destroying namespace "runtimeclass-2651" for this suite. 01/05/23 20:00:00.398
------------------------------
• [0.088 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:00:00.324
    Jan  5 20:00:00.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename runtimeclass 01/05/23 20:00:00.327
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:00.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:00.362
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:31
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:00:00.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] RuntimeClass
      tear down framework | framework.go:193
    STEP: Destroying namespace "runtimeclass-2651" for this suite. 01/05/23 20:00:00.398
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:00:00.418
Jan  5 20:00:00.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 20:00:00.419
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:00.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:00.468
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787
STEP: creating service endpoint-test2 in namespace services-4944 01/05/23 20:00:00.474
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[] 01/05/23 20:00:00.503
Jan  5 20:00:00.508: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
Jan  5 20:00:01.515: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-4944 01/05/23 20:00:01.515
Jan  5 20:00:01.525: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4944" to be "running and ready"
Jan  5 20:00:01.530: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936148ms
Jan  5 20:00:01.530: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:00:03.533: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008417445s
Jan  5 20:00:03.534: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 20:00:03.534: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[pod1:[80]] 01/05/23 20:00:03.538
Jan  5 20:00:03.546: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 01/05/23 20:00:03.546
Jan  5 20:00:03.546: INFO: Creating new exec pod
Jan  5 20:00:03.553: INFO: Waiting up to 5m0s for pod "execpodjh22s" in namespace "services-4944" to be "running"
Jan  5 20:00:03.556: INFO: Pod "execpodjh22s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.555543ms
Jan  5 20:00:05.562: INFO: Pod "execpodjh22s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008631432s
Jan  5 20:00:05.562: INFO: Pod "execpodjh22s" satisfied condition "running"
Jan  5 20:00:06.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:06.707: INFO: rc: 1
Jan  5 20:00:06.707: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 20:00:07.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:07.931: INFO: rc: 1
Jan  5 20:00:07.931: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 20:00:08.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:08.850: INFO: rc: 1
Jan  5 20:00:08.850: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 20:00:09.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:09.861: INFO: rc: 1
Jan  5 20:00:09.861: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 20:00:10.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:10.849: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 20:00:10.849: INFO: stdout: ""
Jan  5 20:00:10.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 10.20.15.118 80'
Jan  5 20:00:10.993: INFO: stderr: "+ nc -v -z -w 2 10.20.15.118 80\nConnection to 10.20.15.118 80 port [tcp/http] succeeded!\n"
Jan  5 20:00:10.993: INFO: stdout: ""
STEP: Creating pod pod2 in namespace services-4944 01/05/23 20:00:10.993
Jan  5 20:00:11.004: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4944" to be "running and ready"
Jan  5 20:00:11.007: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.694781ms
Jan  5 20:00:11.007: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:00:13.010: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005849674s
Jan  5 20:00:13.010: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 20:00:13.010: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[pod1:[80] pod2:[80]] 01/05/23 20:00:13.013
Jan  5 20:00:13.038: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 01/05/23 20:00:13.038
Jan  5 20:00:14.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:14.189: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 20:00:14.189: INFO: stdout: ""
Jan  5 20:00:14.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 10.20.15.118 80'
Jan  5 20:00:14.328: INFO: stderr: "+ nc -v -z -w 2 10.20.15.118 80\nConnection to 10.20.15.118 80 port [tcp/http] succeeded!\n"
Jan  5 20:00:14.328: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-4944 01/05/23 20:00:14.328
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[pod2:[80]] 01/05/23 20:00:14.342
Jan  5 20:00:15.364: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 01/05/23 20:00:15.364
Jan  5 20:00:16.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:18.515: INFO: rc: 1
Jan  5 20:00:18.515: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 20:00:19.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:21.658: INFO: rc: 1
Jan  5 20:00:21.658: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
Command stdout:

stderr:
+ nc -v -z -w 2 endpoint-test2 80
nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 20:00:22.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
Jan  5 20:00:22.647: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jan  5 20:00:22.647: INFO: stdout: ""
Jan  5 20:00:22.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 10.20.15.118 80'
Jan  5 20:00:22.783: INFO: stderr: "+ nc -v -z -w 2 10.20.15.118 80\nConnection to 10.20.15.118 80 port [tcp/http] succeeded!\n"
Jan  5 20:00:22.783: INFO: stdout: ""
STEP: Deleting pod pod2 in namespace services-4944 01/05/23 20:00:22.783
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[] 01/05/23 20:00:22.798
Jan  5 20:00:23.812: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 20:00:23.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-4944" for this suite. 01/05/23 20:00:23.838
------------------------------
• [SLOW TEST] [23.426 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:787

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:00:00.418
    Jan  5 20:00:00.418: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 20:00:00.419
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:00.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:00.468
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:787
    STEP: creating service endpoint-test2 in namespace services-4944 01/05/23 20:00:00.474
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[] 01/05/23 20:00:00.503
    Jan  5 20:00:00.508: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
    Jan  5 20:00:01.515: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-4944 01/05/23 20:00:01.515
    Jan  5 20:00:01.525: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-4944" to be "running and ready"
    Jan  5 20:00:01.530: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.936148ms
    Jan  5 20:00:01.530: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:00:03.533: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008417445s
    Jan  5 20:00:03.534: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 20:00:03.534: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[pod1:[80]] 01/05/23 20:00:03.538
    Jan  5 20:00:03.546: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 01/05/23 20:00:03.546
    Jan  5 20:00:03.546: INFO: Creating new exec pod
    Jan  5 20:00:03.553: INFO: Waiting up to 5m0s for pod "execpodjh22s" in namespace "services-4944" to be "running"
    Jan  5 20:00:03.556: INFO: Pod "execpodjh22s": Phase="Pending", Reason="", readiness=false. Elapsed: 2.555543ms
    Jan  5 20:00:05.562: INFO: Pod "execpodjh22s": Phase="Running", Reason="", readiness=true. Elapsed: 2.008631432s
    Jan  5 20:00:05.562: INFO: Pod "execpodjh22s" satisfied condition "running"
    Jan  5 20:00:06.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:06.707: INFO: rc: 1
    Jan  5 20:00:06.707: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 20:00:07.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:07.931: INFO: rc: 1
    Jan  5 20:00:07.931: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 20:00:08.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:08.850: INFO: rc: 1
    Jan  5 20:00:08.850: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 20:00:09.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:09.861: INFO: rc: 1
    Jan  5 20:00:09.861: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 20:00:10.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:10.849: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 20:00:10.849: INFO: stdout: ""
    Jan  5 20:00:10.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 10.20.15.118 80'
    Jan  5 20:00:10.993: INFO: stderr: "+ nc -v -z -w 2 10.20.15.118 80\nConnection to 10.20.15.118 80 port [tcp/http] succeeded!\n"
    Jan  5 20:00:10.993: INFO: stdout: ""
    STEP: Creating pod pod2 in namespace services-4944 01/05/23 20:00:10.993
    Jan  5 20:00:11.004: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-4944" to be "running and ready"
    Jan  5 20:00:11.007: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.694781ms
    Jan  5 20:00:11.007: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:00:13.010: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.005849674s
    Jan  5 20:00:13.010: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 20:00:13.010: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[pod1:[80] pod2:[80]] 01/05/23 20:00:13.013
    Jan  5 20:00:13.038: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 01/05/23 20:00:13.038
    Jan  5 20:00:14.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:14.189: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 20:00:14.189: INFO: stdout: ""
    Jan  5 20:00:14.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 10.20.15.118 80'
    Jan  5 20:00:14.328: INFO: stderr: "+ nc -v -z -w 2 10.20.15.118 80\nConnection to 10.20.15.118 80 port [tcp/http] succeeded!\n"
    Jan  5 20:00:14.328: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-4944 01/05/23 20:00:14.328
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[pod2:[80]] 01/05/23 20:00:14.342
    Jan  5 20:00:15.364: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 01/05/23 20:00:15.364
    Jan  5 20:00:16.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:18.515: INFO: rc: 1
    Jan  5 20:00:18.515: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 20:00:19.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:21.658: INFO: rc: 1
    Jan  5 20:00:21.658: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 endpoint-test2 80
    nc: connect to endpoint-test2 port 80 (tcp) timed out: Operation in progress
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 20:00:22.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 endpoint-test2 80'
    Jan  5 20:00:22.647: INFO: stderr: "+ nc -v -z -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jan  5 20:00:22.647: INFO: stdout: ""
    Jan  5 20:00:22.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-4944 exec execpodjh22s -- /bin/sh -x -c nc -v -z -w 2 10.20.15.118 80'
    Jan  5 20:00:22.783: INFO: stderr: "+ nc -v -z -w 2 10.20.15.118 80\nConnection to 10.20.15.118 80 port [tcp/http] succeeded!\n"
    Jan  5 20:00:22.783: INFO: stdout: ""
    STEP: Deleting pod pod2 in namespace services-4944 01/05/23 20:00:22.783
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4944 to expose endpoints map[] 01/05/23 20:00:22.798
    Jan  5 20:00:23.812: INFO: successfully validated that service endpoint-test2 in namespace services-4944 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:00:23.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-4944" for this suite. 01/05/23 20:00:23.838
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:00:23.847
Jan  5 20:00:23.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 20:00:23.848
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:23.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:23.865
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 20:00:23.868
Jan  5 20:00:23.878: INFO: Waiting up to 5m0s for pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df" in namespace "emptydir-4992" to be "Succeeded or Failed"
Jan  5 20:00:23.880: INFO: Pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.518354ms
Jan  5 20:00:25.886: INFO: Pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008775617s
Jan  5 20:00:27.885: INFO: Pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006828369s
STEP: Saw pod success 01/05/23 20:00:27.885
Jan  5 20:00:27.885: INFO: Pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df" satisfied condition "Succeeded or Failed"
Jan  5 20:00:27.887: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-461bc145-91eb-45d4-ab46-ea0eea9867df container test-container: <nil>
STEP: delete the pod 01/05/23 20:00:27.894
Jan  5 20:00:27.903: INFO: Waiting for pod pod-461bc145-91eb-45d4-ab46-ea0eea9867df to disappear
Jan  5 20:00:27.906: INFO: Pod pod-461bc145-91eb-45d4-ab46-ea0eea9867df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 20:00:27.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-4992" for this suite. 01/05/23 20:00:27.909
------------------------------
• [4.068 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:147

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:00:23.847
    Jan  5 20:00:23.847: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 20:00:23.848
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:23.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:23.865
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:147
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 20:00:23.868
    Jan  5 20:00:23.878: INFO: Waiting up to 5m0s for pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df" in namespace "emptydir-4992" to be "Succeeded or Failed"
    Jan  5 20:00:23.880: INFO: Pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.518354ms
    Jan  5 20:00:25.886: INFO: Pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008775617s
    Jan  5 20:00:27.885: INFO: Pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006828369s
    STEP: Saw pod success 01/05/23 20:00:27.885
    Jan  5 20:00:27.885: INFO: Pod "pod-461bc145-91eb-45d4-ab46-ea0eea9867df" satisfied condition "Succeeded or Failed"
    Jan  5 20:00:27.887: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-461bc145-91eb-45d4-ab46-ea0eea9867df container test-container: <nil>
    STEP: delete the pod 01/05/23 20:00:27.894
    Jan  5 20:00:27.903: INFO: Waiting for pod pod-461bc145-91eb-45d4-ab46-ea0eea9867df to disappear
    Jan  5 20:00:27.906: INFO: Pod pod-461bc145-91eb-45d4-ab46-ea0eea9867df no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:00:27.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-4992" for this suite. 01/05/23 20:00:27.909
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:00:27.917
Jan  5 20:00:27.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 20:00:27.919
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:27.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:27.934
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 20:00:27.948
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:00:28.685
STEP: Deploying the webhook pod 01/05/23 20:00:28.689
STEP: Wait for the deployment to be ready 01/05/23 20:00:28.699
Jan  5 20:00:28.705: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 20:00:30.715
STEP: Verifying the service has paired with the endpoint 01/05/23 20:00:30.726
Jan  5 20:00:31.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 20:00:31.731
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 20:00:31.752
STEP: Creating a dummy validating-webhook-configuration object 01/05/23 20:00:31.772
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/05/23 20:00:31.782
STEP: Creating a dummy mutating-webhook-configuration object 01/05/23 20:00:31.786
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/05/23 20:00:31.796
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:00:31.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-1572" for this suite. 01/05/23 20:00:31.853
STEP: Destroying namespace "webhook-1572-markers" for this suite. 01/05/23 20:00:31.858
------------------------------
• [3.950 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:277

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:00:27.917
    Jan  5 20:00:27.918: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 20:00:27.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:27.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:27.934
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 20:00:27.948
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:00:28.685
    STEP: Deploying the webhook pod 01/05/23 20:00:28.689
    STEP: Wait for the deployment to be ready 01/05/23 20:00:28.699
    Jan  5 20:00:28.705: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 20:00:30.715
    STEP: Verifying the service has paired with the endpoint 01/05/23 20:00:30.726
    Jan  5 20:00:31.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:277
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 20:00:31.731
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 01/05/23 20:00:31.752
    STEP: Creating a dummy validating-webhook-configuration object 01/05/23 20:00:31.772
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 01/05/23 20:00:31.782
    STEP: Creating a dummy mutating-webhook-configuration object 01/05/23 20:00:31.786
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 01/05/23 20:00:31.796
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:00:31.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-1572" for this suite. 01/05/23 20:00:31.853
    STEP: Destroying namespace "webhook-1572-markers" for this suite. 01/05/23 20:00:31.858
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:00:31.869
Jan  5 20:00:31.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename statefulset 01/05/23 20:00:31.871
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:31.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:31.887
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-4416 01/05/23 20:00:31.891
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:739
STEP: Looking for a node to schedule stateful set and pod 01/05/23 20:00:31.899
STEP: Creating pod with conflicting port in namespace statefulset-4416 01/05/23 20:00:31.91
STEP: Waiting until pod test-pod will start running in namespace statefulset-4416 01/05/23 20:00:31.929
Jan  5 20:00:31.929: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4416" to be "running"
Jan  5 20:00:31.933: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.85576ms
Jan  5 20:00:33.936: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007244932s
Jan  5 20:00:33.937: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-4416 01/05/23 20:00:33.937
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4416 01/05/23 20:00:33.942
Jan  5 20:00:33.956: INFO: Observed stateful pod in namespace: statefulset-4416, name: ss-0, uid: 6e343c70-3228-4b3b-9a8c-6c86219536be, status phase: Pending. Waiting for statefulset controller to delete.
Jan  5 20:00:33.970: INFO: Observed stateful pod in namespace: statefulset-4416, name: ss-0, uid: 6e343c70-3228-4b3b-9a8c-6c86219536be, status phase: Failed. Waiting for statefulset controller to delete.
Jan  5 20:00:33.978: INFO: Observed stateful pod in namespace: statefulset-4416, name: ss-0, uid: 6e343c70-3228-4b3b-9a8c-6c86219536be, status phase: Failed. Waiting for statefulset controller to delete.
Jan  5 20:00:33.983: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4416
STEP: Removing pod with conflicting port in namespace statefulset-4416 01/05/23 20:00:33.983
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4416 and will be in running state 01/05/23 20:00:33.998
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  5 20:00:36.007: INFO: Deleting all statefulset in ns statefulset-4416
Jan  5 20:00:36.010: INFO: Scaling statefulset ss to 0
Jan  5 20:00:46.024: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 20:00:46.032: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  5 20:00:46.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-4416" for this suite. 01/05/23 20:00:46.046
------------------------------
• [SLOW TEST] [14.190 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:00:31.869
    Jan  5 20:00:31.869: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename statefulset 01/05/23 20:00:31.871
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:31.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:31.887
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-4416 01/05/23 20:00:31.891
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:739
    STEP: Looking for a node to schedule stateful set and pod 01/05/23 20:00:31.899
    STEP: Creating pod with conflicting port in namespace statefulset-4416 01/05/23 20:00:31.91
    STEP: Waiting until pod test-pod will start running in namespace statefulset-4416 01/05/23 20:00:31.929
    Jan  5 20:00:31.929: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-4416" to be "running"
    Jan  5 20:00:31.933: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.85576ms
    Jan  5 20:00:33.936: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.007244932s
    Jan  5 20:00:33.937: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-4416 01/05/23 20:00:33.937
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4416 01/05/23 20:00:33.942
    Jan  5 20:00:33.956: INFO: Observed stateful pod in namespace: statefulset-4416, name: ss-0, uid: 6e343c70-3228-4b3b-9a8c-6c86219536be, status phase: Pending. Waiting for statefulset controller to delete.
    Jan  5 20:00:33.970: INFO: Observed stateful pod in namespace: statefulset-4416, name: ss-0, uid: 6e343c70-3228-4b3b-9a8c-6c86219536be, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  5 20:00:33.978: INFO: Observed stateful pod in namespace: statefulset-4416, name: ss-0, uid: 6e343c70-3228-4b3b-9a8c-6c86219536be, status phase: Failed. Waiting for statefulset controller to delete.
    Jan  5 20:00:33.983: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4416
    STEP: Removing pod with conflicting port in namespace statefulset-4416 01/05/23 20:00:33.983
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4416 and will be in running state 01/05/23 20:00:33.998
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  5 20:00:36.007: INFO: Deleting all statefulset in ns statefulset-4416
    Jan  5 20:00:36.010: INFO: Scaling statefulset ss to 0
    Jan  5 20:00:46.024: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 20:00:46.032: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:00:46.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-4416" for this suite. 01/05/23 20:00:46.046
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:00:46.06
Jan  5 20:00:46.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 20:00:46.062
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:46.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:46.075
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228
STEP: creating service in namespace services-7055 01/05/23 20:00:46.078
STEP: creating service affinity-nodeport in namespace services-7055 01/05/23 20:00:46.078
STEP: creating replication controller affinity-nodeport in namespace services-7055 01/05/23 20:00:46.09
I0105 20:00:46.100247      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7055, replica count: 3
I0105 20:00:49.150826      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jan  5 20:00:49.159: INFO: Creating new exec pod
Jan  5 20:00:49.167: INFO: Waiting up to 5m0s for pod "execpod-affinityfcldp" in namespace "services-7055" to be "running"
Jan  5 20:00:49.170: INFO: Pod "execpod-affinityfcldp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351623ms
Jan  5 20:00:51.174: INFO: Pod "execpod-affinityfcldp": Phase="Running", Reason="", readiness=true. Elapsed: 2.006383002s
Jan  5 20:00:51.174: INFO: Pod "execpod-affinityfcldp" satisfied condition "running"
Jan  5 20:00:52.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
Jan  5 20:00:52.316: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jan  5 20:00:52.316: INFO: stdout: ""
Jan  5 20:00:52.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c nc -v -z -w 2 10.20.3.184 80'
Jan  5 20:00:52.448: INFO: stderr: "+ nc -v -z -w 2 10.20.3.184 80\nConnection to 10.20.3.184 80 port [tcp/http] succeeded!\n"
Jan  5 20:00:52.448: INFO: stdout: ""
Jan  5 20:00:52.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c nc -v -z -w 2 10.196.0.39 31338'
Jan  5 20:00:52.592: INFO: stderr: "+ nc -v -z -w 2 10.196.0.39 31338\nConnection to 10.196.0.39 31338 port [tcp/*] succeeded!\n"
Jan  5 20:00:52.592: INFO: stdout: ""
Jan  5 20:00:52.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c nc -v -z -w 2 10.196.0.38 31338'
Jan  5 20:00:52.729: INFO: stderr: "+ nc -v -z -w 2 10.196.0.38 31338\nConnection to 10.196.0.38 31338 port [tcp/*] succeeded!\n"
Jan  5 20:00:52.729: INFO: stdout: ""
Jan  5 20:00:52.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.196.0.39:31338/ ; done'
Jan  5 20:00:52.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n"
Jan  5 20:00:52.971: INFO: stdout: "\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn"
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
Jan  5 20:00:52.971: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7055, will wait for the garbage collector to delete the pods 01/05/23 20:00:52.988
Jan  5 20:00:53.049: INFO: Deleting ReplicationController affinity-nodeport took: 6.908669ms
Jan  5 20:00:53.149: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.388951ms
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 20:00:55.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7055" for this suite. 01/05/23 20:00:55.071
------------------------------
• [SLOW TEST] [9.015 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2228

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:00:46.06
    Jan  5 20:00:46.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 20:00:46.062
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:46.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:46.075
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2228
    STEP: creating service in namespace services-7055 01/05/23 20:00:46.078
    STEP: creating service affinity-nodeport in namespace services-7055 01/05/23 20:00:46.078
    STEP: creating replication controller affinity-nodeport in namespace services-7055 01/05/23 20:00:46.09
    I0105 20:00:46.100247      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7055, replica count: 3
    I0105 20:00:49.150826      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jan  5 20:00:49.159: INFO: Creating new exec pod
    Jan  5 20:00:49.167: INFO: Waiting up to 5m0s for pod "execpod-affinityfcldp" in namespace "services-7055" to be "running"
    Jan  5 20:00:49.170: INFO: Pod "execpod-affinityfcldp": Phase="Pending", Reason="", readiness=false. Elapsed: 2.351623ms
    Jan  5 20:00:51.174: INFO: Pod "execpod-affinityfcldp": Phase="Running", Reason="", readiness=true. Elapsed: 2.006383002s
    Jan  5 20:00:51.174: INFO: Pod "execpod-affinityfcldp" satisfied condition "running"
    Jan  5 20:00:52.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c nc -v -z -w 2 affinity-nodeport 80'
    Jan  5 20:00:52.316: INFO: stderr: "+ nc -v -z -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jan  5 20:00:52.316: INFO: stdout: ""
    Jan  5 20:00:52.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c nc -v -z -w 2 10.20.3.184 80'
    Jan  5 20:00:52.448: INFO: stderr: "+ nc -v -z -w 2 10.20.3.184 80\nConnection to 10.20.3.184 80 port [tcp/http] succeeded!\n"
    Jan  5 20:00:52.448: INFO: stdout: ""
    Jan  5 20:00:52.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c nc -v -z -w 2 10.196.0.39 31338'
    Jan  5 20:00:52.592: INFO: stderr: "+ nc -v -z -w 2 10.196.0.39 31338\nConnection to 10.196.0.39 31338 port [tcp/*] succeeded!\n"
    Jan  5 20:00:52.592: INFO: stdout: ""
    Jan  5 20:00:52.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c nc -v -z -w 2 10.196.0.38 31338'
    Jan  5 20:00:52.729: INFO: stderr: "+ nc -v -z -w 2 10.196.0.38 31338\nConnection to 10.196.0.38 31338 port [tcp/*] succeeded!\n"
    Jan  5 20:00:52.729: INFO: stdout: ""
    Jan  5 20:00:52.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7055 exec execpod-affinityfcldp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.196.0.39:31338/ ; done'
    Jan  5 20:00:52.971: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.196.0.39:31338/\n"
    Jan  5 20:00:52.971: INFO: stdout: "\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn\naffinity-nodeport-j5hjn"
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Received response from host: affinity-nodeport-j5hjn
    Jan  5 20:00:52.971: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7055, will wait for the garbage collector to delete the pods 01/05/23 20:00:52.988
    Jan  5 20:00:53.049: INFO: Deleting ReplicationController affinity-nodeport took: 6.908669ms
    Jan  5 20:00:53.149: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.388951ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:00:55.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7055" for this suite. 01/05/23 20:00:55.071
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:00:55.077
Jan  5 20:00:55.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 20:00:55.078
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:55.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:55.094
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109
STEP: Creating configMap with name configmap-test-volume-map-12559859-ffb8-46d1-859a-55759ff4270b 01/05/23 20:00:55.097
STEP: Creating a pod to test consume configMaps 01/05/23 20:00:55.101
Jan  5 20:00:55.111: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd" in namespace "configmap-3947" to be "Succeeded or Failed"
Jan  5 20:00:55.115: INFO: Pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.500617ms
Jan  5 20:00:57.120: INFO: Pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00860902s
Jan  5 20:00:59.119: INFO: Pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008112776s
STEP: Saw pod success 01/05/23 20:00:59.119
Jan  5 20:00:59.119: INFO: Pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd" satisfied condition "Succeeded or Failed"
Jan  5 20:00:59.122: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd container agnhost-container: <nil>
STEP: delete the pod 01/05/23 20:00:59.128
Jan  5 20:00:59.141: INFO: Waiting for pod pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd to disappear
Jan  5 20:00:59.145: INFO: Pod pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:00:59.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3947" for this suite. 01/05/23 20:00:59.15
------------------------------
• [4.078 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:00:55.077
    Jan  5 20:00:55.077: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 20:00:55.078
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:55.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:55.094
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:109
    STEP: Creating configMap with name configmap-test-volume-map-12559859-ffb8-46d1-859a-55759ff4270b 01/05/23 20:00:55.097
    STEP: Creating a pod to test consume configMaps 01/05/23 20:00:55.101
    Jan  5 20:00:55.111: INFO: Waiting up to 5m0s for pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd" in namespace "configmap-3947" to be "Succeeded or Failed"
    Jan  5 20:00:55.115: INFO: Pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.500617ms
    Jan  5 20:00:57.120: INFO: Pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00860902s
    Jan  5 20:00:59.119: INFO: Pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008112776s
    STEP: Saw pod success 01/05/23 20:00:59.119
    Jan  5 20:00:59.119: INFO: Pod "pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd" satisfied condition "Succeeded or Failed"
    Jan  5 20:00:59.122: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 20:00:59.128
    Jan  5 20:00:59.141: INFO: Waiting for pod pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd to disappear
    Jan  5 20:00:59.145: INFO: Pod pod-configmaps-b1a7be5d-8f50-4562-845b-b9d210c098dd no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:00:59.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3947" for this suite. 01/05/23 20:00:59.15
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:00:59.158
Jan  5 20:00:59.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename security-context-test 01/05/23 20:00:59.159
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:59.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:59.177
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:50
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:486
Jan  5 20:00:59.188: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b" in namespace "security-context-test-6941" to be "Succeeded or Failed"
Jan  5 20:00:59.195: INFO: Pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.931182ms
Jan  5 20:01:01.200: INFO: Pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011850377s
Jan  5 20:01:03.199: INFO: Pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011163535s
Jan  5 20:01:03.199: INFO: Pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  5 20:01:03.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-test-6941" for this suite. 01/05/23 20:01:03.203
------------------------------
• [4.051 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:430
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:486

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:00:59.158
    Jan  5 20:00:59.158: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename security-context-test 01/05/23 20:00:59.159
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:00:59.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:00:59.177
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:50
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:486
    Jan  5 20:00:59.188: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b" in namespace "security-context-test-6941" to be "Succeeded or Failed"
    Jan  5 20:00:59.195: INFO: Pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.931182ms
    Jan  5 20:01:01.200: INFO: Pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011850377s
    Jan  5 20:01:03.199: INFO: Pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011163535s
    Jan  5 20:01:03.199: INFO: Pod "busybox-readonly-false-312a9404-d263-4bc9-96c3-25401f4cac1b" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:01:03.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-test-6941" for this suite. 01/05/23 20:01:03.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:01:03.215
Jan  5 20:01:03.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename subpath 01/05/23 20:01:03.216
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:01:03.229
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:01:03.233
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 01/05/23 20:01:03.236
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-whwq 01/05/23 20:01:03.245
STEP: Creating a pod to test atomic-volume-subpath 01/05/23 20:01:03.245
Jan  5 20:01:03.253: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-whwq" in namespace "subpath-2703" to be "Succeeded or Failed"
Jan  5 20:01:03.257: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.701362ms
Jan  5 20:01:05.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00760241s
Jan  5 20:01:07.263: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 4.01026127s
Jan  5 20:01:09.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 6.007361266s
Jan  5 20:01:11.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 8.006577251s
Jan  5 20:01:13.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 10.006774077s
Jan  5 20:01:15.262: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 12.008992501s
Jan  5 20:01:17.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 14.006985493s
Jan  5 20:01:19.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 16.007026776s
Jan  5 20:01:21.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 18.007625791s
Jan  5 20:01:23.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 20.007327224s
Jan  5 20:01:25.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=false. Elapsed: 22.008061372s
Jan  5 20:01:27.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007032983s
STEP: Saw pod success 01/05/23 20:01:27.26
Jan  5 20:01:27.260: INFO: Pod "pod-subpath-test-configmap-whwq" satisfied condition "Succeeded or Failed"
Jan  5 20:01:27.263: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-subpath-test-configmap-whwq container test-container-subpath-configmap-whwq: <nil>
STEP: delete the pod 01/05/23 20:01:27.273
Jan  5 20:01:27.285: INFO: Waiting for pod pod-subpath-test-configmap-whwq to disappear
Jan  5 20:01:27.288: INFO: Pod pod-subpath-test-configmap-whwq no longer exists
STEP: Deleting pod pod-subpath-test-configmap-whwq 01/05/23 20:01:27.288
Jan  5 20:01:27.288: INFO: Deleting pod "pod-subpath-test-configmap-whwq" in namespace "subpath-2703"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/node/init/init.go:32
Jan  5 20:01:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Subpath
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Subpath
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Subpath
  tear down framework | framework.go:193
STEP: Destroying namespace "subpath-2703" for this suite. 01/05/23 20:01:27.299
------------------------------
• [SLOW TEST] [24.090 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:01:03.215
    Jan  5 20:01:03.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename subpath 01/05/23 20:01:03.216
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:01:03.229
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:01:03.233
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 01/05/23 20:01:03.236
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-whwq 01/05/23 20:01:03.245
    STEP: Creating a pod to test atomic-volume-subpath 01/05/23 20:01:03.245
    Jan  5 20:01:03.253: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-whwq" in namespace "subpath-2703" to be "Succeeded or Failed"
    Jan  5 20:01:03.257: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Pending", Reason="", readiness=false. Elapsed: 3.701362ms
    Jan  5 20:01:05.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 2.00760241s
    Jan  5 20:01:07.263: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 4.01026127s
    Jan  5 20:01:09.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 6.007361266s
    Jan  5 20:01:11.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 8.006577251s
    Jan  5 20:01:13.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 10.006774077s
    Jan  5 20:01:15.262: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 12.008992501s
    Jan  5 20:01:17.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 14.006985493s
    Jan  5 20:01:19.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 16.007026776s
    Jan  5 20:01:21.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 18.007625791s
    Jan  5 20:01:23.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=true. Elapsed: 20.007327224s
    Jan  5 20:01:25.261: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Running", Reason="", readiness=false. Elapsed: 22.008061372s
    Jan  5 20:01:27.260: INFO: Pod "pod-subpath-test-configmap-whwq": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.007032983s
    STEP: Saw pod success 01/05/23 20:01:27.26
    Jan  5 20:01:27.260: INFO: Pod "pod-subpath-test-configmap-whwq" satisfied condition "Succeeded or Failed"
    Jan  5 20:01:27.263: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-subpath-test-configmap-whwq container test-container-subpath-configmap-whwq: <nil>
    STEP: delete the pod 01/05/23 20:01:27.273
    Jan  5 20:01:27.285: INFO: Waiting for pod pod-subpath-test-configmap-whwq to disappear
    Jan  5 20:01:27.288: INFO: Pod pod-subpath-test-configmap-whwq no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-whwq 01/05/23 20:01:27.288
    Jan  5 20:01:27.288: INFO: Deleting pod "pod-subpath-test-configmap-whwq" in namespace "subpath-2703"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:01:27.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Subpath
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Subpath
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Subpath
      tear down framework | framework.go:193
    STEP: Destroying namespace "subpath-2703" for this suite. 01/05/23 20:01:27.299
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:01:27.306
Jan  5 20:01:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename statefulset 01/05/23 20:01:27.307
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:01:27.317
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:01:27.32
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5220 01/05/23 20:01:27.323
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:587
STEP: Initializing watcher for selector baz=blah,foo=bar 01/05/23 20:01:27.33
STEP: Creating stateful set ss in namespace statefulset-5220 01/05/23 20:01:27.334
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5220 01/05/23 20:01:27.341
Jan  5 20:01:27.343: INFO: Found 0 stateful pods, waiting for 1
Jan  5 20:01:37.347: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/05/23 20:01:37.347
Jan  5 20:01:37.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 20:01:37.496: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 20:01:37.496: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 20:01:37.496: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 20:01:37.499: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jan  5 20:01:47.503: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 20:01:47.503: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 20:01:47.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999782s
Jan  5 20:01:48.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99770522s
Jan  5 20:01:49.523: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994396656s
Jan  5 20:01:50.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991680242s
Jan  5 20:01:51.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988085757s
Jan  5 20:01:52.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.984688155s
Jan  5 20:01:53.537: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.981476514s
Jan  5 20:01:54.540: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978082229s
Jan  5 20:01:55.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.974568311s
Jan  5 20:01:56.547: INFO: Verifying statefulset ss doesn't scale past 1 for another 971.023597ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5220 01/05/23 20:01:57.548
Jan  5 20:01:57.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 20:01:57.749: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 20:01:57.749: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 20:01:57.749: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 20:01:57.752: INFO: Found 1 stateful pods, waiting for 3
Jan  5 20:02:07.757: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:02:07.757: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:02:07.757: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 01/05/23 20:02:07.757
STEP: Scale down will halt with unhealthy stateful pod 01/05/23 20:02:07.758
Jan  5 20:02:07.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 20:02:07.968: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 20:02:07.968: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 20:02:07.968: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 20:02:07.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 20:02:08.111: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 20:02:08.111: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 20:02:08.111: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 20:02:08.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 20:02:08.256: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 20:02:08.256: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 20:02:08.256: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 20:02:08.256: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 20:02:08.258: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jan  5 20:02:18.265: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 20:02:18.265: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 20:02:18.265: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jan  5 20:02:18.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999809s
Jan  5 20:02:19.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996340306s
Jan  5 20:02:20.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992163436s
Jan  5 20:02:21.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988486001s
Jan  5 20:02:22.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984994333s
Jan  5 20:02:23.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981112271s
Jan  5 20:02:24.315: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976163333s
Jan  5 20:02:25.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960351027s
Jan  5 20:02:26.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956728811s
Jan  5 20:02:27.326: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.030995ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5220 01/05/23 20:02:28.326
Jan  5 20:02:28.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 20:02:28.464: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 20:02:28.464: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 20:02:28.464: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 20:02:28.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 20:02:28.603: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 20:02:28.603: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 20:02:28.603: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 20:02:28.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 20:02:28.757: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 20:02:28.757: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 20:02:28.757: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jan  5 20:02:28.757: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 01/05/23 20:02:38.772
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  5 20:02:38.772: INFO: Deleting all statefulset in ns statefulset-5220
Jan  5 20:02:38.775: INFO: Scaling statefulset ss to 0
Jan  5 20:02:38.784: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 20:02:38.786: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  5 20:02:38.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5220" for this suite. 01/05/23 20:02:38.808
------------------------------
• [SLOW TEST] [71.508 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:587

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:01:27.306
    Jan  5 20:01:27.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename statefulset 01/05/23 20:01:27.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:01:27.317
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:01:27.32
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5220 01/05/23 20:01:27.323
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:587
    STEP: Initializing watcher for selector baz=blah,foo=bar 01/05/23 20:01:27.33
    STEP: Creating stateful set ss in namespace statefulset-5220 01/05/23 20:01:27.334
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5220 01/05/23 20:01:27.341
    Jan  5 20:01:27.343: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 20:01:37.347: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 01/05/23 20:01:37.347
    Jan  5 20:01:37.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 20:01:37.496: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 20:01:37.496: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 20:01:37.496: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 20:01:37.499: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jan  5 20:01:47.503: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 20:01:47.503: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 20:01:47.517: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999782s
    Jan  5 20:01:48.520: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.99770522s
    Jan  5 20:01:49.523: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994396656s
    Jan  5 20:01:50.526: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991680242s
    Jan  5 20:01:51.530: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988085757s
    Jan  5 20:01:52.533: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.984688155s
    Jan  5 20:01:53.537: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.981476514s
    Jan  5 20:01:54.540: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.978082229s
    Jan  5 20:01:55.544: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.974568311s
    Jan  5 20:01:56.547: INFO: Verifying statefulset ss doesn't scale past 1 for another 971.023597ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5220 01/05/23 20:01:57.548
    Jan  5 20:01:57.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 20:01:57.749: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 20:01:57.749: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 20:01:57.749: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 20:01:57.752: INFO: Found 1 stateful pods, waiting for 3
    Jan  5 20:02:07.757: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:02:07.757: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:02:07.757: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 01/05/23 20:02:07.757
    STEP: Scale down will halt with unhealthy stateful pod 01/05/23 20:02:07.758
    Jan  5 20:02:07.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 20:02:07.968: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 20:02:07.968: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 20:02:07.968: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 20:02:07.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 20:02:08.111: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 20:02:08.111: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 20:02:08.111: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 20:02:08.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 20:02:08.256: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 20:02:08.256: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 20:02:08.256: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 20:02:08.256: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 20:02:08.258: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Jan  5 20:02:18.265: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 20:02:18.265: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 20:02:18.265: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jan  5 20:02:18.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999809s
    Jan  5 20:02:19.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996340306s
    Jan  5 20:02:20.287: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992163436s
    Jan  5 20:02:21.291: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988486001s
    Jan  5 20:02:22.295: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984994333s
    Jan  5 20:02:23.299: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.981112271s
    Jan  5 20:02:24.315: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976163333s
    Jan  5 20:02:25.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.960351027s
    Jan  5 20:02:26.323: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956728811s
    Jan  5 20:02:27.326: INFO: Verifying statefulset ss doesn't scale past 3 for another 953.030995ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5220 01/05/23 20:02:28.326
    Jan  5 20:02:28.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 20:02:28.464: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 20:02:28.464: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 20:02:28.464: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 20:02:28.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 20:02:28.603: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 20:02:28.603: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 20:02:28.603: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 20:02:28.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-5220 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 20:02:28.757: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 20:02:28.757: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 20:02:28.757: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jan  5 20:02:28.757: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 01/05/23 20:02:38.772
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  5 20:02:38.772: INFO: Deleting all statefulset in ns statefulset-5220
    Jan  5 20:02:38.775: INFO: Scaling statefulset ss to 0
    Jan  5 20:02:38.784: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 20:02:38.786: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:02:38.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5220" for this suite. 01/05/23 20:02:38.808
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
[BeforeEach] [sig-node] Downward API
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:02:38.816
Jan  5 20:02:38.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename downward-api 01/05/23 20:02:38.818
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:02:38.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:02:38.832
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:31
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267
STEP: Creating a pod to test downward api env vars 01/05/23 20:02:38.835
Jan  5 20:02:38.844: INFO: Waiting up to 5m0s for pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7" in namespace "downward-api-9719" to be "Succeeded or Failed"
Jan  5 20:02:38.848: INFO: Pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.956542ms
Jan  5 20:02:40.852: INFO: Pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007813092s
Jan  5 20:02:42.852: INFO: Pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007121088s
STEP: Saw pod success 01/05/23 20:02:42.852
Jan  5 20:02:42.852: INFO: Pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7" satisfied condition "Succeeded or Failed"
Jan  5 20:02:42.854: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7 container dapi-container: <nil>
STEP: delete the pod 01/05/23 20:02:42.861
Jan  5 20:02:42.872: INFO: Waiting for pod downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7 to disappear
Jan  5 20:02:42.876: INFO: Pod downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/node/init/init.go:32
Jan  5 20:02:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Downward API
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Downward API
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Downward API
  tear down framework | framework.go:193
STEP: Destroying namespace "downward-api-9719" for this suite. 01/05/23 20:02:42.879
------------------------------
• [4.069 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:02:38.816
    Jan  5 20:02:38.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename downward-api 01/05/23 20:02:38.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:02:38.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:02:38.832
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:31
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:267
    STEP: Creating a pod to test downward api env vars 01/05/23 20:02:38.835
    Jan  5 20:02:38.844: INFO: Waiting up to 5m0s for pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7" in namespace "downward-api-9719" to be "Succeeded or Failed"
    Jan  5 20:02:38.848: INFO: Pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.956542ms
    Jan  5 20:02:40.852: INFO: Pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007813092s
    Jan  5 20:02:42.852: INFO: Pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007121088s
    STEP: Saw pod success 01/05/23 20:02:42.852
    Jan  5 20:02:42.852: INFO: Pod "downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7" satisfied condition "Succeeded or Failed"
    Jan  5 20:02:42.854: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7 container dapi-container: <nil>
    STEP: delete the pod 01/05/23 20:02:42.861
    Jan  5 20:02:42.872: INFO: Waiting for pod downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7 to disappear
    Jan  5 20:02:42.876: INFO: Pod downward-api-0f0fa894-f622-4b49-87e7-5fed950b1ed7 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:02:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Downward API
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Downward API
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Downward API
      tear down framework | framework.go:193
    STEP: Destroying namespace "downward-api-9719" for this suite. 01/05/23 20:02:42.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:02:42.89
Jan  5 20:02:42.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 20:02:42.891
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:02:42.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:02:42.908
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130
STEP: Creating the pod 01/05/23 20:02:42.912
Jan  5 20:02:42.920: INFO: Waiting up to 5m0s for pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10" in namespace "projected-2429" to be "running and ready"
Jan  5 20:02:42.925: INFO: Pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10": Phase="Pending", Reason="", readiness=false. Elapsed: 5.186041ms
Jan  5 20:02:42.925: INFO: The phase of Pod labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:02:44.928: INFO: Pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10": Phase="Running", Reason="", readiness=true. Elapsed: 2.007896955s
Jan  5 20:02:44.928: INFO: The phase of Pod labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10 is Running (Ready = true)
Jan  5 20:02:44.928: INFO: Pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10" satisfied condition "running and ready"
Jan  5 20:02:45.450: INFO: Successfully updated pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 20:02:47.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-2429" for this suite. 01/05/23 20:02:47.467
------------------------------
• [4.583 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:02:42.89
    Jan  5 20:02:42.890: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 20:02:42.891
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:02:42.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:02:42.908
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:130
    STEP: Creating the pod 01/05/23 20:02:42.912
    Jan  5 20:02:42.920: INFO: Waiting up to 5m0s for pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10" in namespace "projected-2429" to be "running and ready"
    Jan  5 20:02:42.925: INFO: Pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10": Phase="Pending", Reason="", readiness=false. Elapsed: 5.186041ms
    Jan  5 20:02:42.925: INFO: The phase of Pod labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:02:44.928: INFO: Pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10": Phase="Running", Reason="", readiness=true. Elapsed: 2.007896955s
    Jan  5 20:02:44.928: INFO: The phase of Pod labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10 is Running (Ready = true)
    Jan  5 20:02:44.928: INFO: Pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10" satisfied condition "running and ready"
    Jan  5 20:02:45.450: INFO: Successfully updated pod "labelsupdate1a66e21f-45ab-4ca8-825a-6c5dba38ed10"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:02:47.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-2429" for this suite. 01/05/23 20:02:47.467
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
[BeforeEach] [sig-network] EndpointSlice
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:02:47.474
Jan  5 20:02:47.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename endpointslice 01/05/23 20:02:47.475
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:02:47.488
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:02:47.491
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:52
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353
STEP: getting /apis 01/05/23 20:02:47.494
STEP: getting /apis/discovery.k8s.io 01/05/23 20:02:47.497
STEP: getting /apis/discovery.k8s.iov1 01/05/23 20:02:47.498
STEP: creating 01/05/23 20:02:47.499
STEP: getting 01/05/23 20:02:47.512
STEP: listing 01/05/23 20:02:47.514
STEP: watching 01/05/23 20:02:47.517
Jan  5 20:02:47.517: INFO: starting watch
STEP: cluster-wide listing 01/05/23 20:02:47.518
STEP: cluster-wide watching 01/05/23 20:02:47.52
Jan  5 20:02:47.521: INFO: starting watch
STEP: patching 01/05/23 20:02:47.522
STEP: updating 01/05/23 20:02:47.526
Jan  5 20:02:47.532: INFO: waiting for watch events with expected annotations
Jan  5 20:02:47.532: INFO: saw patched and updated annotations
STEP: deleting 01/05/23 20:02:47.533
STEP: deleting a collection 01/05/23 20:02:47.546
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/node/init/init.go:32
Jan  5 20:02:47.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] EndpointSlice
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] EndpointSlice
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] EndpointSlice
  tear down framework | framework.go:193
STEP: Destroying namespace "endpointslice-2502" for this suite. 01/05/23 20:02:47.561
------------------------------
• [0.092 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:353

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:02:47.474
    Jan  5 20:02:47.474: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename endpointslice 01/05/23 20:02:47.475
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:02:47.488
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:02:47.491
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:52
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:353
    STEP: getting /apis 01/05/23 20:02:47.494
    STEP: getting /apis/discovery.k8s.io 01/05/23 20:02:47.497
    STEP: getting /apis/discovery.k8s.iov1 01/05/23 20:02:47.498
    STEP: creating 01/05/23 20:02:47.499
    STEP: getting 01/05/23 20:02:47.512
    STEP: listing 01/05/23 20:02:47.514
    STEP: watching 01/05/23 20:02:47.517
    Jan  5 20:02:47.517: INFO: starting watch
    STEP: cluster-wide listing 01/05/23 20:02:47.518
    STEP: cluster-wide watching 01/05/23 20:02:47.52
    Jan  5 20:02:47.521: INFO: starting watch
    STEP: patching 01/05/23 20:02:47.522
    STEP: updating 01/05/23 20:02:47.526
    Jan  5 20:02:47.532: INFO: waiting for watch events with expected annotations
    Jan  5 20:02:47.532: INFO: saw patched and updated annotations
    STEP: deleting 01/05/23 20:02:47.533
    STEP: deleting a collection 01/05/23 20:02:47.546
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:02:47.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] EndpointSlice
      tear down framework | framework.go:193
    STEP: Destroying namespace "endpointslice-2502" for this suite. 01/05/23 20:02:47.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:02:47.582
Jan  5 20:02:47.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename cronjob 01/05/23 20:02:47.583
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:02:47.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:02:47.609
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 01/05/23 20:02:47.613
STEP: Ensuring no jobs are scheduled 01/05/23 20:02:47.631
STEP: Ensuring no job exists by listing jobs explicitly 01/05/23 20:07:47.638
STEP: Removing cronjob 01/05/23 20:07:47.641
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  5 20:07:47.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9363" for this suite. 01/05/23 20:07:47.649
------------------------------
• [SLOW TEST] [300.076 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:02:47.582
    Jan  5 20:02:47.582: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename cronjob 01/05/23 20:02:47.583
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:02:47.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:02:47.609
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 01/05/23 20:02:47.613
    STEP: Ensuring no jobs are scheduled 01/05/23 20:02:47.631
    STEP: Ensuring no job exists by listing jobs explicitly 01/05/23 20:07:47.638
    STEP: Removing cronjob 01/05/23 20:07:47.641
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:07:47.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9363" for this suite. 01/05/23 20:07:47.649
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:07:47.666
Jan  5 20:07:47.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-probe 01/05/23 20:07:47.668
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:07:47.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:07:47.682
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  5 20:08:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4626" for this suite. 01/05/23 20:08:47.706
------------------------------
• [SLOW TEST] [60.045 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:07:47.666
    Jan  5 20:07:47.666: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-probe 01/05/23 20:07:47.668
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:07:47.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:07:47.682
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:108
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:08:47.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4626" for this suite. 01/05/23 20:08:47.706
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:08:47.717
Jan  5 20:08:47.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename gc 01/05/23 20:08:47.719
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:08:47.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:08:47.734
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:31
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 01/05/23 20:08:47.737
STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 20:08:47.743
STEP: delete the deployment 01/05/23 20:08:48.25
STEP: wait for all rs to be garbage collected 01/05/23 20:08:48.255
STEP: expected 0 rs, got 1 rs 01/05/23 20:08:48.26
STEP: expected 0 pods, got 2 pods 01/05/23 20:08:48.264
STEP: Gathering metrics 01/05/23 20:08:48.773
W0105 20:08:48.784165      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jan  5 20:08:48.784: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/node/init/init.go:32
Jan  5 20:08:48.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Garbage collector
  tear down framework | framework.go:193
STEP: Destroying namespace "gc-1155" for this suite. 01/05/23 20:08:48.788
------------------------------
• [1.076 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:08:47.717
    Jan  5 20:08:47.717: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename gc 01/05/23 20:08:47.719
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:08:47.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:08:47.734
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:31
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 01/05/23 20:08:47.737
    STEP: Wait for the Deployment to create new ReplicaSet 01/05/23 20:08:47.743
    STEP: delete the deployment 01/05/23 20:08:48.25
    STEP: wait for all rs to be garbage collected 01/05/23 20:08:48.255
    STEP: expected 0 rs, got 1 rs 01/05/23 20:08:48.26
    STEP: expected 0 pods, got 2 pods 01/05/23 20:08:48.264
    STEP: Gathering metrics 01/05/23 20:08:48.773
    W0105 20:08:48.784165      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jan  5 20:08:48.784: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:08:48.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Garbage collector
      tear down framework | framework.go:193
    STEP: Destroying namespace "gc-1155" for this suite. 01/05/23 20:08:48.788
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
[BeforeEach] [sig-apps] Daemon set [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:08:48.793
Jan  5 20:08:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename daemonsets 01/05/23 20:08:48.794
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:08:48.805
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:08:48.808
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:146
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374
Jan  5 20:08:48.825: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 20:08:48.83
Jan  5 20:08:48.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 20:08:48.838: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 20:08:49.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 20:08:49.845: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
Jan  5 20:08:50.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 20:08:50.846: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 01/05/23 20:08:50.855
STEP: Check that daemon pods images are updated. 01/05/23 20:08:50.873
Jan  5 20:08:50.878: INFO: Wrong image for pod: daemon-set-5ckjm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:50.878: INFO: Wrong image for pod: daemon-set-jdlq7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:50.878: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:51.886: INFO: Wrong image for pod: daemon-set-jdlq7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:51.886: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:52.886: INFO: Wrong image for pod: daemon-set-jdlq7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:52.886: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:53.889: INFO: Wrong image for pod: daemon-set-jdlq7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:53.889: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:53.889: INFO: Pod daemon-set-n5ltq is not available
Jan  5 20:08:54.887: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:55.887: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
Jan  5 20:08:55.887: INFO: Pod daemon-set-wx4tb is not available
Jan  5 20:08:57.887: INFO: Pod daemon-set-6w5fp is not available
STEP: Check that daemon pods are still running on every node of the cluster. 01/05/23 20:08:57.89
Jan  5 20:08:57.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jan  5 20:08:57.896: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
Jan  5 20:08:58.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jan  5 20:08:58.908: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:111
STEP: Deleting DaemonSet "daemon-set" 01/05/23 20:08:58.92
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3673, will wait for the garbage collector to delete the pods 01/05/23 20:08:58.92
Jan  5 20:08:58.980: INFO: Deleting DaemonSet.extensions daemon-set took: 6.523071ms
Jan  5 20:08:59.082: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.293759ms
Jan  5 20:09:01.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jan  5 20:09:01.785: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jan  5 20:09:01.788: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"96732"},"items":null}

Jan  5 20:09:01.791: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"96732"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:09:01.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "daemonsets-3673" for this suite. 01/05/23 20:09:01.805
------------------------------
• [SLOW TEST] [13.018 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:08:48.793
    Jan  5 20:08:48.793: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename daemonsets 01/05/23 20:08:48.794
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:08:48.805
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:08:48.808
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:146
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:374
    Jan  5 20:08:48.825: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 01/05/23 20:08:48.83
    Jan  5 20:08:48.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 20:08:48.838: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 20:08:49.845: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 20:08:49.845: INFO: Node gke-gke-1-26-default-pool-05283374-16pz is running 0 daemon pod, expected 1
    Jan  5 20:08:50.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 20:08:50.846: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 01/05/23 20:08:50.855
    STEP: Check that daemon pods images are updated. 01/05/23 20:08:50.873
    Jan  5 20:08:50.878: INFO: Wrong image for pod: daemon-set-5ckjm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:50.878: INFO: Wrong image for pod: daemon-set-jdlq7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:50.878: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:51.886: INFO: Wrong image for pod: daemon-set-jdlq7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:51.886: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:52.886: INFO: Wrong image for pod: daemon-set-jdlq7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:52.886: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:53.889: INFO: Wrong image for pod: daemon-set-jdlq7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:53.889: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:53.889: INFO: Pod daemon-set-n5ltq is not available
    Jan  5 20:08:54.887: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:55.887: INFO: Wrong image for pod: daemon-set-mlm8k. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
    Jan  5 20:08:55.887: INFO: Pod daemon-set-wx4tb is not available
    Jan  5 20:08:57.887: INFO: Pod daemon-set-6w5fp is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 01/05/23 20:08:57.89
    Jan  5 20:08:57.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jan  5 20:08:57.896: INFO: Node gke-gke-1-26-default-pool-05283374-qmj7 is running 0 daemon pod, expected 1
    Jan  5 20:08:58.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jan  5 20:08:58.908: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:111
    STEP: Deleting DaemonSet "daemon-set" 01/05/23 20:08:58.92
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3673, will wait for the garbage collector to delete the pods 01/05/23 20:08:58.92
    Jan  5 20:08:58.980: INFO: Deleting DaemonSet.extensions daemon-set took: 6.523071ms
    Jan  5 20:08:59.082: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.293759ms
    Jan  5 20:09:01.785: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jan  5 20:09:01.785: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jan  5 20:09:01.788: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"96732"},"items":null}

    Jan  5 20:09:01.791: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"96732"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:09:01.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Daemon set [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "daemonsets-3673" for this suite. 01/05/23 20:09:01.805
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:09:01.816
Jan  5 20:09:01.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename statefulset 01/05/23 20:09:01.817
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:09:01.83
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:09:01.833
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-5025 01/05/23 20:09:01.837
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:848
STEP: Creating statefulset ss in namespace statefulset-5025 01/05/23 20:09:01.849
Jan  5 20:09:01.861: INFO: Found 0 stateful pods, waiting for 1
Jan  5 20:09:11.865: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 01/05/23 20:09:11.87
STEP: updating a scale subresource 01/05/23 20:09:11.872
STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 20:09:11.877
STEP: Patch a scale subresource 01/05/23 20:09:11.88
STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 20:09:11.886
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  5 20:09:11.889: INFO: Deleting all statefulset in ns statefulset-5025
Jan  5 20:09:11.893: INFO: Scaling statefulset ss to 0
Jan  5 20:09:21.917: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 20:09:21.919: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  5 20:09:21.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-5025" for this suite. 01/05/23 20:09:21.936
------------------------------
• [SLOW TEST] [20.128 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:09:01.816
    Jan  5 20:09:01.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename statefulset 01/05/23 20:09:01.817
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:09:01.83
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:09:01.833
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-5025 01/05/23 20:09:01.837
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:848
    STEP: Creating statefulset ss in namespace statefulset-5025 01/05/23 20:09:01.849
    Jan  5 20:09:01.861: INFO: Found 0 stateful pods, waiting for 1
    Jan  5 20:09:11.865: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 01/05/23 20:09:11.87
    STEP: updating a scale subresource 01/05/23 20:09:11.872
    STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 20:09:11.877
    STEP: Patch a scale subresource 01/05/23 20:09:11.88
    STEP: verifying the statefulset Spec.Replicas was modified 01/05/23 20:09:11.886
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  5 20:09:11.889: INFO: Deleting all statefulset in ns statefulset-5025
    Jan  5 20:09:11.893: INFO: Scaling statefulset ss to 0
    Jan  5 20:09:21.917: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 20:09:21.919: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:09:21.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-5025" for this suite. 01/05/23 20:09:21.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:09:21.946
Jan  5 20:09:21.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename statefulset 01/05/23 20:09:21.947
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:09:21.961
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:09:21.964
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-324 01/05/23 20:09:21.967
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:317
STEP: Creating a new StatefulSet 01/05/23 20:09:21.975
Jan  5 20:09:21.985: INFO: Found 0 stateful pods, waiting for 3
Jan  5 20:09:31.989: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:09:31.989: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:09:31.989: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/05/23 20:09:31.996
Jan  5 20:09:32.014: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/05/23 20:09:32.014
STEP: Not applying an update when the partition is greater than the number of replicas 01/05/23 20:09:42.029
STEP: Performing a canary update 01/05/23 20:09:42.029
Jan  5 20:09:42.049: INFO: Updating stateful set ss2
Jan  5 20:09:42.055: INFO: Waiting for Pod statefulset-324/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
STEP: Restoring Pods to the correct revision when they are deleted 01/05/23 20:09:52.061
Jan  5 20:09:52.108: INFO: Found 2 stateful pods, waiting for 3
Jan  5 20:10:02.112: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:10:02.112: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:10:02.112: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 01/05/23 20:10:02.117
Jan  5 20:10:02.137: INFO: Updating stateful set ss2
Jan  5 20:10:02.143: INFO: Waiting for Pod statefulset-324/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
Jan  5 20:10:12.171: INFO: Updating stateful set ss2
Jan  5 20:10:12.177: INFO: Waiting for StatefulSet statefulset-324/ss2 to complete update
Jan  5 20:10:12.177: INFO: Waiting for Pod statefulset-324/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  5 20:10:22.184: INFO: Deleting all statefulset in ns statefulset-324
Jan  5 20:10:22.186: INFO: Scaling statefulset ss2 to 0
Jan  5 20:10:32.203: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 20:10:32.206: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  5 20:10:32.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-324" for this suite. 01/05/23 20:10:32.221
------------------------------
• [SLOW TEST] [70.284 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:317

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:09:21.946
    Jan  5 20:09:21.946: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename statefulset 01/05/23 20:09:21.947
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:09:21.961
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:09:21.964
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-324 01/05/23 20:09:21.967
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:317
    STEP: Creating a new StatefulSet 01/05/23 20:09:21.975
    Jan  5 20:09:21.985: INFO: Found 0 stateful pods, waiting for 3
    Jan  5 20:09:31.989: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:09:31.989: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:09:31.989: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/05/23 20:09:31.996
    Jan  5 20:09:32.014: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/05/23 20:09:32.014
    STEP: Not applying an update when the partition is greater than the number of replicas 01/05/23 20:09:42.029
    STEP: Performing a canary update 01/05/23 20:09:42.029
    Jan  5 20:09:42.049: INFO: Updating stateful set ss2
    Jan  5 20:09:42.055: INFO: Waiting for Pod statefulset-324/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    STEP: Restoring Pods to the correct revision when they are deleted 01/05/23 20:09:52.061
    Jan  5 20:09:52.108: INFO: Found 2 stateful pods, waiting for 3
    Jan  5 20:10:02.112: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:10:02.112: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:10:02.112: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 01/05/23 20:10:02.117
    Jan  5 20:10:02.137: INFO: Updating stateful set ss2
    Jan  5 20:10:02.143: INFO: Waiting for Pod statefulset-324/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    Jan  5 20:10:12.171: INFO: Updating stateful set ss2
    Jan  5 20:10:12.177: INFO: Waiting for StatefulSet statefulset-324/ss2 to complete update
    Jan  5 20:10:12.177: INFO: Waiting for Pod statefulset-324/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  5 20:10:22.184: INFO: Deleting all statefulset in ns statefulset-324
    Jan  5 20:10:22.186: INFO: Scaling statefulset ss2 to 0
    Jan  5 20:10:32.203: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 20:10:32.206: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:10:32.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-324" for this suite. 01/05/23 20:10:32.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:10:32.233
Jan  5 20:10:32.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 20:10:32.234
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:32.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:32.251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 20:10:32.265
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:10:32.581
STEP: Deploying the webhook pod 01/05/23 20:10:32.586
STEP: Wait for the deployment to be ready 01/05/23 20:10:32.597
Jan  5 20:10:32.602: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 01/05/23 20:10:34.611
STEP: Verifying the service has paired with the endpoint 01/05/23 20:10:34.62
Jan  5 20:10:35.620: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/05/23 20:10:35.624
STEP: create a pod that should be updated by the webhook 01/05/23 20:10:35.644
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:10:35.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-747" for this suite. 01/05/23 20:10:35.714
STEP: Destroying namespace "webhook-747-markers" for this suite. 01/05/23 20:10:35.72
------------------------------
• [3.492 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:264

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:10:32.233
    Jan  5 20:10:32.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 20:10:32.234
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:32.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:32.251
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 20:10:32.265
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:10:32.581
    STEP: Deploying the webhook pod 01/05/23 20:10:32.586
    STEP: Wait for the deployment to be ready 01/05/23 20:10:32.597
    Jan  5 20:10:32.602: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 01/05/23 20:10:34.611
    STEP: Verifying the service has paired with the endpoint 01/05/23 20:10:34.62
    Jan  5 20:10:35.620: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:264
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 01/05/23 20:10:35.624
    STEP: create a pod that should be updated by the webhook 01/05/23 20:10:35.644
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:10:35.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-747" for this suite. 01/05/23 20:10:35.714
    STEP: Destroying namespace "webhook-747-markers" for this suite. 01/05/23 20:10:35.72
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:10:35.727
Jan  5 20:10:35.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sysctl 01/05/23 20:10:35.729
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:35.742
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:35.745
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 01/05/23 20:10:35.748
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:10:35.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  tear down framework | framework.go:193
STEP: Destroying namespace "sysctl-4199" for this suite. 01/05/23 20:10:35.757
------------------------------
• [0.034 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:10:35.727
    Jan  5 20:10:35.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sysctl 01/05/23 20:10:35.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:35.742
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:35.745
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 01/05/23 20:10:35.748
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:10:35.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sysctl-4199" for this suite. 01/05/23 20:10:35.757
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
[BeforeEach] [sig-node] Pods
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:10:35.763
Jan  5 20:10:35.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pods 01/05/23 20:10:35.765
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:35.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:35.785
[BeforeEach] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:194
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204
STEP: creating pod 01/05/23 20:10:35.788
Jan  5 20:10:35.798: INFO: Waiting up to 5m0s for pod "pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1" in namespace "pods-5747" to be "running and ready"
Jan  5 20:10:35.802: INFO: Pod "pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.482858ms
Jan  5 20:10:35.802: INFO: The phase of Pod pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:10:37.806: INFO: Pod "pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008447058s
Jan  5 20:10:37.807: INFO: The phase of Pod pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1 is Running (Ready = true)
Jan  5 20:10:37.807: INFO: Pod "pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1" satisfied condition "running and ready"
Jan  5 20:10:37.812: INFO: Pod pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1 has hostIP: 10.196.0.39
[AfterEach] [sig-node] Pods
  test/e2e/framework/node/init/init.go:32
Jan  5 20:10:37.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Pods
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Pods
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Pods
  tear down framework | framework.go:193
STEP: Destroying namespace "pods-5747" for this suite. 01/05/23 20:10:37.818
------------------------------
• [2.064 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:10:35.763
    Jan  5 20:10:35.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pods 01/05/23 20:10:35.765
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:35.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:35.785
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:194
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:204
    STEP: creating pod 01/05/23 20:10:35.788
    Jan  5 20:10:35.798: INFO: Waiting up to 5m0s for pod "pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1" in namespace "pods-5747" to be "running and ready"
    Jan  5 20:10:35.802: INFO: Pod "pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.482858ms
    Jan  5 20:10:35.802: INFO: The phase of Pod pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:10:37.806: INFO: Pod "pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008447058s
    Jan  5 20:10:37.807: INFO: The phase of Pod pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1 is Running (Ready = true)
    Jan  5 20:10:37.807: INFO: Pod "pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1" satisfied condition "running and ready"
    Jan  5 20:10:37.812: INFO: Pod pod-hostip-7cc58438-e282-41d6-ab4f-24331b97a6e1 has hostIP: 10.196.0.39
    [AfterEach] [sig-node] Pods
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:10:37.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Pods
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Pods
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Pods
      tear down framework | framework.go:193
    STEP: Destroying namespace "pods-5747" for this suite. 01/05/23 20:10:37.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:10:37.854
Jan  5 20:10:37.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 20:10:37.856
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:37.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:37.874
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119
STEP: Creating secret with name projected-secret-test-80cc9626-f8a7-42cb-817f-af21d05fc1ef 01/05/23 20:10:37.879
STEP: Creating a pod to test consume secrets 01/05/23 20:10:37.883
Jan  5 20:10:37.890: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0" in namespace "projected-5468" to be "Succeeded or Failed"
Jan  5 20:10:37.894: INFO: Pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.220529ms
Jan  5 20:10:39.898: INFO: Pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00688286s
Jan  5 20:10:41.898: INFO: Pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006891976s
STEP: Saw pod success 01/05/23 20:10:41.898
Jan  5 20:10:41.898: INFO: Pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0" satisfied condition "Succeeded or Failed"
Jan  5 20:10:41.900: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 20:10:41.918
Jan  5 20:10:41.929: INFO: Waiting for pod pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0 to disappear
Jan  5 20:10:41.938: INFO: Pod pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  5 20:10:41.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-5468" for this suite. 01/05/23 20:10:41.945
------------------------------
• [4.099 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:119

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:10:37.854
    Jan  5 20:10:37.855: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 20:10:37.856
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:37.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:37.874
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:119
    STEP: Creating secret with name projected-secret-test-80cc9626-f8a7-42cb-817f-af21d05fc1ef 01/05/23 20:10:37.879
    STEP: Creating a pod to test consume secrets 01/05/23 20:10:37.883
    Jan  5 20:10:37.890: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0" in namespace "projected-5468" to be "Succeeded or Failed"
    Jan  5 20:10:37.894: INFO: Pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.220529ms
    Jan  5 20:10:39.898: INFO: Pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00688286s
    Jan  5 20:10:41.898: INFO: Pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006891976s
    STEP: Saw pod success 01/05/23 20:10:41.898
    Jan  5 20:10:41.898: INFO: Pod "pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0" satisfied condition "Succeeded or Failed"
    Jan  5 20:10:41.900: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 20:10:41.918
    Jan  5 20:10:41.929: INFO: Waiting for pod pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0 to disappear
    Jan  5 20:10:41.938: INFO: Pod pod-projected-secrets-8bd39fa6-a9a2-4d10-8da6-fbab80f39ee0 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:10:41.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-5468" for this suite. 01/05/23 20:10:41.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:10:41.957
Jan  5 20:10:41.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pod-network-test 01/05/23 20:10:41.959
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:41.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:41.974
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-3957 01/05/23 20:10:41.977
STEP: creating a selector 01/05/23 20:10:41.978
STEP: Creating the service pods in kubernetes 01/05/23 20:10:41.978
Jan  5 20:10:41.978: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 20:10:42.017: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3957" to be "running and ready"
Jan  5 20:10:42.026: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.971597ms
Jan  5 20:10:42.026: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:10:44.031: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013831015s
Jan  5 20:10:44.031: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:10:46.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013301311s
Jan  5 20:10:46.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:10:48.031: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014151827s
Jan  5 20:10:48.031: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:10:50.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012974739s
Jan  5 20:10:50.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:10:52.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012963216s
Jan  5 20:10:52.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:10:54.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.014489382s
Jan  5 20:10:54.032: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 20:10:54.032: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 20:10:54.035: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3957" to be "running and ready"
Jan  5 20:10:54.040: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.774775ms
Jan  5 20:10:54.040: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 20:10:54.040: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  5 20:10:54.042: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3957" to be "running and ready"
Jan  5 20:10:54.044: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.143088ms
Jan  5 20:10:54.045: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  5 20:10:54.045: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 20:10:54.047
Jan  5 20:10:54.067: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3957" to be "running"
Jan  5 20:10:54.072: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877442ms
Jan  5 20:10:56.077: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009162692s
Jan  5 20:10:56.077: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 20:10:56.079: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3957" to be "running"
Jan  5 20:10:56.082: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.191129ms
Jan  5 20:10:56.082: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  5 20:10:56.084: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  5 20:10:56.084: INFO: Going to poll 10.16.2.249 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  5 20:10:56.086: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.16.2.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3957 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:10:56.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:10:56.087: INFO: ExecWithOptions: Clientset creation
Jan  5 20:10:56.087: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-3957/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.16.2.249+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 20:10:57.150: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  5 20:10:57.150: INFO: Going to poll 10.16.0.204 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  5 20:10:57.153: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.16.0.204 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3957 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:10:57.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:10:57.153: INFO: ExecWithOptions: Clientset creation
Jan  5 20:10:57.153: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-3957/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.16.0.204+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 20:10:58.211: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  5 20:10:58.211: INFO: Going to poll 10.16.1.48 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jan  5 20:10:58.215: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.16.1.48 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3957 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:10:58.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:10:58.216: INFO: ExecWithOptions: Clientset creation
Jan  5 20:10:58.216: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-3957/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.16.1.48+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 20:10:59.277: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  5 20:10:59.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-3957" for this suite. 01/05/23 20:10:59.282
------------------------------
• [SLOW TEST] [17.330 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:10:41.957
    Jan  5 20:10:41.957: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 20:10:41.959
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:41.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:41.974
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-3957 01/05/23 20:10:41.977
    STEP: creating a selector 01/05/23 20:10:41.978
    STEP: Creating the service pods in kubernetes 01/05/23 20:10:41.978
    Jan  5 20:10:41.978: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 20:10:42.017: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3957" to be "running and ready"
    Jan  5 20:10:42.026: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.971597ms
    Jan  5 20:10:42.026: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:10:44.031: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.013831015s
    Jan  5 20:10:44.031: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:10:46.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.013301311s
    Jan  5 20:10:46.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:10:48.031: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.014151827s
    Jan  5 20:10:48.031: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:10:50.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012974739s
    Jan  5 20:10:50.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:10:52.030: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.012963216s
    Jan  5 20:10:52.030: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:10:54.032: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.014489382s
    Jan  5 20:10:54.032: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 20:10:54.032: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 20:10:54.035: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3957" to be "running and ready"
    Jan  5 20:10:54.040: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 4.774775ms
    Jan  5 20:10:54.040: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 20:10:54.040: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  5 20:10:54.042: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3957" to be "running and ready"
    Jan  5 20:10:54.044: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.143088ms
    Jan  5 20:10:54.045: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  5 20:10:54.045: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 20:10:54.047
    Jan  5 20:10:54.067: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3957" to be "running"
    Jan  5 20:10:54.072: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877442ms
    Jan  5 20:10:56.077: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009162692s
    Jan  5 20:10:56.077: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 20:10:56.079: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-3957" to be "running"
    Jan  5 20:10:56.082: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.191129ms
    Jan  5 20:10:56.082: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  5 20:10:56.084: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  5 20:10:56.084: INFO: Going to poll 10.16.2.249 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  5 20:10:56.086: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.16.2.249 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3957 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:10:56.086: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:10:56.087: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:10:56.087: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-3957/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.16.2.249+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 20:10:57.150: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  5 20:10:57.150: INFO: Going to poll 10.16.0.204 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  5 20:10:57.153: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.16.0.204 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3957 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:10:57.153: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:10:57.153: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:10:57.153: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-3957/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.16.0.204+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 20:10:58.211: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  5 20:10:58.211: INFO: Going to poll 10.16.1.48 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jan  5 20:10:58.215: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.16.1.48 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3957 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:10:58.215: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:10:58.216: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:10:58.216: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-3957/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.16.1.48+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 20:10:59.277: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:10:59.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-3957" for this suite. 01/05/23 20:10:59.282
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:10:59.29
Jan  5 20:10:59.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-probe 01/05/23 20:10:59.291
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:59.303
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:59.306
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184
STEP: Creating pod liveness-c88cc561-5208-466d-98fc-bad9e079d797 in namespace container-probe-4887 01/05/23 20:10:59.309
Jan  5 20:10:59.322: INFO: Waiting up to 5m0s for pod "liveness-c88cc561-5208-466d-98fc-bad9e079d797" in namespace "container-probe-4887" to be "not pending"
Jan  5 20:10:59.328: INFO: Pod "liveness-c88cc561-5208-466d-98fc-bad9e079d797": Phase="Pending", Reason="", readiness=false. Elapsed: 6.342786ms
Jan  5 20:11:01.331: INFO: Pod "liveness-c88cc561-5208-466d-98fc-bad9e079d797": Phase="Running", Reason="", readiness=true. Elapsed: 2.009247625s
Jan  5 20:11:01.331: INFO: Pod "liveness-c88cc561-5208-466d-98fc-bad9e079d797" satisfied condition "not pending"
Jan  5 20:11:01.331: INFO: Started pod liveness-c88cc561-5208-466d-98fc-bad9e079d797 in namespace container-probe-4887
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 20:11:01.332
Jan  5 20:11:01.337: INFO: Initial restart count of pod liveness-c88cc561-5208-466d-98fc-bad9e079d797 is 0
STEP: deleting the pod 01/05/23 20:15:01.782
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  5 20:15:01.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-4887" for this suite. 01/05/23 20:15:01.801
------------------------------
• [SLOW TEST] [242.518 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:10:59.29
    Jan  5 20:10:59.290: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-probe 01/05/23 20:10:59.291
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:10:59.303
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:10:59.306
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:184
    STEP: Creating pod liveness-c88cc561-5208-466d-98fc-bad9e079d797 in namespace container-probe-4887 01/05/23 20:10:59.309
    Jan  5 20:10:59.322: INFO: Waiting up to 5m0s for pod "liveness-c88cc561-5208-466d-98fc-bad9e079d797" in namespace "container-probe-4887" to be "not pending"
    Jan  5 20:10:59.328: INFO: Pod "liveness-c88cc561-5208-466d-98fc-bad9e079d797": Phase="Pending", Reason="", readiness=false. Elapsed: 6.342786ms
    Jan  5 20:11:01.331: INFO: Pod "liveness-c88cc561-5208-466d-98fc-bad9e079d797": Phase="Running", Reason="", readiness=true. Elapsed: 2.009247625s
    Jan  5 20:11:01.331: INFO: Pod "liveness-c88cc561-5208-466d-98fc-bad9e079d797" satisfied condition "not pending"
    Jan  5 20:11:01.331: INFO: Started pod liveness-c88cc561-5208-466d-98fc-bad9e079d797 in namespace container-probe-4887
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 20:11:01.332
    Jan  5 20:11:01.337: INFO: Initial restart count of pod liveness-c88cc561-5208-466d-98fc-bad9e079d797 is 0
    STEP: deleting the pod 01/05/23 20:15:01.782
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:15:01.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-4887" for this suite. 01/05/23 20:15:01.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
[BeforeEach] [sig-apps] DisruptionController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:15:01.812
Jan  5 20:15:01.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename disruption 01/05/23 20:15:01.813
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:15:01.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:15:01.831
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:72
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347
STEP: Creating a pdb that targets all three pods in a test replica set 01/05/23 20:15:01.834
STEP: Waiting for the pdb to be processed 01/05/23 20:15:01.839
STEP: First trying to evict a pod which shouldn't be evictable 01/05/23 20:15:03.853
STEP: Waiting for all pods to be running 01/05/23 20:15:03.853
Jan  5 20:15:03.856: INFO: pods: 0 < 3
STEP: locating a running pod 01/05/23 20:15:05.86
STEP: Updating the pdb to allow a pod to be evicted 01/05/23 20:15:05.869
STEP: Waiting for the pdb to be processed 01/05/23 20:15:05.876
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 20:15:07.881
STEP: Waiting for all pods to be running 01/05/23 20:15:07.881
STEP: Waiting for the pdb to observed all healthy pods 01/05/23 20:15:07.884
STEP: Patching the pdb to disallow a pod to be evicted 01/05/23 20:15:07.908
STEP: Waiting for the pdb to be processed 01/05/23 20:15:07.926
STEP: Waiting for all pods to be running 01/05/23 20:15:09.934
STEP: locating a running pod 01/05/23 20:15:09.937
STEP: Deleting the pdb to allow a pod to be evicted 01/05/23 20:15:09.945
STEP: Waiting for the pdb to be deleted 01/05/23 20:15:09.95
STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 20:15:09.952
STEP: Waiting for all pods to be running 01/05/23 20:15:09.952
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/node/init/init.go:32
Jan  5 20:15:09.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] DisruptionController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] DisruptionController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] DisruptionController
  tear down framework | framework.go:193
STEP: Destroying namespace "disruption-4934" for this suite. 01/05/23 20:15:09.983
------------------------------
• [SLOW TEST] [8.180 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:347

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:15:01.812
    Jan  5 20:15:01.812: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename disruption 01/05/23 20:15:01.813
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:15:01.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:15:01.831
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:72
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:347
    STEP: Creating a pdb that targets all three pods in a test replica set 01/05/23 20:15:01.834
    STEP: Waiting for the pdb to be processed 01/05/23 20:15:01.839
    STEP: First trying to evict a pod which shouldn't be evictable 01/05/23 20:15:03.853
    STEP: Waiting for all pods to be running 01/05/23 20:15:03.853
    Jan  5 20:15:03.856: INFO: pods: 0 < 3
    STEP: locating a running pod 01/05/23 20:15:05.86
    STEP: Updating the pdb to allow a pod to be evicted 01/05/23 20:15:05.869
    STEP: Waiting for the pdb to be processed 01/05/23 20:15:05.876
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 20:15:07.881
    STEP: Waiting for all pods to be running 01/05/23 20:15:07.881
    STEP: Waiting for the pdb to observed all healthy pods 01/05/23 20:15:07.884
    STEP: Patching the pdb to disallow a pod to be evicted 01/05/23 20:15:07.908
    STEP: Waiting for the pdb to be processed 01/05/23 20:15:07.926
    STEP: Waiting for all pods to be running 01/05/23 20:15:09.934
    STEP: locating a running pod 01/05/23 20:15:09.937
    STEP: Deleting the pdb to allow a pod to be evicted 01/05/23 20:15:09.945
    STEP: Waiting for the pdb to be deleted 01/05/23 20:15:09.95
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 01/05/23 20:15:09.952
    STEP: Waiting for all pods to be running 01/05/23 20:15:09.952
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:15:09.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] DisruptionController
      tear down framework | framework.go:193
    STEP: Destroying namespace "disruption-4934" for this suite. 01/05/23 20:15:09.983
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:15:09.994
Jan  5 20:15:09.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename deployment 01/05/23 20:15:09.995
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:15:10.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:15:10.024
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jan  5 20:15:10.038: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jan  5 20:15:15.045: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 01/05/23 20:15:15.046
Jan  5 20:15:15.046: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/05/23 20:15:15.076
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 20:15:15.144: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4938  88161519-5f69-4274-9ad2-f34a7e8c2295 100519 1 2023-01-05 20:15:15 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-05 20:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fca7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

Jan  5 20:15:15.166: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jan  5 20:15:15.167: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jan  5 20:15:15.167: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4938  8efede9b-dce7-4bc7-b1f9-41bbafd9a2b4 100520 1 2023-01-05 20:15:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 88161519-5f69-4274-9ad2-f34a7e8c2295 0xc004ae8907 0xc004ae8908}] [] [{e2e.test Update apps/v1 2023-01-05 20:15:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:15:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 20:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"88161519-5f69-4274-9ad2-f34a7e8c2295\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004ae89c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 20:15:15.196: INFO: Pod "test-cleanup-controller-z6gr2" is available:
&Pod{ObjectMeta:{test-cleanup-controller-z6gr2 test-cleanup-controller- deployment-4938  2f0a53cd-2a35-4f83-853e-df0f1c6cd5c2 100476 0 2023-01-05 20:15:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 8efede9b-dce7-4bc7-b1f9-41bbafd9a2b4 0xc004ae8cd7 0xc004ae8cd8}] [] [{kube-controller-manager Update v1 2023-01-05 20:15:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8efede9b-dce7-4bc7-b1f9-41bbafd9a2b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:15:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bctk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bctk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:15:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:15:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:15:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:15:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.252,StartTime:2023-01-05 20:15:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:15:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a6622eafb15411f69f815aa82523be6a1df29a07fbc06836189f14eddba99a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  5 20:15:15.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-4938" for this suite. 01/05/23 20:15:15.221
------------------------------
• [SLOW TEST] [5.268 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:15:09.994
    Jan  5 20:15:09.994: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename deployment 01/05/23 20:15:09.995
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:15:10.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:15:10.024
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jan  5 20:15:10.038: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jan  5 20:15:15.045: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 01/05/23 20:15:15.046
    Jan  5 20:15:15.046: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 01/05/23 20:15:15.076
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 20:15:15.144: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4938  88161519-5f69-4274-9ad2-f34a7e8c2295 100519 1 2023-01-05 20:15:15 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-01-05 20:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005fca7c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

    Jan  5 20:15:15.166: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
    Jan  5 20:15:15.167: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
    Jan  5 20:15:15.167: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-4938  8efede9b-dce7-4bc7-b1f9-41bbafd9a2b4 100520 1 2023-01-05 20:15:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 88161519-5f69-4274-9ad2-f34a7e8c2295 0xc004ae8907 0xc004ae8908}] [] [{e2e.test Update apps/v1 2023-01-05 20:15:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:15:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-01-05 20:15:15 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"88161519-5f69-4274-9ad2-f34a7e8c2295\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004ae89c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 20:15:15.196: INFO: Pod "test-cleanup-controller-z6gr2" is available:
    &Pod{ObjectMeta:{test-cleanup-controller-z6gr2 test-cleanup-controller- deployment-4938  2f0a53cd-2a35-4f83-853e-df0f1c6cd5c2 100476 0 2023-01-05 20:15:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 8efede9b-dce7-4bc7-b1f9-41bbafd9a2b4 0xc004ae8cd7 0xc004ae8cd8}] [] [{kube-controller-manager Update v1 2023-01-05 20:15:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8efede9b-dce7-4bc7-b1f9-41bbafd9a2b4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:15:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bctk9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bctk9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:15:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:15:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:15:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:15:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.252,StartTime:2023-01-05 20:15:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:15:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6a6622eafb15411f69f815aa82523be6a1df29a07fbc06836189f14eddba99a2,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:15:15.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-4938" for this suite. 01/05/23 20:15:15.221
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:15:15.265
Jan  5 20:15:15.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-probe 01/05/23 20:15:15.266
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:15:15.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:15:15.355
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199
STEP: Creating pod liveness-43c95100-6b37-496c-ab4f-5546ebebbdef in namespace container-probe-8066 01/05/23 20:15:15.365
Jan  5 20:15:15.383: INFO: Waiting up to 5m0s for pod "liveness-43c95100-6b37-496c-ab4f-5546ebebbdef" in namespace "container-probe-8066" to be "not pending"
Jan  5 20:15:15.397: INFO: Pod "liveness-43c95100-6b37-496c-ab4f-5546ebebbdef": Phase="Pending", Reason="", readiness=false. Elapsed: 14.161175ms
Jan  5 20:15:17.400: INFO: Pod "liveness-43c95100-6b37-496c-ab4f-5546ebebbdef": Phase="Running", Reason="", readiness=true. Elapsed: 2.017228346s
Jan  5 20:15:17.401: INFO: Pod "liveness-43c95100-6b37-496c-ab4f-5546ebebbdef" satisfied condition "not pending"
Jan  5 20:15:17.401: INFO: Started pod liveness-43c95100-6b37-496c-ab4f-5546ebebbdef in namespace container-probe-8066
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 20:15:17.401
Jan  5 20:15:17.404: INFO: Initial restart count of pod liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is 0
Jan  5 20:15:37.441: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 1 (20.03720249s elapsed)
Jan  5 20:15:57.477: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 2 (40.073195564s elapsed)
Jan  5 20:16:17.513: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 3 (1m0.109309033s elapsed)
Jan  5 20:16:37.548: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 4 (1m20.144471026s elapsed)
Jan  5 20:17:39.677: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 5 (2m22.273075437s elapsed)
STEP: deleting the pod 01/05/23 20:17:39.677
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  5 20:17:39.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-8066" for this suite. 01/05/23 20:17:39.697
------------------------------
• [SLOW TEST] [144.442 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:199

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:15:15.265
    Jan  5 20:15:15.265: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-probe 01/05/23 20:15:15.266
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:15:15.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:15:15.355
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:199
    STEP: Creating pod liveness-43c95100-6b37-496c-ab4f-5546ebebbdef in namespace container-probe-8066 01/05/23 20:15:15.365
    Jan  5 20:15:15.383: INFO: Waiting up to 5m0s for pod "liveness-43c95100-6b37-496c-ab4f-5546ebebbdef" in namespace "container-probe-8066" to be "not pending"
    Jan  5 20:15:15.397: INFO: Pod "liveness-43c95100-6b37-496c-ab4f-5546ebebbdef": Phase="Pending", Reason="", readiness=false. Elapsed: 14.161175ms
    Jan  5 20:15:17.400: INFO: Pod "liveness-43c95100-6b37-496c-ab4f-5546ebebbdef": Phase="Running", Reason="", readiness=true. Elapsed: 2.017228346s
    Jan  5 20:15:17.401: INFO: Pod "liveness-43c95100-6b37-496c-ab4f-5546ebebbdef" satisfied condition "not pending"
    Jan  5 20:15:17.401: INFO: Started pod liveness-43c95100-6b37-496c-ab4f-5546ebebbdef in namespace container-probe-8066
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 20:15:17.401
    Jan  5 20:15:17.404: INFO: Initial restart count of pod liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is 0
    Jan  5 20:15:37.441: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 1 (20.03720249s elapsed)
    Jan  5 20:15:57.477: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 2 (40.073195564s elapsed)
    Jan  5 20:16:17.513: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 3 (1m0.109309033s elapsed)
    Jan  5 20:16:37.548: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 4 (1m20.144471026s elapsed)
    Jan  5 20:17:39.677: INFO: Restart count of pod container-probe-8066/liveness-43c95100-6b37-496c-ab4f-5546ebebbdef is now 5 (2m22.273075437s elapsed)
    STEP: deleting the pod 01/05/23 20:17:39.677
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:17:39.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-8066" for this suite. 01/05/23 20:17:39.697
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:17:39.72
Jan  5 20:17:39.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 20:17:39.724
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:17:39.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:17:39.746
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:394
STEP: creating all guestbook components 01/05/23 20:17:39.75
Jan  5 20:17:39.750: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jan  5 20:17:39.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
Jan  5 20:17:40.201: INFO: stderr: ""
Jan  5 20:17:40.201: INFO: stdout: "service/agnhost-replica created\n"
Jan  5 20:17:40.201: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jan  5 20:17:40.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
Jan  5 20:17:40.524: INFO: stderr: ""
Jan  5 20:17:40.524: INFO: stdout: "service/agnhost-primary created\n"
Jan  5 20:17:40.524: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jan  5 20:17:40.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
Jan  5 20:17:40.820: INFO: stderr: ""
Jan  5 20:17:40.820: INFO: stdout: "service/frontend created\n"
Jan  5 20:17:40.820: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jan  5 20:17:40.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
Jan  5 20:17:41.073: INFO: stderr: ""
Jan  5 20:17:41.073: INFO: stdout: "deployment.apps/frontend created\n"
Jan  5 20:17:41.073: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  5 20:17:41.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
Jan  5 20:17:41.337: INFO: stderr: ""
Jan  5 20:17:41.337: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jan  5 20:17:41.337: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.43
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jan  5 20:17:41.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
Jan  5 20:17:42.302: INFO: stderr: ""
Jan  5 20:17:42.302: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 01/05/23 20:17:42.302
Jan  5 20:17:42.302: INFO: Waiting for all frontend pods to be Running.
Jan  5 20:17:47.359: INFO: Waiting for frontend to serve content.
Jan  5 20:17:47.372: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
Jan  5 20:17:52.385: INFO: Trying to add a new entry to the guestbook.
Jan  5 20:17:52.394: INFO: Verifying that added entry can be retrieved.
Jan  5 20:17:52.401: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
STEP: using delete to clean up resources 01/05/23 20:17:57.411
Jan  5 20:17:57.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
Jan  5 20:17:57.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 20:17:57.496: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 20:17:57.496
Jan  5 20:17:57.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
Jan  5 20:17:57.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 20:17:57.614: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 20:17:57.614
Jan  5 20:17:57.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
Jan  5 20:17:57.774: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 20:17:57.774: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 20:17:57.774
Jan  5 20:17:57.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
Jan  5 20:17:57.905: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 20:17:57.905: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 20:17:57.906
Jan  5 20:17:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
Jan  5 20:17:58.126: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 20:17:58.126: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 01/05/23 20:17:58.126
Jan  5 20:17:58.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
Jan  5 20:17:58.361: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jan  5 20:17:58.361: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 20:17:58.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-484" for this suite. 01/05/23 20:17:58.366
------------------------------
• [SLOW TEST] [18.665 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:369
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:17:39.72
    Jan  5 20:17:39.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 20:17:39.724
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:17:39.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:17:39.746
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:394
    STEP: creating all guestbook components 01/05/23 20:17:39.75
    Jan  5 20:17:39.750: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jan  5 20:17:39.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
    Jan  5 20:17:40.201: INFO: stderr: ""
    Jan  5 20:17:40.201: INFO: stdout: "service/agnhost-replica created\n"
    Jan  5 20:17:40.201: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jan  5 20:17:40.201: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
    Jan  5 20:17:40.524: INFO: stderr: ""
    Jan  5 20:17:40.524: INFO: stdout: "service/agnhost-primary created\n"
    Jan  5 20:17:40.524: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jan  5 20:17:40.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
    Jan  5 20:17:40.820: INFO: stderr: ""
    Jan  5 20:17:40.820: INFO: stdout: "service/frontend created\n"
    Jan  5 20:17:40.820: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jan  5 20:17:40.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
    Jan  5 20:17:41.073: INFO: stderr: ""
    Jan  5 20:17:41.073: INFO: stdout: "deployment.apps/frontend created\n"
    Jan  5 20:17:41.073: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  5 20:17:41.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
    Jan  5 20:17:41.337: INFO: stderr: ""
    Jan  5 20:17:41.337: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jan  5 20:17:41.337: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.43
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jan  5 20:17:41.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 create -f -'
    Jan  5 20:17:42.302: INFO: stderr: ""
    Jan  5 20:17:42.302: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 01/05/23 20:17:42.302
    Jan  5 20:17:42.302: INFO: Waiting for all frontend pods to be Running.
    Jan  5 20:17:47.359: INFO: Waiting for frontend to serve content.
    Jan  5 20:17:47.372: INFO: Failed to get response from guestbook. err: the server responded with the status code 417 but did not return more information (get services frontend), response: 
    Jan  5 20:17:52.385: INFO: Trying to add a new entry to the guestbook.
    Jan  5 20:17:52.394: INFO: Verifying that added entry can be retrieved.
    Jan  5 20:17:52.401: INFO: Failed to get response from guestbook. err: <nil>, response: {"data":""}
    STEP: using delete to clean up resources 01/05/23 20:17:57.411
    Jan  5 20:17:57.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
    Jan  5 20:17:57.495: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 20:17:57.496: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 20:17:57.496
    Jan  5 20:17:57.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
    Jan  5 20:17:57.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 20:17:57.614: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 20:17:57.614
    Jan  5 20:17:57.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
    Jan  5 20:17:57.774: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 20:17:57.774: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 20:17:57.774
    Jan  5 20:17:57.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
    Jan  5 20:17:57.905: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 20:17:57.905: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 20:17:57.906
    Jan  5 20:17:57.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
    Jan  5 20:17:58.126: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 20:17:58.126: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 01/05/23 20:17:58.126
    Jan  5 20:17:58.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-484 delete --grace-period=0 --force -f -'
    Jan  5 20:17:58.361: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jan  5 20:17:58.361: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:17:58.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-484" for this suite. 01/05/23 20:17:58.366
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:17:58.385
Jan  5 20:17:58.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename namespaces 01/05/23 20:17:58.386
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:17:58.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:17:58.402
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268
STEP: creating a Namespace 01/05/23 20:17:58.406
STEP: patching the Namespace 01/05/23 20:17:58.417
STEP: get the Namespace and ensuring it has the label 01/05/23 20:17:58.424
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:17:58.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-5983" for this suite. 01/05/23 20:17:58.432
STEP: Destroying namespace "nspatchtest-5c15ec49-c268-4a5c-a520-6b7e505fec6f-1617" for this suite. 01/05/23 20:17:58.438
------------------------------
• [0.059 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:17:58.385
    Jan  5 20:17:58.385: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename namespaces 01/05/23 20:17:58.386
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:17:58.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:17:58.402
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:268
    STEP: creating a Namespace 01/05/23 20:17:58.406
    STEP: patching the Namespace 01/05/23 20:17:58.417
    STEP: get the Namespace and ensuring it has the label 01/05/23 20:17:58.424
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:17:58.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-5983" for this suite. 01/05/23 20:17:58.432
    STEP: Destroying namespace "nspatchtest-5c15ec49-c268-4a5c-a520-6b7e505fec6f-1617" for this suite. 01/05/23 20:17:58.438
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
[BeforeEach] [sig-cli] Kubectl client
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:17:58.447
Jan  5 20:17:58.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubectl 01/05/23 20:17:58.449
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:17:58.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:17:58.467
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:274
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1276
Jan  5 20:17:58.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 create -f -'
Jan  5 20:17:58.915: INFO: stderr: ""
Jan  5 20:17:58.915: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jan  5 20:17:58.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 create -f -'
Jan  5 20:17:59.218: INFO: stderr: ""
Jan  5 20:17:59.218: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 01/05/23 20:17:59.218
Jan  5 20:18:00.224: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 20:18:00.224: INFO: Found 0 / 1
Jan  5 20:18:01.222: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 20:18:01.222: INFO: Found 1 / 1
Jan  5 20:18:01.222: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jan  5 20:18:01.225: INFO: Selector matched 1 pods for map[app:agnhost]
Jan  5 20:18:01.225: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jan  5 20:18:01.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe pod agnhost-primary-4ttpd'
Jan  5 20:18:01.317: INFO: stderr: ""
Jan  5 20:18:01.317: INFO: stdout: "Name:             agnhost-primary-4ttpd\nNamespace:        kubectl-9050\nPriority:         0\nService Account:  default\nNode:             gke-gke-1-26-default-pool-05283374-16pz/10.196.0.39\nStart Time:       Thu, 05 Jan 2023 20:17:58 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.16.2.4\nIPs:\n  IP:           10.16.2.4\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6ee774aec0fa1a1813cc25a1e192fd9116c5b59f65f84c45dfe48c1e979deafc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Jan 2023 20:18:00 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2xrdx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2xrdx:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-9050/agnhost-primary-4ttpd to gke-gke-1-26-default-pool-05283374-16pz\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
Jan  5 20:18:01.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe rc agnhost-primary'
Jan  5 20:18:01.403: INFO: stderr: ""
Jan  5 20:18:01.403: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9050\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-4ttpd\n"
Jan  5 20:18:01.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe service agnhost-primary'
Jan  5 20:18:01.483: INFO: stderr: ""
Jan  5 20:18:01.483: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9050\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       cloud.google.com/neg: {\"ingress\":true}\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.20.1.131\nIPs:               10.20.1.131\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.16.2.4:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jan  5 20:18:01.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe node gke-gke-1-26-default-pool-05283374-16pz'
Jan  5 20:18:01.594: INFO: stderr: ""
Jan  5 20:18:01.594: INFO: stdout: "Name:               gke-gke-1-26-default-pool-05283374-16pz\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=e2-medium\n                    beta.kubernetes.io/os=linux\n                    cloud.google.com/gke-boot-disk=pd-balanced\n                    cloud.google.com/gke-container-runtime=containerd\n                    cloud.google.com/gke-cpu-scaling-level=2\n                    cloud.google.com/gke-logging-variant=DEFAULT\n                    cloud.google.com/gke-max-pods-per-node=110\n                    cloud.google.com/gke-nodepool=default-pool\n                    cloud.google.com/gke-os-distribution=cos\n                    cloud.google.com/gke-stack-type=IPV4\n                    cloud.google.com/machine-family=e2\n                    cloud.google.com/private-node=false\n                    failure-domain.beta.kubernetes.io/region=us-east7\n                    failure-domain.beta.kubernetes.io/zone=us-east7-c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=gke-gke-1-26-default-pool-05283374-16pz\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=e2-medium\n                    topology.gke.io/zone=us-east7-c\n                    topology.kubernetes.io/region=us-east7\n                    topology.kubernetes.io/zone=us-east7-c\nAnnotations:        container.googleapis.com/instance_id: 2990397384176612047\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"pd.csi.storage.gke.io\":\"projects/liggitt-gke-dev/zones/us-east7-c/instances/gke-gke-1-26-default-pool-05283374-16pz\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.gke.io/last-applied-node-labels:\n                      cloud.google.com/gke-boot-disk=pd-balanced,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-cpu-scaling-level=2,clou...\n                    node.gke.io/last-applied-node-taints: \n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Jan 2023 17:46:35 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  gke-gke-1-26-default-pool-05283374-16pz\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 05 Jan 2023 20:17:56 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentUnregisterNetDevice   False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  CorruptDockerOverlay2         False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentKubeletRestart        False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  KernelDeadlock                False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  NetworkUnavailable            False   Thu, 05 Jan 2023 18:58:11 +0000   Thu, 05 Jan 2023 18:58:11 +0000   RouteCreated                    NodeController create implicit route\n  MemoryPressure                False   Thu, 05 Jan 2023 20:13:53 +0000   Thu, 05 Jan 2023 17:45:31 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Thu, 05 Jan 2023 20:13:53 +0000   Thu, 05 Jan 2023 17:45:31 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Thu, 05 Jan 2023 20:13:53 +0000   Thu, 05 Jan 2023 17:45:31 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Thu, 05 Jan 2023 20:13:53 +0000   Thu, 05 Jan 2023 17:46:56 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.196.0.39\n  ExternalIP:  34.161.255.149\n  Hostname:    gke-gke-1-26-default-pool-05283374-16pz\nCapacity:\n  cpu:                2\n  ephemeral-storage:  98831908Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4022996Ki\n  pods:               110\nAllocatable:\n  cpu:                940m\n  ephemeral-storage:  47060071478\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2877140Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 4fa5e02e32166636e45869a69481b187\n  System UUID:                4fa5e02e-3216-6636-e458-69a69481b187\n  Boot ID:                    0cfeb33d-ea45-4786-95c6-eeb8a7c9ccb1\n  Kernel Version:             5.15.65+\n  OS Image:                   Container-Optimized OS from Google\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.26.0-gke.1500\n  Kube-Proxy Version:         v1.26.0-gke.1500\nPodCIDR:                      10.16.2.0/24\nPodCIDRs:                     10.16.2.0/24\nProviderID:                   gce://liggitt-gke-dev/us-east7-c/gke-gke-1-26-default-pool-05283374-16pz\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 fluentbit-gke-5jd2q                                        100m (10%)    0 (0%)      200Mi (7%)       500Mi (17%)    151m\n  kube-system                 gke-metrics-agent-vk8fs                                    6m (0%)       0 (0%)      100Mi (3%)       100Mi (3%)     151m\n  kube-system                 kube-proxy-gke-gke-1-26-default-pool-05283374-16pz         100m (10%)    0 (0%)      0 (0%)           0 (0%)         150m\n  kube-system                 pdcsi-node-2nkb4                                           10m (1%)      0 (0%)      20Mi (0%)        100Mi (3%)     151m\n  kubectl-9050                agnhost-primary-4ttpd                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  sonobuoy                    sonobuoy-e2e-job-7d2488470a1c477d                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                216m (22%)   0 (0%)\n  memory             320Mi (11%)  700Mi (24%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
Jan  5 20:18:01.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe namespace kubectl-9050'
Jan  5 20:18:01.683: INFO: stderr: ""
Jan  5 20:18:01.683: INFO: stdout: "Name:         kubectl-9050\nLabels:       e2e-framework=kubectl\n              e2e-run=8bfd2e17-1f7d-4837-af63-bc3dee1c8575\n              kubernetes.io/metadata.name=kubectl-9050\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:01.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-cli] Kubectl client
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-cli] Kubectl client
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-cli] Kubectl client
  tear down framework | framework.go:193
STEP: Destroying namespace "kubectl-9050" for this suite. 01/05/23 20:18:01.687
------------------------------
• [3.245 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1270
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:17:58.447
    Jan  5 20:17:58.447: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubectl 01/05/23 20:17:58.449
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:17:58.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:17:58.467
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:274
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1276
    Jan  5 20:17:58.471: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 create -f -'
    Jan  5 20:17:58.915: INFO: stderr: ""
    Jan  5 20:17:58.915: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jan  5 20:17:58.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 create -f -'
    Jan  5 20:17:59.218: INFO: stderr: ""
    Jan  5 20:17:59.218: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 01/05/23 20:17:59.218
    Jan  5 20:18:00.224: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 20:18:00.224: INFO: Found 0 / 1
    Jan  5 20:18:01.222: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 20:18:01.222: INFO: Found 1 / 1
    Jan  5 20:18:01.222: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jan  5 20:18:01.225: INFO: Selector matched 1 pods for map[app:agnhost]
    Jan  5 20:18:01.225: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jan  5 20:18:01.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe pod agnhost-primary-4ttpd'
    Jan  5 20:18:01.317: INFO: stderr: ""
    Jan  5 20:18:01.317: INFO: stdout: "Name:             agnhost-primary-4ttpd\nNamespace:        kubectl-9050\nPriority:         0\nService Account:  default\nNode:             gke-gke-1-26-default-pool-05283374-16pz/10.196.0.39\nStart Time:       Thu, 05 Jan 2023 20:17:58 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.16.2.4\nIPs:\n  IP:           10.16.2.4\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://6ee774aec0fa1a1813cc25a1e192fd9116c5b59f65f84c45dfe48c1e979deafc\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 05 Jan 2023 20:18:00 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2xrdx (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2xrdx:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-9050/agnhost-primary-4ttpd to gke-gke-1-26-default-pool-05283374-16pz\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
    Jan  5 20:18:01.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe rc agnhost-primary'
    Jan  5 20:18:01.403: INFO: stderr: ""
    Jan  5 20:18:01.403: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9050\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-4ttpd\n"
    Jan  5 20:18:01.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe service agnhost-primary'
    Jan  5 20:18:01.483: INFO: stderr: ""
    Jan  5 20:18:01.483: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9050\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       cloud.google.com/neg: {\"ingress\":true}\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.20.1.131\nIPs:               10.20.1.131\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.16.2.4:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jan  5 20:18:01.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe node gke-gke-1-26-default-pool-05283374-16pz'
    Jan  5 20:18:01.594: INFO: stderr: ""
    Jan  5 20:18:01.594: INFO: stdout: "Name:               gke-gke-1-26-default-pool-05283374-16pz\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=e2-medium\n                    beta.kubernetes.io/os=linux\n                    cloud.google.com/gke-boot-disk=pd-balanced\n                    cloud.google.com/gke-container-runtime=containerd\n                    cloud.google.com/gke-cpu-scaling-level=2\n                    cloud.google.com/gke-logging-variant=DEFAULT\n                    cloud.google.com/gke-max-pods-per-node=110\n                    cloud.google.com/gke-nodepool=default-pool\n                    cloud.google.com/gke-os-distribution=cos\n                    cloud.google.com/gke-stack-type=IPV4\n                    cloud.google.com/machine-family=e2\n                    cloud.google.com/private-node=false\n                    failure-domain.beta.kubernetes.io/region=us-east7\n                    failure-domain.beta.kubernetes.io/zone=us-east7-c\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=gke-gke-1-26-default-pool-05283374-16pz\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/instance-type=e2-medium\n                    topology.gke.io/zone=us-east7-c\n                    topology.kubernetes.io/region=us-east7\n                    topology.kubernetes.io/zone=us-east7-c\nAnnotations:        container.googleapis.com/instance_id: 2990397384176612047\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"pd.csi.storage.gke.io\":\"projects/liggitt-gke-dev/zones/us-east7-c/instances/gke-gke-1-26-default-pool-05283374-16pz\"}\n                    node.alpha.kubernetes.io/ttl: 0\n                    node.gke.io/last-applied-node-labels:\n                      cloud.google.com/gke-boot-disk=pd-balanced,cloud.google.com/gke-container-runtime=containerd,cloud.google.com/gke-cpu-scaling-level=2,clou...\n                    node.gke.io/last-applied-node-taints: \n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 05 Jan 2023 17:46:35 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  gke-gke-1-26-default-pool-05283374-16pz\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 05 Jan 2023 20:17:56 +0000\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  FrequentUnregisterNetDevice   False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  CorruptDockerOverlay2         False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentKubeletRestart        False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  KernelDeadlock                False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Thu, 05 Jan 2023 20:17:25 +0000   Thu, 05 Jan 2023 17:46:37 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  NetworkUnavailable            False   Thu, 05 Jan 2023 18:58:11 +0000   Thu, 05 Jan 2023 18:58:11 +0000   RouteCreated                    NodeController create implicit route\n  MemoryPressure                False   Thu, 05 Jan 2023 20:13:53 +0000   Thu, 05 Jan 2023 17:45:31 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Thu, 05 Jan 2023 20:13:53 +0000   Thu, 05 Jan 2023 17:45:31 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Thu, 05 Jan 2023 20:13:53 +0000   Thu, 05 Jan 2023 17:45:31 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Thu, 05 Jan 2023 20:13:53 +0000   Thu, 05 Jan 2023 17:46:56 +0000   KubeletReady                    kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  10.196.0.39\n  ExternalIP:  34.161.255.149\n  Hostname:    gke-gke-1-26-default-pool-05283374-16pz\nCapacity:\n  cpu:                2\n  ephemeral-storage:  98831908Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             4022996Ki\n  pods:               110\nAllocatable:\n  cpu:                940m\n  ephemeral-storage:  47060071478\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             2877140Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 4fa5e02e32166636e45869a69481b187\n  System UUID:                4fa5e02e-3216-6636-e458-69a69481b187\n  Boot ID:                    0cfeb33d-ea45-4786-95c6-eeb8a7c9ccb1\n  Kernel Version:             5.15.65+\n  OS Image:                   Container-Optimized OS from Google\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.6\n  Kubelet Version:            v1.26.0-gke.1500\n  Kube-Proxy Version:         v1.26.0-gke.1500\nPodCIDR:                      10.16.2.0/24\nPodCIDRs:                     10.16.2.0/24\nProviderID:                   gce://liggitt-gke-dev/us-east7-c/gke-gke-1-26-default-pool-05283374-16pz\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 fluentbit-gke-5jd2q                                        100m (10%)    0 (0%)      200Mi (7%)       500Mi (17%)    151m\n  kube-system                 gke-metrics-agent-vk8fs                                    6m (0%)       0 (0%)      100Mi (3%)       100Mi (3%)     151m\n  kube-system                 kube-proxy-gke-gke-1-26-default-pool-05283374-16pz         100m (10%)    0 (0%)      0 (0%)           0 (0%)         150m\n  kube-system                 pdcsi-node-2nkb4                                           10m (1%)      0 (0%)      20Mi (0%)        100Mi (3%)     151m\n  kubectl-9050                agnhost-primary-4ttpd                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                    sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  sonobuoy                    sonobuoy-e2e-job-7d2488470a1c477d                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         76m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                216m (22%)   0 (0%)\n  memory             320Mi (11%)  700Mi (24%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
    Jan  5 20:18:01.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=kubectl-9050 describe namespace kubectl-9050'
    Jan  5 20:18:01.683: INFO: stderr: ""
    Jan  5 20:18:01.683: INFO: stdout: "Name:         kubectl-9050\nLabels:       e2e-framework=kubectl\n              e2e-run=8bfd2e17-1f7d-4837-af63-bc3dee1c8575\n              kubernetes.io/metadata.name=kubectl-9050\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:01.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-cli] Kubectl client
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubectl-9050" for this suite. 01/05/23 20:18:01.687
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:01.693
Jan  5 20:18:01.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 20:18:01.695
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:01.707
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:01.709
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 20:18:01.726
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:18:02.586
STEP: Deploying the webhook pod 01/05/23 20:18:02.592
STEP: Wait for the deployment to be ready 01/05/23 20:18:02.604
Jan  5 20:18:02.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 20:18:04.621
STEP: Verifying the service has paired with the endpoint 01/05/23 20:18:04.633
Jan  5 20:18:05.633: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197
STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 20:18:05.637
STEP: create a pod that should be denied by the webhook 01/05/23 20:18:05.66
STEP: create a pod that causes the webhook to hang 01/05/23 20:18:05.68
STEP: create a configmap that should be denied by the webhook 01/05/23 20:18:15.69
STEP: create a configmap that should be admitted by the webhook 01/05/23 20:18:15.706
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 20:18:15.719
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 20:18:15.727
STEP: create a namespace that bypass the webhook 01/05/23 20:18:15.733
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/05/23 20:18:15.739
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:15.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5537" for this suite. 01/05/23 20:18:15.808
STEP: Destroying namespace "webhook-5537-markers" for this suite. 01/05/23 20:18:15.814
------------------------------
• [SLOW TEST] [14.127 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:197

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:01.693
    Jan  5 20:18:01.693: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 20:18:01.695
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:01.707
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:01.709
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 20:18:01.726
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:18:02.586
    STEP: Deploying the webhook pod 01/05/23 20:18:02.592
    STEP: Wait for the deployment to be ready 01/05/23 20:18:02.604
    Jan  5 20:18:02.613: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 20:18:04.621
    STEP: Verifying the service has paired with the endpoint 01/05/23 20:18:04.633
    Jan  5 20:18:05.633: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:197
    STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 20:18:05.637
    STEP: create a pod that should be denied by the webhook 01/05/23 20:18:05.66
    STEP: create a pod that causes the webhook to hang 01/05/23 20:18:05.68
    STEP: create a configmap that should be denied by the webhook 01/05/23 20:18:15.69
    STEP: create a configmap that should be admitted by the webhook 01/05/23 20:18:15.706
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 20:18:15.719
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 01/05/23 20:18:15.727
    STEP: create a namespace that bypass the webhook 01/05/23 20:18:15.733
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 01/05/23 20:18:15.739
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:15.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5537" for this suite. 01/05/23 20:18:15.808
    STEP: Destroying namespace "webhook-5537-markers" for this suite. 01/05/23 20:18:15.814
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:15.824
Jan  5 20:18:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 20:18:15.826
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:15.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:15.846
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56
STEP: Creating projection with secret that has name projected-secret-test-0d1f45a4-9209-49c2-941c-12a2f36b6702 01/05/23 20:18:15.851
STEP: Creating a pod to test consume secrets 01/05/23 20:18:15.855
Jan  5 20:18:15.864: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f" in namespace "projected-3816" to be "Succeeded or Failed"
Jan  5 20:18:15.868: INFO: Pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.793343ms
Jan  5 20:18:17.872: INFO: Pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f": Phase="Running", Reason="", readiness=false. Elapsed: 2.007805656s
Jan  5 20:18:19.872: INFO: Pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007323052s
STEP: Saw pod success 01/05/23 20:18:19.872
Jan  5 20:18:19.872: INFO: Pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f" satisfied condition "Succeeded or Failed"
Jan  5 20:18:19.876: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 20:18:19.894
Jan  5 20:18:19.904: INFO: Waiting for pod pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f to disappear
Jan  5 20:18:19.907: INFO: Pod pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:19.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-3816" for this suite. 01/05/23 20:18:19.911
------------------------------
• [4.093 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:15.824
    Jan  5 20:18:15.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 20:18:15.826
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:15.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:15.846
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:56
    STEP: Creating projection with secret that has name projected-secret-test-0d1f45a4-9209-49c2-941c-12a2f36b6702 01/05/23 20:18:15.851
    STEP: Creating a pod to test consume secrets 01/05/23 20:18:15.855
    Jan  5 20:18:15.864: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f" in namespace "projected-3816" to be "Succeeded or Failed"
    Jan  5 20:18:15.868: INFO: Pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.793343ms
    Jan  5 20:18:17.872: INFO: Pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f": Phase="Running", Reason="", readiness=false. Elapsed: 2.007805656s
    Jan  5 20:18:19.872: INFO: Pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007323052s
    STEP: Saw pod success 01/05/23 20:18:19.872
    Jan  5 20:18:19.872: INFO: Pod "pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f" satisfied condition "Succeeded or Failed"
    Jan  5 20:18:19.876: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 20:18:19.894
    Jan  5 20:18:19.904: INFO: Waiting for pod pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f to disappear
    Jan  5 20:18:19.907: INFO: Pod pod-projected-secrets-862e1670-1bfd-41e3-a1b3-530bd70c258f no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:19.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-3816" for this suite. 01/05/23 20:18:19.911
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:19.922
Jan  5 20:18:19.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 20:18:19.924
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:19.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:19.941
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 20:18:19.955
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:18:20.572
STEP: Deploying the webhook pod 01/05/23 20:18:20.578
STEP: Wait for the deployment to be ready 01/05/23 20:18:20.589
Jan  5 20:18:20.599: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 20:18:22.607
STEP: Verifying the service has paired with the endpoint 01/05/23 20:18:22.623
Jan  5 20:18:23.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209
STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 20:18:23.626
STEP: create a pod 01/05/23 20:18:23.647
Jan  5 20:18:23.654: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8196" to be "running"
Jan  5 20:18:23.661: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.15249ms
Jan  5 20:18:25.665: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010906014s
Jan  5 20:18:25.665: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 01/05/23 20:18:25.665
Jan  5 20:18:25.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=webhook-8196 attach --namespace=webhook-8196 to-be-attached-pod -i -c=container1'
Jan  5 20:18:25.759: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:25.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-8196" for this suite. 01/05/23 20:18:25.795
STEP: Destroying namespace "webhook-8196-markers" for this suite. 01/05/23 20:18:25.802
------------------------------
• [SLOW TEST] [5.887 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:209

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:19.922
    Jan  5 20:18:19.923: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 20:18:19.924
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:19.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:19.941
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 20:18:19.955
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:18:20.572
    STEP: Deploying the webhook pod 01/05/23 20:18:20.578
    STEP: Wait for the deployment to be ready 01/05/23 20:18:20.589
    Jan  5 20:18:20.599: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 20:18:22.607
    STEP: Verifying the service has paired with the endpoint 01/05/23 20:18:22.623
    Jan  5 20:18:23.623: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:209
    STEP: Registering the webhook via the AdmissionRegistration API 01/05/23 20:18:23.626
    STEP: create a pod 01/05/23 20:18:23.647
    Jan  5 20:18:23.654: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-8196" to be "running"
    Jan  5 20:18:23.661: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 7.15249ms
    Jan  5 20:18:25.665: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010906014s
    Jan  5 20:18:25.665: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 01/05/23 20:18:25.665
    Jan  5 20:18:25.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=webhook-8196 attach --namespace=webhook-8196 to-be-attached-pod -i -c=container1'
    Jan  5 20:18:25.759: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:25.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-8196" for this suite. 01/05/23 20:18:25.795
    STEP: Destroying namespace "webhook-8196-markers" for this suite. 01/05/23 20:18:25.802
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:25.81
Jan  5 20:18:25.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 20:18:25.812
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:25.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:25.834
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97
STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 20:18:25.837
Jan  5 20:18:25.846: INFO: Waiting up to 5m0s for pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f" in namespace "emptydir-652" to be "Succeeded or Failed"
Jan  5 20:18:25.849: INFO: Pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.022918ms
Jan  5 20:18:27.856: INFO: Pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010469912s
Jan  5 20:18:29.854: INFO: Pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008043353s
STEP: Saw pod success 01/05/23 20:18:29.854
Jan  5 20:18:29.854: INFO: Pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f" satisfied condition "Succeeded or Failed"
Jan  5 20:18:29.857: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f container test-container: <nil>
STEP: delete the pod 01/05/23 20:18:29.863
Jan  5 20:18:29.875: INFO: Waiting for pod pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f to disappear
Jan  5 20:18:29.879: INFO: Pod pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:29.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-652" for this suite. 01/05/23 20:18:29.883
------------------------------
• [4.078 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:25.81
    Jan  5 20:18:25.810: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 20:18:25.812
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:25.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:25.834
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:97
    STEP: Creating a pod to test emptydir 0644 on tmpfs 01/05/23 20:18:25.837
    Jan  5 20:18:25.846: INFO: Waiting up to 5m0s for pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f" in namespace "emptydir-652" to be "Succeeded or Failed"
    Jan  5 20:18:25.849: INFO: Pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.022918ms
    Jan  5 20:18:27.856: INFO: Pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010469912s
    Jan  5 20:18:29.854: INFO: Pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008043353s
    STEP: Saw pod success 01/05/23 20:18:29.854
    Jan  5 20:18:29.854: INFO: Pod "pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f" satisfied condition "Succeeded or Failed"
    Jan  5 20:18:29.857: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f container test-container: <nil>
    STEP: delete the pod 01/05/23 20:18:29.863
    Jan  5 20:18:29.875: INFO: Waiting for pod pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f to disappear
    Jan  5 20:18:29.879: INFO: Pod pod-2bd5fc9f-de53-49d2-a9a3-720afbfd428f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:29.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-652" for this suite. 01/05/23 20:18:29.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
[BeforeEach] [sig-node] Security Context
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:29.893
Jan  5 20:18:29.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename security-context 01/05/23 20:18:29.895
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:29.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:29.911
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:31
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 20:18:29.914
Jan  5 20:18:29.922: INFO: Waiting up to 5m0s for pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59" in namespace "security-context-5163" to be "Succeeded or Failed"
Jan  5 20:18:29.927: INFO: Pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.519012ms
Jan  5 20:18:31.932: INFO: Pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009428348s
Jan  5 20:18:33.929: INFO: Pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007389622s
STEP: Saw pod success 01/05/23 20:18:33.929
Jan  5 20:18:33.930: INFO: Pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59" satisfied condition "Succeeded or Failed"
Jan  5 20:18:33.932: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59 container test-container: <nil>
STEP: delete the pod 01/05/23 20:18:33.939
Jan  5 20:18:33.948: INFO: Waiting for pod security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59 to disappear
Jan  5 20:18:33.951: INFO: Pod security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Security Context
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Security Context
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Security Context
  tear down framework | framework.go:193
STEP: Destroying namespace "security-context-5163" for this suite. 01/05/23 20:18:33.954
------------------------------
• [4.069 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:29.893
    Jan  5 20:18:29.893: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename security-context 01/05/23 20:18:29.895
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:29.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:29.911
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:31
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:129
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 01/05/23 20:18:29.914
    Jan  5 20:18:29.922: INFO: Waiting up to 5m0s for pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59" in namespace "security-context-5163" to be "Succeeded or Failed"
    Jan  5 20:18:29.927: INFO: Pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59": Phase="Pending", Reason="", readiness=false. Elapsed: 4.519012ms
    Jan  5 20:18:31.932: INFO: Pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009428348s
    Jan  5 20:18:33.929: INFO: Pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007389622s
    STEP: Saw pod success 01/05/23 20:18:33.929
    Jan  5 20:18:33.930: INFO: Pod "security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59" satisfied condition "Succeeded or Failed"
    Jan  5 20:18:33.932: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59 container test-container: <nil>
    STEP: delete the pod 01/05/23 20:18:33.939
    Jan  5 20:18:33.948: INFO: Waiting for pod security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59 to disappear
    Jan  5 20:18:33.951: INFO: Pod security-context-139466ca-1e73-4bb2-9d53-1a3bc52cba59 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Security Context
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Security Context
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Security Context
      tear down framework | framework.go:193
    STEP: Destroying namespace "security-context-5163" for this suite. 01/05/23 20:18:33.954
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:33.966
Jan  5 20:18:33.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 20:18:33.967
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:33.983
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:33.986
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46
STEP: Creating projection with secret that has name projected-secret-test-8b02f9e0-5d4f-4e66-8f93-5c07d40be8d0 01/05/23 20:18:33.989
STEP: Creating a pod to test consume secrets 01/05/23 20:18:33.993
Jan  5 20:18:34.003: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c" in namespace "projected-864" to be "Succeeded or Failed"
Jan  5 20:18:34.006: INFO: Pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705318ms
Jan  5 20:18:36.011: INFO: Pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008707677s
Jan  5 20:18:38.010: INFO: Pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007187861s
STEP: Saw pod success 01/05/23 20:18:38.01
Jan  5 20:18:38.010: INFO: Pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c" satisfied condition "Succeeded or Failed"
Jan  5 20:18:38.013: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c container projected-secret-volume-test: <nil>
STEP: delete the pod 01/05/23 20:18:38.018
Jan  5 20:18:38.029: INFO: Waiting for pod pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c to disappear
Jan  5 20:18:38.033: INFO: Pod pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:38.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-864" for this suite. 01/05/23 20:18:38.038
------------------------------
• [4.077 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:33.966
    Jan  5 20:18:33.966: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 20:18:33.967
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:33.983
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:33.986
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:46
    STEP: Creating projection with secret that has name projected-secret-test-8b02f9e0-5d4f-4e66-8f93-5c07d40be8d0 01/05/23 20:18:33.989
    STEP: Creating a pod to test consume secrets 01/05/23 20:18:33.993
    Jan  5 20:18:34.003: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c" in namespace "projected-864" to be "Succeeded or Failed"
    Jan  5 20:18:34.006: INFO: Pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.705318ms
    Jan  5 20:18:36.011: INFO: Pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008707677s
    Jan  5 20:18:38.010: INFO: Pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007187861s
    STEP: Saw pod success 01/05/23 20:18:38.01
    Jan  5 20:18:38.010: INFO: Pod "pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c" satisfied condition "Succeeded or Failed"
    Jan  5 20:18:38.013: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c container projected-secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 20:18:38.018
    Jan  5 20:18:38.029: INFO: Waiting for pod pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c to disappear
    Jan  5 20:18:38.033: INFO: Pod pod-projected-secrets-ce295634-da96-4f0a-8e5e-d60aacd8540c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:38.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-864" for this suite. 01/05/23 20:18:38.038
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:38.046
Jan  5 20:18:38.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 20:18:38.048
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:38.064
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:38.068
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74
STEP: Creating configMap with name projected-configmap-test-volume-5d621988-bb5f-48ab-b7b2-b9c99d898cf9 01/05/23 20:18:38.071
STEP: Creating a pod to test consume configMaps 01/05/23 20:18:38.075
Jan  5 20:18:38.084: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42" in namespace "projected-6382" to be "Succeeded or Failed"
Jan  5 20:18:38.088: INFO: Pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891809ms
Jan  5 20:18:40.091: INFO: Pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007606566s
Jan  5 20:18:42.091: INFO: Pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007210435s
STEP: Saw pod success 01/05/23 20:18:42.091
Jan  5 20:18:42.091: INFO: Pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42" satisfied condition "Succeeded or Failed"
Jan  5 20:18:42.094: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 20:18:42.099
Jan  5 20:18:42.112: INFO: Waiting for pod pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42 to disappear
Jan  5 20:18:42.114: INFO: Pod pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:42.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-6382" for this suite. 01/05/23 20:18:42.118
------------------------------
• [4.078 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:38.046
    Jan  5 20:18:38.047: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 20:18:38.048
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:38.064
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:38.068
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:74
    STEP: Creating configMap with name projected-configmap-test-volume-5d621988-bb5f-48ab-b7b2-b9c99d898cf9 01/05/23 20:18:38.071
    STEP: Creating a pod to test consume configMaps 01/05/23 20:18:38.075
    Jan  5 20:18:38.084: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42" in namespace "projected-6382" to be "Succeeded or Failed"
    Jan  5 20:18:38.088: INFO: Pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891809ms
    Jan  5 20:18:40.091: INFO: Pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007606566s
    Jan  5 20:18:42.091: INFO: Pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007210435s
    STEP: Saw pod success 01/05/23 20:18:42.091
    Jan  5 20:18:42.091: INFO: Pod "pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42" satisfied condition "Succeeded or Failed"
    Jan  5 20:18:42.094: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 20:18:42.099
    Jan  5 20:18:42.112: INFO: Waiting for pod pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42 to disappear
    Jan  5 20:18:42.114: INFO: Pod pod-projected-configmaps-31b58569-f3b5-4a03-9f31-9ff0e76bda42 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:42.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-6382" for this suite. 01/05/23 20:18:42.118
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:42.129
Jan  5 20:18:42.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename webhook 01/05/23 20:18:42.131
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:42.142
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:42.146
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:90
STEP: Setting up server cert 01/05/23 20:18:42.161
STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:18:42.511
STEP: Deploying the webhook pod 01/05/23 20:18:42.516
STEP: Wait for the deployment to be ready 01/05/23 20:18:42.528
Jan  5 20:18:42.534: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 01/05/23 20:18:44.543
STEP: Verifying the service has paired with the endpoint 01/05/23 20:18:44.557
Jan  5 20:18:45.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221
Jan  5 20:18:45.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/05/23 20:18:46.073
STEP: Creating a custom resource that should be denied by the webhook 01/05/23 20:18:46.096
STEP: Creating a custom resource whose deletion would be denied by the webhook 01/05/23 20:18:48.138
STEP: Updating the custom resource with disallowed data should be denied 01/05/23 20:18:48.144
STEP: Deleting the custom resource should be denied 01/05/23 20:18:48.154
STEP: Remove the offending key and value from the custom resource data 01/05/23 20:18:48.16
STEP: Deleting the updated custom resource should be successful 01/05/23 20:18:48.168
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:48.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:105
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "webhook-5842" for this suite. 01/05/23 20:18:48.738
STEP: Destroying namespace "webhook-5842-markers" for this suite. 01/05/23 20:18:48.744
------------------------------
• [SLOW TEST] [6.620 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:221

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:42.129
    Jan  5 20:18:42.130: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename webhook 01/05/23 20:18:42.131
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:42.142
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:42.146
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:90
    STEP: Setting up server cert 01/05/23 20:18:42.161
    STEP: Create role binding to let webhook read extension-apiserver-authentication 01/05/23 20:18:42.511
    STEP: Deploying the webhook pod 01/05/23 20:18:42.516
    STEP: Wait for the deployment to be ready 01/05/23 20:18:42.528
    Jan  5 20:18:42.534: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 01/05/23 20:18:44.543
    STEP: Verifying the service has paired with the endpoint 01/05/23 20:18:44.557
    Jan  5 20:18:45.558: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:221
    Jan  5 20:18:45.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 01/05/23 20:18:46.073
    STEP: Creating a custom resource that should be denied by the webhook 01/05/23 20:18:46.096
    STEP: Creating a custom resource whose deletion would be denied by the webhook 01/05/23 20:18:48.138
    STEP: Updating the custom resource with disallowed data should be denied 01/05/23 20:18:48.144
    STEP: Deleting the custom resource should be denied 01/05/23 20:18:48.154
    STEP: Remove the offending key and value from the custom resource data 01/05/23 20:18:48.16
    STEP: Deleting the updated custom resource should be successful 01/05/23 20:18:48.168
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:48.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:105
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "webhook-5842" for this suite. 01/05/23 20:18:48.738
    STEP: Destroying namespace "webhook-5842-markers" for this suite. 01/05/23 20:18:48.744
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:48.75
Jan  5 20:18:48.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 20:18:48.751
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:48.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:48.783
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442
STEP: set up a multi version CRD 01/05/23 20:18:48.786
Jan  5 20:18:48.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: mark a version not serverd 01/05/23 20:18:52.717
STEP: check the unserved version gets removed 01/05/23 20:18:52.736
STEP: check the other version is not changed 01/05/23 20:18:53.824
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:57.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-6227" for this suite. 01/05/23 20:18:57.103
------------------------------
• [SLOW TEST] [8.359 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:442

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:48.75
    Jan  5 20:18:48.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 20:18:48.751
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:48.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:48.783
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:442
    STEP: set up a multi version CRD 01/05/23 20:18:48.786
    Jan  5 20:18:48.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: mark a version not serverd 01/05/23 20:18:52.717
    STEP: check the unserved version gets removed 01/05/23 20:18:52.736
    STEP: check the other version is not changed 01/05/23 20:18:53.824
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:57.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-6227" for this suite. 01/05/23 20:18:57.103
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
[BeforeEach] [sig-storage] CSIInlineVolumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:57.113
Jan  5 20:18:57.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename csiinlinevolumes 01/05/23 20:18:57.115
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:57.129
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:57.132
[BeforeEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131
STEP: creating 01/05/23 20:18:57.135
STEP: getting 01/05/23 20:18:57.154
STEP: listing in namespace 01/05/23 20:18:57.158
STEP: patching 01/05/23 20:18:57.16
STEP: deleting 01/05/23 20:18:57.169
[AfterEach] [sig-storage] CSIInlineVolumes
  test/e2e/framework/node/init/init.go:32
Jan  5 20:18:57.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
  tear down framework | framework.go:193
STEP: Destroying namespace "csiinlinevolumes-7096" for this suite. 01/05/23 20:18:57.191
------------------------------
• [0.082 seconds]
[sig-storage] CSIInlineVolumes
test/e2e/storage/utils/framework.go:23
  should support CSIVolumeSource in Pod API [Conformance]
  test/e2e/storage/csi_inline.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIInlineVolumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:57.113
    Jan  5 20:18:57.113: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename csiinlinevolumes 01/05/23 20:18:57.115
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:57.129
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:57.132
    [BeforeEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSIVolumeSource in Pod API [Conformance]
      test/e2e/storage/csi_inline.go:131
    STEP: creating 01/05/23 20:18:57.135
    STEP: getting 01/05/23 20:18:57.154
    STEP: listing in namespace 01/05/23 20:18:57.158
    STEP: patching 01/05/23 20:18:57.16
    STEP: deleting 01/05/23 20:18:57.169
    [AfterEach] [sig-storage] CSIInlineVolumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:18:57.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] CSIInlineVolumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "csiinlinevolumes-7096" for this suite. 01/05/23 20:18:57.191
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:18:57.202
Jan  5 20:18:57.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 20:18:57.203
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:57.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:57.228
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/05/23 20:18:57.24
Jan  5 20:18:57.252: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4818" to be "running and ready"
Jan  5 20:18:57.257: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229941ms
Jan  5 20:18:57.257: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:18:59.261: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008606082s
Jan  5 20:18:59.261: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 20:18:59.261: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:134
STEP: create the pod with lifecycle hook 01/05/23 20:18:59.264
Jan  5 20:18:59.270: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4818" to be "running and ready"
Jan  5 20:18:59.275: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.709651ms
Jan  5 20:18:59.275: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:19:01.280: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009361179s
Jan  5 20:19:01.280: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jan  5 20:19:01.280: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 01/05/23 20:19:01.282
STEP: delete the pod with lifecycle hook 01/05/23 20:19:01.3
Jan  5 20:19:01.307: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  5 20:19:01.311: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  5 20:19:03.313: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  5 20:19:03.316: INFO: Pod pod-with-poststart-exec-hook still exists
Jan  5 20:19:05.311: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jan  5 20:19:05.315: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:05.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-4818" for this suite. 01/05/23 20:19:05.319
------------------------------
• [SLOW TEST] [8.122 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:134

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:18:57.202
    Jan  5 20:18:57.202: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 20:18:57.203
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:18:57.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:18:57.228
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 20:18:57.24
    Jan  5 20:18:57.252: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4818" to be "running and ready"
    Jan  5 20:18:57.257: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.229941ms
    Jan  5 20:18:57.257: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:18:59.261: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.008606082s
    Jan  5 20:18:59.261: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 20:18:59.261: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:134
    STEP: create the pod with lifecycle hook 01/05/23 20:18:59.264
    Jan  5 20:18:59.270: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4818" to be "running and ready"
    Jan  5 20:18:59.275: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.709651ms
    Jan  5 20:18:59.275: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:19:01.280: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.009361179s
    Jan  5 20:19:01.280: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jan  5 20:19:01.280: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 01/05/23 20:19:01.282
    STEP: delete the pod with lifecycle hook 01/05/23 20:19:01.3
    Jan  5 20:19:01.307: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  5 20:19:01.311: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  5 20:19:03.313: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  5 20:19:03.316: INFO: Pod pod-with-poststart-exec-hook still exists
    Jan  5 20:19:05.311: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jan  5 20:19:05.315: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:05.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-4818" for this suite. 01/05/23 20:19:05.319
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
[BeforeEach] [sig-storage] Projected configMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:05.326
Jan  5 20:19:05.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 20:19:05.328
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:05.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:05.346
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174
STEP: Creating configMap with name cm-test-opt-del-656e18e1-7a1d-4528-9c09-2e0db51f635d 01/05/23 20:19:05.352
STEP: Creating configMap with name cm-test-opt-upd-9c9baee8-c1b0-4a9a-af99-1e8974294f99 01/05/23 20:19:05.357
STEP: Creating the pod 01/05/23 20:19:05.361
Jan  5 20:19:05.374: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020" in namespace "projected-8218" to be "running and ready"
Jan  5 20:19:05.378: INFO: Pod "pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020148ms
Jan  5 20:19:05.378: INFO: The phase of Pod pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:19:07.382: INFO: Pod "pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020": Phase="Running", Reason="", readiness=true. Elapsed: 2.007990126s
Jan  5 20:19:07.382: INFO: The phase of Pod pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020 is Running (Ready = true)
Jan  5 20:19:07.382: INFO: Pod "pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-656e18e1-7a1d-4528-9c09-2e0db51f635d 01/05/23 20:19:07.402
STEP: Updating configmap cm-test-opt-upd-9c9baee8-c1b0-4a9a-af99-1e8974294f99 01/05/23 20:19:07.406
STEP: Creating configMap with name cm-test-opt-create-c3ad47fd-e652-42e6-bb4f-ac68df708827 01/05/23 20:19:07.412
STEP: waiting to observe update in volume 01/05/23 20:19:07.416
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:09.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected configMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected configMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected configMap
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8218" for this suite. 01/05/23 20:19:09.456
------------------------------
• [4.138 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:05.326
    Jan  5 20:19:05.326: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 20:19:05.328
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:05.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:05.346
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:174
    STEP: Creating configMap with name cm-test-opt-del-656e18e1-7a1d-4528-9c09-2e0db51f635d 01/05/23 20:19:05.352
    STEP: Creating configMap with name cm-test-opt-upd-9c9baee8-c1b0-4a9a-af99-1e8974294f99 01/05/23 20:19:05.357
    STEP: Creating the pod 01/05/23 20:19:05.361
    Jan  5 20:19:05.374: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020" in namespace "projected-8218" to be "running and ready"
    Jan  5 20:19:05.378: INFO: Pod "pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020148ms
    Jan  5 20:19:05.378: INFO: The phase of Pod pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:19:07.382: INFO: Pod "pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020": Phase="Running", Reason="", readiness=true. Elapsed: 2.007990126s
    Jan  5 20:19:07.382: INFO: The phase of Pod pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020 is Running (Ready = true)
    Jan  5 20:19:07.382: INFO: Pod "pod-projected-configmaps-baf57b84-c845-4c34-9f52-02240d107020" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-656e18e1-7a1d-4528-9c09-2e0db51f635d 01/05/23 20:19:07.402
    STEP: Updating configmap cm-test-opt-upd-9c9baee8-c1b0-4a9a-af99-1e8974294f99 01/05/23 20:19:07.406
    STEP: Creating configMap with name cm-test-opt-create-c3ad47fd-e652-42e6-bb4f-ac68df708827 01/05/23 20:19:07.412
    STEP: waiting to observe update in volume 01/05/23 20:19:07.416
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:09.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected configMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8218" for this suite. 01/05/23 20:19:09.456
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:09.467
Jan  5 20:19:09.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 20:19:09.468
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:09.49
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:09.496
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138
STEP: Creating configMap that has name configmap-test-emptyKey-c830d38a-e591-4296-9f61-c04c0b67b024 01/05/23 20:19:09.5
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:09.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-6815" for this suite. 01/05/23 20:19:09.506
------------------------------
• [0.048 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:138

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:09.467
    Jan  5 20:19:09.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 20:19:09.468
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:09.49
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:09.496
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:138
    STEP: Creating configMap that has name configmap-test-emptyKey-c830d38a-e591-4296-9f61-c04c0b67b024 01/05/23 20:19:09.5
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:09.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-6815" for this suite. 01/05/23 20:19:09.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:09.52
Jan  5 20:19:09.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 20:19:09.521
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:09.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:09.553
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87
STEP: Creating a pod to test emptydir volume type on tmpfs 01/05/23 20:19:09.558
Jan  5 20:19:09.570: INFO: Waiting up to 5m0s for pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7" in namespace "emptydir-9934" to be "Succeeded or Failed"
Jan  5 20:19:09.579: INFO: Pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.211405ms
Jan  5 20:19:11.582: INFO: Pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012334488s
Jan  5 20:19:13.582: INFO: Pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011883776s
STEP: Saw pod success 01/05/23 20:19:13.582
Jan  5 20:19:13.582: INFO: Pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7" satisfied condition "Succeeded or Failed"
Jan  5 20:19:13.584: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-de918cb0-41b8-4880-b520-9afe696f2bd7 container test-container: <nil>
STEP: delete the pod 01/05/23 20:19:13.59
Jan  5 20:19:13.602: INFO: Waiting for pod pod-de918cb0-41b8-4880-b520-9afe696f2bd7 to disappear
Jan  5 20:19:13.605: INFO: Pod pod-de918cb0-41b8-4880-b520-9afe696f2bd7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:13.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9934" for this suite. 01/05/23 20:19:13.61
------------------------------
• [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:09.52
    Jan  5 20:19:09.520: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 20:19:09.521
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:09.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:09.553
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:87
    STEP: Creating a pod to test emptydir volume type on tmpfs 01/05/23 20:19:09.558
    Jan  5 20:19:09.570: INFO: Waiting up to 5m0s for pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7" in namespace "emptydir-9934" to be "Succeeded or Failed"
    Jan  5 20:19:09.579: INFO: Pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.211405ms
    Jan  5 20:19:11.582: INFO: Pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012334488s
    Jan  5 20:19:13.582: INFO: Pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011883776s
    STEP: Saw pod success 01/05/23 20:19:13.582
    Jan  5 20:19:13.582: INFO: Pod "pod-de918cb0-41b8-4880-b520-9afe696f2bd7" satisfied condition "Succeeded or Failed"
    Jan  5 20:19:13.584: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-de918cb0-41b8-4880-b520-9afe696f2bd7 container test-container: <nil>
    STEP: delete the pod 01/05/23 20:19:13.59
    Jan  5 20:19:13.602: INFO: Waiting for pod pod-de918cb0-41b8-4880-b520-9afe696f2bd7 to disappear
    Jan  5 20:19:13.605: INFO: Pod pod-de918cb0-41b8-4880-b520-9afe696f2bd7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:13.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9934" for this suite. 01/05/23 20:19:13.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
[BeforeEach] [sig-storage] Projected downwardAPI
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:13.619
Jan  5 20:19:13.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 20:19:13.621
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:13.631
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:13.635
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:44
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207
STEP: Creating a pod to test downward API volume plugin 01/05/23 20:19:13.638
Jan  5 20:19:13.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a" in namespace "projected-4456" to be "Succeeded or Failed"
Jan  5 20:19:13.658: INFO: Pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.902533ms
Jan  5 20:19:15.664: INFO: Pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011998108s
Jan  5 20:19:17.661: INFO: Pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009451571s
STEP: Saw pod success 01/05/23 20:19:17.662
Jan  5 20:19:17.662: INFO: Pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a" satisfied condition "Succeeded or Failed"
Jan  5 20:19:17.665: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a container client-container: <nil>
STEP: delete the pod 01/05/23 20:19:17.674
Jan  5 20:19:17.685: INFO: Waiting for pod downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a to disappear
Jan  5 20:19:17.689: INFO: Pod downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:17.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected downwardAPI
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-4456" for this suite. 01/05/23 20:19:17.695
------------------------------
• [4.080 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:13.619
    Jan  5 20:19:13.619: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 20:19:13.621
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:13.631
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:13.635
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:44
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:207
    STEP: Creating a pod to test downward API volume plugin 01/05/23 20:19:13.638
    Jan  5 20:19:13.651: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a" in namespace "projected-4456" to be "Succeeded or Failed"
    Jan  5 20:19:13.658: INFO: Pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.902533ms
    Jan  5 20:19:15.664: INFO: Pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011998108s
    Jan  5 20:19:17.661: INFO: Pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009451571s
    STEP: Saw pod success 01/05/23 20:19:17.662
    Jan  5 20:19:17.662: INFO: Pod "downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a" satisfied condition "Succeeded or Failed"
    Jan  5 20:19:17.665: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a container client-container: <nil>
    STEP: delete the pod 01/05/23 20:19:17.674
    Jan  5 20:19:17.685: INFO: Waiting for pod downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a to disappear
    Jan  5 20:19:17.689: INFO: Pod downwardapi-volume-f710b80e-ab32-4e93-a1b6-56dbe9a2c37a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:17.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected downwardAPI
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-4456" for this suite. 01/05/23 20:19:17.695
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
[BeforeEach] [sig-node] Container Lifecycle Hook
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:17.716
Jan  5 20:19:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 20:19:17.718
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:17.731
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:17.734
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:77
STEP: create the container to handle the HTTPGet hook request. 01/05/23 20:19:17.742
Jan  5 20:19:17.752: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9191" to be "running and ready"
Jan  5 20:19:17.755: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.952246ms
Jan  5 20:19:17.756: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:19:19.759: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007095925s
Jan  5 20:19:19.759: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jan  5 20:19:19.759: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:212
STEP: create the pod with lifecycle hook 01/05/23 20:19:19.762
Jan  5 20:19:19.768: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9191" to be "running and ready"
Jan  5 20:19:19.772: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519116ms
Jan  5 20:19:19.772: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:19:21.775: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007015015s
Jan  5 20:19:21.776: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jan  5 20:19:21.776: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 01/05/23 20:19:21.778
Jan  5 20:19:21.784: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 20:19:21.788: INFO: Pod pod-with-prestop-http-hook still exists
Jan  5 20:19:23.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 20:19:23.798: INFO: Pod pod-with-prestop-http-hook still exists
Jan  5 20:19:25.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jan  5 20:19:25.792: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 01/05/23 20:19:25.792
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:25.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
  tear down framework | framework.go:193
STEP: Destroying namespace "container-lifecycle-hook-9191" for this suite. 01/05/23 20:19:25.805
------------------------------
• [SLOW TEST] [8.099 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:212

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:17.716
    Jan  5 20:19:17.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-lifecycle-hook 01/05/23 20:19:17.718
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:17.731
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:17.734
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:77
    STEP: create the container to handle the HTTPGet hook request. 01/05/23 20:19:17.742
    Jan  5 20:19:17.752: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-9191" to be "running and ready"
    Jan  5 20:19:17.755: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.952246ms
    Jan  5 20:19:17.756: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:19:19.759: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.007095925s
    Jan  5 20:19:19.759: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jan  5 20:19:19.759: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:212
    STEP: create the pod with lifecycle hook 01/05/23 20:19:19.762
    Jan  5 20:19:19.768: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-9191" to be "running and ready"
    Jan  5 20:19:19.772: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 3.519116ms
    Jan  5 20:19:19.772: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:19:21.775: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007015015s
    Jan  5 20:19:21.776: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jan  5 20:19:21.776: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 01/05/23 20:19:21.778
    Jan  5 20:19:21.784: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 20:19:21.788: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  5 20:19:23.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 20:19:23.798: INFO: Pod pod-with-prestop-http-hook still exists
    Jan  5 20:19:25.789: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jan  5 20:19:25.792: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 01/05/23 20:19:25.792
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:25.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Lifecycle Hook
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-lifecycle-hook-9191" for this suite. 01/05/23 20:19:25.805
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:25.822
Jan  5 20:19:25.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 20:19:25.823
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:25.837
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:25.842
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117
STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 20:19:25.845
Jan  5 20:19:25.857: INFO: Waiting up to 5m0s for pod "pod-59beac16-ed84-458b-a84a-d0027ae77573" in namespace "emptydir-2784" to be "Succeeded or Failed"
Jan  5 20:19:25.861: INFO: Pod "pod-59beac16-ed84-458b-a84a-d0027ae77573": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115192ms
Jan  5 20:19:27.864: INFO: Pod "pod-59beac16-ed84-458b-a84a-d0027ae77573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007706007s
Jan  5 20:19:29.865: INFO: Pod "pod-59beac16-ed84-458b-a84a-d0027ae77573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008655215s
STEP: Saw pod success 01/05/23 20:19:29.865
Jan  5 20:19:29.865: INFO: Pod "pod-59beac16-ed84-458b-a84a-d0027ae77573" satisfied condition "Succeeded or Failed"
Jan  5 20:19:29.868: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-59beac16-ed84-458b-a84a-d0027ae77573 container test-container: <nil>
STEP: delete the pod 01/05/23 20:19:29.873
Jan  5 20:19:29.884: INFO: Waiting for pod pod-59beac16-ed84-458b-a84a-d0027ae77573 to disappear
Jan  5 20:19:29.888: INFO: Pod pod-59beac16-ed84-458b-a84a-d0027ae77573 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:29.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-2784" for this suite. 01/05/23 20:19:29.892
------------------------------
• [4.075 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:25.822
    Jan  5 20:19:25.822: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 20:19:25.823
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:25.837
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:25.842
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:117
    STEP: Creating a pod to test emptydir 0777 on tmpfs 01/05/23 20:19:25.845
    Jan  5 20:19:25.857: INFO: Waiting up to 5m0s for pod "pod-59beac16-ed84-458b-a84a-d0027ae77573" in namespace "emptydir-2784" to be "Succeeded or Failed"
    Jan  5 20:19:25.861: INFO: Pod "pod-59beac16-ed84-458b-a84a-d0027ae77573": Phase="Pending", Reason="", readiness=false. Elapsed: 4.115192ms
    Jan  5 20:19:27.864: INFO: Pod "pod-59beac16-ed84-458b-a84a-d0027ae77573": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007706007s
    Jan  5 20:19:29.865: INFO: Pod "pod-59beac16-ed84-458b-a84a-d0027ae77573": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008655215s
    STEP: Saw pod success 01/05/23 20:19:29.865
    Jan  5 20:19:29.865: INFO: Pod "pod-59beac16-ed84-458b-a84a-d0027ae77573" satisfied condition "Succeeded or Failed"
    Jan  5 20:19:29.868: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-59beac16-ed84-458b-a84a-d0027ae77573 container test-container: <nil>
    STEP: delete the pod 01/05/23 20:19:29.873
    Jan  5 20:19:29.884: INFO: Waiting for pod pod-59beac16-ed84-458b-a84a-d0027ae77573 to disappear
    Jan  5 20:19:29.888: INFO: Pod pod-59beac16-ed84-458b-a84a-d0027ae77573 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:29.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-2784" for this suite. 01/05/23 20:19:29.892
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:29.897
Jan  5 20:19:29.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 20:19:29.899
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:29.911
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:29.914
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7599 01/05/23 20:19:29.917
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 20:19:29.93
STEP: creating service externalsvc in namespace services-7599 01/05/23 20:19:29.93
STEP: creating replication controller externalsvc in namespace services-7599 01/05/23 20:19:29.941
I0105 20:19:29.951991      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7599, replica count: 2
I0105 20:19:33.003465      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 01/05/23 20:19:33.006
Jan  5 20:19:33.018: INFO: Creating new exec pod
Jan  5 20:19:33.026: INFO: Waiting up to 5m0s for pod "execpodlzxmj" in namespace "services-7599" to be "running"
Jan  5 20:19:33.030: INFO: Pod "execpodlzxmj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472008ms
Jan  5 20:19:35.034: INFO: Pod "execpodlzxmj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007271956s
Jan  5 20:19:35.034: INFO: Pod "execpodlzxmj" satisfied condition "running"
Jan  5 20:19:35.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7599 exec execpodlzxmj -- /bin/sh -x -c nslookup nodeport-service.services-7599.svc.cluster.local'
Jan  5 20:19:35.215: INFO: stderr: "+ nslookup nodeport-service.services-7599.svc.cluster.local\n"
Jan  5 20:19:35.215: INFO: stdout: "Server:\t\t10.20.0.10\nAddress:\t10.20.0.10#53\n\nnodeport-service.services-7599.svc.cluster.local\tcanonical name = externalsvc.services-7599.svc.cluster.local.\nName:\texternalsvc.services-7599.svc.cluster.local\nAddress: 10.20.7.173\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7599, will wait for the garbage collector to delete the pods 01/05/23 20:19:35.215
Jan  5 20:19:35.275: INFO: Deleting ReplicationController externalsvc took: 6.450179ms
Jan  5 20:19:35.375: INFO: Terminating ReplicationController externalsvc pods took: 100.464232ms
Jan  5 20:19:37.190: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7599" for this suite. 01/05/23 20:19:37.21
------------------------------
• [SLOW TEST] [7.318 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1557

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:29.897
    Jan  5 20:19:29.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 20:19:29.899
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:29.911
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:29.914
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1557
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-7599 01/05/23 20:19:29.917
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 01/05/23 20:19:29.93
    STEP: creating service externalsvc in namespace services-7599 01/05/23 20:19:29.93
    STEP: creating replication controller externalsvc in namespace services-7599 01/05/23 20:19:29.941
    I0105 20:19:29.951991      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7599, replica count: 2
    I0105 20:19:33.003465      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 01/05/23 20:19:33.006
    Jan  5 20:19:33.018: INFO: Creating new exec pod
    Jan  5 20:19:33.026: INFO: Waiting up to 5m0s for pod "execpodlzxmj" in namespace "services-7599" to be "running"
    Jan  5 20:19:33.030: INFO: Pod "execpodlzxmj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.472008ms
    Jan  5 20:19:35.034: INFO: Pod "execpodlzxmj": Phase="Running", Reason="", readiness=true. Elapsed: 2.007271956s
    Jan  5 20:19:35.034: INFO: Pod "execpodlzxmj" satisfied condition "running"
    Jan  5 20:19:35.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7599 exec execpodlzxmj -- /bin/sh -x -c nslookup nodeport-service.services-7599.svc.cluster.local'
    Jan  5 20:19:35.215: INFO: stderr: "+ nslookup nodeport-service.services-7599.svc.cluster.local\n"
    Jan  5 20:19:35.215: INFO: stdout: "Server:\t\t10.20.0.10\nAddress:\t10.20.0.10#53\n\nnodeport-service.services-7599.svc.cluster.local\tcanonical name = externalsvc.services-7599.svc.cluster.local.\nName:\texternalsvc.services-7599.svc.cluster.local\nAddress: 10.20.7.173\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7599, will wait for the garbage collector to delete the pods 01/05/23 20:19:35.215
    Jan  5 20:19:35.275: INFO: Deleting ReplicationController externalsvc took: 6.450179ms
    Jan  5 20:19:35.375: INFO: Terminating ReplicationController externalsvc pods took: 100.464232ms
    Jan  5 20:19:37.190: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:37.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7599" for this suite. 01/05/23 20:19:37.21
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:37.216
Jan  5 20:19:37.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 20:19:37.218
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:37.231
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:37.234
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47
STEP: Creating configMap with name configmap-test-volume-3a5b680f-7224-4d43-9a3a-35721e63caed 01/05/23 20:19:37.237
STEP: Creating a pod to test consume configMaps 01/05/23 20:19:37.244
Jan  5 20:19:37.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5" in namespace "configmap-4533" to be "Succeeded or Failed"
Jan  5 20:19:37.259: INFO: Pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.454046ms
Jan  5 20:19:39.264: INFO: Pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008856839s
Jan  5 20:19:41.264: INFO: Pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009392442s
STEP: Saw pod success 01/05/23 20:19:41.264
Jan  5 20:19:41.264: INFO: Pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5" satisfied condition "Succeeded or Failed"
Jan  5 20:19:41.266: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 20:19:41.274
Jan  5 20:19:41.286: INFO: Waiting for pod pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5 to disappear
Jan  5 20:19:41.289: INFO: Pod pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:41.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-4533" for this suite. 01/05/23 20:19:41.293
------------------------------
• [4.084 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:47

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:37.216
    Jan  5 20:19:37.217: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 20:19:37.218
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:37.231
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:37.234
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:47
    STEP: Creating configMap with name configmap-test-volume-3a5b680f-7224-4d43-9a3a-35721e63caed 01/05/23 20:19:37.237
    STEP: Creating a pod to test consume configMaps 01/05/23 20:19:37.244
    Jan  5 20:19:37.255: INFO: Waiting up to 5m0s for pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5" in namespace "configmap-4533" to be "Succeeded or Failed"
    Jan  5 20:19:37.259: INFO: Pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.454046ms
    Jan  5 20:19:39.264: INFO: Pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008856839s
    Jan  5 20:19:41.264: INFO: Pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009392442s
    STEP: Saw pod success 01/05/23 20:19:41.264
    Jan  5 20:19:41.264: INFO: Pod "pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5" satisfied condition "Succeeded or Failed"
    Jan  5 20:19:41.266: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 20:19:41.274
    Jan  5 20:19:41.286: INFO: Waiting for pod pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5 to disappear
    Jan  5 20:19:41.289: INFO: Pod pod-configmaps-aa513b41-dcdc-46c4-91fe-b5a7714c3ec5 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:41.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-4533" for this suite. 01/05/23 20:19:41.293
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:41.306
Jan  5 20:19:41.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename deployment 01/05/23 20:19:41.307
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:41.325
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:41.328
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jan  5 20:19:41.331: INFO: Creating simple deployment test-new-deployment
Jan  5 20:19:41.342: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
STEP: getting scale subresource 01/05/23 20:19:43.357
STEP: updating a scale subresource 01/05/23 20:19:43.359
STEP: verifying the deployment Spec.Replicas was modified 01/05/23 20:19:43.365
STEP: Patch a scale subresource 01/05/23 20:19:43.367
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 20:19:43.387: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-716  fabfdfdf-d27e-4c6a-9a86-fe119cc74fde 103706 3 2023-01-05 20:19:41 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-05 20:19:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00447ff38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 20:19:43 +0000 UTC,LastTransitionTime:2023-01-05 20:19:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-05 20:19:43 +0000 UTC,LastTransitionTime:2023-01-05 20:19:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jan  5 20:19:43.392: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-716  5f62fce4-6567-4435-995c-6b2dd0ee7fce 103707 2 2023-01-05 20:19:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment fabfdfdf-d27e-4c6a-9a86-fe119cc74fde 0xc000dac397 0xc000dac398}] [] [{kube-controller-manager Update apps/v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fabfdfdf-d27e-4c6a-9a86-fe119cc74fde\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dac428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jan  5 20:19:43.396: INFO: Pod "test-new-deployment-7f5969cbc7-4qwmk" is not available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4qwmk test-new-deployment-7f5969cbc7- deployment-716  d10679a0-3c9b-4007-bdb9-4975e07c5536 103709 0 2023-01-05 20:19:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5f62fce4-6567-4435-995c-6b2dd0ee7fce 0xc000dac837 0xc000dac838}] [] [{kube-controller-manager Update v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f62fce4-6567-4435-995c-6b2dd0ee7fce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndjf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndjf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jan  5 20:19:43.397: INFO: Pod "test-new-deployment-7f5969cbc7-8jv2s" is available:
&Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8jv2s test-new-deployment-7f5969cbc7- deployment-716  4aca6fe4-48c6-490a-8c52-2c4c15c3a398 103700 0 2023-01-05 20:19:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5f62fce4-6567-4435-995c-6b2dd0ee7fce 0xc000dacd40 0xc000dacd41}] [] [{kube-controller-manager Update v1 2023-01-05 20:19:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f62fce4-6567-4435-995c-6b2dd0ee7fce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jnklc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jnklc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.21,StartTime:2023-01-05 20:19:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:19:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://391ceb4ee1003dc729a9052aa6e482602411f9c2ecd1b3263b3140ce3ee43d1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:43.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-716" for this suite. 01/05/23 20:19:43.407
------------------------------
• [2.112 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:41.306
    Jan  5 20:19:41.306: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename deployment 01/05/23 20:19:41.307
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:41.325
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:41.328
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jan  5 20:19:41.331: INFO: Creating simple deployment test-new-deployment
    Jan  5 20:19:41.342: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
    STEP: getting scale subresource 01/05/23 20:19:43.357
    STEP: updating a scale subresource 01/05/23 20:19:43.359
    STEP: verifying the deployment Spec.Replicas was modified 01/05/23 20:19:43.365
    STEP: Patch a scale subresource 01/05/23 20:19:43.367
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 20:19:43.387: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-716  fabfdfdf-d27e-4c6a-9a86-fe119cc74fde 103706 3 2023-01-05 20:19:41 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-01-05 20:19:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00447ff38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-01-05 20:19:43 +0000 UTC,LastTransitionTime:2023-01-05 20:19:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-7f5969cbc7" has successfully progressed.,LastUpdateTime:2023-01-05 20:19:43 +0000 UTC,LastTransitionTime:2023-01-05 20:19:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jan  5 20:19:43.392: INFO: New ReplicaSet "test-new-deployment-7f5969cbc7" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-7f5969cbc7  deployment-716  5f62fce4-6567-4435-995c-6b2dd0ee7fce 103707 2 2023-01-05 20:19:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment fabfdfdf-d27e-4c6a-9a86-fe119cc74fde 0xc000dac397 0xc000dac398}] [] [{kube-controller-manager Update apps/v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"fabfdfdf-d27e-4c6a-9a86-fe119cc74fde\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7f5969cbc7,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000dac428 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jan  5 20:19:43.396: INFO: Pod "test-new-deployment-7f5969cbc7-4qwmk" is not available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-4qwmk test-new-deployment-7f5969cbc7- deployment-716  d10679a0-3c9b-4007-bdb9-4975e07c5536 103709 0 2023-01-05 20:19:43 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5f62fce4-6567-4435-995c-6b2dd0ee7fce 0xc000dac837 0xc000dac838}] [] [{kube-controller-manager Update v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f62fce4-6567-4435-995c-6b2dd0ee7fce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ndjf9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ndjf9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jan  5 20:19:43.397: INFO: Pod "test-new-deployment-7f5969cbc7-8jv2s" is available:
    &Pod{ObjectMeta:{test-new-deployment-7f5969cbc7-8jv2s test-new-deployment-7f5969cbc7- deployment-716  4aca6fe4-48c6-490a-8c52-2c4c15c3a398 103700 0 2023-01-05 20:19:41 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7f5969cbc7] map[] [{apps/v1 ReplicaSet test-new-deployment-7f5969cbc7 5f62fce4-6567-4435-995c-6b2dd0ee7fce 0xc000dacd40 0xc000dacd41}] [] [{kube-controller-manager Update v1 2023-01-05 20:19:41 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5f62fce4-6567-4435-995c-6b2dd0ee7fce\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:19:43 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.21\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jnklc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jnklc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:41 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:19:41 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.21,StartTime:2023-01-05 20:19:41 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:19:42 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://391ceb4ee1003dc729a9052aa6e482602411f9c2ecd1b3263b3140ce3ee43d1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.21,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:43.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-716" for this suite. 01/05/23 20:19:43.407
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:43.421
Jan  5 20:19:43.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename certificates 01/05/23 20:19:43.422
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:43.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:43.454
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 01/05/23 20:19:44.285
STEP: getting /apis/certificates.k8s.io 01/05/23 20:19:44.288
STEP: getting /apis/certificates.k8s.io/v1 01/05/23 20:19:44.29
STEP: creating 01/05/23 20:19:44.291
STEP: getting 01/05/23 20:19:44.314
STEP: listing 01/05/23 20:19:44.317
STEP: watching 01/05/23 20:19:44.32
Jan  5 20:19:44.320: INFO: starting watch
STEP: patching 01/05/23 20:19:44.321
STEP: updating 01/05/23 20:19:44.328
Jan  5 20:19:44.333: INFO: waiting for watch events with expected annotations
Jan  5 20:19:44.333: INFO: saw patched and updated annotations
STEP: getting /approval 01/05/23 20:19:44.333
STEP: patching /approval 01/05/23 20:19:44.336
STEP: updating /approval 01/05/23 20:19:44.342
STEP: getting /status 01/05/23 20:19:44.348
STEP: patching /status 01/05/23 20:19:44.35
STEP: updating /status 01/05/23 20:19:44.359
STEP: deleting 01/05/23 20:19:44.366
STEP: deleting a collection 01/05/23 20:19:44.378
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:44.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "certificates-9927" for this suite. 01/05/23 20:19:44.391
------------------------------
• [0.974 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:43.421
    Jan  5 20:19:43.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename certificates 01/05/23 20:19:43.422
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:43.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:43.454
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 01/05/23 20:19:44.285
    STEP: getting /apis/certificates.k8s.io 01/05/23 20:19:44.288
    STEP: getting /apis/certificates.k8s.io/v1 01/05/23 20:19:44.29
    STEP: creating 01/05/23 20:19:44.291
    STEP: getting 01/05/23 20:19:44.314
    STEP: listing 01/05/23 20:19:44.317
    STEP: watching 01/05/23 20:19:44.32
    Jan  5 20:19:44.320: INFO: starting watch
    STEP: patching 01/05/23 20:19:44.321
    STEP: updating 01/05/23 20:19:44.328
    Jan  5 20:19:44.333: INFO: waiting for watch events with expected annotations
    Jan  5 20:19:44.333: INFO: saw patched and updated annotations
    STEP: getting /approval 01/05/23 20:19:44.333
    STEP: patching /approval 01/05/23 20:19:44.336
    STEP: updating /approval 01/05/23 20:19:44.342
    STEP: getting /status 01/05/23 20:19:44.348
    STEP: patching /status 01/05/23 20:19:44.35
    STEP: updating /status 01/05/23 20:19:44.359
    STEP: deleting 01/05/23 20:19:44.366
    STEP: deleting a collection 01/05/23 20:19:44.378
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:44.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "certificates-9927" for this suite. 01/05/23 20:19:44.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
[BeforeEach] [sig-storage] Projected secret
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:44.4
Jan  5 20:19:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename projected 01/05/23 20:19:44.401
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:44.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:44.417
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:31
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215
STEP: Creating secret with name s-test-opt-del-bc6351cc-ecdd-438e-a2f4-4bfbc02e6979 01/05/23 20:19:44.422
STEP: Creating secret with name s-test-opt-upd-8a58a7d3-5746-4191-958a-97b690b6e79f 01/05/23 20:19:44.426
STEP: Creating the pod 01/05/23 20:19:44.43
Jan  5 20:19:44.440: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2" in namespace "projected-8768" to be "running and ready"
Jan  5 20:19:44.445: INFO: Pod "pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.883329ms
Jan  5 20:19:44.445: INFO: The phase of Pod pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:19:46.449: INFO: Pod "pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008214227s
Jan  5 20:19:46.449: INFO: The phase of Pod pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2 is Running (Ready = true)
Jan  5 20:19:46.449: INFO: Pod "pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-bc6351cc-ecdd-438e-a2f4-4bfbc02e6979 01/05/23 20:19:46.47
STEP: Updating secret s-test-opt-upd-8a58a7d3-5746-4191-958a-97b690b6e79f 01/05/23 20:19:46.475
STEP: Creating secret with name s-test-opt-create-91e9925b-c72f-4009-86d8-1e1f94c26665 01/05/23 20:19:46.48
STEP: waiting to observe update in volume 01/05/23 20:19:46.484
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:48.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Projected secret
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Projected secret
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Projected secret
  tear down framework | framework.go:193
STEP: Destroying namespace "projected-8768" for this suite. 01/05/23 20:19:48.526
------------------------------
• [4.131 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:44.4
    Jan  5 20:19:44.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename projected 01/05/23 20:19:44.401
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:44.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:44.417
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:31
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:215
    STEP: Creating secret with name s-test-opt-del-bc6351cc-ecdd-438e-a2f4-4bfbc02e6979 01/05/23 20:19:44.422
    STEP: Creating secret with name s-test-opt-upd-8a58a7d3-5746-4191-958a-97b690b6e79f 01/05/23 20:19:44.426
    STEP: Creating the pod 01/05/23 20:19:44.43
    Jan  5 20:19:44.440: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2" in namespace "projected-8768" to be "running and ready"
    Jan  5 20:19:44.445: INFO: Pod "pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.883329ms
    Jan  5 20:19:44.445: INFO: The phase of Pod pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:19:46.449: INFO: Pod "pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008214227s
    Jan  5 20:19:46.449: INFO: The phase of Pod pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2 is Running (Ready = true)
    Jan  5 20:19:46.449: INFO: Pod "pod-projected-secrets-a8e6df55-4a10-426a-ba64-6242828c79a2" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-bc6351cc-ecdd-438e-a2f4-4bfbc02e6979 01/05/23 20:19:46.47
    STEP: Updating secret s-test-opt-upd-8a58a7d3-5746-4191-958a-97b690b6e79f 01/05/23 20:19:46.475
    STEP: Creating secret with name s-test-opt-create-91e9925b-c72f-4009-86d8-1e1f94c26665 01/05/23 20:19:46.48
    STEP: waiting to observe update in volume 01/05/23 20:19:46.484
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:48.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Projected secret
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Projected secret
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Projected secret
      tear down framework | framework.go:193
    STEP: Destroying namespace "projected-8768" for this suite. 01/05/23 20:19:48.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
[BeforeEach] [sig-node] Container Runtime
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:48.535
Jan  5 20:19:48.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-runtime 01/05/23 20:19:48.536
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:48.548
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:48.552
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:31
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:195
STEP: create the container 01/05/23 20:19:48.557
STEP: wait for the container to reach Succeeded 01/05/23 20:19:48.568
STEP: get the container status 01/05/23 20:19:52.586
STEP: the container should be terminated 01/05/23 20:19:52.588
STEP: the termination message should be set 01/05/23 20:19:52.588
Jan  5 20:19:52.588: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 01/05/23 20:19:52.588
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:52.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Container Runtime
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Container Runtime
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Container Runtime
  tear down framework | framework.go:193
STEP: Destroying namespace "container-runtime-1341" for this suite. 01/05/23 20:19:52.605
------------------------------
• [4.074 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:44
    on terminated container
    test/e2e/common/node/runtime.go:137
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:48.535
    Jan  5 20:19:48.535: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-runtime 01/05/23 20:19:48.536
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:48.548
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:48.552
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:31
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:195
    STEP: create the container 01/05/23 20:19:48.557
    STEP: wait for the container to reach Succeeded 01/05/23 20:19:48.568
    STEP: get the container status 01/05/23 20:19:52.586
    STEP: the container should be terminated 01/05/23 20:19:52.588
    STEP: the termination message should be set 01/05/23 20:19:52.588
    Jan  5 20:19:52.588: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 01/05/23 20:19:52.588
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:52.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Container Runtime
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Container Runtime
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Container Runtime
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-runtime-1341" for this suite. 01/05/23 20:19:52.605
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
[BeforeEach] [sig-node] Containers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:52.613
Jan  5 20:19:52.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename containers 01/05/23 20:19:52.615
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:52.628
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:52.632
[BeforeEach] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73
STEP: Creating a pod to test override command 01/05/23 20:19:52.634
Jan  5 20:19:52.643: INFO: Waiting up to 5m0s for pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399" in namespace "containers-9668" to be "Succeeded or Failed"
Jan  5 20:19:52.645: INFO: Pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399": Phase="Pending", Reason="", readiness=false. Elapsed: 1.998294ms
Jan  5 20:19:54.648: INFO: Pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005471446s
Jan  5 20:19:56.648: INFO: Pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005443618s
STEP: Saw pod success 01/05/23 20:19:56.648
Jan  5 20:19:56.648: INFO: Pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399" satisfied condition "Succeeded or Failed"
Jan  5 20:19:56.651: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod client-containers-464e25a3-370c-47fb-aed2-e03b98966399 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 20:19:56.657
Jan  5 20:19:56.668: INFO: Waiting for pod client-containers-464e25a3-370c-47fb-aed2-e03b98966399 to disappear
Jan  5 20:19:56.674: INFO: Pod client-containers-464e25a3-370c-47fb-aed2-e03b98966399 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/node/init/init.go:32
Jan  5 20:19:56.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Containers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Containers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Containers
  tear down framework | framework.go:193
STEP: Destroying namespace "containers-9668" for this suite. 01/05/23 20:19:56.677
------------------------------
• [4.069 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:52.613
    Jan  5 20:19:52.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename containers 01/05/23 20:19:52.615
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:52.628
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:52.632
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:73
    STEP: Creating a pod to test override command 01/05/23 20:19:52.634
    Jan  5 20:19:52.643: INFO: Waiting up to 5m0s for pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399" in namespace "containers-9668" to be "Succeeded or Failed"
    Jan  5 20:19:52.645: INFO: Pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399": Phase="Pending", Reason="", readiness=false. Elapsed: 1.998294ms
    Jan  5 20:19:54.648: INFO: Pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005471446s
    Jan  5 20:19:56.648: INFO: Pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.005443618s
    STEP: Saw pod success 01/05/23 20:19:56.648
    Jan  5 20:19:56.648: INFO: Pod "client-containers-464e25a3-370c-47fb-aed2-e03b98966399" satisfied condition "Succeeded or Failed"
    Jan  5 20:19:56.651: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-qmj7 pod client-containers-464e25a3-370c-47fb-aed2-e03b98966399 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 20:19:56.657
    Jan  5 20:19:56.668: INFO: Waiting for pod client-containers-464e25a3-370c-47fb-aed2-e03b98966399 to disappear
    Jan  5 20:19:56.674: INFO: Pod client-containers-464e25a3-370c-47fb-aed2-e03b98966399 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:19:56.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Containers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Containers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Containers
      tear down framework | framework.go:193
    STEP: Destroying namespace "containers-9668" for this suite. 01/05/23 20:19:56.677
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:19:56.692
Jan  5 20:19:56.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 20:19:56.693
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:56.709
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:56.712
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010
STEP: Creating resourceQuota "e2e-rq-status-t8clb" 01/05/23 20:19:56.717
Jan  5 20:19:56.724: INFO: Resource quota "e2e-rq-status-t8clb" reports spec: hard cpu limit of 500m
Jan  5 20:19:56.724: INFO: Resource quota "e2e-rq-status-t8clb" reports spec: hard memory limit of 500Mi
STEP: Updating resourceQuota "e2e-rq-status-t8clb" /status 01/05/23 20:19:56.724
STEP: Confirm /status for "e2e-rq-status-t8clb" resourceQuota via watch 01/05/23 20:19:56.734
Jan  5 20:19:56.735: INFO: observed resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList(nil)
Jan  5 20:19:56.735: INFO: Found resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan  5 20:19:56.735: INFO: ResourceQuota "e2e-rq-status-t8clb" /status was updated
STEP: Patching hard spec values for cpu & memory 01/05/23 20:19:56.738
Jan  5 20:19:56.742: INFO: Resource quota "e2e-rq-status-t8clb" reports spec: hard cpu limit of 1
Jan  5 20:19:56.742: INFO: Resource quota "e2e-rq-status-t8clb" reports spec: hard memory limit of 1Gi
STEP: Patching "e2e-rq-status-t8clb" /status 01/05/23 20:19:56.743
STEP: Confirm /status for "e2e-rq-status-t8clb" resourceQuota via watch 01/05/23 20:19:56.749
Jan  5 20:19:56.750: INFO: observed resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
Jan  5 20:19:56.750: INFO: Found resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
Jan  5 20:19:56.751: INFO: ResourceQuota "e2e-rq-status-t8clb" /status was patched
STEP: Get "e2e-rq-status-t8clb" /status 01/05/23 20:19:56.751
Jan  5 20:19:56.754: INFO: Resourcequota "e2e-rq-status-t8clb" reports status: hard cpu of 1
Jan  5 20:19:56.754: INFO: Resourcequota "e2e-rq-status-t8clb" reports status: hard memory of 1Gi
STEP: Repatching "e2e-rq-status-t8clb" /status before checking Spec is unchanged 01/05/23 20:19:56.756
Jan  5 20:19:56.760: INFO: Resourcequota "e2e-rq-status-t8clb" reports status: hard cpu of 2
Jan  5 20:19:56.761: INFO: Resourcequota "e2e-rq-status-t8clb" reports status: hard memory of 2Gi
Jan  5 20:19:56.762: INFO: Found resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
Jan  5 20:22:46.768: INFO: ResourceQuota "e2e-rq-status-t8clb" Spec was unchanged and /status reset
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 20:22:46.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-4164" for this suite. 01/05/23 20:22:46.773
------------------------------
• [SLOW TEST] [170.087 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should apply changes to a resourcequota status [Conformance]
  test/e2e/apimachinery/resource_quota.go:1010

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:19:56.692
    Jan  5 20:19:56.692: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 20:19:56.693
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:19:56.709
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:19:56.712
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply changes to a resourcequota status [Conformance]
      test/e2e/apimachinery/resource_quota.go:1010
    STEP: Creating resourceQuota "e2e-rq-status-t8clb" 01/05/23 20:19:56.717
    Jan  5 20:19:56.724: INFO: Resource quota "e2e-rq-status-t8clb" reports spec: hard cpu limit of 500m
    Jan  5 20:19:56.724: INFO: Resource quota "e2e-rq-status-t8clb" reports spec: hard memory limit of 500Mi
    STEP: Updating resourceQuota "e2e-rq-status-t8clb" /status 01/05/23 20:19:56.724
    STEP: Confirm /status for "e2e-rq-status-t8clb" resourceQuota via watch 01/05/23 20:19:56.734
    Jan  5 20:19:56.735: INFO: observed resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList(nil)
    Jan  5 20:19:56.735: INFO: Found resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan  5 20:19:56.735: INFO: ResourceQuota "e2e-rq-status-t8clb" /status was updated
    STEP: Patching hard spec values for cpu & memory 01/05/23 20:19:56.738
    Jan  5 20:19:56.742: INFO: Resource quota "e2e-rq-status-t8clb" reports spec: hard cpu limit of 1
    Jan  5 20:19:56.742: INFO: Resource quota "e2e-rq-status-t8clb" reports spec: hard memory limit of 1Gi
    STEP: Patching "e2e-rq-status-t8clb" /status 01/05/23 20:19:56.743
    STEP: Confirm /status for "e2e-rq-status-t8clb" resourceQuota via watch 01/05/23 20:19:56.749
    Jan  5 20:19:56.750: INFO: observed resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
    Jan  5 20:19:56.750: INFO: Found resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
    Jan  5 20:19:56.751: INFO: ResourceQuota "e2e-rq-status-t8clb" /status was patched
    STEP: Get "e2e-rq-status-t8clb" /status 01/05/23 20:19:56.751
    Jan  5 20:19:56.754: INFO: Resourcequota "e2e-rq-status-t8clb" reports status: hard cpu of 1
    Jan  5 20:19:56.754: INFO: Resourcequota "e2e-rq-status-t8clb" reports status: hard memory of 1Gi
    STEP: Repatching "e2e-rq-status-t8clb" /status before checking Spec is unchanged 01/05/23 20:19:56.756
    Jan  5 20:19:56.760: INFO: Resourcequota "e2e-rq-status-t8clb" reports status: hard cpu of 2
    Jan  5 20:19:56.761: INFO: Resourcequota "e2e-rq-status-t8clb" reports status: hard memory of 2Gi
    Jan  5 20:19:56.762: INFO: Found resourceQuota "e2e-rq-status-t8clb" in namespace "resourcequota-4164" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
    Jan  5 20:22:46.768: INFO: ResourceQuota "e2e-rq-status-t8clb" Spec was unchanged and /status reset
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:22:46.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-4164" for this suite. 01/05/23 20:22:46.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
[BeforeEach] [sig-apps] ReplicationController
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:22:46.784
Jan  5 20:22:46.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename replication-controller 01/05/23 20:22:46.786
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:22:46.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:22:46.804
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:57
[It] should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402
STEP: Creating ReplicationController "e2e-rc-p84jd" 01/05/23 20:22:46.807
Jan  5 20:22:46.811: INFO: Get Replication Controller "e2e-rc-p84jd" to confirm replicas
Jan  5 20:22:47.814: INFO: Get Replication Controller "e2e-rc-p84jd" to confirm replicas
Jan  5 20:22:47.821: INFO: Found 1 replicas for "e2e-rc-p84jd" replication controller
STEP: Getting scale subresource for ReplicationController "e2e-rc-p84jd" 01/05/23 20:22:47.821
STEP: Updating a scale subresource 01/05/23 20:22:47.826
STEP: Verifying replicas where modified for replication controller "e2e-rc-p84jd" 01/05/23 20:22:47.84
Jan  5 20:22:47.863: INFO: Get Replication Controller "e2e-rc-p84jd" to confirm replicas
Jan  5 20:22:47.867: INFO: Found 2 replicas for "e2e-rc-p84jd" replication controller
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/node/init/init.go:32
Jan  5 20:22:47.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] ReplicationController
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] ReplicationController
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] ReplicationController
  tear down framework | framework.go:193
STEP: Destroying namespace "replication-controller-4095" for this suite. 01/05/23 20:22:47.892
------------------------------
• [1.117 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should get and update a ReplicationController scale [Conformance]
  test/e2e/apps/rc.go:402

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:22:46.784
    Jan  5 20:22:46.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename replication-controller 01/05/23 20:22:46.786
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:22:46.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:22:46.804
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:57
    [It] should get and update a ReplicationController scale [Conformance]
      test/e2e/apps/rc.go:402
    STEP: Creating ReplicationController "e2e-rc-p84jd" 01/05/23 20:22:46.807
    Jan  5 20:22:46.811: INFO: Get Replication Controller "e2e-rc-p84jd" to confirm replicas
    Jan  5 20:22:47.814: INFO: Get Replication Controller "e2e-rc-p84jd" to confirm replicas
    Jan  5 20:22:47.821: INFO: Found 1 replicas for "e2e-rc-p84jd" replication controller
    STEP: Getting scale subresource for ReplicationController "e2e-rc-p84jd" 01/05/23 20:22:47.821
    STEP: Updating a scale subresource 01/05/23 20:22:47.826
    STEP: Verifying replicas where modified for replication controller "e2e-rc-p84jd" 01/05/23 20:22:47.84
    Jan  5 20:22:47.863: INFO: Get Replication Controller "e2e-rc-p84jd" to confirm replicas
    Jan  5 20:22:47.867: INFO: Found 2 replicas for "e2e-rc-p84jd" replication controller
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:22:47.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] ReplicationController
      tear down framework | framework.go:193
    STEP: Destroying namespace "replication-controller-4095" for this suite. 01/05/23 20:22:47.892
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:22:47.907
Jan  5 20:22:47.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubelet-test 01/05/23 20:22:47.91
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:22:47.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:22:47.929
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  5 20:22:47.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-275" for this suite. 01/05/23 20:22:47.973
------------------------------
• [0.075 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:22:47.907
    Jan  5 20:22:47.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 20:22:47.91
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:22:47.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:22:47.929
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:22:47.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-275" for this suite. 01/05/23 20:22:47.973
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:22:48
Jan  5 20:22:48.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename cronjob 01/05/23 20:22:48.009
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:22:48.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:22:48.045
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 01/05/23 20:22:48.055
STEP: Ensuring more than one job is running at a time 01/05/23 20:22:48.062
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/05/23 20:24:02.066
STEP: Removing cronjob 01/05/23 20:24:02.069
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  5 20:24:02.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-3802" for this suite. 01/05/23 20:24:02.078
------------------------------
• [SLOW TEST] [74.092 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:22:48
    Jan  5 20:22:48.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename cronjob 01/05/23 20:22:48.009
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:22:48.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:22:48.045
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 01/05/23 20:22:48.055
    STEP: Ensuring more than one job is running at a time 01/05/23 20:22:48.062
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 01/05/23 20:24:02.066
    STEP: Removing cronjob 01/05/23 20:24:02.069
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:24:02.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-3802" for this suite. 01/05/23 20:24:02.078
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:24:02.094
Jan  5 20:24:02.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 20:24:02.096
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:24:02.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:24:02.118
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187
STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 20:24:02.122
Jan  5 20:24:02.138: INFO: Waiting up to 5m0s for pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4" in namespace "emptydir-9120" to be "Succeeded or Failed"
Jan  5 20:24:02.147: INFO: Pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.246556ms
Jan  5 20:24:04.151: INFO: Pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012515536s
Jan  5 20:24:06.151: INFO: Pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012478428s
STEP: Saw pod success 01/05/23 20:24:06.151
Jan  5 20:24:06.151: INFO: Pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4" satisfied condition "Succeeded or Failed"
Jan  5 20:24:06.154: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-7fb9133a-db1f-4942-b302-d91f13d35ea4 container test-container: <nil>
STEP: delete the pod 01/05/23 20:24:06.172
Jan  5 20:24:06.186: INFO: Waiting for pod pod-7fb9133a-db1f-4942-b302-d91f13d35ea4 to disappear
Jan  5 20:24:06.189: INFO: Pod pod-7fb9133a-db1f-4942-b302-d91f13d35ea4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 20:24:06.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-9120" for this suite. 01/05/23 20:24:06.194
------------------------------
• [4.105 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:187

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:24:02.094
    Jan  5 20:24:02.094: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 20:24:02.096
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:24:02.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:24:02.118
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:187
    STEP: Creating a pod to test emptydir 0777 on node default medium 01/05/23 20:24:02.122
    Jan  5 20:24:02.138: INFO: Waiting up to 5m0s for pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4" in namespace "emptydir-9120" to be "Succeeded or Failed"
    Jan  5 20:24:02.147: INFO: Pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.246556ms
    Jan  5 20:24:04.151: INFO: Pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012515536s
    Jan  5 20:24:06.151: INFO: Pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012478428s
    STEP: Saw pod success 01/05/23 20:24:06.151
    Jan  5 20:24:06.151: INFO: Pod "pod-7fb9133a-db1f-4942-b302-d91f13d35ea4" satisfied condition "Succeeded or Failed"
    Jan  5 20:24:06.154: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-7fb9133a-db1f-4942-b302-d91f13d35ea4 container test-container: <nil>
    STEP: delete the pod 01/05/23 20:24:06.172
    Jan  5 20:24:06.186: INFO: Waiting for pod pod-7fb9133a-db1f-4942-b302-d91f13d35ea4 to disappear
    Jan  5 20:24:06.189: INFO: Pod pod-7fb9133a-db1f-4942-b302-d91f13d35ea4 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:24:06.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-9120" for this suite. 01/05/23 20:24:06.194
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:24:06.203
Jan  5 20:24:06.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 20:24:06.204
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:24:06.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:24:06.22
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236
Jan  5 20:24:06.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 20:24:08.04
Jan  5 20:24:08.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 --namespace=crd-publish-openapi-7158 create -f -'
Jan  5 20:24:08.837: INFO: stderr: ""
Jan  5 20:24:08.837: INFO: stdout: "e2e-test-crd-publish-openapi-5041-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  5 20:24:08.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 --namespace=crd-publish-openapi-7158 delete e2e-test-crd-publish-openapi-5041-crds test-cr'
Jan  5 20:24:08.934: INFO: stderr: ""
Jan  5 20:24:08.934: INFO: stdout: "e2e-test-crd-publish-openapi-5041-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jan  5 20:24:08.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 --namespace=crd-publish-openapi-7158 apply -f -'
Jan  5 20:24:09.188: INFO: stderr: ""
Jan  5 20:24:09.188: INFO: stdout: "e2e-test-crd-publish-openapi-5041-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jan  5 20:24:09.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 --namespace=crd-publish-openapi-7158 delete e2e-test-crd-publish-openapi-5041-crds test-cr'
Jan  5 20:24:09.269: INFO: stderr: ""
Jan  5 20:24:09.269: INFO: stdout: "e2e-test-crd-publish-openapi-5041-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 01/05/23 20:24:09.269
Jan  5 20:24:09.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 explain e2e-test-crd-publish-openapi-5041-crds'
Jan  5 20:24:09.521: INFO: stderr: ""
Jan  5 20:24:09.521: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5041-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:24:11.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-publish-openapi-7158" for this suite. 01/05/23 20:24:11.661
------------------------------
• [SLOW TEST] [5.464 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:236

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:24:06.203
    Jan  5 20:24:06.203: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-publish-openapi 01/05/23 20:24:06.204
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:24:06.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:24:06.22
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:236
    Jan  5 20:24:06.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 01/05/23 20:24:08.04
    Jan  5 20:24:08.041: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 --namespace=crd-publish-openapi-7158 create -f -'
    Jan  5 20:24:08.837: INFO: stderr: ""
    Jan  5 20:24:08.837: INFO: stdout: "e2e-test-crd-publish-openapi-5041-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  5 20:24:08.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 --namespace=crd-publish-openapi-7158 delete e2e-test-crd-publish-openapi-5041-crds test-cr'
    Jan  5 20:24:08.934: INFO: stderr: ""
    Jan  5 20:24:08.934: INFO: stdout: "e2e-test-crd-publish-openapi-5041-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jan  5 20:24:08.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 --namespace=crd-publish-openapi-7158 apply -f -'
    Jan  5 20:24:09.188: INFO: stderr: ""
    Jan  5 20:24:09.188: INFO: stdout: "e2e-test-crd-publish-openapi-5041-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jan  5 20:24:09.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 --namespace=crd-publish-openapi-7158 delete e2e-test-crd-publish-openapi-5041-crds test-cr'
    Jan  5 20:24:09.269: INFO: stderr: ""
    Jan  5 20:24:09.269: INFO: stdout: "e2e-test-crd-publish-openapi-5041-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 01/05/23 20:24:09.269
    Jan  5 20:24:09.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=crd-publish-openapi-7158 explain e2e-test-crd-publish-openapi-5041-crds'
    Jan  5 20:24:09.521: INFO: stderr: ""
    Jan  5 20:24:09.521: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-5041-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:24:11.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-publish-openapi-7158" for this suite. 01/05/23 20:24:11.661
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:24:11.674
Jan  5 20:24:11.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename watch 01/05/23 20:24:11.676
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:24:11.688
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:24:11.691
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:31
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 01/05/23 20:24:11.694
STEP: starting a background goroutine to produce watch events 01/05/23 20:24:11.697
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/05/23 20:24:11.697
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/node/init/init.go:32
Jan  5 20:24:14.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Watchers
  tear down framework | framework.go:193
STEP: Destroying namespace "watch-8618" for this suite. 01/05/23 20:24:14.532
------------------------------
• [2.908 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:24:11.674
    Jan  5 20:24:11.675: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename watch 01/05/23 20:24:11.676
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:24:11.688
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:24:11.691
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:31
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 01/05/23 20:24:11.694
    STEP: starting a background goroutine to produce watch events 01/05/23 20:24:11.697
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 01/05/23 20:24:11.697
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:24:14.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Watchers
      tear down framework | framework.go:193
    STEP: Destroying namespace "watch-8618" for this suite. 01/05/23 20:24:14.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:24:14.585
Jan  5 20:24:14.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename taint-single-pod 01/05/23 20:24:14.586
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:24:14.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:24:14.604
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:170
Jan  5 20:24:14.606: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 20:25:14.634: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293
Jan  5 20:25:14.637: INFO: Starting informer...
STEP: Starting pod... 01/05/23 20:25:14.637
Jan  5 20:25:14.852: INFO: Pod is running on gke-gke-1-26-default-pool-05283374-16pz. Tainting Node
STEP: Trying to apply a taint on the Node 01/05/23 20:25:14.852
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 20:25:14.869
STEP: Waiting short time to make sure Pod is queued for deletion 01/05/23 20:25:14.874
Jan  5 20:25:14.874: INFO: Pod wasn't evicted. Proceeding
Jan  5 20:25:14.874: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 20:25:14.897
STEP: Waiting some time to make sure that toleration time passed. 01/05/23 20:25:14.901
Jan  5 20:26:29.903: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:26:29.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "taint-single-pod-3057" for this suite. 01/05/23 20:26:29.909
------------------------------
• [SLOW TEST] [135.329 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:24:14.585
    Jan  5 20:24:14.585: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename taint-single-pod 01/05/23 20:24:14.586
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:24:14.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:24:14.604
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:170
    Jan  5 20:24:14.606: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 20:25:14.634: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:293
    Jan  5 20:25:14.637: INFO: Starting informer...
    STEP: Starting pod... 01/05/23 20:25:14.637
    Jan  5 20:25:14.852: INFO: Pod is running on gke-gke-1-26-default-pool-05283374-16pz. Tainting Node
    STEP: Trying to apply a taint on the Node 01/05/23 20:25:14.852
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 20:25:14.869
    STEP: Waiting short time to make sure Pod is queued for deletion 01/05/23 20:25:14.874
    Jan  5 20:25:14.874: INFO: Pod wasn't evicted. Proceeding
    Jan  5 20:25:14.874: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 01/05/23 20:25:14.897
    STEP: Waiting some time to make sure that toleration time passed. 01/05/23 20:25:14.901
    Jan  5 20:26:29.903: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:26:29.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "taint-single-pod-3057" for this suite. 01/05/23 20:26:29.909
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:26:29.917
Jan  5 20:26:29.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 20:26:29.919
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:26:29.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:26:29.935
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848
STEP: creating service multi-endpoint-test in namespace services-7744 01/05/23 20:26:29.938
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[] 01/05/23 20:26:29.955
Jan  5 20:26:29.958: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
Jan  5 20:26:30.965: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-7744 01/05/23 20:26:30.965
Jan  5 20:26:30.973: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7744" to be "running and ready"
Jan  5 20:26:30.976: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402787ms
Jan  5 20:26:30.976: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:26:32.980: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007017423s
Jan  5 20:26:32.980: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 20:26:32.980: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[pod1:[100]] 01/05/23 20:26:32.983
Jan  5 20:26:32.991: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-7744 01/05/23 20:26:32.991
Jan  5 20:26:32.998: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7744" to be "running and ready"
Jan  5 20:26:33.001: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512864ms
Jan  5 20:26:33.001: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:26:35.006: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008009767s
Jan  5 20:26:35.006: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 20:26:35.006: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[pod1:[100] pod2:[101]] 01/05/23 20:26:35.01
Jan  5 20:26:35.026: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 01/05/23 20:26:35.026
Jan  5 20:26:35.026: INFO: Creating new exec pod
Jan  5 20:26:35.034: INFO: Waiting up to 5m0s for pod "execpodnhh8h" in namespace "services-7744" to be "running"
Jan  5 20:26:35.038: INFO: Pod "execpodnhh8h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.166688ms
Jan  5 20:26:37.046: INFO: Pod "execpodnhh8h": Phase="Running", Reason="", readiness=true. Elapsed: 2.012267977s
Jan  5 20:26:37.046: INFO: Pod "execpodnhh8h" satisfied condition "running"
Jan  5 20:26:38.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan  5 20:26:38.225: INFO: rc: 1
Jan  5 20:26:38.225: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80:
Command stdout:

stderr:
+ nc -v -z -w 2 multi-endpoint-test 80
nc: connect to multi-endpoint-test port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 20:26:39.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan  5 20:26:39.423: INFO: rc: 1
Jan  5 20:26:39.423: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80:
Command stdout:

stderr:
+ nc -v -z -w 2 multi-endpoint-test 80
nc: connect to multi-endpoint-test port 80 (tcp) failed: Connection refused
command terminated with exit code 1

error:
exit status 1
Retrying...
Jan  5 20:26:40.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
Jan  5 20:26:40.381: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jan  5 20:26:40.381: INFO: stdout: ""
Jan  5 20:26:40.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 10.20.3.3 80'
Jan  5 20:26:40.537: INFO: stderr: "+ nc -v -z -w 2 10.20.3.3 80\nConnection to 10.20.3.3 80 port [tcp/http] succeeded!\n"
Jan  5 20:26:40.537: INFO: stdout: ""
Jan  5 20:26:40.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
Jan  5 20:26:40.679: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jan  5 20:26:40.679: INFO: stdout: ""
Jan  5 20:26:40.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 10.20.3.3 81'
Jan  5 20:26:40.851: INFO: stderr: "+ nc -v -z -w 2 10.20.3.3 81\nConnection to 10.20.3.3 81 port [tcp/*] succeeded!\n"
Jan  5 20:26:40.851: INFO: stdout: ""
STEP: Deleting pod pod1 in namespace services-7744 01/05/23 20:26:40.851
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[pod2:[101]] 01/05/23 20:26:40.867
Jan  5 20:26:40.878: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-7744 01/05/23 20:26:40.878
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[] 01/05/23 20:26:40.901
Jan  5 20:26:40.910: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 20:26:40.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-7744" for this suite. 01/05/23 20:26:40.939
------------------------------
• [SLOW TEST] [11.030 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:848

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:26:29.917
    Jan  5 20:26:29.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 20:26:29.919
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:26:29.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:26:29.935
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:848
    STEP: creating service multi-endpoint-test in namespace services-7744 01/05/23 20:26:29.938
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[] 01/05/23 20:26:29.955
    Jan  5 20:26:29.958: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
    Jan  5 20:26:30.965: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-7744 01/05/23 20:26:30.965
    Jan  5 20:26:30.973: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-7744" to be "running and ready"
    Jan  5 20:26:30.976: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.402787ms
    Jan  5 20:26:30.976: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:26:32.980: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007017423s
    Jan  5 20:26:32.980: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 20:26:32.980: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[pod1:[100]] 01/05/23 20:26:32.983
    Jan  5 20:26:32.991: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-7744 01/05/23 20:26:32.991
    Jan  5 20:26:32.998: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-7744" to be "running and ready"
    Jan  5 20:26:33.001: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.512864ms
    Jan  5 20:26:33.001: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:26:35.006: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008009767s
    Jan  5 20:26:35.006: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 20:26:35.006: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[pod1:[100] pod2:[101]] 01/05/23 20:26:35.01
    Jan  5 20:26:35.026: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 01/05/23 20:26:35.026
    Jan  5 20:26:35.026: INFO: Creating new exec pod
    Jan  5 20:26:35.034: INFO: Waiting up to 5m0s for pod "execpodnhh8h" in namespace "services-7744" to be "running"
    Jan  5 20:26:35.038: INFO: Pod "execpodnhh8h": Phase="Pending", Reason="", readiness=false. Elapsed: 4.166688ms
    Jan  5 20:26:37.046: INFO: Pod "execpodnhh8h": Phase="Running", Reason="", readiness=true. Elapsed: 2.012267977s
    Jan  5 20:26:37.046: INFO: Pod "execpodnhh8h" satisfied condition "running"
    Jan  5 20:26:38.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan  5 20:26:38.225: INFO: rc: 1
    Jan  5 20:26:38.225: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 multi-endpoint-test 80
    nc: connect to multi-endpoint-test port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 20:26:39.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan  5 20:26:39.423: INFO: rc: 1
    Jan  5 20:26:39.423: INFO: Service reachability failing with error: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80:
    Command stdout:

    stderr:
    + nc -v -z -w 2 multi-endpoint-test 80
    nc: connect to multi-endpoint-test port 80 (tcp) failed: Connection refused
    command terminated with exit code 1

    error:
    exit status 1
    Retrying...
    Jan  5 20:26:40.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 80'
    Jan  5 20:26:40.381: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jan  5 20:26:40.381: INFO: stdout: ""
    Jan  5 20:26:40.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 10.20.3.3 80'
    Jan  5 20:26:40.537: INFO: stderr: "+ nc -v -z -w 2 10.20.3.3 80\nConnection to 10.20.3.3 80 port [tcp/http] succeeded!\n"
    Jan  5 20:26:40.537: INFO: stdout: ""
    Jan  5 20:26:40.537: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 multi-endpoint-test 81'
    Jan  5 20:26:40.679: INFO: stderr: "+ nc -v -z -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jan  5 20:26:40.679: INFO: stdout: ""
    Jan  5 20:26:40.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=services-7744 exec execpodnhh8h -- /bin/sh -x -c nc -v -z -w 2 10.20.3.3 81'
    Jan  5 20:26:40.851: INFO: stderr: "+ nc -v -z -w 2 10.20.3.3 81\nConnection to 10.20.3.3 81 port [tcp/*] succeeded!\n"
    Jan  5 20:26:40.851: INFO: stdout: ""
    STEP: Deleting pod pod1 in namespace services-7744 01/05/23 20:26:40.851
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[pod2:[101]] 01/05/23 20:26:40.867
    Jan  5 20:26:40.878: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-7744 01/05/23 20:26:40.878
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7744 to expose endpoints map[] 01/05/23 20:26:40.901
    Jan  5 20:26:40.910: INFO: successfully validated that service multi-endpoint-test in namespace services-7744 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:26:40.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-7744" for this suite. 01/05/23 20:26:40.939
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:26:40.948
Jan  5 20:26:40.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 20:26:40.949
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:26:40.967
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:26:40.975
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654
STEP: creating a collection of services 01/05/23 20:26:40.981
Jan  5 20:26:40.981: INFO: Creating e2e-svc-a-zmwmd
Jan  5 20:26:40.993: INFO: Creating e2e-svc-b-25cpg
Jan  5 20:26:41.005: INFO: Creating e2e-svc-c-cdchk
STEP: deleting service collection 01/05/23 20:26:41.022
Jan  5 20:26:41.045: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 20:26:41.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-2366" for this suite. 01/05/23 20:26:41.049
------------------------------
• [0.107 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3654

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:26:40.948
    Jan  5 20:26:40.948: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 20:26:40.949
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:26:40.967
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:26:40.975
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3654
    STEP: creating a collection of services 01/05/23 20:26:40.981
    Jan  5 20:26:40.981: INFO: Creating e2e-svc-a-zmwmd
    Jan  5 20:26:40.993: INFO: Creating e2e-svc-b-25cpg
    Jan  5 20:26:41.005: INFO: Creating e2e-svc-c-cdchk
    STEP: deleting service collection 01/05/23 20:26:41.022
    Jan  5 20:26:41.045: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:26:41.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-2366" for this suite. 01/05/23 20:26:41.049
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:26:41.063
Jan  5 20:26:41.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-preemption 01/05/23 20:26:41.065
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:26:41.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:26:41.086
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:96
Jan  5 20:26:41.101: INFO: Waiting up to 1m0s for all nodes to be ready
Jan  5 20:27:41.132: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:27:41.134
Jan  5 20:27:41.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 20:27:41.135
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:41.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:41.152
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:763
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:806
Jan  5 20:27:41.167: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jan  5 20:27:41.171: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/node/init/init.go:32
Jan  5 20:27:41.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:779
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:27:41.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:84
[DeferCleanup (Each)] PriorityClass endpoints
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] PriorityClass endpoints
  dump namespaces | framework.go:196
[DeferCleanup (Each)] PriorityClass endpoints
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-path-8382" for this suite. 01/05/23 20:27:41.249
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-preemption-5556" for this suite. 01/05/23 20:27:41.255
------------------------------
• [SLOW TEST] [60.198 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:756
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:806

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:26:41.063
    Jan  5 20:26:41.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-preemption 01/05/23 20:26:41.065
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:26:41.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:26:41.086
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:96
    Jan  5 20:26:41.101: INFO: Waiting up to 1m0s for all nodes to be ready
    Jan  5 20:27:41.132: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:27:41.134
    Jan  5 20:27:41.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-preemption-path 01/05/23 20:27:41.135
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:41.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:41.152
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:763
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:806
    Jan  5 20:27:41.167: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jan  5 20:27:41.171: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:27:41.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:779
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:27:41.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:84
    [DeferCleanup (Each)] PriorityClass endpoints
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] PriorityClass endpoints
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] PriorityClass endpoints
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-path-8382" for this suite. 01/05/23 20:27:41.249
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPreemption [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-preemption-5556" for this suite. 01/05/23 20:27:41.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
[BeforeEach] [sig-node] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:27:41.268
Jan  5 20:27:41.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 20:27:41.27
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:41.282
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:41.286
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46
STEP: Creating secret with name secret-test-2390b8fc-d216-4480-8961-30957f0490f2 01/05/23 20:27:41.29
STEP: Creating a pod to test consume secrets 01/05/23 20:27:41.3
Jan  5 20:27:41.313: INFO: Waiting up to 5m0s for pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a" in namespace "secrets-7131" to be "Succeeded or Failed"
Jan  5 20:27:41.318: INFO: Pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.693531ms
Jan  5 20:27:43.322: INFO: Pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009011545s
Jan  5 20:27:45.321: INFO: Pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008295922s
STEP: Saw pod success 01/05/23 20:27:45.321
Jan  5 20:27:45.321: INFO: Pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a" satisfied condition "Succeeded or Failed"
Jan  5 20:27:45.324: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a container secret-env-test: <nil>
STEP: delete the pod 01/05/23 20:27:45.34
Jan  5 20:27:45.365: INFO: Waiting for pod pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a to disappear
Jan  5 20:27:45.368: INFO: Pod pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 20:27:45.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-7131" for this suite. 01/05/23 20:27:45.371
------------------------------
• [4.109 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:27:41.268
    Jan  5 20:27:41.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 20:27:41.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:41.282
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:41.286
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:46
    STEP: Creating secret with name secret-test-2390b8fc-d216-4480-8961-30957f0490f2 01/05/23 20:27:41.29
    STEP: Creating a pod to test consume secrets 01/05/23 20:27:41.3
    Jan  5 20:27:41.313: INFO: Waiting up to 5m0s for pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a" in namespace "secrets-7131" to be "Succeeded or Failed"
    Jan  5 20:27:41.318: INFO: Pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.693531ms
    Jan  5 20:27:43.322: INFO: Pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009011545s
    Jan  5 20:27:45.321: INFO: Pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008295922s
    STEP: Saw pod success 01/05/23 20:27:45.321
    Jan  5 20:27:45.321: INFO: Pod "pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a" satisfied condition "Succeeded or Failed"
    Jan  5 20:27:45.324: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a container secret-env-test: <nil>
    STEP: delete the pod 01/05/23 20:27:45.34
    Jan  5 20:27:45.365: INFO: Waiting for pod pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a to disappear
    Jan  5 20:27:45.368: INFO: Pod pod-secrets-c8bc9dab-2d02-43d6-af83-5bcf919e644a no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:27:45.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-7131" for this suite. 01/05/23 20:27:45.371
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:27:45.38
Jan  5 20:27:45.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 20:27:45.382
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:45.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:45.419
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125
STEP: Creating secret with name secret-test-56fce591-9198-4311-b962-7af0070d8f88 01/05/23 20:27:45.422
STEP: Creating a pod to test consume secrets 01/05/23 20:27:45.429
Jan  5 20:27:45.440: INFO: Waiting up to 5m0s for pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5" in namespace "secrets-5215" to be "Succeeded or Failed"
Jan  5 20:27:45.445: INFO: Pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.814626ms
Jan  5 20:27:47.448: INFO: Pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008154381s
Jan  5 20:27:49.449: INFO: Pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009786944s
STEP: Saw pod success 01/05/23 20:27:49.45
Jan  5 20:27:49.450: INFO: Pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5" satisfied condition "Succeeded or Failed"
Jan  5 20:27:49.462: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-69b04751-c461-433b-9959-1da08770fbb5 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 20:27:49.471
Jan  5 20:27:49.483: INFO: Waiting for pod pod-secrets-69b04751-c461-433b-9959-1da08770fbb5 to disappear
Jan  5 20:27:49.488: INFO: Pod pod-secrets-69b04751-c461-433b-9959-1da08770fbb5 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 20:27:49.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-5215" for this suite. 01/05/23 20:27:49.492
------------------------------
• [4.116 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:27:45.38
    Jan  5 20:27:45.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 20:27:45.382
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:45.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:45.419
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:125
    STEP: Creating secret with name secret-test-56fce591-9198-4311-b962-7af0070d8f88 01/05/23 20:27:45.422
    STEP: Creating a pod to test consume secrets 01/05/23 20:27:45.429
    Jan  5 20:27:45.440: INFO: Waiting up to 5m0s for pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5" in namespace "secrets-5215" to be "Succeeded or Failed"
    Jan  5 20:27:45.445: INFO: Pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.814626ms
    Jan  5 20:27:47.448: INFO: Pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008154381s
    Jan  5 20:27:49.449: INFO: Pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009786944s
    STEP: Saw pod success 01/05/23 20:27:49.45
    Jan  5 20:27:49.450: INFO: Pod "pod-secrets-69b04751-c461-433b-9959-1da08770fbb5" satisfied condition "Succeeded or Failed"
    Jan  5 20:27:49.462: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-69b04751-c461-433b-9959-1da08770fbb5 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 20:27:49.471
    Jan  5 20:27:49.483: INFO: Waiting for pod pod-secrets-69b04751-c461-433b-9959-1da08770fbb5 to disappear
    Jan  5 20:27:49.488: INFO: Pod pod-secrets-69b04751-c461-433b-9959-1da08770fbb5 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:27:49.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-5215" for this suite. 01/05/23 20:27:49.492
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:27:49.504
Jan  5 20:27:49.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 20:27:49.505
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:49.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:49.538
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74
STEP: Creating configMap with name configmap-test-volume-2381b1bc-bb1c-4a27-b8af-d912a77a329b 01/05/23 20:27:49.542
STEP: Creating a pod to test consume configMaps 01/05/23 20:27:49.547
Jan  5 20:27:49.562: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276" in namespace "configmap-641" to be "Succeeded or Failed"
Jan  5 20:27:49.566: INFO: Pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276": Phase="Pending", Reason="", readiness=false. Elapsed: 3.56155ms
Jan  5 20:27:51.569: INFO: Pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006645313s
Jan  5 20:27:53.569: INFO: Pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006779778s
STEP: Saw pod success 01/05/23 20:27:53.569
Jan  5 20:27:53.569: INFO: Pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276" satisfied condition "Succeeded or Failed"
Jan  5 20:27:53.572: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 20:27:53.578
Jan  5 20:27:53.593: INFO: Waiting for pod pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276 to disappear
Jan  5 20:27:53.595: INFO: Pod pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:27:53.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-641" for this suite. 01/05/23 20:27:53.6
------------------------------
• [4.100 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:74

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:27:49.504
    Jan  5 20:27:49.504: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 20:27:49.505
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:49.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:49.538
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:74
    STEP: Creating configMap with name configmap-test-volume-2381b1bc-bb1c-4a27-b8af-d912a77a329b 01/05/23 20:27:49.542
    STEP: Creating a pod to test consume configMaps 01/05/23 20:27:49.547
    Jan  5 20:27:49.562: INFO: Waiting up to 5m0s for pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276" in namespace "configmap-641" to be "Succeeded or Failed"
    Jan  5 20:27:49.566: INFO: Pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276": Phase="Pending", Reason="", readiness=false. Elapsed: 3.56155ms
    Jan  5 20:27:51.569: INFO: Pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006645313s
    Jan  5 20:27:53.569: INFO: Pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006779778s
    STEP: Saw pod success 01/05/23 20:27:53.569
    Jan  5 20:27:53.569: INFO: Pod "pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276" satisfied condition "Succeeded or Failed"
    Jan  5 20:27:53.572: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 20:27:53.578
    Jan  5 20:27:53.593: INFO: Waiting for pod pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276 to disappear
    Jan  5 20:27:53.595: INFO: Pod pod-configmaps-4ec38c33-7412-4962-8084-bc39ada9a276 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:27:53.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-641" for this suite. 01/05/23 20:27:53.6
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:27:53.605
Jan  5 20:27:53.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename kubelet-test 01/05/23 20:27:53.607
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:53.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:53.622
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jan  5 20:27:53.637: INFO: Waiting up to 5m0s for pod "busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59" in namespace "kubelet-test-2317" to be "running and ready"
Jan  5 20:27:53.640: INFO: Pod "busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905053ms
Jan  5 20:27:53.640: INFO: The phase of Pod busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:27:55.643: INFO: Pod "busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59": Phase="Running", Reason="", readiness=true. Elapsed: 2.006184976s
Jan  5 20:27:55.643: INFO: The phase of Pod busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59 is Running (Ready = true)
Jan  5 20:27:55.643: INFO: Pod "busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/node/init/init.go:32
Jan  5 20:27:55.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Kubelet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Kubelet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Kubelet
  tear down framework | framework.go:193
STEP: Destroying namespace "kubelet-test-2317" for this suite. 01/05/23 20:27:55.657
------------------------------
• [2.058 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:27:53.605
    Jan  5 20:27:53.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename kubelet-test 01/05/23 20:27:53.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:53.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:53.622
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jan  5 20:27:53.637: INFO: Waiting up to 5m0s for pod "busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59" in namespace "kubelet-test-2317" to be "running and ready"
    Jan  5 20:27:53.640: INFO: Pod "busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59": Phase="Pending", Reason="", readiness=false. Elapsed: 2.905053ms
    Jan  5 20:27:53.640: INFO: The phase of Pod busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:27:55.643: INFO: Pod "busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59": Phase="Running", Reason="", readiness=true. Elapsed: 2.006184976s
    Jan  5 20:27:55.643: INFO: The phase of Pod busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59 is Running (Ready = true)
    Jan  5 20:27:55.643: INFO: Pod "busybox-scheduling-1e982e93-45e9-4d9c-8f74-0afc4557cb59" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:27:55.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Kubelet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Kubelet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Kubelet
      tear down framework | framework.go:193
    STEP: Destroying namespace "kubelet-test-2317" for this suite. 01/05/23 20:27:55.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:27:55.67
Jan  5 20:27:55.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename deployment 01/05/23 20:27:55.672
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:55.685
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:55.688
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 01/05/23 20:27:55.695
STEP: waiting for Deployment to be created 01/05/23 20:27:55.7
STEP: waiting for all Replicas to be Ready 01/05/23 20:27:55.702
Jan  5 20:27:55.703: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 20:27:55.704: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 20:27:55.713: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 20:27:55.714: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 20:27:55.731: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 20:27:55.731: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 20:27:55.773: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 20:27:55.773: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jan  5 20:27:57.371: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  5 20:27:57.371: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jan  5 20:27:57.536: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 01/05/23 20:27:57.536
W0105 20:27:57.545697      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jan  5 20:27:57.547: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 01/05/23 20:27:57.548
Jan  5 20:27:57.551: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:27:57.553: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:57.553: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:57.554: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:57.554: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:57.560: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:57.560: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:57.588: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:57.589: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:57.607: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:27:57.607: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:27:57.625: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:27:57.626: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:27:59.392: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:59.392: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:27:59.422: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
STEP: listing Deployments 01/05/23 20:27:59.423
Jan  5 20:27:59.427: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 01/05/23 20:27:59.428
Jan  5 20:27:59.446: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 01/05/23 20:27:59.446
Jan  5 20:27:59.453: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:27:59.460: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:27:59.484: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:27:59.510: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:27:59.522: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:27:59.536: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:28:00.555: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:28:01.423: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:28:01.459: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:28:01.491: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jan  5 20:28:02.570: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 01/05/23 20:28:02.597
STEP: fetching the DeploymentStatus 01/05/23 20:28:02.604
Jan  5 20:28:02.609: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:28:02.609: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:28:02.611: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 3
Jan  5 20:28:02.611: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:28:02.611: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
Jan  5 20:28:02.611: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 3
STEP: deleting the Deployment 01/05/23 20:28:02.611
Jan  5 20:28:02.620: INFO: observed event type MODIFIED
Jan  5 20:28:02.620: INFO: observed event type MODIFIED
Jan  5 20:28:02.620: INFO: observed event type MODIFIED
Jan  5 20:28:02.620: INFO: observed event type MODIFIED
Jan  5 20:28:02.621: INFO: observed event type MODIFIED
Jan  5 20:28:02.621: INFO: observed event type MODIFIED
Jan  5 20:28:02.621: INFO: observed event type MODIFIED
Jan  5 20:28:02.621: INFO: observed event type MODIFIED
Jan  5 20:28:02.621: INFO: observed event type MODIFIED
Jan  5 20:28:02.621: INFO: observed event type MODIFIED
Jan  5 20:28:02.622: INFO: observed event type MODIFIED
Jan  5 20:28:02.622: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jan  5 20:28:02.626: INFO: Log out all the ReplicaSets if there is no deployment created
Jan  5 20:28:02.632: INFO: ReplicaSet "test-deployment-7b7876f9d6":
&ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-5641  43d69750-8b20-44cf-9f12-87f20c425b36 108788 2 2023-01-05 20:27:59 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 262682db-40af-45b1-8e59-7262d7c2db27 0xc0042b1a67 0xc0042b1a68}] [] [{kube-controller-manager Update apps/v1 2023-01-05 20:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"262682db-40af-45b1-8e59-7262d7c2db27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b1af0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jan  5 20:28:02.637: INFO: pod: "test-deployment-7b7876f9d6-g6qnv":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-g6qnv test-deployment-7b7876f9d6- deployment-5641  c51bc697-e79e-46aa-b23f-4b564ad75ecc 108787 0 2023-01-05 20:28:01 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 43d69750-8b20-44cf-9f12-87f20c425b36 0xc0041e7b97 0xc0041e7b98}] [] [{kube-controller-manager Update v1 2023-01-05 20:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43d69750-8b20-44cf-9f12-87f20c425b36\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2pql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2pql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.62,StartTime:2023-01-05 20:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:28:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1a451f81c436558ee783a48a98284fd928d5d2cc0dd0b1f6691ac4cdc2d3c5eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  5 20:28:02.639: INFO: pod: "test-deployment-7b7876f9d6-t45wq":
&Pod{ObjectMeta:{test-deployment-7b7876f9d6-t45wq test-deployment-7b7876f9d6- deployment-5641  ea001288-fa2f-4a84-9a7a-47a7d70b8020 108754 0 2023-01-05 20:27:59 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 43d69750-8b20-44cf-9f12-87f20c425b36 0xc0041e7d77 0xc0041e7d78}] [] [{kube-controller-manager Update v1 2023-01-05 20:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43d69750-8b20-44cf-9f12-87f20c425b36\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:28:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6nsrf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6nsrf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:27:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.37,StartTime:2023-01-05 20:27:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:28:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dac827948d6d1ea5782b8bdfd155e67529959df89bd0c74cca25e66373375ccc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  5 20:28:02.640: INFO: ReplicaSet "test-deployment-7df74c55ff":
&ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-5641  ec6ccfb0-055d-416b-ac4c-e7b827b39e00 108793 4 2023-01-05 20:27:57 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 262682db-40af-45b1-8e59-7262d7c2db27 0xc0042b1b57 0xc0042b1b58}] [] [{kube-controller-manager Update apps/v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"262682db-40af-45b1-8e59-7262d7c2db27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b1be0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jan  5 20:28:02.644: INFO: pod: "test-deployment-7df74c55ff-q92kd":
&Pod{ObjectMeta:{test-deployment-7df74c55ff-q92kd test-deployment-7df74c55ff- deployment-5641  22c18d0c-2410-499a-a469-2318a301df1b 108790 0 2023-01-05 20:27:59 +0000 UTC 2023-01-05 20:28:03 +0000 UTC 0xc0043ab188 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff ec6ccfb0-055d-416b-ac4c-e7b827b39e00 0xc0043ab1b7 0xc0043ab1b8}] [] [{kube-controller-manager Update v1 2023-01-05 20:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec6ccfb0-055d-416b-ac4c-e7b827b39e00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlcdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlcdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:27:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.61,StartTime:2023-01-05 20:27:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:28:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://c9c9df02fd3019fe007d00b8d9a20e9744b049b04d5cc79af08a2cd18af9ea54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jan  5 20:28:02.645: INFO: ReplicaSet "test-deployment-f4dbc4647":
&ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5641  5d36082d-354e-4d4d-a8fa-93756a8e6ab1 108700 3 2023-01-05 20:27:55 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 262682db-40af-45b1-8e59-7262d7c2db27 0xc0042b1c47 0xc0042b1c48}] [] [{kube-controller-manager Update apps/v1 2023-01-05 20:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"262682db-40af-45b1-8e59-7262d7c2db27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:27:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b1cd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/node/init/init.go:32
Jan  5 20:28:02.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] Deployment
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] Deployment
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] Deployment
  tear down framework | framework.go:193
STEP: Destroying namespace "deployment-5641" for this suite. 01/05/23 20:28:02.653
------------------------------
• [SLOW TEST] [6.988 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:27:55.67
    Jan  5 20:27:55.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename deployment 01/05/23 20:27:55.672
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:27:55.685
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:27:55.688
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 01/05/23 20:27:55.695
    STEP: waiting for Deployment to be created 01/05/23 20:27:55.7
    STEP: waiting for all Replicas to be Ready 01/05/23 20:27:55.702
    Jan  5 20:27:55.703: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 20:27:55.704: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 20:27:55.713: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 20:27:55.714: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 20:27:55.731: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 20:27:55.731: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 20:27:55.773: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 20:27:55.773: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jan  5 20:27:57.371: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  5 20:27:57.371: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jan  5 20:27:57.536: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 01/05/23 20:27:57.536
    W0105 20:27:57.545697      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jan  5 20:27:57.547: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 01/05/23 20:27:57.548
    Jan  5 20:27:57.551: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 0
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:27:57.552: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:27:57.553: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:57.553: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:57.554: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:57.554: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:57.560: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:57.560: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:57.588: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:57.589: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:57.607: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:27:57.607: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:27:57.625: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:27:57.626: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:27:59.392: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:59.392: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:27:59.422: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    STEP: listing Deployments 01/05/23 20:27:59.423
    Jan  5 20:27:59.427: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 01/05/23 20:27:59.428
    Jan  5 20:27:59.446: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 01/05/23 20:27:59.446
    Jan  5 20:27:59.453: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:27:59.460: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:27:59.484: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:27:59.510: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:27:59.522: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:27:59.536: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:28:00.555: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:28:01.423: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:28:01.459: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:28:01.491: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jan  5 20:28:02.570: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 01/05/23 20:28:02.597
    STEP: fetching the DeploymentStatus 01/05/23 20:28:02.604
    Jan  5 20:28:02.609: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:28:02.609: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 1
    Jan  5 20:28:02.610: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:28:02.611: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 3
    Jan  5 20:28:02.611: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:28:02.611: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 2
    Jan  5 20:28:02.611: INFO: observed Deployment test-deployment in namespace deployment-5641 with ReadyReplicas 3
    STEP: deleting the Deployment 01/05/23 20:28:02.611
    Jan  5 20:28:02.620: INFO: observed event type MODIFIED
    Jan  5 20:28:02.620: INFO: observed event type MODIFIED
    Jan  5 20:28:02.620: INFO: observed event type MODIFIED
    Jan  5 20:28:02.620: INFO: observed event type MODIFIED
    Jan  5 20:28:02.621: INFO: observed event type MODIFIED
    Jan  5 20:28:02.621: INFO: observed event type MODIFIED
    Jan  5 20:28:02.621: INFO: observed event type MODIFIED
    Jan  5 20:28:02.621: INFO: observed event type MODIFIED
    Jan  5 20:28:02.621: INFO: observed event type MODIFIED
    Jan  5 20:28:02.621: INFO: observed event type MODIFIED
    Jan  5 20:28:02.622: INFO: observed event type MODIFIED
    Jan  5 20:28:02.622: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jan  5 20:28:02.626: INFO: Log out all the ReplicaSets if there is no deployment created
    Jan  5 20:28:02.632: INFO: ReplicaSet "test-deployment-7b7876f9d6":
    &ReplicaSet{ObjectMeta:{test-deployment-7b7876f9d6  deployment-5641  43d69750-8b20-44cf-9f12-87f20c425b36 108788 2 2023-01-05 20:27:59 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 262682db-40af-45b1-8e59-7262d7c2db27 0xc0042b1a67 0xc0042b1a68}] [] [{kube-controller-manager Update apps/v1 2023-01-05 20:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"262682db-40af-45b1-8e59-7262d7c2db27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7b7876f9d6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b1af0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jan  5 20:28:02.637: INFO: pod: "test-deployment-7b7876f9d6-g6qnv":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-g6qnv test-deployment-7b7876f9d6- deployment-5641  c51bc697-e79e-46aa-b23f-4b564ad75ecc 108787 0 2023-01-05 20:28:01 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 43d69750-8b20-44cf-9f12-87f20c425b36 0xc0041e7b97 0xc0041e7b98}] [] [{kube-controller-manager Update v1 2023-01-05 20:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43d69750-8b20-44cf-9f12-87f20c425b36\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t2pql,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t2pql,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.62,StartTime:2023-01-05 20:28:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:28:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1a451f81c436558ee783a48a98284fd928d5d2cc0dd0b1f6691ac4cdc2d3c5eb,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.62,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  5 20:28:02.639: INFO: pod: "test-deployment-7b7876f9d6-t45wq":
    &Pod{ObjectMeta:{test-deployment-7b7876f9d6-t45wq test-deployment-7b7876f9d6- deployment-5641  ea001288-fa2f-4a84-9a7a-47a7d70b8020 108754 0 2023-01-05 20:27:59 +0000 UTC <nil> <nil> map[pod-template-hash:7b7876f9d6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7b7876f9d6 43d69750-8b20-44cf-9f12-87f20c425b36 0xc0041e7d77 0xc0041e7d78}] [] [{kube-controller-manager Update v1 2023-01-05 20:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43d69750-8b20-44cf-9f12-87f20c425b36\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:28:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.2.37\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6nsrf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6nsrf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-16pz,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:27:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.39,PodIP:10.16.2.37,StartTime:2023-01-05 20:27:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:28:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://dac827948d6d1ea5782b8bdfd155e67529959df89bd0c74cca25e66373375ccc,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.2.37,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  5 20:28:02.640: INFO: ReplicaSet "test-deployment-7df74c55ff":
    &ReplicaSet{ObjectMeta:{test-deployment-7df74c55ff  deployment-5641  ec6ccfb0-055d-416b-ac4c-e7b827b39e00 108793 4 2023-01-05 20:27:57 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 262682db-40af-45b1-8e59-7262d7c2db27 0xc0042b1b57 0xc0042b1b58}] [] [{kube-controller-manager Update apps/v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"262682db-40af-45b1-8e59-7262d7c2db27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7df74c55ff,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b1be0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jan  5 20:28:02.644: INFO: pod: "test-deployment-7df74c55ff-q92kd":
    &Pod{ObjectMeta:{test-deployment-7df74c55ff-q92kd test-deployment-7df74c55ff- deployment-5641  22c18d0c-2410-499a-a469-2318a301df1b 108790 0 2023-01-05 20:27:59 +0000 UTC 2023-01-05 20:28:03 +0000 UTC 0xc0043ab188 map[pod-template-hash:7df74c55ff test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-7df74c55ff ec6ccfb0-055d-416b-ac4c-e7b827b39e00 0xc0043ab1b7 0xc0043ab1b8}] [] [{kube-controller-manager Update v1 2023-01-05 20:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ec6ccfb0-055d-416b-ac4c-e7b827b39e00\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-01-05 20:28:00 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.16.1.61\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlcdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlcdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:gke-gke-1-26-default-pool-05283374-qmj7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:27:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:28:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-01-05 20:27:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.196.0.38,PodIP:10.16.1.61,StartTime:2023-01-05 20:27:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-01-05 20:28:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://c9c9df02fd3019fe007d00b8d9a20e9744b049b04d5cc79af08a2cd18af9ea54,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.16.1.61,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jan  5 20:28:02.645: INFO: ReplicaSet "test-deployment-f4dbc4647":
    &ReplicaSet{ObjectMeta:{test-deployment-f4dbc4647  deployment-5641  5d36082d-354e-4d4d-a8fa-93756a8e6ab1 108700 3 2023-01-05 20:27:55 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 262682db-40af-45b1-8e59-7262d7c2db27 0xc0042b1c47 0xc0042b1c48}] [] [{kube-controller-manager Update apps/v1 2023-01-05 20:27:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"262682db-40af-45b1-8e59-7262d7c2db27\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-01-05 20:27:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: f4dbc4647,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:f4dbc4647 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042b1cd0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:28:02.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] Deployment
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] Deployment
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] Deployment
      tear down framework | framework.go:193
    STEP: Destroying namespace "deployment-5641" for this suite. 01/05/23 20:28:02.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:28:02.662
Jan  5 20:28:02.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 20:28:02.664
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:28:02.682
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:28:02.685
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428
STEP: creating a Service 01/05/23 20:28:02.69
STEP: watching for the Service to be added 01/05/23 20:28:02.701
Jan  5 20:28:02.704: INFO: Found Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jan  5 20:28:02.704: INFO: Service test-service-x4bq4 created
STEP: Getting /status 01/05/23 20:28:02.704
Jan  5 20:28:02.708: INFO: Service test-service-x4bq4 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 01/05/23 20:28:02.708
STEP: watching for the Service to be patched 01/05/23 20:28:02.715
Jan  5 20:28:02.717: INFO: observed Service test-service-x4bq4 in namespace services-3149 with annotations: map[cloud.google.com/neg:{"ingress":true}] & LoadBalancer: {[]}
Jan  5 20:28:02.717: INFO: Found Service test-service-x4bq4 in namespace services-3149 with annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jan  5 20:28:02.718: INFO: Service test-service-x4bq4 has service status patched
STEP: updating the ServiceStatus 01/05/23 20:28:02.718
Jan  5 20:28:02.726: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 01/05/23 20:28:02.727
Jan  5 20:28:02.730: INFO: Observed Service test-service-x4bq4 in namespace services-3149 with annotations: map[cloud.google.com/neg:{"ingress":true}] & Conditions: {[]}
Jan  5 20:28:02.730: INFO: Observed event: &Service{ObjectMeta:{test-service-x4bq4  services-3149  a1235e6b-db91-46f9-8985-5319682ac2b2 108807 0 2023-01-05 20:28:02 +0000 UTC <nil> <nil> map[test-service-static:true] map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.20.1.195,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.20.1.195],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jan  5 20:28:02.730: INFO: Found Service test-service-x4bq4 in namespace services-3149 with annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jan  5 20:28:02.731: INFO: Service test-service-x4bq4 has service status updated
STEP: patching the service 01/05/23 20:28:02.731
STEP: watching for the Service to be patched 01/05/23 20:28:02.751
Jan  5 20:28:02.755: INFO: observed Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service-static:true]
Jan  5 20:28:02.755: INFO: observed Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service-static:true]
Jan  5 20:28:02.756: INFO: observed Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service-static:true]
Jan  5 20:28:02.756: INFO: Found Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service:patched test-service-static:true]
Jan  5 20:28:02.757: INFO: Service test-service-x4bq4 patched
STEP: deleting the service 01/05/23 20:28:02.757
STEP: watching for the Service to be deleted 01/05/23 20:28:02.766
Jan  5 20:28:02.769: INFO: Observed event: ADDED
Jan  5 20:28:02.769: INFO: Observed event: MODIFIED
Jan  5 20:28:02.770: INFO: Observed event: MODIFIED
Jan  5 20:28:02.770: INFO: Observed event: MODIFIED
Jan  5 20:28:02.770: INFO: Found Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service:patched test-service-static:true] & annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true]
Jan  5 20:28:02.770: INFO: Service test-service-x4bq4 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 20:28:02.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-3149" for this suite. 01/05/23 20:28:02.774
------------------------------
• [0.120 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3428

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:28:02.662
    Jan  5 20:28:02.662: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 20:28:02.664
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:28:02.682
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:28:02.685
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3428
    STEP: creating a Service 01/05/23 20:28:02.69
    STEP: watching for the Service to be added 01/05/23 20:28:02.701
    Jan  5 20:28:02.704: INFO: Found Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jan  5 20:28:02.704: INFO: Service test-service-x4bq4 created
    STEP: Getting /status 01/05/23 20:28:02.704
    Jan  5 20:28:02.708: INFO: Service test-service-x4bq4 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 01/05/23 20:28:02.708
    STEP: watching for the Service to be patched 01/05/23 20:28:02.715
    Jan  5 20:28:02.717: INFO: observed Service test-service-x4bq4 in namespace services-3149 with annotations: map[cloud.google.com/neg:{"ingress":true}] & LoadBalancer: {[]}
    Jan  5 20:28:02.717: INFO: Found Service test-service-x4bq4 in namespace services-3149 with annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jan  5 20:28:02.718: INFO: Service test-service-x4bq4 has service status patched
    STEP: updating the ServiceStatus 01/05/23 20:28:02.718
    Jan  5 20:28:02.726: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 01/05/23 20:28:02.727
    Jan  5 20:28:02.730: INFO: Observed Service test-service-x4bq4 in namespace services-3149 with annotations: map[cloud.google.com/neg:{"ingress":true}] & Conditions: {[]}
    Jan  5 20:28:02.730: INFO: Observed event: &Service{ObjectMeta:{test-service-x4bq4  services-3149  a1235e6b-db91-46f9-8985-5319682ac2b2 108807 0 2023-01-05 20:28:02 +0000 UTC <nil> <nil> map[test-service-static:true] map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] [] [] [{e2e.test Update v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-01-05 20:28:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.20.1.195,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.20.1.195],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jan  5 20:28:02.730: INFO: Found Service test-service-x4bq4 in namespace services-3149 with annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jan  5 20:28:02.731: INFO: Service test-service-x4bq4 has service status updated
    STEP: patching the service 01/05/23 20:28:02.731
    STEP: watching for the Service to be patched 01/05/23 20:28:02.751
    Jan  5 20:28:02.755: INFO: observed Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service-static:true]
    Jan  5 20:28:02.755: INFO: observed Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service-static:true]
    Jan  5 20:28:02.756: INFO: observed Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service-static:true]
    Jan  5 20:28:02.756: INFO: Found Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service:patched test-service-static:true]
    Jan  5 20:28:02.757: INFO: Service test-service-x4bq4 patched
    STEP: deleting the service 01/05/23 20:28:02.757
    STEP: watching for the Service to be deleted 01/05/23 20:28:02.766
    Jan  5 20:28:02.769: INFO: Observed event: ADDED
    Jan  5 20:28:02.769: INFO: Observed event: MODIFIED
    Jan  5 20:28:02.770: INFO: Observed event: MODIFIED
    Jan  5 20:28:02.770: INFO: Observed event: MODIFIED
    Jan  5 20:28:02.770: INFO: Found Service test-service-x4bq4 in namespace services-3149 with labels: map[test-service:patched test-service-static:true] & annotations: map[cloud.google.com/neg:{"ingress":true} patchedstatus:true]
    Jan  5 20:28:02.770: INFO: Service test-service-x4bq4 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:28:02.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-3149" for this suite. 01/05/23 20:28:02.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:28:02.816
Jan  5 20:28:02.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename hostport 01/05/23 20:28:02.818
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:28:02.835
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:28:02.838
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/05/23 20:28:02.846
Jan  5 20:28:02.863: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3262" to be "running and ready"
Jan  5 20:28:02.868: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.859094ms
Jan  5 20:28:02.868: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:28:04.871: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008209791s
Jan  5 20:28:04.871: INFO: The phase of Pod pod1 is Running (Ready = true)
Jan  5 20:28:04.871: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.196.0.39 on the node which pod1 resides and expect scheduled 01/05/23 20:28:04.871
Jan  5 20:28:04.883: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3262" to be "running and ready"
Jan  5 20:28:04.887: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990522ms
Jan  5 20:28:04.887: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:28:06.891: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008206764s
Jan  5 20:28:06.891: INFO: The phase of Pod pod2 is Running (Ready = true)
Jan  5 20:28:06.891: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.196.0.39 but use UDP protocol on the node which pod2 resides 01/05/23 20:28:06.891
Jan  5 20:28:06.900: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3262" to be "running and ready"
Jan  5 20:28:06.904: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217746ms
Jan  5 20:28:06.904: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:28:08.909: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007655262s
Jan  5 20:28:08.909: INFO: The phase of Pod pod3 is Running (Ready = true)
Jan  5 20:28:08.909: INFO: Pod "pod3" satisfied condition "running and ready"
Jan  5 20:28:08.915: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3262" to be "running and ready"
Jan  5 20:28:08.919: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.656002ms
Jan  5 20:28:08.919: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:28:10.923: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008073792s
Jan  5 20:28:10.923: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jan  5 20:28:10.923: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/05/23 20:28:10.929
Jan  5 20:28:10.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.196.0.39 http://127.0.0.1:54323/hostname] Namespace:hostport-3262 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:28:10.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:28:10.930: INFO: ExecWithOptions: Clientset creation
Jan  5 20:28:10.930: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/hostport-3262/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.196.0.39+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.196.0.39, port: 54323 01/05/23 20:28:11.003
Jan  5 20:28:11.003: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.196.0.39:54323/hostname] Namespace:hostport-3262 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:28:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:28:11.004: INFO: ExecWithOptions: Clientset creation
Jan  5 20:28:11.004: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/hostport-3262/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.196.0.39%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.196.0.39, port: 54323 UDP 01/05/23 20:28:11.088
Jan  5 20:28:11.088: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.196.0.39 54323] Namespace:hostport-3262 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:28:11.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:28:11.089: INFO: ExecWithOptions: Clientset creation
Jan  5 20:28:11.089: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/hostport-3262/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.196.0.39+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/node/init/init.go:32
Jan  5 20:28:16.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] HostPort
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] HostPort
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] HostPort
  tear down framework | framework.go:193
STEP: Destroying namespace "hostport-3262" for this suite. 01/05/23 20:28:16.179
------------------------------
• [SLOW TEST] [13.368 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:28:02.816
    Jan  5 20:28:02.817: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename hostport 01/05/23 20:28:02.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:28:02.835
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:28:02.838
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 01/05/23 20:28:02.846
    Jan  5 20:28:02.863: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-3262" to be "running and ready"
    Jan  5 20:28:02.868: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 4.859094ms
    Jan  5 20:28:02.868: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:28:04.871: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008209791s
    Jan  5 20:28:04.871: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jan  5 20:28:04.871: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.196.0.39 on the node which pod1 resides and expect scheduled 01/05/23 20:28:04.871
    Jan  5 20:28:04.883: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-3262" to be "running and ready"
    Jan  5 20:28:04.887: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.990522ms
    Jan  5 20:28:04.887: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:28:06.891: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008206764s
    Jan  5 20:28:06.891: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jan  5 20:28:06.891: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.196.0.39 but use UDP protocol on the node which pod2 resides 01/05/23 20:28:06.891
    Jan  5 20:28:06.900: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-3262" to be "running and ready"
    Jan  5 20:28:06.904: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.217746ms
    Jan  5 20:28:06.904: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:28:08.909: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007655262s
    Jan  5 20:28:08.909: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jan  5 20:28:08.909: INFO: Pod "pod3" satisfied condition "running and ready"
    Jan  5 20:28:08.915: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-3262" to be "running and ready"
    Jan  5 20:28:08.919: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.656002ms
    Jan  5 20:28:08.919: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:28:10.923: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008073792s
    Jan  5 20:28:10.923: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jan  5 20:28:10.923: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 01/05/23 20:28:10.929
    Jan  5 20:28:10.929: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.196.0.39 http://127.0.0.1:54323/hostname] Namespace:hostport-3262 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:28:10.929: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:28:10.930: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:28:10.930: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/hostport-3262/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.196.0.39+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.196.0.39, port: 54323 01/05/23 20:28:11.003
    Jan  5 20:28:11.003: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.196.0.39:54323/hostname] Namespace:hostport-3262 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:28:11.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:28:11.004: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:28:11.004: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/hostport-3262/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.196.0.39%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.196.0.39, port: 54323 UDP 01/05/23 20:28:11.088
    Jan  5 20:28:11.088: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.196.0.39 54323] Namespace:hostport-3262 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:28:11.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:28:11.089: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:28:11.089: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/hostport-3262/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.196.0.39+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:28:16.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] HostPort
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] HostPort
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] HostPort
      tear down framework | framework.go:193
    STEP: Destroying namespace "hostport-3262" for this suite. 01/05/23 20:28:16.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:28:16.19
Jan  5 20:28:16.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 20:28:16.192
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:28:16.207
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:28:16.21
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124
STEP: Creating configMap with name configmap-test-upd-3eb6dbd8-d77f-48b0-b95b-802c4f64a194 01/05/23 20:28:16.219
STEP: Creating the pod 01/05/23 20:28:16.224
Jan  5 20:28:16.234: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7" in namespace "configmap-3158" to be "running and ready"
Jan  5 20:28:16.239: INFO: Pod "pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.060195ms
Jan  5 20:28:16.239: INFO: The phase of Pod pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:28:18.242: INFO: Pod "pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008134822s
Jan  5 20:28:18.243: INFO: The phase of Pod pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7 is Running (Ready = true)
Jan  5 20:28:18.243: INFO: Pod "pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-3eb6dbd8-d77f-48b0-b95b-802c4f64a194 01/05/23 20:28:18.262
STEP: waiting to observe update in volume 01/05/23 20:28:18.267
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:28:20.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-3158" for this suite. 01/05/23 20:28:20.288
------------------------------
• [4.103 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:28:16.19
    Jan  5 20:28:16.190: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 20:28:16.192
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:28:16.207
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:28:16.21
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:124
    STEP: Creating configMap with name configmap-test-upd-3eb6dbd8-d77f-48b0-b95b-802c4f64a194 01/05/23 20:28:16.219
    STEP: Creating the pod 01/05/23 20:28:16.224
    Jan  5 20:28:16.234: INFO: Waiting up to 5m0s for pod "pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7" in namespace "configmap-3158" to be "running and ready"
    Jan  5 20:28:16.239: INFO: Pod "pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.060195ms
    Jan  5 20:28:16.239: INFO: The phase of Pod pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:28:18.242: INFO: Pod "pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7": Phase="Running", Reason="", readiness=true. Elapsed: 2.008134822s
    Jan  5 20:28:18.243: INFO: The phase of Pod pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7 is Running (Ready = true)
    Jan  5 20:28:18.243: INFO: Pod "pod-configmaps-7a256e70-9611-43b9-9ef0-da25fe034fe7" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-3eb6dbd8-d77f-48b0-b95b-802c4f64a194 01/05/23 20:28:18.262
    STEP: waiting to observe update in volume 01/05/23 20:28:18.267
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:28:20.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-3158" for this suite. 01/05/23 20:28:20.288
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
[BeforeEach] [sig-apps] StatefulSet
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:28:20.298
Jan  5 20:28:20.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename statefulset 01/05/23 20:28:20.299
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:28:20.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:28:20.314
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:98
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:113
STEP: Creating service test in namespace statefulset-8329 01/05/23 20:28:20.317
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:306
STEP: Creating a new StatefulSet 01/05/23 20:28:20.325
Jan  5 20:28:20.336: INFO: Found 0 stateful pods, waiting for 3
Jan  5 20:28:30.343: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:28:30.343: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:28:30.343: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jan  5 20:28:30.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-8329 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 20:28:30.522: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 20:28:30.522: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 20:28:30.522: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/05/23 20:28:40.536
Jan  5 20:28:40.557: INFO: Updating stateful set ss2
STEP: Creating a new revision 01/05/23 20:28:40.557
STEP: Updating Pods in reverse ordinal order 01/05/23 20:28:50.57
Jan  5 20:28:50.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-8329 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 20:28:50.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 20:28:50.713: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 20:28:50.713: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 01/05/23 20:29:00.73
Jan  5 20:29:00.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-8329 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jan  5 20:29:00.900: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jan  5 20:29:00.900: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jan  5 20:29:00.900: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jan  5 20:29:10.933: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 01/05/23 20:29:20.948
Jan  5 20:29:20.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-8329 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jan  5 20:29:21.090: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jan  5 20:29:21.090: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jan  5 20:29:21.090: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:124
Jan  5 20:29:31.111: INFO: Deleting all statefulset in ns statefulset-8329
Jan  5 20:29:31.113: INFO: Scaling statefulset ss2 to 0
Jan  5 20:29:41.131: INFO: Waiting for statefulset status.replicas updated to 0
Jan  5 20:29:41.133: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/node/init/init.go:32
Jan  5 20:29:41.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] StatefulSet
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] StatefulSet
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] StatefulSet
  tear down framework | framework.go:193
STEP: Destroying namespace "statefulset-8329" for this suite. 01/05/23 20:29:41.148
------------------------------
• [SLOW TEST] [80.860 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:103
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:306

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:28:20.298
    Jan  5 20:28:20.298: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename statefulset 01/05/23 20:28:20.299
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:28:20.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:28:20.314
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:98
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:113
    STEP: Creating service test in namespace statefulset-8329 01/05/23 20:28:20.317
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:306
    STEP: Creating a new StatefulSet 01/05/23 20:28:20.325
    Jan  5 20:28:20.336: INFO: Found 0 stateful pods, waiting for 3
    Jan  5 20:28:30.343: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:28:30.343: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:28:30.343: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jan  5 20:28:30.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-8329 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 20:28:30.522: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 20:28:30.522: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 20:28:30.522: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 01/05/23 20:28:40.536
    Jan  5 20:28:40.557: INFO: Updating stateful set ss2
    STEP: Creating a new revision 01/05/23 20:28:40.557
    STEP: Updating Pods in reverse ordinal order 01/05/23 20:28:50.57
    Jan  5 20:28:50.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-8329 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 20:28:50.713: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 20:28:50.713: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 20:28:50.713: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 01/05/23 20:29:00.73
    Jan  5 20:29:00.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-8329 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jan  5 20:29:00.900: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jan  5 20:29:00.900: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jan  5 20:29:00.900: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jan  5 20:29:10.933: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 01/05/23 20:29:20.948
    Jan  5 20:29:20.951: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3260951545 --namespace=statefulset-8329 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jan  5 20:29:21.090: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jan  5 20:29:21.090: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jan  5 20:29:21.090: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:124
    Jan  5 20:29:31.111: INFO: Deleting all statefulset in ns statefulset-8329
    Jan  5 20:29:31.113: INFO: Scaling statefulset ss2 to 0
    Jan  5 20:29:41.131: INFO: Waiting for statefulset status.replicas updated to 0
    Jan  5 20:29:41.133: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:29:41.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] StatefulSet
      tear down framework | framework.go:193
    STEP: Destroying namespace "statefulset-8329" for this suite. 01/05/23 20:29:41.148
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
[BeforeEach] [sig-node] Probing container
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:29:41.162
Jan  5 20:29:41.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename container-probe 01/05/23 20:29:41.164
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:29:41.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:29:41.181
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:63
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135
STEP: Creating pod busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0 in namespace container-probe-5880 01/05/23 20:29:41.184
Jan  5 20:29:41.193: INFO: Waiting up to 5m0s for pod "busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0" in namespace "container-probe-5880" to be "not pending"
Jan  5 20:29:41.198: INFO: Pod "busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.638161ms
Jan  5 20:29:43.201: INFO: Pod "busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007603317s
Jan  5 20:29:43.201: INFO: Pod "busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0" satisfied condition "not pending"
Jan  5 20:29:43.201: INFO: Started pod busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0 in namespace container-probe-5880
STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 20:29:43.201
Jan  5 20:29:43.203: INFO: Initial restart count of pod busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0 is 0
Jan  5 20:30:33.305: INFO: Restart count of pod container-probe-5880/busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0 is now 1 (50.101608868s elapsed)
STEP: deleting the pod 01/05/23 20:30:33.305
[AfterEach] [sig-node] Probing container
  test/e2e/framework/node/init/init.go:32
Jan  5 20:30:33.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] Probing container
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] Probing container
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] Probing container
  tear down framework | framework.go:193
STEP: Destroying namespace "container-probe-5880" for this suite. 01/05/23 20:30:33.322
------------------------------
• [SLOW TEST] [52.164 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:29:41.162
    Jan  5 20:29:41.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename container-probe 01/05/23 20:29:41.164
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:29:41.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:29:41.181
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:63
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:135
    STEP: Creating pod busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0 in namespace container-probe-5880 01/05/23 20:29:41.184
    Jan  5 20:29:41.193: INFO: Waiting up to 5m0s for pod "busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0" in namespace "container-probe-5880" to be "not pending"
    Jan  5 20:29:41.198: INFO: Pod "busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.638161ms
    Jan  5 20:29:43.201: INFO: Pod "busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0": Phase="Running", Reason="", readiness=true. Elapsed: 2.007603317s
    Jan  5 20:29:43.201: INFO: Pod "busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0" satisfied condition "not pending"
    Jan  5 20:29:43.201: INFO: Started pod busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0 in namespace container-probe-5880
    STEP: checking the pod's current state and verifying that restartCount is present 01/05/23 20:29:43.201
    Jan  5 20:29:43.203: INFO: Initial restart count of pod busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0 is 0
    Jan  5 20:30:33.305: INFO: Restart count of pod container-probe-5880/busybox-f067a34f-3386-4ee2-8ed1-9e0740a76cd0 is now 1 (50.101608868s elapsed)
    STEP: deleting the pod 01/05/23 20:30:33.305
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:30:33.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] Probing container
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] Probing container
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] Probing container
      tear down framework | framework.go:193
    STEP: Destroying namespace "container-probe-5880" for this suite. 01/05/23 20:30:33.322
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
[BeforeEach] [sig-network] Services
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:30:33.328
Jan  5 20:30:33.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename services 01/05/23 20:30:33.329
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:30:33.343
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:30:33.346
[BeforeEach] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:766
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244
STEP: creating an Endpoint 01/05/23 20:30:33.353
STEP: waiting for available Endpoint 01/05/23 20:30:33.36
STEP: listing all Endpoints 01/05/23 20:30:33.361
STEP: updating the Endpoint 01/05/23 20:30:33.364
STEP: fetching the Endpoint 01/05/23 20:30:33.37
STEP: patching the Endpoint 01/05/23 20:30:33.372
STEP: fetching the Endpoint 01/05/23 20:30:33.384
STEP: deleting the Endpoint by Collection 01/05/23 20:30:33.387
STEP: waiting for Endpoint deletion 01/05/23 20:30:33.393
STEP: fetching the Endpoint 01/05/23 20:30:33.395
[AfterEach] [sig-network] Services
  test/e2e/framework/node/init/init.go:32
Jan  5 20:30:33.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Services
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Services
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Services
  tear down framework | framework.go:193
STEP: Destroying namespace "services-1890" for this suite. 01/05/23 20:30:33.401
------------------------------
• [0.080 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3244

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:30:33.328
    Jan  5 20:30:33.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename services 01/05/23 20:30:33.329
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:30:33.343
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:30:33.346
    [BeforeEach] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:766
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3244
    STEP: creating an Endpoint 01/05/23 20:30:33.353
    STEP: waiting for available Endpoint 01/05/23 20:30:33.36
    STEP: listing all Endpoints 01/05/23 20:30:33.361
    STEP: updating the Endpoint 01/05/23 20:30:33.364
    STEP: fetching the Endpoint 01/05/23 20:30:33.37
    STEP: patching the Endpoint 01/05/23 20:30:33.372
    STEP: fetching the Endpoint 01/05/23 20:30:33.384
    STEP: deleting the Endpoint by Collection 01/05/23 20:30:33.387
    STEP: waiting for Endpoint deletion 01/05/23 20:30:33.393
    STEP: fetching the Endpoint 01/05/23 20:30:33.395
    [AfterEach] [sig-network] Services
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:30:33.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Services
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Services
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Services
      tear down framework | framework.go:193
    STEP: Destroying namespace "services-1890" for this suite. 01/05/23 20:30:33.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
[BeforeEach] [sig-node] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:30:33.413
Jan  5 20:30:33.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 20:30:33.415
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:30:33.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:30:33.431
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93
STEP: Creating configMap configmap-2995/configmap-test-56acccd4-dd41-4f2d-939e-0be01550007c 01/05/23 20:30:33.436
STEP: Creating a pod to test consume configMaps 01/05/23 20:30:33.44
Jan  5 20:30:33.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277" in namespace "configmap-2995" to be "Succeeded or Failed"
Jan  5 20:30:33.455: INFO: Pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277": Phase="Pending", Reason="", readiness=false. Elapsed: 5.62964ms
Jan  5 20:30:35.459: INFO: Pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009602502s
Jan  5 20:30:37.458: INFO: Pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008977447s
STEP: Saw pod success 01/05/23 20:30:37.458
Jan  5 20:30:37.458: INFO: Pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277" satisfied condition "Succeeded or Failed"
Jan  5 20:30:37.461: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-5d06111f-27b2-481d-8652-b23766781277 container env-test: <nil>
STEP: delete the pod 01/05/23 20:30:37.479
Jan  5 20:30:37.490: INFO: Waiting for pod pod-configmaps-5d06111f-27b2-481d-8652-b23766781277 to disappear
Jan  5 20:30:37.493: INFO: Pod pod-configmaps-5d06111f-27b2-481d-8652-b23766781277 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:30:37.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-node] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-node] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-node] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2995" for this suite. 01/05/23 20:30:37.497
------------------------------
• [4.092 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:30:33.413
    Jan  5 20:30:33.413: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 20:30:33.415
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:30:33.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:30:33.431
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:93
    STEP: Creating configMap configmap-2995/configmap-test-56acccd4-dd41-4f2d-939e-0be01550007c 01/05/23 20:30:33.436
    STEP: Creating a pod to test consume configMaps 01/05/23 20:30:33.44
    Jan  5 20:30:33.449: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277" in namespace "configmap-2995" to be "Succeeded or Failed"
    Jan  5 20:30:33.455: INFO: Pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277": Phase="Pending", Reason="", readiness=false. Elapsed: 5.62964ms
    Jan  5 20:30:35.459: INFO: Pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009602502s
    Jan  5 20:30:37.458: INFO: Pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008977447s
    STEP: Saw pod success 01/05/23 20:30:37.458
    Jan  5 20:30:37.458: INFO: Pod "pod-configmaps-5d06111f-27b2-481d-8652-b23766781277" satisfied condition "Succeeded or Failed"
    Jan  5 20:30:37.461: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-5d06111f-27b2-481d-8652-b23766781277 container env-test: <nil>
    STEP: delete the pod 01/05/23 20:30:37.479
    Jan  5 20:30:37.490: INFO: Waiting for pod pod-configmaps-5d06111f-27b2-481d-8652-b23766781277 to disappear
    Jan  5 20:30:37.493: INFO: Pod pod-configmaps-5d06111f-27b2-481d-8652-b23766781277 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:30:37.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-node] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-node] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-node] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2995" for this suite. 01/05/23 20:30:37.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:30:37.509
Jan  5 20:30:37.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 20:30:37.51
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:30:37.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:30:37.528
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423
STEP: Creating configMap with name configmap-test-volume-bff6223c-e49e-40b5-a1d0-515ca627a584 01/05/23 20:30:37.531
STEP: Creating a pod to test consume configMaps 01/05/23 20:30:37.535
Jan  5 20:30:37.546: INFO: Waiting up to 5m0s for pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d" in namespace "configmap-1270" to be "Succeeded or Failed"
Jan  5 20:30:37.550: INFO: Pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051564ms
Jan  5 20:30:39.554: INFO: Pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008043098s
Jan  5 20:30:41.555: INFO: Pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008779061s
STEP: Saw pod success 01/05/23 20:30:41.555
Jan  5 20:30:41.555: INFO: Pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d" satisfied condition "Succeeded or Failed"
Jan  5 20:30:41.558: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d container configmap-volume-test: <nil>
STEP: delete the pod 01/05/23 20:30:41.564
Jan  5 20:30:41.575: INFO: Waiting for pod pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d to disappear
Jan  5 20:30:41.578: INFO: Pod pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:30:41.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-1270" for this suite. 01/05/23 20:30:41.582
------------------------------
• [4.079 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:423

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:30:37.509
    Jan  5 20:30:37.509: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 20:30:37.51
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:30:37.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:30:37.528
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:423
    STEP: Creating configMap with name configmap-test-volume-bff6223c-e49e-40b5-a1d0-515ca627a584 01/05/23 20:30:37.531
    STEP: Creating a pod to test consume configMaps 01/05/23 20:30:37.535
    Jan  5 20:30:37.546: INFO: Waiting up to 5m0s for pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d" in namespace "configmap-1270" to be "Succeeded or Failed"
    Jan  5 20:30:37.550: INFO: Pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.051564ms
    Jan  5 20:30:39.554: INFO: Pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008043098s
    Jan  5 20:30:41.555: INFO: Pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008779061s
    STEP: Saw pod success 01/05/23 20:30:41.555
    Jan  5 20:30:41.555: INFO: Pod "pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d" satisfied condition "Succeeded or Failed"
    Jan  5 20:30:41.558: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d container configmap-volume-test: <nil>
    STEP: delete the pod 01/05/23 20:30:41.564
    Jan  5 20:30:41.575: INFO: Waiting for pod pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d to disappear
    Jan  5 20:30:41.578: INFO: Pod pod-configmaps-a74db4bb-2062-4b68-9e40-996c0e91857d no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:30:41.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-1270" for this suite. 01/05/23 20:30:41.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:30:41.605
Jan  5 20:30:41.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename pod-network-test 01/05/23 20:30:41.607
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:30:41.622
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:30:41.628
[BeforeEach] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:31
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-5302 01/05/23 20:30:41.631
STEP: creating a selector 01/05/23 20:30:41.631
STEP: Creating the service pods in kubernetes 01/05/23 20:30:41.631
Jan  5 20:30:41.631: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jan  5 20:30:41.669: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5302" to be "running and ready"
Jan  5 20:30:41.675: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538743ms
Jan  5 20:30:41.675: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:30:43.679: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009400344s
Jan  5 20:30:43.679: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:30:45.678: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008819913s
Jan  5 20:30:45.678: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:30:47.678: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009256652s
Jan  5 20:30:47.678: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:30:49.678: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008651141s
Jan  5 20:30:49.678: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:30:51.678: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008939491s
Jan  5 20:30:51.678: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jan  5 20:30:53.679: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010017601s
Jan  5 20:30:53.679: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jan  5 20:30:53.679: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jan  5 20:30:53.682: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5302" to be "running and ready"
Jan  5 20:30:53.684: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.22961ms
Jan  5 20:30:53.684: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan  5 20:30:55.688: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.006659015s
Jan  5 20:30:55.689: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan  5 20:30:57.688: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.005973977s
Jan  5 20:30:57.688: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan  5 20:30:59.689: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.007297552s
Jan  5 20:30:59.689: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan  5 20:31:01.689: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.006969242s
Jan  5 20:31:01.689: INFO: The phase of Pod netserver-1 is Running (Ready = false)
Jan  5 20:31:03.689: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.007589033s
Jan  5 20:31:03.690: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jan  5 20:31:03.690: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jan  5 20:31:03.692: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5302" to be "running and ready"
Jan  5 20:31:03.694: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.242323ms
Jan  5 20:31:03.694: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jan  5 20:31:03.694: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 01/05/23 20:31:03.697
Jan  5 20:31:03.712: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5302" to be "running"
Jan  5 20:31:03.718: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.744722ms
Jan  5 20:31:05.722: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0098846s
Jan  5 20:31:05.722: INFO: Pod "test-container-pod" satisfied condition "running"
Jan  5 20:31:05.725: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5302" to be "running"
Jan  5 20:31:05.727: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.05964ms
Jan  5 20:31:05.727: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jan  5 20:31:05.729: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jan  5 20:31:05.729: INFO: Going to poll 10.16.2.47 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  5 20:31:05.731: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.16.2.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5302 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:31:05.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:31:05.732: INFO: ExecWithOptions: Clientset creation
Jan  5 20:31:05.732: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5302/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.16.2.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 20:31:05.796: INFO: Found all 1 expected endpoints: [netserver-0]
Jan  5 20:31:05.797: INFO: Going to poll 10.16.0.212 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  5 20:31:05.800: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.16.0.212:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5302 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:31:05.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:31:05.801: INFO: ExecWithOptions: Clientset creation
Jan  5 20:31:05.801: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5302/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.16.0.212%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 20:31:05.882: INFO: Found all 1 expected endpoints: [netserver-1]
Jan  5 20:31:05.882: INFO: Going to poll 10.16.1.67 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jan  5 20:31:05.886: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.16.1.67:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5302 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jan  5 20:31:05.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
Jan  5 20:31:05.887: INFO: ExecWithOptions: Clientset creation
Jan  5 20:31:05.888: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5302/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.16.1.67%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jan  5 20:31:05.955: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/node/init/init.go:32
Jan  5 20:31:05.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-network] Networking
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-network] Networking
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-network] Networking
  tear down framework | framework.go:193
STEP: Destroying namespace "pod-network-test-5302" for this suite. 01/05/23 20:31:05.959
------------------------------
• [SLOW TEST] [24.363 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:30:41.605
    Jan  5 20:30:41.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename pod-network-test 01/05/23 20:30:41.607
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:30:41.622
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:30:41.628
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:31
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-5302 01/05/23 20:30:41.631
    STEP: creating a selector 01/05/23 20:30:41.631
    STEP: Creating the service pods in kubernetes 01/05/23 20:30:41.631
    Jan  5 20:30:41.631: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jan  5 20:30:41.669: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-5302" to be "running and ready"
    Jan  5 20:30:41.675: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538743ms
    Jan  5 20:30:41.675: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:30:43.679: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009400344s
    Jan  5 20:30:43.679: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:30:45.678: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008819913s
    Jan  5 20:30:45.678: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:30:47.678: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009256652s
    Jan  5 20:30:47.678: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:30:49.678: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.008651141s
    Jan  5 20:30:49.678: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:30:51.678: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008939491s
    Jan  5 20:30:51.678: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jan  5 20:30:53.679: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.010017601s
    Jan  5 20:30:53.679: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jan  5 20:30:53.679: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jan  5 20:30:53.682: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-5302" to be "running and ready"
    Jan  5 20:30:53.684: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.22961ms
    Jan  5 20:30:53.684: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan  5 20:30:55.688: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 2.006659015s
    Jan  5 20:30:55.689: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan  5 20:30:57.688: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 4.005973977s
    Jan  5 20:30:57.688: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan  5 20:30:59.689: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 6.007297552s
    Jan  5 20:30:59.689: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan  5 20:31:01.689: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=false. Elapsed: 8.006969242s
    Jan  5 20:31:01.689: INFO: The phase of Pod netserver-1 is Running (Ready = false)
    Jan  5 20:31:03.689: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.007589033s
    Jan  5 20:31:03.690: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jan  5 20:31:03.690: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jan  5 20:31:03.692: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-5302" to be "running and ready"
    Jan  5 20:31:03.694: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 2.242323ms
    Jan  5 20:31:03.694: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jan  5 20:31:03.694: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 01/05/23 20:31:03.697
    Jan  5 20:31:03.712: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-5302" to be "running"
    Jan  5 20:31:03.718: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.744722ms
    Jan  5 20:31:05.722: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.0098846s
    Jan  5 20:31:05.722: INFO: Pod "test-container-pod" satisfied condition "running"
    Jan  5 20:31:05.725: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-5302" to be "running"
    Jan  5 20:31:05.727: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.05964ms
    Jan  5 20:31:05.727: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jan  5 20:31:05.729: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jan  5 20:31:05.729: INFO: Going to poll 10.16.2.47 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  5 20:31:05.731: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.16.2.47:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5302 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:31:05.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:31:05.732: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:31:05.732: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5302/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.16.2.47%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 20:31:05.796: INFO: Found all 1 expected endpoints: [netserver-0]
    Jan  5 20:31:05.797: INFO: Going to poll 10.16.0.212 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  5 20:31:05.800: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.16.0.212:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5302 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:31:05.800: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:31:05.801: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:31:05.801: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5302/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.16.0.212%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 20:31:05.882: INFO: Found all 1 expected endpoints: [netserver-1]
    Jan  5 20:31:05.882: INFO: Going to poll 10.16.1.67 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jan  5 20:31:05.886: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.16.1.67:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5302 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jan  5 20:31:05.886: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    Jan  5 20:31:05.887: INFO: ExecWithOptions: Clientset creation
    Jan  5 20:31:05.888: INFO: ExecWithOptions: execute(POST https://10.20.0.1:443/api/v1/namespaces/pod-network-test-5302/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.16.1.67%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jan  5 20:31:05.955: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:31:05.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-network] Networking
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-network] Networking
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-network] Networking
      tear down framework | framework.go:193
    STEP: Destroying namespace "pod-network-test-5302" for this suite. 01/05/23 20:31:05.959
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
[BeforeEach] [sig-storage] EmptyDir volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:31:05.967
Jan  5 20:31:05.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir 01/05/23 20:31:05.968
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:05.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:05.982
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107
STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 20:31:05.984
Jan  5 20:31:05.993: INFO: Waiting up to 5m0s for pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3" in namespace "emptydir-8976" to be "Succeeded or Failed"
Jan  5 20:31:05.996: INFO: Pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.830605ms
Jan  5 20:31:07.999: INFO: Pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3": Phase="Running", Reason="", readiness=false. Elapsed: 2.005861682s
Jan  5 20:31:10.001: INFO: Pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007688657s
STEP: Saw pod success 01/05/23 20:31:10.001
Jan  5 20:31:10.001: INFO: Pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3" satisfied condition "Succeeded or Failed"
Jan  5 20:31:10.003: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-f79cc1e4-4d30-4a63-9447-100afcda10f3 container test-container: <nil>
STEP: delete the pod 01/05/23 20:31:10.01
Jan  5 20:31:10.023: INFO: Waiting for pod pod-f79cc1e4-4d30-4a63-9447-100afcda10f3 to disappear
Jan  5 20:31:10.026: INFO: Pod pod-f79cc1e4-4d30-4a63-9447-100afcda10f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 20:31:10.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-8976" for this suite. 01/05/23 20:31:10.031
------------------------------
• [4.069 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:31:05.967
    Jan  5 20:31:05.967: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir 01/05/23 20:31:05.968
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:05.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:05.982
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:107
    STEP: Creating a pod to test emptydir 0666 on tmpfs 01/05/23 20:31:05.984
    Jan  5 20:31:05.993: INFO: Waiting up to 5m0s for pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3" in namespace "emptydir-8976" to be "Succeeded or Failed"
    Jan  5 20:31:05.996: INFO: Pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.830605ms
    Jan  5 20:31:07.999: INFO: Pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3": Phase="Running", Reason="", readiness=false. Elapsed: 2.005861682s
    Jan  5 20:31:10.001: INFO: Pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007688657s
    STEP: Saw pod success 01/05/23 20:31:10.001
    Jan  5 20:31:10.001: INFO: Pod "pod-f79cc1e4-4d30-4a63-9447-100afcda10f3" satisfied condition "Succeeded or Failed"
    Jan  5 20:31:10.003: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-f79cc1e4-4d30-4a63-9447-100afcda10f3 container test-container: <nil>
    STEP: delete the pod 01/05/23 20:31:10.01
    Jan  5 20:31:10.023: INFO: Waiting for pod pod-f79cc1e4-4d30-4a63-9447-100afcda10f3 to disappear
    Jan  5 20:31:10.026: INFO: Pod pod-f79cc1e4-4d30-4a63-9447-100afcda10f3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:31:10.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-8976" for this suite. 01/05/23 20:31:10.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:31:10.043
Jan  5 20:31:10.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename namespaces 01/05/23 20:31:10.044
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:10.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:10.065
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:31
[It] should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366
STEP: Updating Namespace "namespaces-644" 01/05/23 20:31:10.068
Jan  5 20:31:10.075: INFO: Namespace "namespaces-644" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8bfd2e17-1f7d-4837-af63-bc3dee1c8575", "kubernetes.io/metadata.name":"namespaces-644", "namespaces-644":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:31:10.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "namespaces-644" for this suite. 01/05/23 20:31:10.079
------------------------------
• [0.041 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply an update to a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:366

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:31:10.043
    Jan  5 20:31:10.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename namespaces 01/05/23 20:31:10.044
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:10.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:10.065
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [It] should apply an update to a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:366
    STEP: Updating Namespace "namespaces-644" 01/05/23 20:31:10.068
    Jan  5 20:31:10.075: INFO: Namespace "namespaces-644" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"8bfd2e17-1f7d-4837-af63-bc3dee1c8575", "kubernetes.io/metadata.name":"namespaces-644", "namespaces-644":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:31:10.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Namespaces [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "namespaces-644" for this suite. 01/05/23 20:31:10.079
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
[BeforeEach] [sig-api-machinery] ResourceQuota
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:31:10.09
Jan  5 20:31:10.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename resourcequota 01/05/23 20:31:10.092
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:10.107
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:10.11
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:31
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943
STEP: Creating a ResourceQuota 01/05/23 20:31:10.114
STEP: Getting a ResourceQuota 01/05/23 20:31:10.12
STEP: Listing all ResourceQuotas with LabelSelector 01/05/23 20:31:10.124
STEP: Patching the ResourceQuota 01/05/23 20:31:10.128
STEP: Deleting a Collection of ResourceQuotas 01/05/23 20:31:10.133
STEP: Verifying the deleted ResourceQuota 01/05/23 20:31:10.144
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/node/init/init.go:32
Jan  5 20:31:10.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
  tear down framework | framework.go:193
STEP: Destroying namespace "resourcequota-5427" for this suite. 01/05/23 20:31:10.15
------------------------------
• [0.065 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:943

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:31:10.09
    Jan  5 20:31:10.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename resourcequota 01/05/23 20:31:10.092
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:10.107
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:10.11
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:31
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:943
    STEP: Creating a ResourceQuota 01/05/23 20:31:10.114
    STEP: Getting a ResourceQuota 01/05/23 20:31:10.12
    STEP: Listing all ResourceQuotas with LabelSelector 01/05/23 20:31:10.124
    STEP: Patching the ResourceQuota 01/05/23 20:31:10.128
    STEP: Deleting a Collection of ResourceQuotas 01/05/23 20:31:10.133
    STEP: Verifying the deleted ResourceQuota 01/05/23 20:31:10.144
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:31:10.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] ResourceQuota
      tear down framework | framework.go:193
    STEP: Destroying namespace "resourcequota-5427" for this suite. 01/05/23 20:31:10.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:31:10.156
Jan  5 20:31:10.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename svcaccounts 01/05/23 20:31:10.157
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:10.17
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:10.173
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78
Jan  5 20:31:10.197: INFO: Waiting up to 5m0s for pod "pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781" in namespace "svcaccounts-3722" to be "running"
Jan  5 20:31:10.200: INFO: Pod "pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718855ms
Jan  5 20:31:12.203: INFO: Pod "pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781": Phase="Running", Reason="", readiness=true. Elapsed: 2.006074333s
Jan  5 20:31:12.203: INFO: Pod "pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781" satisfied condition "running"
STEP: reading a file in the container 01/05/23 20:31:12.203
Jan  5 20:31:12.203: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3722 pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 01/05/23 20:31:12.348
Jan  5 20:31:12.349: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3722 pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 01/05/23 20:31:12.494
Jan  5 20:31:12.494: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3722 pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jan  5 20:31:12.632: INFO: Got root ca configmap in namespace "svcaccounts-3722"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  5 20:31:12.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-3722" for this suite. 01/05/23 20:31:12.637
------------------------------
• [2.486 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:31:10.156
    Jan  5 20:31:10.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 20:31:10.157
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:10.17
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:10.173
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:78
    Jan  5 20:31:10.197: INFO: Waiting up to 5m0s for pod "pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781" in namespace "svcaccounts-3722" to be "running"
    Jan  5 20:31:10.200: INFO: Pod "pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781": Phase="Pending", Reason="", readiness=false. Elapsed: 2.718855ms
    Jan  5 20:31:12.203: INFO: Pod "pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781": Phase="Running", Reason="", readiness=true. Elapsed: 2.006074333s
    Jan  5 20:31:12.203: INFO: Pod "pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781" satisfied condition "running"
    STEP: reading a file in the container 01/05/23 20:31:12.203
    Jan  5 20:31:12.203: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3722 pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 01/05/23 20:31:12.348
    Jan  5 20:31:12.349: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3722 pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 01/05/23 20:31:12.494
    Jan  5 20:31:12.494: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3722 pod-service-account-0de47cc9-71d3-4ade-a98a-d7733b4b3781 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jan  5 20:31:12.632: INFO: Got root ca configmap in namespace "svcaccounts-3722"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:31:12.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-3722" for this suite. 01/05/23 20:31:12.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:31:12.643
Jan  5 20:31:12.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 20:31:12.644
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:12.657
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:12.663
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89
STEP: Creating secret with name secret-test-map-ef858143-886f-4190-995a-3942e95c5a6e 01/05/23 20:31:12.665
STEP: Creating a pod to test consume secrets 01/05/23 20:31:12.669
Jan  5 20:31:12.678: INFO: Waiting up to 5m0s for pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000" in namespace "secrets-4241" to be "Succeeded or Failed"
Jan  5 20:31:12.683: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195702ms
Jan  5 20:31:14.686: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007187542s
Jan  5 20:31:16.687: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008438446s
Jan  5 20:31:18.689: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010050854s
STEP: Saw pod success 01/05/23 20:31:18.689
Jan  5 20:31:18.689: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000" satisfied condition "Succeeded or Failed"
Jan  5 20:31:18.691: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 20:31:18.699
Jan  5 20:31:18.714: INFO: Waiting for pod pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000 to disappear
Jan  5 20:31:18.717: INFO: Pod pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 20:31:18.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-4241" for this suite. 01/05/23 20:31:18.722
------------------------------
• [SLOW TEST] [6.084 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:31:12.643
    Jan  5 20:31:12.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 20:31:12.644
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:12.657
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:12.663
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:89
    STEP: Creating secret with name secret-test-map-ef858143-886f-4190-995a-3942e95c5a6e 01/05/23 20:31:12.665
    STEP: Creating a pod to test consume secrets 01/05/23 20:31:12.669
    Jan  5 20:31:12.678: INFO: Waiting up to 5m0s for pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000" in namespace "secrets-4241" to be "Succeeded or Failed"
    Jan  5 20:31:12.683: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000": Phase="Pending", Reason="", readiness=false. Elapsed: 4.195702ms
    Jan  5 20:31:14.686: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007187542s
    Jan  5 20:31:16.687: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008438446s
    Jan  5 20:31:18.689: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010050854s
    STEP: Saw pod success 01/05/23 20:31:18.689
    Jan  5 20:31:18.689: INFO: Pod "pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000" satisfied condition "Succeeded or Failed"
    Jan  5 20:31:18.691: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 20:31:18.699
    Jan  5 20:31:18.714: INFO: Waiting for pod pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000 to disappear
    Jan  5 20:31:18.717: INFO: Pod pod-secrets-3a65b281-baf3-41bd-899a-da1fb3b25000 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:31:18.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-4241" for this suite. 01/05/23 20:31:18.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:31:18.728
Jan  5 20:31:18.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename cronjob 01/05/23 20:31:18.729
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:18.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:18.743
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:31
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 01/05/23 20:31:18.746
STEP: Ensuring a job is scheduled 01/05/23 20:31:18.751
STEP: Ensuring exactly one is scheduled 01/05/23 20:32:00.756
STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 20:32:00.758
STEP: Ensuring the job is replaced with a new one 01/05/23 20:32:00.761
STEP: Removing cronjob 01/05/23 20:33:00.769
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/node/init/init.go:32
Jan  5 20:33:00.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-apps] CronJob
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-apps] CronJob
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-apps] CronJob
  tear down framework | framework.go:193
STEP: Destroying namespace "cronjob-9936" for this suite. 01/05/23 20:33:00.788
------------------------------
• [SLOW TEST] [102.071 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:31:18.728
    Jan  5 20:31:18.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename cronjob 01/05/23 20:31:18.729
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:31:18.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:31:18.743
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:31
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 01/05/23 20:31:18.746
    STEP: Ensuring a job is scheduled 01/05/23 20:31:18.751
    STEP: Ensuring exactly one is scheduled 01/05/23 20:32:00.756
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 01/05/23 20:32:00.758
    STEP: Ensuring the job is replaced with a new one 01/05/23 20:32:00.761
    STEP: Removing cronjob 01/05/23 20:33:00.769
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:33:00.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-apps] CronJob
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-apps] CronJob
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-apps] CronJob
      tear down framework | framework.go:193
    STEP: Destroying namespace "cronjob-9936" for this suite. 01/05/23 20:33:00.788
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:33:00.816
Jan  5 20:33:00.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename crd-watch 01/05/23 20:33:00.818
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:33:00.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:33:00.834
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:31
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jan  5 20:33:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Creating first CR  01/05/23 20:33:03.395
Jan  5 20:33:03.399: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:03Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:03Z]] name:name1 resourceVersion:112002 uid:410d7f76-24ad-4f08-bcf2-6a85ca5b21de] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 01/05/23 20:33:13.399
Jan  5 20:33:13.405: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:13Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:13Z]] name:name2 resourceVersion:112096 uid:0a2aece2-2e3d-40e3-8cc2-537ace94be74] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 01/05/23 20:33:23.405
Jan  5 20:33:23.411: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:03Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:23Z]] name:name1 resourceVersion:112183 uid:410d7f76-24ad-4f08-bcf2-6a85ca5b21de] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 01/05/23 20:33:33.413
Jan  5 20:33:33.421: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:33Z]] name:name2 resourceVersion:112279 uid:0a2aece2-2e3d-40e3-8cc2-537ace94be74] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 01/05/23 20:33:43.421
Jan  5 20:33:43.427: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:03Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:23Z]] name:name1 resourceVersion:112367 uid:410d7f76-24ad-4f08-bcf2-6a85ca5b21de] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 01/05/23 20:33:53.43
Jan  5 20:33:53.436: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:33Z]] name:name2 resourceVersion:112459 uid:0a2aece2-2e3d-40e3-8cc2-537ace94be74] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:34:03.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  tear down framework | framework.go:193
STEP: Destroying namespace "crd-watch-1244" for this suite. 01/05/23 20:34:03.954
------------------------------
• [SLOW TEST] [63.144 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:33:00.816
    Jan  5 20:33:00.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename crd-watch 01/05/23 20:33:00.818
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:33:00.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:33:00.834
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:31
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jan  5 20:33:00.838: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Creating first CR  01/05/23 20:33:03.395
    Jan  5 20:33:03.399: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:03Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:03Z]] name:name1 resourceVersion:112002 uid:410d7f76-24ad-4f08-bcf2-6a85ca5b21de] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 01/05/23 20:33:13.399
    Jan  5 20:33:13.405: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:13Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:13Z]] name:name2 resourceVersion:112096 uid:0a2aece2-2e3d-40e3-8cc2-537ace94be74] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 01/05/23 20:33:23.405
    Jan  5 20:33:23.411: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:03Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:23Z]] name:name1 resourceVersion:112183 uid:410d7f76-24ad-4f08-bcf2-6a85ca5b21de] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 01/05/23 20:33:33.413
    Jan  5 20:33:33.421: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:33Z]] name:name2 resourceVersion:112279 uid:0a2aece2-2e3d-40e3-8cc2-537ace94be74] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 01/05/23 20:33:43.421
    Jan  5 20:33:43.427: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:03Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:23Z]] name:name1 resourceVersion:112367 uid:410d7f76-24ad-4f08-bcf2-6a85ca5b21de] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 01/05/23 20:33:53.43
    Jan  5 20:33:53.436: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-01-05T20:33:13Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-01-05T20:33:33Z]] name:name2 resourceVersion:112459 uid:0a2aece2-2e3d-40e3-8cc2-537ace94be74] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:34:03.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      tear down framework | framework.go:193
    STEP: Destroying namespace "crd-watch-1244" for this suite. 01/05/23 20:34:03.954
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
[BeforeEach] [sig-storage] ConfigMap
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:34:03.963
Jan  5 20:34:03.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename configmap 01/05/23 20:34:03.965
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:03.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:03.978
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:31
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99
STEP: Creating configMap with name configmap-test-volume-map-7486280b-cea9-4b5a-a36d-4e45ca1637aa 01/05/23 20:34:03.982
STEP: Creating a pod to test consume configMaps 01/05/23 20:34:03.986
Jan  5 20:34:03.994: INFO: Waiting up to 5m0s for pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3" in namespace "configmap-2974" to be "Succeeded or Failed"
Jan  5 20:34:03.997: INFO: Pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908246ms
Jan  5 20:34:06.000: INFO: Pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005909705s
Jan  5 20:34:08.000: INFO: Pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006286842s
STEP: Saw pod success 01/05/23 20:34:08
Jan  5 20:34:08.001: INFO: Pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3" satisfied condition "Succeeded or Failed"
Jan  5 20:34:08.003: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3 container agnhost-container: <nil>
STEP: delete the pod 01/05/23 20:34:08.018
Jan  5 20:34:08.031: INFO: Waiting for pod pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3 to disappear
Jan  5 20:34:08.036: INFO: Pod pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/node/init/init.go:32
Jan  5 20:34:08.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] ConfigMap
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] ConfigMap
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] ConfigMap
  tear down framework | framework.go:193
STEP: Destroying namespace "configmap-2974" for this suite. 01/05/23 20:34:08.04
------------------------------
• [4.082 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:34:03.963
    Jan  5 20:34:03.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename configmap 01/05/23 20:34:03.965
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:03.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:03.978
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:31
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:99
    STEP: Creating configMap with name configmap-test-volume-map-7486280b-cea9-4b5a-a36d-4e45ca1637aa 01/05/23 20:34:03.982
    STEP: Creating a pod to test consume configMaps 01/05/23 20:34:03.986
    Jan  5 20:34:03.994: INFO: Waiting up to 5m0s for pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3" in namespace "configmap-2974" to be "Succeeded or Failed"
    Jan  5 20:34:03.997: INFO: Pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908246ms
    Jan  5 20:34:06.000: INFO: Pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005909705s
    Jan  5 20:34:08.000: INFO: Pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006286842s
    STEP: Saw pod success 01/05/23 20:34:08
    Jan  5 20:34:08.001: INFO: Pod "pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3" satisfied condition "Succeeded or Failed"
    Jan  5 20:34:08.003: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3 container agnhost-container: <nil>
    STEP: delete the pod 01/05/23 20:34:08.018
    Jan  5 20:34:08.031: INFO: Waiting for pod pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3 to disappear
    Jan  5 20:34:08.036: INFO: Pod pod-configmaps-26769691-3471-448c-959d-ddc2fe75c6c3 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:34:08.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] ConfigMap
      tear down framework | framework.go:193
    STEP: Destroying namespace "configmap-2974" for this suite. 01/05/23 20:34:08.04
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:34:08.047
Jan  5 20:34:08.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-pred 01/05/23 20:34:08.049
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:08.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:08.067
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  5 20:34:08.070: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 20:34:08.078: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 20:34:08.081: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-16pz before test
Jan  5 20:34:08.087: INFO: fluentbit-gke-5jd2q from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.087: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 20:34:08.087: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 20:34:08.087: INFO: gke-metrics-agent-vk8fs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.087: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 20:34:08.087: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 20:34:08.087: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-16pz from kube-system started at 2023-01-05 17:43:36 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.087: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 20:34:08.088: INFO: pdcsi-node-2nkb4 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.088: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 20:34:08.088: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 20:34:08.088: INFO: sonobuoy from sonobuoy started at 2023-01-05 19:01:43 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.088: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 20:34:08.088: INFO: sonobuoy-e2e-job-7d2488470a1c477d from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.088: INFO: 	Container e2e ready: true, restart count 0
Jan  5 20:34:08.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 20:34:08.089: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 20:34:08.089: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 20:34:08.089: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-dbpc before test
Jan  5 20:34:08.095: INFO: event-exporter-gke-6dc4db644f-dh2bd from kube-system started at 2023-01-05 17:46:55 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.095: INFO: 	Container event-exporter ready: true, restart count 0
Jan  5 20:34:08.095: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Jan  5 20:34:08.095: INFO: fluentbit-gke-xzr85 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.095: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 20:34:08.096: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 20:34:08.096: INFO: gke-metrics-agent-hc4fg from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.096: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 20:34:08.096: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 20:34:08.096: INFO: konnectivity-agent-57b5db4478-7sktm from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.096: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 20:34:08.097: INFO: konnectivity-agent-autoscaler-797b8755f5-bbvjz from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.097: INFO: 	Container autoscaler ready: true, restart count 0
Jan  5 20:34:08.097: INFO: kube-dns-549f79cb95-5mb7d from kube-system started at 2023-01-05 17:46:55 +0000 UTC (4 container statuses recorded)
Jan  5 20:34:08.097: INFO: 	Container dnsmasq ready: true, restart count 0
Jan  5 20:34:08.097: INFO: 	Container kubedns ready: true, restart count 0
Jan  5 20:34:08.097: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jan  5 20:34:08.097: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 20:34:08.098: INFO: kube-dns-autoscaler-758c4689b9-pl85w from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.098: INFO: 	Container autoscaler ready: true, restart count 0
Jan  5 20:34:08.098: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.098: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 20:34:08.098: INFO: l7-default-backend-7bcfd7bc79-lqt6s from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.098: INFO: 	Container default-http-backend ready: true, restart count 0
Jan  5 20:34:08.098: INFO: pdcsi-node-dq4p5 from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.099: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 20:34:08.099: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 20:34:08.099: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 20:34:08.099: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 20:34:08.099: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-qmj7 before test
Jan  5 20:34:08.105: INFO: fluentbit-gke-n7chw from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 20:34:08.105: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 20:34:08.105: INFO: gke-metrics-agent-w95hs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 20:34:08.105: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 20:34:08.105: INFO: konnectivity-agent-57b5db4478-b22kt from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 20:34:08.105: INFO: konnectivity-agent-57b5db4478-cqgvz from kube-system started at 2023-01-05 19:58:37 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 20:34:08.105: INFO: kube-dns-549f79cb95-7zg82 from kube-system started at 2023-01-05 17:47:07 +0000 UTC (4 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container dnsmasq ready: true, restart count 0
Jan  5 20:34:08.105: INFO: 	Container kubedns ready: true, restart count 0
Jan  5 20:34:08.105: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jan  5 20:34:08.105: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 20:34:08.105: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 20:34:08.105: INFO: metrics-server-v0.5.2-7b9768bdd5-skdgv from kube-system started at 2023-01-05 19:58:37 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 20:34:08.105: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan  5 20:34:08.105: INFO: pdcsi-node-vdldr from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 20:34:08.105: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 20:34:08.105: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:08.105: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 20:34:08.105: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443
STEP: Trying to schedule Pod with nonempty NodeSelector. 01/05/23 20:34:08.105
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.173783eec585df82], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/05/23 20:34:08.135
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:34:09.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7214" for this suite. 01/05/23 20:34:09.135
------------------------------
• [1.093 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:34:08.047
    Jan  5 20:34:08.048: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-pred 01/05/23 20:34:08.049
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:08.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:08.067
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  5 20:34:08.070: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 20:34:08.078: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 20:34:08.081: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-16pz before test
    Jan  5 20:34:08.087: INFO: fluentbit-gke-5jd2q from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.087: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 20:34:08.087: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 20:34:08.087: INFO: gke-metrics-agent-vk8fs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.087: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 20:34:08.087: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 20:34:08.087: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-16pz from kube-system started at 2023-01-05 17:43:36 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.087: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 20:34:08.088: INFO: pdcsi-node-2nkb4 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.088: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 20:34:08.088: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 20:34:08.088: INFO: sonobuoy from sonobuoy started at 2023-01-05 19:01:43 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.088: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 20:34:08.088: INFO: sonobuoy-e2e-job-7d2488470a1c477d from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.088: INFO: 	Container e2e ready: true, restart count 0
    Jan  5 20:34:08.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 20:34:08.089: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 20:34:08.089: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 20:34:08.089: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-dbpc before test
    Jan  5 20:34:08.095: INFO: event-exporter-gke-6dc4db644f-dh2bd from kube-system started at 2023-01-05 17:46:55 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.095: INFO: 	Container event-exporter ready: true, restart count 0
    Jan  5 20:34:08.095: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
    Jan  5 20:34:08.095: INFO: fluentbit-gke-xzr85 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.095: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 20:34:08.096: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 20:34:08.096: INFO: gke-metrics-agent-hc4fg from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.096: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 20:34:08.096: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 20:34:08.096: INFO: konnectivity-agent-57b5db4478-7sktm from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.096: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 20:34:08.097: INFO: konnectivity-agent-autoscaler-797b8755f5-bbvjz from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.097: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  5 20:34:08.097: INFO: kube-dns-549f79cb95-5mb7d from kube-system started at 2023-01-05 17:46:55 +0000 UTC (4 container statuses recorded)
    Jan  5 20:34:08.097: INFO: 	Container dnsmasq ready: true, restart count 0
    Jan  5 20:34:08.097: INFO: 	Container kubedns ready: true, restart count 0
    Jan  5 20:34:08.097: INFO: 	Container prometheus-to-sd ready: true, restart count 0
    Jan  5 20:34:08.097: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 20:34:08.098: INFO: kube-dns-autoscaler-758c4689b9-pl85w from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.098: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  5 20:34:08.098: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.098: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 20:34:08.098: INFO: l7-default-backend-7bcfd7bc79-lqt6s from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.098: INFO: 	Container default-http-backend ready: true, restart count 0
    Jan  5 20:34:08.098: INFO: pdcsi-node-dq4p5 from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.099: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 20:34:08.099: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 20:34:08.099: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.099: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 20:34:08.099: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 20:34:08.099: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-qmj7 before test
    Jan  5 20:34:08.105: INFO: fluentbit-gke-n7chw from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: gke-metrics-agent-w95hs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: konnectivity-agent-57b5db4478-b22kt from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: konnectivity-agent-57b5db4478-cqgvz from kube-system started at 2023-01-05 19:58:37 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: kube-dns-549f79cb95-7zg82 from kube-system started at 2023-01-05 17:47:07 +0000 UTC (4 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container dnsmasq ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: 	Container kubedns ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: 	Container prometheus-to-sd ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: metrics-server-v0.5.2-7b9768bdd5-skdgv from kube-system started at 2023-01-05 19:58:37 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: pdcsi-node-vdldr from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:08.105: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 20:34:08.105: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:443
    STEP: Trying to schedule Pod with nonempty NodeSelector. 01/05/23 20:34:08.105
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.173783eec585df82], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] 01/05/23 20:34:08.135
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:34:09.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7214" for this suite. 01/05/23 20:34:09.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
[BeforeEach] [sig-storage] Secrets
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:34:09.159
Jan  5 20:34:09.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename secrets 01/05/23 20:34:09.16
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:09.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:09.177
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:31
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99
STEP: Creating secret with name secret-test-dc2e1c4d-21f4-42e4-be43-bfd8996561dc 01/05/23 20:34:09.194
STEP: Creating a pod to test consume secrets 01/05/23 20:34:09.198
Jan  5 20:34:09.207: INFO: Waiting up to 5m0s for pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0" in namespace "secrets-3907" to be "Succeeded or Failed"
Jan  5 20:34:09.211: INFO: Pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295752ms
Jan  5 20:34:11.215: INFO: Pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007885669s
Jan  5 20:34:13.216: INFO: Pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009007972s
STEP: Saw pod success 01/05/23 20:34:13.216
Jan  5 20:34:13.217: INFO: Pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0" satisfied condition "Succeeded or Failed"
Jan  5 20:34:13.221: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0 container secret-volume-test: <nil>
STEP: delete the pod 01/05/23 20:34:13.23
Jan  5 20:34:13.245: INFO: Waiting for pod pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0 to disappear
Jan  5 20:34:13.248: INFO: Pod pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/node/init/init.go:32
Jan  5 20:34:13.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] Secrets
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] Secrets
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] Secrets
  tear down framework | framework.go:193
STEP: Destroying namespace "secrets-3907" for this suite. 01/05/23 20:34:13.254
STEP: Destroying namespace "secret-namespace-8707" for this suite. 01/05/23 20:34:13.26
------------------------------
• [4.105 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:99

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:34:09.159
    Jan  5 20:34:09.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename secrets 01/05/23 20:34:09.16
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:09.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:09.177
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:31
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:99
    STEP: Creating secret with name secret-test-dc2e1c4d-21f4-42e4-be43-bfd8996561dc 01/05/23 20:34:09.194
    STEP: Creating a pod to test consume secrets 01/05/23 20:34:09.198
    Jan  5 20:34:09.207: INFO: Waiting up to 5m0s for pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0" in namespace "secrets-3907" to be "Succeeded or Failed"
    Jan  5 20:34:09.211: INFO: Pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.295752ms
    Jan  5 20:34:11.215: INFO: Pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007885669s
    Jan  5 20:34:13.216: INFO: Pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009007972s
    STEP: Saw pod success 01/05/23 20:34:13.216
    Jan  5 20:34:13.217: INFO: Pod "pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0" satisfied condition "Succeeded or Failed"
    Jan  5 20:34:13.221: INFO: Trying to get logs from node gke-gke-1-26-default-pool-05283374-16pz pod pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0 container secret-volume-test: <nil>
    STEP: delete the pod 01/05/23 20:34:13.23
    Jan  5 20:34:13.245: INFO: Waiting for pod pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0 to disappear
    Jan  5 20:34:13.248: INFO: Pod pod-secrets-4429831e-66f1-4a08-a3f3-14942744d7d0 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:34:13.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] Secrets
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] Secrets
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] Secrets
      tear down framework | framework.go:193
    STEP: Destroying namespace "secrets-3907" for this suite. 01/05/23 20:34:13.254
    STEP: Destroying namespace "secret-namespace-8707" for this suite. 01/05/23 20:34:13.26
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:34:13.269
Jan  5 20:34:13.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename sched-pred 01/05/23 20:34:13.27
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:13.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:13.284
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:97
Jan  5 20:34:13.287: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jan  5 20:34:13.293: INFO: Waiting for terminating namespaces to be deleted...
Jan  5 20:34:13.296: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-16pz before test
Jan  5 20:34:13.306: INFO: fluentbit-gke-5jd2q from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.306: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 20:34:13.306: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 20:34:13.306: INFO: gke-metrics-agent-vk8fs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.306: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 20:34:13.306: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 20:34:13.307: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-16pz from kube-system started at 2023-01-05 17:43:36 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.307: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 20:34:13.307: INFO: pdcsi-node-2nkb4 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.307: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 20:34:13.307: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 20:34:13.307: INFO: sonobuoy from sonobuoy started at 2023-01-05 19:01:43 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.307: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jan  5 20:34:13.307: INFO: sonobuoy-e2e-job-7d2488470a1c477d from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.308: INFO: 	Container e2e ready: true, restart count 0
Jan  5 20:34:13.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 20:34:13.308: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 20:34:13.308: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 20:34:13.308: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-dbpc before test
Jan  5 20:34:13.314: INFO: event-exporter-gke-6dc4db644f-dh2bd from kube-system started at 2023-01-05 17:46:55 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.314: INFO: 	Container event-exporter ready: true, restart count 0
Jan  5 20:34:13.315: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
Jan  5 20:34:13.315: INFO: fluentbit-gke-xzr85 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.315: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 20:34:13.315: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 20:34:13.315: INFO: gke-metrics-agent-hc4fg from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.315: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 20:34:13.315: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 20:34:13.315: INFO: konnectivity-agent-57b5db4478-7sktm from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.315: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 20:34:13.315: INFO: konnectivity-agent-autoscaler-797b8755f5-bbvjz from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.315: INFO: 	Container autoscaler ready: true, restart count 0
Jan  5 20:34:13.315: INFO: kube-dns-549f79cb95-5mb7d from kube-system started at 2023-01-05 17:46:55 +0000 UTC (4 container statuses recorded)
Jan  5 20:34:13.315: INFO: 	Container dnsmasq ready: true, restart count 0
Jan  5 20:34:13.315: INFO: 	Container kubedns ready: true, restart count 0
Jan  5 20:34:13.315: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jan  5 20:34:13.315: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 20:34:13.315: INFO: kube-dns-autoscaler-758c4689b9-pl85w from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.315: INFO: 	Container autoscaler ready: true, restart count 0
Jan  5 20:34:13.315: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.315: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 20:34:13.316: INFO: l7-default-backend-7bcfd7bc79-lqt6s from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.316: INFO: 	Container default-http-backend ready: true, restart count 0
Jan  5 20:34:13.316: INFO: pdcsi-node-dq4p5 from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.316: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 20:34:13.316: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 20:34:13.316: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.316: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 20:34:13.316: INFO: 	Container systemd-logs ready: true, restart count 0
Jan  5 20:34:13.316: INFO: 
Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-qmj7 before test
Jan  5 20:34:13.324: INFO: fluentbit-gke-n7chw from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container fluentbit ready: true, restart count 0
Jan  5 20:34:13.324: INFO: 	Container fluentbit-gke ready: true, restart count 0
Jan  5 20:34:13.324: INFO: gke-metrics-agent-w95hs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container core-metrics-exporter ready: true, restart count 0
Jan  5 20:34:13.324: INFO: 	Container gke-metrics-agent ready: true, restart count 0
Jan  5 20:34:13.324: INFO: konnectivity-agent-57b5db4478-b22kt from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 20:34:13.324: INFO: konnectivity-agent-57b5db4478-cqgvz from kube-system started at 2023-01-05 19:58:37 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container konnectivity-agent ready: true, restart count 0
Jan  5 20:34:13.324: INFO: kube-dns-549f79cb95-7zg82 from kube-system started at 2023-01-05 17:47:07 +0000 UTC (4 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container dnsmasq ready: true, restart count 0
Jan  5 20:34:13.324: INFO: 	Container kubedns ready: true, restart count 0
Jan  5 20:34:13.324: INFO: 	Container prometheus-to-sd ready: true, restart count 0
Jan  5 20:34:13.324: INFO: 	Container sidecar ready: true, restart count 0
Jan  5 20:34:13.324: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container kube-proxy ready: true, restart count 0
Jan  5 20:34:13.324: INFO: metrics-server-v0.5.2-7b9768bdd5-skdgv from kube-system started at 2023-01-05 19:58:37 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container metrics-server ready: true, restart count 0
Jan  5 20:34:13.324: INFO: 	Container metrics-server-nanny ready: true, restart count 0
Jan  5 20:34:13.324: INFO: pdcsi-node-vdldr from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Jan  5 20:34:13.324: INFO: 	Container gce-pd-driver ready: true, restart count 0
Jan  5 20:34:13.324: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
Jan  5 20:34:13.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jan  5 20:34:13.324: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466
STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 20:34:13.325
Jan  5 20:34:13.332: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7591" to be "running"
Jan  5 20:34:13.335: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789406ms
Jan  5 20:34:15.346: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013946653s
Jan  5 20:34:17.338: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.006259816s
Jan  5 20:34:17.338: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 20:34:17.341
STEP: Trying to apply a random label on the found node. 01/05/23 20:34:17.355
STEP: verifying the node has the label kubernetes.io/e2e-e5a72edf-1e95-4422-9b8d-8fc55b21625f 42 01/05/23 20:34:17.37
STEP: Trying to relaunch the pod, now with labels. 01/05/23 20:34:17.382
Jan  5 20:34:17.396: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7591" to be "not pending"
Jan  5 20:34:17.400: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976365ms
Jan  5 20:34:19.405: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009255279s
Jan  5 20:34:21.404: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.00813771s
Jan  5 20:34:21.404: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-e5a72edf-1e95-4422-9b8d-8fc55b21625f off the node gke-gke-1-26-default-pool-05283374-16pz 01/05/23 20:34:21.407
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e5a72edf-1e95-4422-9b8d-8fc55b21625f 01/05/23 20:34:21.424
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/node/init/init.go:32
Jan  5 20:34:21.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:88
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
  tear down framework | framework.go:193
STEP: Destroying namespace "sched-pred-7591" for this suite. 01/05/23 20:34:21.437
------------------------------
• [SLOW TEST] [8.175 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:466

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:34:13.269
    Jan  5 20:34:13.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename sched-pred 01/05/23 20:34:13.27
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:13.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:13.284
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:97
    Jan  5 20:34:13.287: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jan  5 20:34:13.293: INFO: Waiting for terminating namespaces to be deleted...
    Jan  5 20:34:13.296: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-16pz before test
    Jan  5 20:34:13.306: INFO: fluentbit-gke-5jd2q from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.306: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 20:34:13.306: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 20:34:13.306: INFO: gke-metrics-agent-vk8fs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.306: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 20:34:13.306: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 20:34:13.307: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-16pz from kube-system started at 2023-01-05 17:43:36 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.307: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 20:34:13.307: INFO: pdcsi-node-2nkb4 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.307: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 20:34:13.307: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 20:34:13.307: INFO: sonobuoy from sonobuoy started at 2023-01-05 19:01:43 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.307: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jan  5 20:34:13.307: INFO: sonobuoy-e2e-job-7d2488470a1c477d from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.308: INFO: 	Container e2e ready: true, restart count 0
    Jan  5 20:34:13.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 20:34:13.308: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-ssl2x from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.308: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 20:34:13.308: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 20:34:13.308: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-dbpc before test
    Jan  5 20:34:13.314: INFO: event-exporter-gke-6dc4db644f-dh2bd from kube-system started at 2023-01-05 17:46:55 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.314: INFO: 	Container event-exporter ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: 	Container prometheus-to-sd-exporter ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: fluentbit-gke-xzr85 from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.315: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: gke-metrics-agent-hc4fg from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.315: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: konnectivity-agent-57b5db4478-7sktm from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.315: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: konnectivity-agent-autoscaler-797b8755f5-bbvjz from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.315: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: kube-dns-549f79cb95-5mb7d from kube-system started at 2023-01-05 17:46:55 +0000 UTC (4 container statuses recorded)
    Jan  5 20:34:13.315: INFO: 	Container dnsmasq ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: 	Container kubedns ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: 	Container prometheus-to-sd ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: kube-dns-autoscaler-758c4689b9-pl85w from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.315: INFO: 	Container autoscaler ready: true, restart count 0
    Jan  5 20:34:13.315: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-dbpc from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.315: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 20:34:13.316: INFO: l7-default-backend-7bcfd7bc79-lqt6s from kube-system started at 2023-01-05 17:46:55 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.316: INFO: 	Container default-http-backend ready: true, restart count 0
    Jan  5 20:34:13.316: INFO: pdcsi-node-dq4p5 from kube-system started at 2023-01-05 17:46:35 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.316: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 20:34:13.316: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 20:34:13.316: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-bwv5z from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.316: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 20:34:13.316: INFO: 	Container systemd-logs ready: true, restart count 0
    Jan  5 20:34:13.316: INFO: 
    Logging pods the apiserver thinks is on node gke-gke-1-26-default-pool-05283374-qmj7 before test
    Jan  5 20:34:13.324: INFO: fluentbit-gke-n7chw from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container fluentbit ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: 	Container fluentbit-gke ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: gke-metrics-agent-w95hs from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container core-metrics-exporter ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: 	Container gke-metrics-agent ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: konnectivity-agent-57b5db4478-b22kt from kube-system started at 2023-01-05 17:47:11 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: konnectivity-agent-57b5db4478-cqgvz from kube-system started at 2023-01-05 19:58:37 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: kube-dns-549f79cb95-7zg82 from kube-system started at 2023-01-05 17:47:07 +0000 UTC (4 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container dnsmasq ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: 	Container kubedns ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: 	Container prometheus-to-sd ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: 	Container sidecar ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: kube-proxy-gke-gke-1-26-default-pool-05283374-qmj7 from kube-system started at 2023-01-05 17:43:35 +0000 UTC (1 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container kube-proxy ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: metrics-server-v0.5.2-7b9768bdd5-skdgv from kube-system started at 2023-01-05 19:58:37 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container metrics-server ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: 	Container metrics-server-nanny ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: pdcsi-node-vdldr from kube-system started at 2023-01-05 17:46:36 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: sonobuoy-systemd-logs-daemon-set-13fbdd5c3661494c-jslq9 from sonobuoy started at 2023-01-05 19:01:44 +0000 UTC (2 container statuses recorded)
    Jan  5 20:34:13.324: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jan  5 20:34:13.324: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:466
    STEP: Trying to launch a pod without a label to get a node which can launch it. 01/05/23 20:34:13.325
    Jan  5 20:34:13.332: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7591" to be "running"
    Jan  5 20:34:13.335: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.789406ms
    Jan  5 20:34:15.346: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013946653s
    Jan  5 20:34:17.338: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.006259816s
    Jan  5 20:34:17.338: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 01/05/23 20:34:17.341
    STEP: Trying to apply a random label on the found node. 01/05/23 20:34:17.355
    STEP: verifying the node has the label kubernetes.io/e2e-e5a72edf-1e95-4422-9b8d-8fc55b21625f 42 01/05/23 20:34:17.37
    STEP: Trying to relaunch the pod, now with labels. 01/05/23 20:34:17.382
    Jan  5 20:34:17.396: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-7591" to be "not pending"
    Jan  5 20:34:17.400: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 3.976365ms
    Jan  5 20:34:19.405: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009255279s
    Jan  5 20:34:21.404: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.00813771s
    Jan  5 20:34:21.404: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-e5a72edf-1e95-4422-9b8d-8fc55b21625f off the node gke-gke-1-26-default-pool-05283374-16pz 01/05/23 20:34:21.407
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-e5a72edf-1e95-4422-9b8d-8fc55b21625f 01/05/23 20:34:21.424
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:34:21.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:88
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-scheduling] SchedulerPredicates [Serial]
      tear down framework | framework.go:193
    STEP: Destroying namespace "sched-pred-7591" for this suite. 01/05/23 20:34:21.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:34:21.458
Jan  5 20:34:21.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 20:34:21.462
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:21.476
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:21.479
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:31
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jan  5 20:34:21.499: INFO: Waiting up to 5m0s for pod "pod-secrets-43dd473a-b975-40e1-b700-b23872b67358" in namespace "emptydir-wrapper-1587" to be "running and ready"
Jan  5 20:34:21.503: INFO: Pod "pod-secrets-43dd473a-b975-40e1-b700-b23872b67358": Phase="Pending", Reason="", readiness=false. Elapsed: 3.919607ms
Jan  5 20:34:21.503: INFO: The phase of Pod pod-secrets-43dd473a-b975-40e1-b700-b23872b67358 is Pending, waiting for it to be Running (with Ready = true)
Jan  5 20:34:23.507: INFO: Pod "pod-secrets-43dd473a-b975-40e1-b700-b23872b67358": Phase="Running", Reason="", readiness=true. Elapsed: 2.008012968s
Jan  5 20:34:23.507: INFO: The phase of Pod pod-secrets-43dd473a-b975-40e1-b700-b23872b67358 is Running (Ready = true)
Jan  5 20:34:23.507: INFO: Pod "pod-secrets-43dd473a-b975-40e1-b700-b23872b67358" satisfied condition "running and ready"
STEP: Cleaning up the secret 01/05/23 20:34:23.51
STEP: Cleaning up the configmap 01/05/23 20:34:23.514
STEP: Cleaning up the pod 01/05/23 20:34:23.519
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/node/init/init.go:32
Jan  5 20:34:23.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
  tear down framework | framework.go:193
STEP: Destroying namespace "emptydir-wrapper-1587" for this suite. 01/05/23 20:34:23.537
------------------------------
• [2.084 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:34:21.458
    Jan  5 20:34:21.458: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename emptydir-wrapper 01/05/23 20:34:21.462
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:21.476
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:21.479
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:31
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jan  5 20:34:21.499: INFO: Waiting up to 5m0s for pod "pod-secrets-43dd473a-b975-40e1-b700-b23872b67358" in namespace "emptydir-wrapper-1587" to be "running and ready"
    Jan  5 20:34:21.503: INFO: Pod "pod-secrets-43dd473a-b975-40e1-b700-b23872b67358": Phase="Pending", Reason="", readiness=false. Elapsed: 3.919607ms
    Jan  5 20:34:21.503: INFO: The phase of Pod pod-secrets-43dd473a-b975-40e1-b700-b23872b67358 is Pending, waiting for it to be Running (with Ready = true)
    Jan  5 20:34:23.507: INFO: Pod "pod-secrets-43dd473a-b975-40e1-b700-b23872b67358": Phase="Running", Reason="", readiness=true. Elapsed: 2.008012968s
    Jan  5 20:34:23.507: INFO: The phase of Pod pod-secrets-43dd473a-b975-40e1-b700-b23872b67358 is Running (Ready = true)
    Jan  5 20:34:23.507: INFO: Pod "pod-secrets-43dd473a-b975-40e1-b700-b23872b67358" satisfied condition "running and ready"
    STEP: Cleaning up the secret 01/05/23 20:34:23.51
    STEP: Cleaning up the configmap 01/05/23 20:34:23.514
    STEP: Cleaning up the pod 01/05/23 20:34:23.519
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:34:23.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-storage] EmptyDir wrapper volumes
      tear down framework | framework.go:193
    STEP: Destroying namespace "emptydir-wrapper-1587" for this suite. 01/05/23 20:34:23.537
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:34:23.546
Jan  5 20:34:23.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename aggregator 01/05/23 20:34:23.547
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:23.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:23.564
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:31
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jan  5 20:34:23.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 01/05/23 20:34:23.569
Jan  5 20:34:24.049: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jan  5 20:34:26.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:28.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:30.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:32.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:34.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:36.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:38.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:40.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:42.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:44.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:46.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:48.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jan  5 20:34:50.227: INFO: Waited 130.674876ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 01/05/23 20:34:50.279
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/05/23 20:34:50.282
STEP: List APIServices 01/05/23 20:34:50.287
Jan  5 20:34:50.293: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/node/init/init.go:32
Jan  5 20:34:51.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-api-machinery] Aggregator
  tear down framework | framework.go:193
STEP: Destroying namespace "aggregator-6192" for this suite. 01/05/23 20:34:51.055
------------------------------
• [SLOW TEST] [27.561 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:34:23.546
    Jan  5 20:34:23.546: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename aggregator 01/05/23 20:34:23.547
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:23.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:23.564
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:31
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jan  5 20:34:23.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 01/05/23 20:34:23.569
    Jan  5 20:34:24.049: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jan  5 20:34:26.089: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:28.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:30.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:32.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:34.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:36.095: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:38.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:40.092: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:42.093: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:44.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:46.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:48.094: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.January, 5, 20, 34, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-55bd96fd47\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jan  5 20:34:50.227: INFO: Waited 130.674876ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 01/05/23 20:34:50.279
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 01/05/23 20:34:50.282
    STEP: List APIServices 01/05/23 20:34:50.287
    Jan  5 20:34:50.293: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:34:51.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-api-machinery] Aggregator
      tear down framework | framework.go:193
    STEP: Destroying namespace "aggregator-6192" for this suite. 01/05/23 20:34:51.055
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
[BeforeEach] [sig-auth] ServiceAccounts
  set up framework | framework.go:178
STEP: Creating a kubernetes client 01/05/23 20:34:51.112
Jan  5 20:34:51.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
STEP: Building a namespace api object, basename svcaccounts 01/05/23 20:34:51.113
STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:51.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:51.134
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:31
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649
STEP: creating a ServiceAccount 01/05/23 20:34:51.136
STEP: watching for the ServiceAccount to be added 01/05/23 20:34:51.154
STEP: patching the ServiceAccount 01/05/23 20:34:51.157
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/05/23 20:34:51.167
STEP: deleting the ServiceAccount 01/05/23 20:34:51.171
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/node/init/init.go:32
Jan  5 20:34:51.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  test/e2e/framework/metrics/init/init.go:33
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  dump namespaces | framework.go:196
[DeferCleanup (Each)] [sig-auth] ServiceAccounts
  tear down framework | framework.go:193
STEP: Destroying namespace "svcaccounts-5546" for this suite. 01/05/23 20:34:51.19
------------------------------
• [0.083 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:649

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      set up framework | framework.go:178
    STEP: Creating a kubernetes client 01/05/23 20:34:51.112
    Jan  5 20:34:51.112: INFO: >>> kubeConfig: /tmp/kubeconfig-3260951545
    STEP: Building a namespace api object, basename svcaccounts 01/05/23 20:34:51.113
    STEP: Waiting for a default service account to be provisioned in namespace 01/05/23 20:34:51.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 01/05/23 20:34:51.134
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:31
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:649
    STEP: creating a ServiceAccount 01/05/23 20:34:51.136
    STEP: watching for the ServiceAccount to be added 01/05/23 20:34:51.154
    STEP: patching the ServiceAccount 01/05/23 20:34:51.157
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 01/05/23 20:34:51.167
    STEP: deleting the ServiceAccount 01/05/23 20:34:51.171
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/node/init/init.go:32
    Jan  5 20:34:51.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      test/e2e/framework/metrics/init/init.go:33
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      dump namespaces | framework.go:196
    [DeferCleanup (Each)] [sig-auth] ServiceAccounts
      tear down framework | framework.go:193
    STEP: Destroying namespace "svcaccounts-5546" for this suite. 01/05/23 20:34:51.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:88
Jan  5 20:34:51.201: INFO: Running AfterSuite actions on node 1
Jan  5 20:34:51.201: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.001 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:88
    Jan  5 20:34:51.201: INFO: Running AfterSuite actions on node 1
    Jan  5 20:34:51.201: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:153
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:153

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:153
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:529
------------------------------
[ReportAfterSuite] PASSED [0.128 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:529

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:529
  << End Captured GinkgoWriter Output
------------------------------

Ran 368 of 7069 Specs in 5584.527 seconds
SUCCESS! -- 368 Passed | 0 Failed | 0 Pending | 6701 Skipped
PASS

Ginkgo ran 1 suite in 1h33m5.311450281s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.4.0[0m

